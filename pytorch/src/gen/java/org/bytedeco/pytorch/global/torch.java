// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.global;

import org.bytedeco.pytorch.*;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.functions.*;
import org.bytedeco.pytorch.Module;
import org.bytedeco.javacpp.annotation.Cast;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

public class torch extends org.bytedeco.pytorch.presets.torch {
    static { Loader.load(); }

// Targeting ../BoolOptional.java


// Targeting ../ByteOptional.java


// Targeting ../IntOptional.java


// Targeting ../LongOptional.java


// Targeting ../FloatOptional.java


// Targeting ../DoubleOptional.java


// Targeting ../SizeTOptional.java


// Targeting ../StringOptional.java


// Targeting ../BoolVectorOptional.java


// Targeting ../LongVectorOptional.java


// Targeting ../DoubleVectorOptional.java


// Targeting ../SizeTVectorOptional.java


// Targeting ../StringVectorOptional.java


// Targeting ../StrideVectorOptional.java


// Targeting ../ShapeSymbolVectorOptional.java


// Targeting ../TensorVectorOptional.java


// Targeting ../DeviceOptional.java


// Targeting ../LongArrayRefOptional.java


// Targeting ../DoubleArrayRefOptional.java


// Targeting ../SymIntArrayRefOptional.java


// Targeting ../LayoutOptional.java


// Targeting ../MemoryFormatOptional.java


// Targeting ../ScalarOptional.java


// Targeting ../ScalarTypeOptional.java


// Targeting ../AliasInfoOptional.java


// Targeting ../IValueOptional.java


// Targeting ../CppSignatureOptional.java


// Targeting ../DispatchKeyOptional.java


// Targeting ../OperatorHandleOptional.java


// Targeting ../OperatorNameOptional.java


// Targeting ../QualifiedNameOptional.java


// Targeting ../StreamOptional.java


// Targeting ../StrideOptional.java


// Targeting ../TypePtrOptional.java


// Targeting ../ClassTypePropertyOptional.java


// Targeting ../AliasTypeSetOptional.java


// Targeting ../FunctionSchemaOptional.java


// Targeting ../SymDimVectorOptional.java


// Targeting ../SymIntOptional.java


// Targeting ../IValueOptional.java


// Targeting ../DimVectorOptional.java


// Targeting ../DimnameOptional.java


// Targeting ../DimnameListOptional.java


// Targeting ../GeneratorOptional.java


// Targeting ../TensorOptional.java


// Targeting ../TensorArrayRefOptional.java


// Targeting ../TypeMetaOptional.java


// Targeting ../ExecutorExecutionModeOptional.java


// Targeting ../InlinedCallStackOptional.java


// Targeting ../ScopeOptional.java


// Targeting ../ModuleInstanceInfoOptional.java


// Targeting ../SourceRangeOptional.java


// Targeting ../MethodOptional.java


// Targeting ../NamedValueOptional.java


// Targeting ../ValueOptional.java


// Targeting ../LongExpandingArrayOptional.java


// Targeting ../DoubleExpandingArrayOptional.java


// Targeting ../T_StringSizeTSizeT_TOptional.java


// Targeting ../T_TypePtrLong_TOptional.java


// Targeting ../StringViewOptional.java


// Targeting ../StringViewVectorOptional.java


// Targeting ../PointerPairOptional.java


// Targeting ../WeakStorageVectorOptional.java


// Targeting ../BatchSizeOptional.java


// Targeting ../ExampleOptional.java


// Targeting ../ExampleVectorOptional.java


// Targeting ../TensorExampleOptional.java


// Targeting ../TensorExampleVectorOptional.java


// Targeting ../T_TensorTensor_TOptional.java


// Targeting ../Nonlinearity.java


// Targeting ../FanModeType.java


// Targeting ../ConvPaddingMode.java


// Targeting ../Conv1dPadding.java


// Targeting ../Conv2dPadding.java


// Targeting ../Conv3dPadding.java


// Targeting ../EmbeddingBagMode.java


// Targeting ../PaddingMode.java


// Targeting ../LossReduction.java


// Targeting ../KLDivLossReduction.java


// Targeting ../GridSampleMode.java


// Targeting ../GridSamplePaddingMode.java


// Targeting ../RNNBaseMode.java


// Targeting ../RNNNonlinearity.java


// Targeting ../UpsampleMode.java


// Targeting ../InterpolateMode.java


// Targeting ../TensorDeque.java


// Targeting ../RecordFunctionHandleIntList.java


// Targeting ../StringStringMap.java


// Targeting ../StringLongMap.java


// Targeting ../ActivityTypeSet.java


// Targeting ../DimnameVector.java


// Targeting ../FunctionPreHookVector.java


// Targeting ../FunctionPostHookVector.java


// Targeting ../DefVector.java


// Targeting ../PropertyVector.java


// Targeting ../OptimizerParamGroupVector.java


// Targeting ../FunctionSchemaVector.java


// Targeting ../DataPtrVector.java


// Targeting ../WeakStorageVector.java


// Targeting ../StringTensorDictItemVector.java


// Targeting ../StringAnyModuleDictItemVector.java


// Targeting ../StringSharedModuleDictItemVector.java


// Targeting ../BoolVector.java


// Targeting ../BytePointerVector.java


// Targeting ../LongVector.java


// Targeting ../DoubleVector.java


// Targeting ../SizeTVector.java


// Targeting ../StringVector.java


// Targeting ../StringViewVector.java


// Targeting ../StringLongVector.java


// Targeting ../IValueVector.java


// Targeting ../QEngineVector.java


// Targeting ../ScalarTypeVector.java


// Targeting ../SymbolVector.java


// Targeting ../LongOptionalVector.java


// Targeting ../IValueOptionalVector.java


// Targeting ../SharedClassTypeVector.java


// Targeting ../TypeVector.java


// Targeting ../StrideVector.java


// Targeting ../ShapeSymbolVector.java


// Targeting ../TensorImplVector.java


// Targeting ../EdgeVector.java


// Targeting ../TensorVector.java


// Targeting ../TensorIndexVector.java


// Targeting ../TensorOptionalVector.java


// Targeting ../FunctionVector.java


// Targeting ../GraphVector.java


// Targeting ../OperatorVector.java


// Targeting ../ResolverVector.java


// Targeting ../ValueVector.java


// Targeting ../JitNodeVector.java


// Targeting ../AnyModuleVector.java


// Targeting ../SharedModuleVector.java


// Targeting ../StringTensorVector.java


// Targeting ../StringAnyModuleVector.java


// Targeting ../StringSharedModuleVector.java


// Targeting ../FusionStrategy.java


// Targeting ../SymIntVector.java


// Targeting ../SharedSugaredValueVector.java


// Targeting ../ExampleVector.java


// Targeting ../TensorExampleVector.java


// Targeting ../ExampleVector.java


// Targeting ../EnumNameValue.java


// Targeting ../StringTensorPair.java


// Targeting ../StringAnyModulePair.java


// Targeting ../StringSharedModulePair.java


// Targeting ../RecordFunctionHandleIntPair.java


// Targeting ../PointerPair.java


// Targeting ../SizeTMatchedSchemaPair.java


// Targeting ../T_DataPtrSizeT_T.java


// Targeting ../T_IntInt_T.java


// Targeting ../T_LongLong_T.java


// Targeting ../T_TensorTensor_T.java


// Targeting ../T_TensorTensorTensor_T.java


// Targeting ../T_TensorTensorTensorTensor_T.java


// Targeting ../T_TensorTensorTensorTensorTensor_T.java


// Targeting ../T_TensorTensorTensorTensorTensorTensorTensor_T.java


// Targeting ../T_TensorTensorTensorTensorVector_T.java


// Targeting ../T_TensorTensorDoubleLong_T.java


// Targeting ../T_TensorT_TensorTensor_T_T.java


// Targeting ../T_TensorMaybeOwnedTensorMaybeOwned_T.java


// Targeting ../T_TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwned_T.java


// Targeting ../T_PackedSequenceTensor_T.java


// Targeting ../T_PackedSequenceT_TensorTensor_T_T.java


// Targeting ../T_StringSizeTSizeT_T.java


// Targeting ../T_TensorTensorVector_T.java


// Targeting ../T_TensorTensorVectorTensorVector_T.java


// Targeting ../T_TypePtrLong_T.java


// Targeting ../HashAliasedIValueMap.java


// Targeting ../StringBoolMap.java


// Targeting ../StringSizeTMap.java


// Targeting ../ExtraFilesMap.java


// Targeting ../TypeEnv.java


// Targeting ../StringIValueMap.java


// Targeting ../StringValueMap.java


// Targeting ../ValueValueMap.java


// Targeting ../ArgumentSpecExecutionPlanMap.java


// Targeting ../TreeRefStringMap.java


// Targeting ../StringSet.java


// Targeting ../HashAliasedIValues.java


// Targeting ../SymbolSet.java


// Targeting ../TensorImplSet.java


// Targeting ../NodeSet.java


// Targeting ../DeviceTypeSet.java


// Parsed from torch/csrc/utils/python_stub.h

// #pragma once


// Parsed from c10/macros/cmake_macros.h

// #ifndef C10_MACROS_CMAKE_MACROS_H_
// #define C10_MACROS_CMAKE_MACROS_H_

// Automatically generated header file for the C10 library.
// Do not include this file directly. Instead, include c10/macros/Macros.h.

// #define C10_BUILD_SHARED_LIBS
/* #undef C10_USE_GLOG */
/* #undef C10_USE_GFLAGS */
/* #undef C10_USE_NUMA */
/* #undef C10_USE_MSVC_STATIC_RUNTIME */

// #endif // C10_MACROS_CMAKE_MACROS_H_


// Parsed from c10/macros/Export.h

// #ifndef C10_MACROS_EXPORT_H_
// #define C10_MACROS_EXPORT_H_

/* Header file to define the common scaffolding for exported symbols.
 *
 * Export is by itself a quite tricky situation to deal with, and if you are
 * hitting this file, make sure you start with the background here:
 * - Linux: https://gcc.gnu.org/wiki/Visibility
 * - Windows:
 * https://docs.microsoft.com/en-us/cpp/cpp/dllexport-dllimport?view=vs-2017
 *
 * Do NOT include this file directly. Instead, use c10/macros/Macros.h
 */

// You do not need to edit this part of file unless you are changing the core
// pytorch export abstractions.
//
// This part defines the C10 core export and import macros. This is controlled
// by whether we are building shared libraries or not, which is determined
// during build time and codified in c10/core/cmake_macros.h.
// When the library is built as a shared lib, EXPORT and IMPORT will contain
// visibility attributes. If it is being built as a static lib, then EXPORT
// and IMPORT basically have no effect.

// As a rule of thumb, you should almost NEVER mix static and shared builds for
// libraries that depend on c10. AKA, if c10 is built as a static library, we
// recommend everything dependent on c10 to be built statically. If c10 is built
// as a shared library, everything dependent on it should be built as shared. In
// the PyTorch project, all native libraries shall use the macro
// C10_BUILD_SHARED_LIB to check whether pytorch is building shared or static
// libraries.

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #ifdef _WIN32
// #else // _WIN32
// #if defined(__GNUC__)
// #define C10_EXPORT __attribute__((__visibility__("default")))
// #define C10_HIDDEN __attribute__((__visibility__("hidden")))
// #else // defined(__GNUC__)
// #define C10_EXPORT
// #define C10_HIDDEN
// #endif // defined(__GNUC__)
// #define C10_IMPORT C10_EXPORT
// #endif // _WIN32

// #ifdef NO_EXPORT
// #undef C10_EXPORT
// #define C10_EXPORT
// #endif

// Definition of an adaptive XX_API macro, that depends on whether you are
// building the library itself or not, routes to XX_EXPORT and XX_IMPORT.
// Basically, you will need to do this for each shared library that you are
// building, and the instruction is as follows: assuming that you are building
// a library called libawesome.so. You should:
// (1) for your cmake target (usually done by "add_library(awesome, ...)"),
//     define a macro called AWESOME_BUILD_MAIN_LIB using
//     target_compile_options.
// (2) define the AWESOME_API macro similar to the one below.
// And in the source file of your awesome library, use AWESOME_API to
// annotate public symbols.

// Here, for the C10 library, we will define the macro C10_API for both import
// and export.

// This one is being used by libc10.so
// #ifdef C10_BUILD_MAIN_LIB
// #define C10_API C10_EXPORT
// #else
// #define C10_API C10_IMPORT
// #endif

// This one is being used by libtorch.so
// #ifdef CAFFE2_BUILD_MAIN_LIB
// #define TORCH_API C10_EXPORT
// #else
// #define TORCH_API C10_IMPORT
// #endif

// You may be wondering: Whose brilliant idea was it to split torch_cuda into
// two pieces with confusing names?
// Once upon a time, there _was_ only TORCH_CUDA_API. All was happy until we
// tried to compile PyTorch for CUDA 11.1, which ran into relocation marker
// issues when linking big binaries.
// (https://github.com/pytorch/pytorch/issues/39968) We had two choices:
//    (1) Stop supporting so many GPU architectures
//    (2) Do something else
// We chose #2 and decided to split the behemoth that was torch_cuda into two
// smaller libraries, one with most of the core kernel functions (torch_cuda_cu)
// and the other that had..well..everything else (torch_cuda_cpp). The idea was
// this: instead of linking our static libraries (like the hefty
// libcudnn_static.a) with another huge library, torch_cuda, and run into pesky
// relocation marker issues, we could link our static libraries to a smaller
// part of torch_cuda (torch_cuda_cpp) and avoid the issues.

// libtorch_cuda_cu.so
// #ifdef TORCH_CUDA_CU_BUILD_MAIN_LIB
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// libtorch_cuda_cpp.so
// #ifdef TORCH_CUDA_CPP_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #endif

// libtorch_cuda.so (where torch_cuda_cu and torch_cuda_cpp are a part of the
// same api)
// #ifdef TORCH_CUDA_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif !defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// #if defined(TORCH_HIP_BUILD_MAIN_LIB)
// #define TORCH_HIP_API C10_EXPORT
// #else
// #define TORCH_HIP_API C10_IMPORT
// #endif

// Enums only need to be exported on windows for non-CUDA files
// #if defined(_WIN32) && defined(__CUDACC__)
// #define C10_API_ENUM C10_API
// #else
// #define C10_API_ENUM
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from torch/csrc/Export.h

// #pragma once

// #include <c10/macros/Export.h>

// #ifdef THP_BUILD_MAIN_LIB
// #define TORCH_PYTHON_API C10_EXPORT
// #else
// #define TORCH_PYTHON_API C10_IMPORT
// #endif


// Parsed from c10/macros/Macros.h

// #ifndef C10_MACROS_MACROS_H_
// #define C10_MACROS_MACROS_H_
// #include <cassert>

/* Main entry for c10/macros.
 *
 * In your code, include c10/macros/Macros.h directly, instead of individual
 * files in this folder.
 */

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #include <c10/macros/Export.h>

// #if defined(__clang__)
// #define __ubsan_ignore_float_divide_by_zero__
//   __attribute__((no_sanitize("float-divide-by-zero")))
// #define __ubsan_ignore_undefined__ __attribute__((no_sanitize("undefined")))
// #define __ubsan_ignore_signed_int_overflow__
//   __attribute__((no_sanitize("signed-integer-overflow")))
// #define __ubsan_ignore_pointer_overflow__
//   __attribute__((no_sanitize("pointer-overflow")))
// #define __ubsan_ignore_function__ __attribute__((no_sanitize("function")))
// #else
// #define __ubsan_ignore_float_divide_by_zero__
// #define __ubsan_ignore_undefined__
// #define __ubsan_ignore_signed_int_overflow__
// #define __ubsan_ignore_pointer_overflow__
// #define __ubsan_ignore_function__
// #endif

// Detect address sanitizer as some stuff doesn't work with it
// #undef C10_ASAN_ENABLED

// for clang
// #if defined(__has_feature)
// #if ((__has_feature(address_sanitizer)))
public static final int C10_ASAN_ENABLED = 1;
// #endif
// #endif

// for gcc
// #if defined(__SANITIZE_ADDRESS__)
// #if __SANITIZE_ADDRESS__
// #if !defined(C10_ASAN_ENABLED)
// #endif
// #endif
// #endif

// #if !defined(C10_ASAN_ENABLED)
// #endif

// Disable the copy and assignment operator for a class. Note that this will
// disable the usage of the class in std containers.
// #define C10_DISABLE_COPY_AND_ASSIGN(classname)
//   classname(const classname&) = delete;
//   classname& operator=(const classname&) = delete

// #define C10_CONCATENATE_IMPL(s1, s2) s1##s2
// #define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)

// #define C10_MACRO_EXPAND(args) args

// #define C10_STRINGIZE_IMPL(x) #x
// #define C10_STRINGIZE(x) C10_STRINGIZE_IMPL(x)

/**
 * C10_ANONYMOUS_VARIABLE(str) introduces an identifier starting with
 * str and ending with a number that varies with the line.
 */
// #ifdef __COUNTER__
// #else
// #define C10_UID __LINE__
// #define C10_ANONYMOUS_VARIABLE(str) C10_CONCATENATE(str, __LINE__)
// #endif

// #ifdef __has_cpp_attribute
// #define C10_HAS_CPP_ATTRIBUTE(x) __has_cpp_attribute(x)
// #else
// #define C10_HAS_CPP_ATTRIBUTE(x) (0)
// #endif

/** C10_NODISCARD - Warn if a type or return value is discarded. */

// Technically, we should check if __cplusplus > 201402L here, because
// [[nodiscard]] is only defined in C++17.  However, some compilers
// we care about don't advertise being C++17 (e.g., clang), but
// support the attribute anyway.  In fact, this is not just a good idea,
// it's the law: clang::warn_unused_result doesn't work on nvcc + clang
// and the best workaround for this case is to use [[nodiscard]]
// instead; see https://github.com/pytorch/pytorch/issues/13118
//
// Note to future editors: if you have noticed that a compiler is
// misbehaving (e.g., it advertises support, but the support doesn't
// actually work, or it is emitting warnings).  Some compilers which
// are strict about the matter include MSVC, which will complain:
//
//  error C2429: attribute 'nodiscard' requires compiler flag '/std:c++latest'
//
// Exhibits:
//  - MSVC 19.14: https://godbolt.org/z/Dzd7gn (requires /std:c++latest)
//  - Clang 8.0.0: https://godbolt.org/z/3PYL4Z (always advertises support)
//  - gcc 8.3: https://godbolt.org/z/4tLMQS (always advertises support)
// #if C10_HAS_CPP_ATTRIBUTE(nodiscard)
// #define C10_NODISCARD [[nodiscard]]
// Workaround for llvm.org/PR23435, since clang 3.6 and below emit a spurious
// error when __has_cpp_attribute is given a scoped attribute in C mode.
// #elif __cplusplus && C10_HAS_CPP_ATTRIBUTE(clang::warn_unused_result)
// TODO: It's possible this is still triggering
// https://github.com/pytorch/pytorch/issues/13118 on Windows; if it is, better
// fix it.
// #define C10_NODISCARD [[clang::warn_unused_result]]
// #else
// #define C10_NODISCARD
// #endif

// suppress an unused variable.
// #if defined(_MSC_VER) && !defined(__clang__)
// #define C10_UNUSED __pragma(warning(suppress : 4100 4101))
// #else
// #define C10_UNUSED __attribute__((__unused__))
// #endif //_MSC_VER

// #if !defined(__has_attribute)
// #define __has_attribute(x) 0
// #endif

// Direct port of LLVM_ATTRIBUTE_USED.
// #if __has_attribute(used)
// #define C10_USED __attribute__((__used__))
// #else
// #define C10_USED
// #endif

// #define C10_RESTRICT __restrict

// Simply define the namespace, in case a dependent library want to refer to
// the c10 namespace but not any nontrivial files.
 // namespace c10

 // namespace c10

 // namespace c10

// Since C10 is the core library for caffe2 (and aten), we will simply reroute
// all abstractions defined in c10 to be available in caffe2 as well.
// This is only for backwards compatibility. Please use the symbols from the
// c10 namespace where possible.



 // namespace at

// WARNING!!! THIS IS A GIANT HACK!!!
// This line means you cannot simultaneously include c10/hip
// and c10/cuda and then use them from the at::cuda namespace.
// This is true in practice, because HIPIFY works inplace on
// files in ATen/cuda, so it assumes that c10::hip is available
// from at::cuda.  This namespace makes that happen.  When
// HIPIFY is no longer out-of-place, we can switch the cuda
// here to hip and everyone is happy.

 // namespace at

// C10_LIKELY/C10_UNLIKELY
//
// These macros provide parentheses, so you can use these macros as:
//
//    if C10_LIKELY(some_expr) {
//      ...
//    }
//
// NB: static_cast to boolean is mandatory in C++, because __builtin_expect
// takes a long argument, which means you may trigger the wrong conversion
// without it.
//
// #if defined(__GNUC__) || defined(__ICL) || defined(__clang__)
// #define C10_LIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 1))
// #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))
// #else
// #define C10_LIKELY(expr) (expr)
// #define C10_UNLIKELY(expr) (expr)
// #endif

/** C10_NOINLINE - Functions whose declaration is annotated with this will not
 *  be inlined. */
// #ifdef __GNUC__
// #define C10_NOINLINE __attribute__((noinline))
// #elif _MSC_VER
// #define C10_NOINLINE __declspec(noinline)
// #else
// #define C10_NOINLINE
// #endif

// #if defined(_MSC_VER)
// #elif __has_attribute(always_inline) || defined(__GNUC__)
// #define C10_ALWAYS_INLINE __attribute__((__always_inline__)) inline
// #else
// #define C10_ALWAYS_INLINE inline
// #endif

// #if defined(_MSC_VER)
// #elif defined(__GNUC__)
// #define C10_ATTR_VISIBILITY_HIDDEN __attribute__((__visibility__("hidden")))
// #else
// #define C10_ATTR_VISIBILITY_HIDDEN
// #endif

// #define C10_ERASE C10_ALWAYS_INLINE C10_ATTR_VISIBILITY_HIDDEN

// C10_FALLTHROUGH - Annotate fallthrough to the next case in a switch.
// #if C10_HAS_CPP_ATTRIBUTE(fallthrough)
// #define C10_FALLTHROUGH [[fallthrough]]
// #else
// #define C10_FALLTHROUGH
// #endif

// #include <cstdint>

// #ifdef __HIPCC__
// Unlike CUDA, HIP requires a HIP header to be included for __host__ to work.
// We do this #include here so that C10_HOST_DEVICE and friends will Just Work.
// See https://github.com/ROCm-Developer-Tools/HIP/issues/441
// #include <hip/hip_runtime.h>
// #endif

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define C10_HOST_DEVICE
// #define C10_HOST
// #define C10_DEVICE
// #endif

// #if defined(USE_ROCM)
// #else
// #define C10_HIP_HOST_DEVICE
// #endif

// #if defined(USE_ROCM)
// #else
// #define C10_WARP_SIZE 32
// #endif

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif

// CUDA_KERNEL_ASSERT checks the assertion
// even when NDEBUG is defined. This is useful for important assertions in CUDA
// code that would otherwise be suppressed when building Release.
// #if defined(__ANDROID__) || defined(__APPLE__) ||
//     (defined(USE_ROCM) && ROCM_VERSION < 40100)
// Those platforms do not support assert()
// #define CUDA_KERNEL_ASSERT(cond)
// #define SYCL_KERNEL_ASSERT(cond)
// #elif defined(_MSC_VER)
// #else // __APPLE__, _MSC_VER
// #if defined(NDEBUG)
// #endif // NDEBUG
// #define CUDA_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(
//         #cond, __FILE__, static_cast<unsigned int>(__LINE__), __func__);
//   }
// #define SYCL_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(
//         #cond, __FILE__, static_cast<unsigned int>(__LINE__), __func__);
//   }
// #endif // __APPLE__

// #ifdef __APPLE__
// #include <TargetConditionals.h>
// #endif

// #if defined(__ANDROID__)
// #elif (
//     defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE))
// #define C10_IOS 1
// #define C10_MOBILE 1
// #endif // ANDROID / IOS

// #if defined(C10_MOBILE) && C10_MOBILE
// #define C10_ALWAYS_INLINE_UNLESS_MOBILE inline
// #else
// #define C10_ALWAYS_INLINE_UNLESS_MOBILE C10_ALWAYS_INLINE
// #endif

// Portable determination of whether type T is trivially copyable.
// Warning: __has_trivial_copy for GCC may not always detect the non-POD
// correctly. For example, T = std::unique_ptr may evaluate to true and be
// treated as POD. This can cause unexpected behavior.
// #if defined(__GNUG__) && __GNUC__ < 5 && !defined(__clang__)
// #define C10_IS_TRIVIALLY_COPYABLE(T) __has_trivial_copy(T)
// #else
// #define C10_IS_TRIVIALLY_COPYABLE(T) std::is_trivially_copyable<T>::value
// #endif

// #if defined(__CUDA_ARCH__)
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// Note [static constexpr char* members for windows NVCC]
// The Windows NVCC compiler doesn't handle static constexpr class members,
// although it's fixed in a later version.
// (see
// https://developercommunity.visualstudio.com/t/intellisense-error-c11-static-constexpr-member-ini/245425)
//
// If we want to ensure that our field is static under all builds, then we need
// to work around it specifically for windows NVCC by making it (a) const, (b)
// defined outside of the class definition We need to define it outside of the
// class definition because of the C++ standard; char* is not an integral type
// (see
// https://stackoverflow.com/questions/24278473/intellisense-a-member-of-type-const-char-const-cannot-have-an-in-class-in)
//
// So instead of this:
// struct Foo {
//     static constexpr const char* name = "foo";
// }
// In Windows NVCC, we end up with this:
// struct Foo {
//     static const char* name;
// }
// const char* Foo::name = "foo";
//
// This gives us a small perf hit for any code that wants to access these field
// members, but right now it isn't used in any perf-critical code paths.
// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #else
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA constexpr

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #endif

// #ifndef HAS_DEMANGLE
// #if defined(__ANDROID__) || defined(_WIN32) || defined(__EMSCRIPTEN__)
public static final int HAS_DEMANGLE = 0;
// #elif defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE)
// #else
// #endif
// #endif // HAS_DEMANGLE

// #define _C10_PRAGMA__(string) _Pragma(#string)
// #define _C10_PRAGMA_(string) _C10_PRAGMA__(string)

// #ifdef __clang__
// #define C10_CLANG_DIAGNOSTIC_PUSH() _Pragma("clang diagnostic push")
// #define C10_CLANG_DIAGNOSTIC_POP() _Pragma("clang diagnostic pop")
// #define C10_CLANG_DIAGNOSTIC_IGNORE(flag)
//   _C10_PRAGMA_(clang diagnostic ignored flag)
// #define C10_CLANG_HAS_WARNING(flag) __has_warning(flag)
// #else
// #define C10_CLANG_DIAGNOSTIC_PUSH()
// #define C10_CLANG_DIAGNOSTIC_POP()
// #define C10_CLANG_DIAGNOSTIC_IGNORE(flag)
// #define C10_CLANG_HAS_WARNING(flag) 0
// #endif

// #ifdef __clang__

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
//   _C10_PRAGMA_(clang diagnostic push)
//   _C10_PRAGMA_(clang diagnostic ignored "-Wunknown-warning-option")
//   _C10_PRAGMA_(clang diagnostic ignored warning)

// #define C10_DIAGNOSTIC_POP() _C10_PRAGMA_(clang diagnostic pop)

// #elif __GNUC__

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
//   _C10_PRAGMA_(GCC diagnostic push)
//   _C10_PRAGMA_(GCC diagnostic ignored "-Wpragmas")
//   _C10_PRAGMA_(GCC diagnostic ignored warning)

// #define C10_DIAGNOSTIC_POP() _C10_PRAGMA_(GCC diagnostic pop)

// #else

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
// #define C10_DIAGNOSTIC_POP()

// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/core/DeviceType.h

// #pragma once

// This is directly synchronized with caffe2/proto/caffe2.proto, but
// doesn't require me to figure out how to get Protobuf headers into
// ATen/core (which would require a lot more build system hacking.)
// If you modify me, keep me synchronized with that file.

// #include <c10/macros/Export.h>

// #include <functional>
// #include <ostream>

// These contains all device types that also have a BackendComponent
// and therefore participate in per-backend functionality dispatch keys.
// This is most backends except PrivateUse2 and PrivateUse3
// #define C10_FORALL_BACKEND_DEVICE_TYPES(_, extra)
//   _(CPU, extra)
//   _(CUDA, extra)
//   _(HIP, extra)
//   _(XLA, extra)
//   _(MPS, extra)
//   _(IPU, extra)
//   _(XPU, extra)
//   _(HPU, extra)
//   _(VE, extra)
//   _(Lazy, extra)
//   _(Meta, extra)
//   _(MTIA, extra)
//   _(PrivateUse1, extra)

@Namespace("c10") public enum DeviceType {
  CPU((byte)(0)),
  CUDA((byte)(1)), // CUDA.
  MKLDNN((byte)(2)), // Reserved for explicit MKLDNN
  OPENGL((byte)(3)), // OpenGL
  OPENCL((byte)(4)), // OpenCL
  IDEEP((byte)(5)), // IDEEP.
  HIP((byte)(6)), // AMD HIP
  FPGA((byte)(7)), // FPGA
  ORT((byte)(8)), // ONNX Runtime / Microsoft
  XLA((byte)(9)), // XLA / TPU
  Vulkan((byte)(10)), // Vulkan
  Metal((byte)(11)), // Metal
  XPU((byte)(12)), // XPU
  MPS((byte)(13)), // MPS
  Meta((byte)(14)), // Meta (tensors with no data)
  HPU((byte)(15)), // HPU / HABANA
  VE((byte)(16)), // SX-Aurora / NEC
  Lazy((byte)(17)), // Lazy Tensors
  IPU((byte)(18)), // Graphcore IPU
  MTIA((byte)(19)), // Meta training and inference devices
  PrivateUse1((byte)(20)), // PrivateUse1 device
  // NB: If you add more devices:
  //  - Change the implementations of DeviceTypeName and isValidDeviceType
  //    in DeviceType.cpp
  //  - Change the number below
  COMPILE_TIME_MAX_DEVICE_TYPES((byte)(21));

    public final byte value;
    private DeviceType(byte v) { this.value = v; }
    private DeviceType(DeviceType e) { this.value = e.value; }
    public DeviceType intern() { for (DeviceType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native DeviceType kCPU();
@Namespace("c10") @MemberGetter public static native DeviceType kCUDA();
@Namespace("c10") @MemberGetter public static native DeviceType kHIP();
@Namespace("c10") @MemberGetter public static native DeviceType kFPGA();
@Namespace("c10") @MemberGetter public static native DeviceType kORT();
@Namespace("c10") @MemberGetter public static native DeviceType kXLA();
@Namespace("c10") @MemberGetter public static native DeviceType kMPS();
@Namespace("c10") @MemberGetter public static native DeviceType kMeta();
@Namespace("c10") @MemberGetter public static native DeviceType kVulkan();
@Namespace("c10") @MemberGetter public static native DeviceType kMetal();
@Namespace("c10") @MemberGetter public static native DeviceType kXPU();
@Namespace("c10") @MemberGetter public static native DeviceType kHPU();
@Namespace("c10") @MemberGetter public static native DeviceType kVE();
@Namespace("c10") @MemberGetter public static native DeviceType kLazy();
@Namespace("c10") @MemberGetter public static native DeviceType kIPU();
@Namespace("c10") @MemberGetter public static native DeviceType kMTIA();
@Namespace("c10") @MemberGetter public static native DeviceType kPrivateUse1();

// define explicit int constant
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_MAX_DEVICE_TYPES();

@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d);

@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(DeviceType d);
@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(@Cast("c10::DeviceType") byte d);

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, DeviceType type);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Cast("c10::DeviceType") byte type);

@Namespace("c10") public static native void register_privateuse1_backend(@StdString BytePointer backend_name);
@Namespace("c10") public static native void register_privateuse1_backend(@StdString String backend_name);
@Namespace("c10") public static native @StdString BytePointer get_privateuse1_backend(@Cast("bool") boolean lower_case/*=true*/);
@Namespace("c10") public static native @StdString BytePointer get_privateuse1_backend();

 // namespace c10
 // namespace std



// Parsed from c10/util/Deprecated.h

// #pragma once

/**
 * This file provides portable macros for marking declarations
 * as deprecated.  You should generally use C10_DEPRECATED,
 * except when marking 'using' declarations as deprecated,
 * in which case you should use C10_DEFINE_DEPRECATED_USING
 * (due to portability concerns).
 */

// Sample usage:
//
//    C10_DEPRECATED void bad_func();
//    struct C10_DEPRECATED BadStruct {
//      ...
//    };

// NB: __cplusplus doesn't work for MSVC, so for now MSVC always uses
// the "__declspec(deprecated)" implementation and not the C++14
// "[[deprecated]]" attribute. We tried enabling "[[deprecated]]" for C++14 on
// MSVC, but ran into issues with some older MSVC versions.
// #if (defined(__cplusplus) && __cplusplus >= 201402L)
// #define C10_DEPRECATED [[deprecated]]
// #define C10_DEPRECATED_MESSAGE(message) [[deprecated(message)]]
// #elif defined(__GNUC__)
// #define C10_DEPRECATED __attribute__((deprecated))
// TODO Is there some way to implement this?
// #define C10_DEPRECATED_MESSAGE(message) __attribute__((deprecated))

// #elif defined(_MSC_VER)
// #else
// #warning "You need to implement C10_DEPRECATED for this compiler"
// #define C10_DEPRECATED
// #endif

// Sample usage:
//
//    C10_DEFINE_DEPRECATED_USING(BadType, int)
//
//   which is the portable version of
//
//    using BadType [[deprecated]] = int;

// technically [[deprecated]] syntax is from c++14 standard, but it works in
// many compilers.
// #if defined(__has_cpp_attribute)
// #if __has_cpp_attribute(deprecated) && !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName [[deprecated]] = TypeThingy;
// #endif
// #endif

// #if defined(_MSC_VER)
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING) && defined(__GNUC__)
// nvcc has a bug where it doesn't understand __attribute__((deprecated))
// declarations even when the host compiler supports it. We'll only use this gcc
// attribute when not cuda, and when using a GCC compiler that doesn't support
// the c++14 syntax we checked for above (available in __GNUC__ >= 5)
// #if !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName __attribute__((deprecated)) = TypeThingy;
// #else
// using cuda + gcc < 5, neither deprecated syntax is available so turning off.
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName = TypeThingy;
// #endif
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING)
// #warning "You need to implement C10_DEFINE_DEPRECATED_USING for this compiler"
// #define C10_DEFINE_DEPRECATED_USING
// #endif


// Parsed from c10/util/reverse_iterator.h

// #pragma once

/**
 * A constexpr std::reverse_iterator for C++11.
 * Implementation taken from libstdc++,
 * https://raw.githubusercontent.com/gcc-mirror/gcc/gcc-9_2_0-release/libstdc%2B%2B-v3/include/bits/stl_iterator.h
 * adapted to our code base and constexpr'ified.
 */

// Copyright (C) 2001-2019 Free Software Foundation, Inc.
//
// This file is part of the GNU ISO C++ Library.  This library is free
// software; you can redistribute it and/or modify it under the
// terms of the GNU General Public License as published by the
// Free Software Foundation; either version 3, or (at your option)
// any later version.

// This library is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// Under Section 7 of GPL version 3, you are granted additional
// permissions described in the GCC Runtime Library Exception, version
// 3.1, as published by the Free Software Foundation.

// You should have received a copy of the GNU General Public License and
// a copy of the GCC Runtime Library Exception along with this program;
// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
// <http://www.gnu.org/licenses/>.

/*
 *
 * Copyright (c) 1994
 * Hewlett-Packard Company
 *
 * Permission to use, copy, modify, distribute and sell this software
 * and its documentation for any purpose is hereby granted without fee,
 * provided that the above copyright notice appear in all copies and
 * that both that copyright notice and this permission notice appear
 * in supporting documentation.  Hewlett-Packard Company makes no
 * representations about the suitability of this software for any
 * purpose.  It is provided "as is" without express or implied warranty.
 *
 *
 * Copyright (c) 1996-1998
 * Silicon Graphics Computer Systems, Inc.
 *
 * Permission to use, copy, modify, distribute and sell this software
 * and its documentation for any purpose is hereby granted without fee,
 * provided that the above copyright notice appear in all copies and
 * that both that copyright notice and this permission notice appear
 * in supporting documentation.  Silicon Graphics makes no
 * representations about the suitability of this software for any
 * purpose.  It is provided "as is" without express or implied warranty.
 */

// #include <c10/util/C++17.h>
// #include <iterator>

 // namespace c10


// Parsed from c10/util/StringUtil.h

// #ifndef C10_UTIL_STRINGUTIL_H_
// #define C10_UTIL_STRINGUTIL_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/string_utils.h>
// #include <c10/util/string_view.h>

// #include <cstddef>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// Obtains the base name from a full path.
@Namespace("c10::detail") public static native @StdString BytePointer StripBasename(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String StripBasename(@StdString String full_path);

@Namespace("c10::detail") public static native @StdString BytePointer ExcludeFileExtension(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String ExcludeFileExtension(@StdString String full_path);
// Targeting ../CompileTimeEmptyString.java











// Specializations for already-a-string types.

// For c10::str() with an empty argument list (which is common in our assert
// macros), we don't want to pay the binary size for constructing and
// destructing a stringstream or even constructing a string.

 // namespace detail

// Convert a list of string-like arguments into a single string.

// Replace all occurrences of "from" substring to "to" string.
// Returns number of replacements
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, @StringView BytePointer from, @StringView BytePointer to);
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, @StringView String from, @StringView String to);
// Targeting ../SourceLocation.java





// unix isprint but insensitive to locale
@Namespace("c10") public static native @Cast("bool") boolean isPrint(@Cast("char") byte s);

@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @StringView BytePointer str);
@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @StringView String str);

 // namespace c10

// #endif // C10_UTIL_STRINGUTIL_H_


// Parsed from c10/util/in_place.h

// #pragma once

// #include <cstddef>

 // namespace c10


// Parsed from c10/util/Exception.h

// #ifndef C10_UTIL_EXCEPTION_H_
// #define C10_UTIL_EXCEPTION_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/StringUtil.h>
// #include <c10/util/variant.h>

// #include <cstddef>
// #include <exception>
// #include <string>
// #include <vector>

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif
// Targeting ../Error.java


// Targeting ../Warning.java



// Issue a warning with a given message. Dispatched to the current
// warning handler.
@Namespace("c10") public static native void warn(@Const @ByRef Warning warning);
// Targeting ../WarningHandler.java



// Note: [Verbatim Warnings]
// Warnings originating in C++ code can appear out-of-place to Python users:
// a user runs a line in Python, but the warning references a line in C++.
// Some parts of PyTorch, like the JIT, are cognizant of this mismatch
// and take care to map warnings back to the user's program, but most
// of PyTorch simply throws a context-free warning. To allow warning
// handlers to add context where appropriate, warn takes the
// "verbatim" flag. When this is false a warning handler might append
// the C++ warning to a Python warning message that relates the warning
// back to the user's program. Callers who have already accounted for
// context in their warnings should set verbatim to true so their warnings
// appear without modification.

/** Sets the global warning handler. This is not thread-safe, so it should
 *  generally be called once during initialization or while holding the GIL
 *  for programs that use python.
 *  User is responsible for keeping the WarningHandler alive until
 *  it is not needed. */
@Namespace("c10::WarningUtils") public static native @NoException(true) void set_warning_handler(WarningHandler handler);
/** Gets the global warning handler. */
@Namespace("c10::WarningUtils") public static native @NoException(true) WarningHandler get_warning_handler();
// Targeting ../WarningHandlerGuard.java



/** The TORCH_WARN_ONCE macro is difficult to test for. Use
 *  setWarnAlways(true) to turn it into TORCH_WARN, which can be
 *  tested for more easily. */
@Namespace("c10::WarningUtils") public static native @NoException(true) void set_warnAlways(@Cast("bool") boolean arg0);
@Namespace("c10::WarningUtils") public static native @Cast("bool") @NoException(true) boolean get_warnAlways();
// Targeting ../WarnAlways.java




// Targeting ../IndexError.java


// Targeting ../ValueError.java


// Targeting ../TypeError.java


// Targeting ../NotImplementedError.java


// Targeting ../EnforceFiniteError.java


// Targeting ../OnnxfiBackendSystemError.java


// Targeting ../LinAlgError.java


// Targeting ../OutOfMemoryError.java


// Targeting ../DistBackendError.java



// A utility function to return an exception std::string by prepending its
// exception type before its what() content
@Namespace("c10") public static native @StdString BytePointer GetExceptionString(@Cast("const std::exception*") @ByRef Pointer e);

 // namespace c10

// Private helper macro for implementing TORCH_INTERNAL_ASSERT and TORCH_CHECK
//
// Note: In the debug build With MSVC, __LINE__ might be of long type (a.k.a
// int32_t), which is different from the definition of `SourceLocation` that
// requires unsigned int (a.k.a uint32_t) and may cause a compile error with the
// message: error C2397: conversion from 'long' to 'uint32_t' requires a
// narrowing conversion Here the static cast is used to pass the build. if this
// is used inside a lambda the __func__ macro expands to operator(), which isn't
// very useful, but hard to fix in a macro so suppressing the warning.
// #define C10_THROW_ERROR(err_type, msg)
//   throw ::c10::err_type(
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, msg)

// Private helper macro for workaround MSVC misexpansion of nested macro
// invocations involving __VA_ARGS__.  See
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define C10_EXPAND_MSVC_WORKAROUND(x) x

// On nvcc, C10_UNLIKELY thwarts missing return statement analysis.  In cases
// where the unlikely expression may be a constant, use this macro to ensure
// return statement analysis keeps working (at the cost of not getting the
// likely/unlikely annotation on nvcc).
// https://github.com/pytorch/pytorch/issues/21418
//
// Currently, this is only used in the error reporting macros below.  If you
// want to use it more generally, move me to Macros.h
//
// TODO: Brian Vaughan observed that we might be able to get this to work on
// nvcc by writing some sort of C++ overload that distinguishes constexpr inputs
// from non-constexpr.  Since there isn't any evidence that losing C10_UNLIKELY
// in nvcc is causing us perf problems, this is not yet implemented, but this
// might be an interesting piece of C++ code for an intrepid bootcamper to
// write.
// #if defined(__CUDACC__)
// #define C10_UNLIKELY_OR_CONST(e) e
// #else
// #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
// #endif

// ----------------------------------------------------------------------------
// Error reporting macros
// ----------------------------------------------------------------------------

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_RETHROW(e, ...) throw
// #else
// #define TORCH_RETHROW(e, ...)
//   do {
//     e.add_context(::c10::str(__VA_ARGS__));
//     throw;
//   } while (false)
// #endif

// A utility macro to provide assert()-like functionality; that is, enforcement
// of internal invariants in code.  It supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the assert
// failure message using operator<< (this is useful to print some variables
// which may be useful for debugging.)
//
// Usage:
//    TORCH_INTERNAL_ASSERT(should_be_true);
//    TORCH_INTERNAL_ASSERT(x == 0, "x = ", x);
//
// Assuming no bugs in PyTorch, the conditions tested by this macro should
// always be true; e.g., it should be possible to disable all of these
// conditions without changing observable user behavior.  If you would like to
// do error reporting for user input, please use TORCH_CHECK instead.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike assert()).
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond " INTERNAL ASSERT FAILED at " C10_STRINGIZE(__FILE__));
//   }
// #else
// It would be nice if we could build a combined string literal out of
// the TORCH_INTERNAL_ASSERT prefix and a user-provided string literal
// as the first argument, but there doesn't seem to be any good way to
// do that while still supporting having a first argument that isn't a
// string literal.
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchInternalAssertFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond
//         " INTERNAL ASSERT FAILED at " C10_STRINGIZE(__FILE__) ":" C10_STRINGIZE(
//             __LINE__) ", please report a bug to PyTorch. ",
//         c10::str(__VA_ARGS__));
//   }
// #endif

// A utility macro to make it easier to test for error conditions from user
// input.  Like TORCH_INTERNAL_ASSERT, it supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the error
// message using operator<< (e.g., you can pass any object which has
// operator<< defined.  Most objects in PyTorch have these definitions!)
//
// Usage:
//    TORCH_CHECK(should_be_true); // A default error message will be provided
//                                 // in this case; but we recommend writing an
//                                 // explicit error message, as it is more
//                                 // user friendly.
//    TORCH_CHECK(x == 0, "Expected x to be 0, but got ", x);
//
// On failure, this macro will raise an exception.  If this exception propagates
// to Python, it will convert into a Python RuntimeError.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike CHECK() from glog.)
//
// #define TORCH_CHECK_WITH(error_t, cond, ...)
//   TORCH_CHECK_WITH_MSG(error_t, cond, "", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK_MSG(cond, type, ...)
//   (#cond #type " CHECK FAILED at " C10_STRINGIZE(__FILE__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #else
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(@Cast("const char*") BytePointer msg);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(String msg);
// If there is just 1 user-provided C-string argument, use it.
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(
    @Cast("const char*") BytePointer arg0,
    @Cast("const char*") BytePointer args);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(
    String arg0,
    String args);
 // namespace detail
 // namespace c10

// #define TORCH_CHECK_MSG(cond, type, ...)
//   (::c10::detail::torchCheckMsgImpl(
//       "Expected " #cond
//       " to be true, but got false.  "
//       "(Could this error message be improved?  If so, "
//       "please report an enhancement request to PyTorch.)",
//       ##__VA_ARGS__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(error_t, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #endif

@Namespace("c10::detail") public static native void torchCheckFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @StdString BytePointer msg);
@Namespace("c10::detail") public static native void torchCheckFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    @StdString String msg);

// The c10::str() call that creates userMsg can have 1 of 3 return
// types depending on the number and types of arguments passed to
// TORCH_INTERNAL_ASSERT.  0 arguments will get a
// CompileTimeEmptyString, 1 const char * will be passed straight
// through, and anything else will get converted to std::string.
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @Cast("const char*") BytePointer userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    String userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @ByVal CompileTimeEmptyString arg4);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    @ByVal CompileTimeEmptyString arg4);

 // namespace detail
 // namespace c10

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", __VA_ARGS__));
//   }
// #else
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", ##__VA_ARGS__));
//   }
// #endif

// An utility macro that does what `TORCH_CHECK` does if compiled in the host
// code, otherwise does nothing. Supposed to be used in the code shared between
// host and device code as an alternative for `TORCH_CHECK`.
// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define TORCH_CHECK_IF_NOT_ON_CUDA(cond, ...) TORCH_CHECK(cond, ##__VA_ARGS__)
// #endif

// Debug only version of TORCH_INTERNAL_ASSERT. This macro only checks in debug
// build, and does nothing in release build.  It is appropriate to use
// in situations where you want to add an assert to a hotpath, but it is
// too expensive to run this assert on production builds.
// #ifdef NDEBUG
// Optimized version - generates no code.
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   while (false)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #else
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #endif

// TODO: We're going to get a lot of similar looking string literals
// this way; check if this actually affects binary size.

// Like TORCH_CHECK, but raises LinAlgError instead of Error.
// #define TORCH_CHECK_LINALG(cond, ...)
//   TORCH_CHECK_WITH_MSG(LinAlgError, cond, "LINALG", __VA_ARGS__)

// Like TORCH_CHECK, but raises IndexErrors instead of Errors.
// #define TORCH_CHECK_INDEX(cond, ...)
//   TORCH_CHECK_WITH_MSG(IndexError, cond, "INDEX", __VA_ARGS__)

// Like TORCH_CHECK, but raises ValueErrors instead of Errors.
// #define TORCH_CHECK_VALUE(cond, ...)
//   TORCH_CHECK_WITH_MSG(ValueError, cond, "VALUE", __VA_ARGS__)

// Like TORCH_CHECK, but raises TypeErrors instead of Errors.
// #define TORCH_CHECK_TYPE(cond, ...)
//   TORCH_CHECK_WITH_MSG(TypeError, cond, "TYPE", __VA_ARGS__)

// Like TORCH_CHECK, but raises NotImplementedErrors instead of Errors.
// #define TORCH_CHECK_NOT_IMPLEMENTED(cond, ...)
//   TORCH_CHECK_WITH_MSG(NotImplementedError, cond, "TYPE", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define WARNING_MESSAGE_STRING(...)
//   ::c10::detail::CompileTimeEmptyString {}
// #else
// #define WARNING_MESSAGE_STRING(...) ::c10::str(__VA_ARGS__)
// #endif

// Report a warning to the user.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef DISABLE_WARN
// #define _TORCH_WARN_WITH(...) ((void)0);
// #else
// #define _TORCH_WARN_WITH(warning_t, ...)
//   ::c10::warn(::c10::Warning(
//       warning_t(),
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//       WARNING_MESSAGE_STRING(__VA_ARGS__),
//       false));
// #endif

// #define TORCH_WARN(...) _TORCH_WARN_WITH(::c10::UserWarning, __VA_ARGS__);

// #define TORCH_WARN_DEPRECATION(...)
//   _TORCH_WARN_WITH(::c10::DeprecationWarning, __VA_ARGS__);

// Report a warning to the user only once.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #define _TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) =
//       [&] {
//         TORCH_WARN(__VA_ARGS__);
//         return true;
//       }()

// #ifdef DISABLE_WARN
// #define TORCH_WARN_ONCE(...) ((void)0);
// #else
// #define TORCH_WARN_ONCE(...)
//   if (::c10::WarningUtils::get_warnAlways()) {
//     TORCH_WARN(__VA_ARGS__);
//   } else {
//     _TORCH_WARN_ONCE(__VA_ARGS__);
//   }
// #endif

// Report an error with a specific argument
// NOTE: using the argument name in TORCH_CHECK's message is preferred
// #define TORCH_CHECK_ARG(cond, argN, ...)
//   TORCH_CHECK(cond, "invalid argument ", argN, ": ", __VA_ARGS__)

// ----------------------------------------------------------------------------
// Deprecated macros
// ----------------------------------------------------------------------------

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ERROR(msg) is deprecated, use TORCH_CHECK(false, msg)
instead.")
*/


/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERT is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/


/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERTM is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/


 // namespace detail
 // namespace c10

// Deprecated alias; this alias was deprecated because people kept mistakenly
// using it for user error checking.  Use TORCH_INTERNAL_ASSERT or TORCH_CHECK
// instead. See https://github.com/pytorch/pytorch/issues/20287 for more
// details.
// #define AT_ASSERT(...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERT();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__));
//   } while (false)

// Deprecated alias, like AT_ASSERT.  The new TORCH_INTERNAL_ASSERT macro
// supports both 0-ary and variadic calls, so having a separate
// message-accepting macro is not necessary.
//
// NB: we MUST include cond explicitly here, as MSVC will miscompile the macro
// expansion, shunting all of __VA_ARGS__ to cond.  An alternate workaround
// can be seen at
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define AT_ASSERTM(cond, ...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERTM();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__));
//   } while (false)

// Deprecated alias; this alias was deprecated because it represents extra API
// surface that makes it hard for people to understand what macro to use.
// Use TORCH_CHECK(false, ...) or TORCH_INTERNAL_ASSERT(false, ...) to
// unconditionally fail at a line of code.
// #define AT_ERROR(...)
//   do {
//     ::c10::detail::deprecated_AT_ERROR();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_CHECK(false, ::c10::str(__VA_ARGS__)));
//   } while (false)

// #endif // C10_UTIL_EXCEPTION_H_


// Parsed from c10/core/Device.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Export.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <iosfwd>
// #include <string>

/** An index representing a specific device; e.g., the 1 in GPU 1.
 *  A DeviceIndex is not independently meaningful without knowing
 *  the DeviceType it is associated; try to use Device rather than
 *  DeviceIndex directly. */
// Targeting ../Device.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Device device);

 // namespace c10
 // namespace std


// Parsed from c10/core/DispatchKey.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Export.h>
// #include <cstdint>
// #include <ostream>
// #include <string>

// Semantically, each value of BackendComponent identifies a "backend" for our
// dispatch. Some functionalities that we may dispatch to are allowed to
// register different handlers for each backend. The BackendComponent is then
// used to figure out which backend implementation to dispatch to.

// In implementation terms, the backend component identifies a specific "bit" in
// a DispatchKeySet. The bits in the DispatchKeySet are split between the bottom
// ~12 "BackendComponent" bits, while the remaining upper bits are assigned to
// functionalities. When we encounter a functionality bit that is known to be
// customizeable per-backend, then we also look at the lower BackendComponent
// bits and take the highest bit to determine which backend's implementation to
// use.

// WARNING!  If you add a new backend component to the end of this list,
// make sure you register it before Meta.
// Meta must be at the end so that meta key in tls triggers meta kernels.
// (But you shouldn't: private use keys should have higher precedence than all
// built-in keys)

// If you add a new (non-privateuse) backend here,
// make sure to add an Autograd<Backend> fallthrough kernel
// in aten/src/ATen/core/VariableFallbackKernel.cpp

// #define C10_FORALL_BACKEND_COMPONENTS(_, extra)
//   _(CPU, extra)
//   _(CUDA, extra)
//   _(HIP, extra)
//   _(XLA, extra)
//   _(MPS, extra)
//   _(IPU, extra)
//   _(XPU, extra)
//   _(HPU, extra)
//   _(VE, extra)
//   _(Lazy, extra)
//   _(MTIA, extra)
//   _(PrivateUse1, extra)
//   _(PrivateUse2, extra)
//   _(PrivateUse3, extra)
//   _(Meta, extra)

// WARNING!  If we add a new per-backend functionality key that has higher
// priority than Autograd, then make sure you update EndOfRuntimeBackendKeys

// #define C10_FORALL_FUNCTIONALITY_KEYS(_)
//   _(Dense, )
//   _(Quantized, Quantized)
//   _(Sparse, Sparse)
//   _(NestedTensor, NestedTensor)
//   _(AutogradFunctionality, Autograd)

@Namespace("c10") public enum BackendComponent {

  // A "backend" is colloquially used to refer to handlers for dispatch
  // which actually implement the numerics of an operation in question.
  //
  // Due to the nature of the enum, these backends are specified in
  // an ordered way, but for most backends this order is not semantically
  // meaningful (e.g., it's valid to reorder these backends without changing
  // semantics).  The only situation when backend ordering is meaningful
  // is when the backend participates in multiple dispatch with another
  // backend; e.g., CPU and CUDA (cuda must have higher priority).

  // These keys don't correspond to individual kernels.
  // Instead, they represent the backends that are allowed to override specific
  // pieces of functionality:
  // - dense kernels (e.g. DispatchKey::CPU)
  // - sparse kernels (e.g. DispatchKey::SparseCPU)
  // - quantized kernels (e.g. DispatchKey::QuantizedCPU)
  // - autograd kernels (e.g. DispatchKey::AutogradCPU)
  // We reserve space in the runtime operator table for this full cross product
  // of
  // [backends in this enum] x [keys below that are explicitly marked as having
  // per-backend functionality]
  //
  // A meta tensor is a tensor without any data associated with it.  (They
  // have also colloquially been referred to as tensors on the "null" device).
  // A meta tensor can be used to dry run operators without actually doing any
  // computation, e.g., add on two meta tensors would give you another meta
  // tensor with the output shape and dtype, but wouldn't actually add anything.

  InvalidBit((byte)(0)),
  CPUBit((byte)(1)),
  CUDABit((byte)(2)),
  HIPBit((byte)(3)),
  XLABit((byte)(4)),
  MPSBit((byte)(5)),
  IPUBit((byte)(6)),
  XPUBit((byte)(7)),
  HPUBit((byte)(8)),
  VEBit((byte)(9)),
  LazyBit((byte)(10)),
  MTIABit((byte)(11)),
  PrivateUse1Bit((byte)(12)),
  PrivateUse2Bit((byte)(13)),
  PrivateUse3Bit((byte)(14)),
  MetaBit((byte)(15)),

  // Define an alias to represent end of backend dispatch keys.
  // If you add new backend keys after PrivateUse3, please also update it here.
  EndOfBackendKeys((byte)(MetaBit.value));

    public final byte value;
    private BackendComponent(byte v) { this.value = v; }
    private BackendComponent(BackendComponent e) { this.value = e.value; }
    public BackendComponent intern() { for (BackendComponent e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Semantically, a dispatch key identifies a possible "level" in our
// dispatch, for which a handler may be registered. Each handler corresponds
// to a type of functionality.
//
// In implementation terms, the dispatch key identifies a specific "bit" in a
// DispatchKeySet.  Higher bit indexes get handled by dispatching first (because
// we "count leading zeros" when we extract the highest priority dispatch
// key.)
//
// Note [DispatchKey Classification]
// This enum actually contains several types of keys, which are explained
// in more detail further down:
// (1) non-customizable backends (e.g. FPGA)
// (2) non-customizable functionalities (e.g. Functionalize)
// (3) functionalized that are customizable per backend (e.g. Dense, Sparse,
// AutogradFunctionality) (4) per-backend instances of customizable
// functionalities (e.g. CPU, SparseCPU, AutogradCPU) (5) alias keys (e.g.
// CompositeImplicitAutograd)
//
// Of the categories above, it's important to note:
// (a) which keys are assigned individual bits in a DispatchKeySet
// (b) which keys are assigned individual slots in the runtime operator table
// ("Runtime keys")
//
// (1), (2) and (3) all get their own dedicated bits in the DispatchKeySet.
// (1), (2) and (4) all get their own dedicated slots in the runtime operator
// table.

// See Note [DispatchKeySet Internal Representation] for more details.
//
// NOTE: Keep the list in sync with `DispatchKey` in torchgen/model.py
@Namespace("c10") public enum DispatchKey {

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ UNDEFINED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // This is not a "real" functionality, but it exists to give us a "nullopt"
  // element we can return for cases when a DispatchKeySet contains no elements.
  // You can think a more semantically accurate definition of DispatchKey is:
  //
  //    using DispatchKey = optional<RealDispatchKey>
  //
  // and Undefined == nullopt.  We didn't actually represent
  // it this way because optional<RealDispatchKey> would take two
  // words, when DispatchKey fits in eight bits.

  Undefined((short)(0)),

  // Define an alias for Undefined to represent CatchAll (long term
  // this will get eliminated, but for now it's convenient)
  CatchAll((short)(Undefined.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ Functionality Keys ~~~~~~~~~~~~~~~~~~~~~~ //
  // Every value in the enum (up to EndOfFunctionalityKeys)
  // corresponds to an individual "functionality" that can be dispatched to.
  // This is represented in the DispatchKeySet by assigning each of these enum
  // values
  // to each of the remaining (64 - len(BackendComponent)) bits.
  //
  // Most of these functionalities have a single handler assigned to them,
  // making them "runtime keys".
  // That map to a single slot in the runtime operator table.
  //
  // A few functionalities are allowed to be customizable per backend.
  // See [Note: Per-Backend Functionality Dispatch Keys] for details.

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Dense((short)(Undefined.value + 1)),

  // Below are non-extensible backends.
  // These are backends that currently don't have their own overrides for
  // Autograd/Sparse/Quantized kernels,
  // and we therefore don't waste space in the runtime operator table allocating
  // space for them.
  // If any of these backends ever need to customize, e.g., Autograd, then we'll
  // need to add a DispatchKey::*Bit for them.

  // TODO: put this in BackendComponents
  FPGA((short)(Undefined.value + 2)), // Xilinx support lives out of tree at
  // https://gitlab.com/pytorch-complex/vitis_kernels

  // TODO: put this in BackendComponents
  // ONNX Runtime, lives out of tree at https://github.com/pytorch/ort and
  // https://github.com/microsoft/onnxruntime, and is also used to test general
  // backend/extension machinery in the core. cf:
  // - test/cpp_extensions/ort_extension.cpp
  // - test/test_torch.py
  // - aten/src/ATen/test/extension_backend_test.cpp
  ORT((short)(Undefined.value + 3)),

  Vulkan((short)(Undefined.value + 4)), // TODO: put this in BackendComponents
  Metal((short)(Undefined.value + 5)), // TODO: put this in BackendComponents

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Quantized((short)(Undefined.value + 6)),

  // This backend is to support custom RNGs; it lets you go
  // to a different kernel if you pass in a generator that is not a
  // traditional CPUGeneratorImpl/CUDAGeneratorImpl.  To make use of this
  // key:
  //  1) set it as a second parameter of at::Generator constructor call in
  //     the user-defined PRNG class.
  //  2) use it as a dispatch key while registering custom kernels
  //     (templatized kernels specialized for user-defined PRNG class)
  // intended for out of tree use; tested by aten/src/ATen/test/rng_test.cpp
  CustomRNGKeyId((short)(Undefined.value + 7)),

  // TODO: Make Mkldnn a functionality key, so we can give it Meta
  // support
  // Here are backends which specify more specialized operators
  // based on the layout of the tensor.  Note that the sparse backends
  // are one case where ordering matters: sparse multi-dispatches with
  // the corresponding dense tensors, and must be handled before them.
  MkldnnCPU((short)(Undefined.value + 8)), // registered at build/aten/src/ATen/RegisterMkldnnCPU.cpp
  // NB: not to be confused with MKLDNN, which is Caffe2 only

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Sparse((short)(Undefined.value + 9)),

  // TODO: Make SparseCsr a functionality key
  SparseCsrCPU((short)(Undefined.value + 10)),
  SparseCsrCUDA((short)(Undefined.value + 11)),

  NestedTensor((short)(Undefined.value + 12)),

  // In some situations, it is not immediately obvious what the correct
  // backend for function is, because the function in question doesn't
  // have any "tensor" arguments.  In this case, a BackendSelect function
  // can be registered to implement the custom determination of the
  // correct backend.
  BackendSelect((short)(Undefined.value + 13)),

  Python((short)(Undefined.value + 14)),

  // Out-of-core key for Fake Tensor in torchdistx.
  // See https://pytorch.org/torchdistx/latest/fake_tensor.html
  // TODO: delete this in favor of Python-implemented fake tensor
  Fake((short)(Undefined.value + 15)),
  // See Note [Out-of-tree vmap+grad prototype]. The purpose of this key
  // is to insert code after the "autograd subsystem" runs, so this key should
  // be directly after ADInplaceOrView and all of the autograd keys.
  FuncTorchDynamicLayerBackMode((short)(Undefined.value + 16)),

  // Alias and mutation removal.
  // If some backends want to opt into only alias removal or only mutation
  // removal,
  // we can consider adding separate keys dedicated to those individual passes.
  // See Note [Functionalization Pass In Core] for details.
  Functionalize((short)(Undefined.value + 17)),

  // The named dispatch key is set for any tensors with named dimensions.
  // Although we have a dispatch key for named tensors, for historical reasons,
  // this dispatch key doesn't do any of the substantive functionality for named
  // tensor (though, hypothetically, it could!)  At the moment, it's just
  // responsible for letting us give good error messages when operations
  // don't support named tensors.
  //
  // NB: If you ever consider moving named tensor functionality into
  // this dispatch key, note that it might be necessary add another dispatch
  // key that triggers before composite operators, in case a composite operator
  // has named dimension propagation that doesn't match that of its
  // constituent parts.
  // TODO: delete this once torchdim lands in functorch
  Named((short)(Undefined.value + 18)),

  // The Conjugate dispatch key is set for any tensors that need to perform
  // conjugation
  // This is implemented at a dispatch level right before any backends run
  Conjugate((short)(Undefined.value + 19)),

  // The Negative dispatch key is set for any tensors that need to perform
  // negation
  // This is implemented at a dispatch level right before any backends run
  Negative((short)(Undefined.value + 20)),

  ZeroTensor((short)(Undefined.value + 21)), // registered at build/aten/src/ATen/RegisterZeroTensor.cpp

  // Note [ADInplaceOrView key]
  // ADInplaceOrView key is used by inplace or view ops to register a kernel
  // that does additional setup for future autograd computation.
  //
  // 1. For inplace ops this kernel does version bump
  // 2. For view ops this kernel does `as_view` setup where we properly setup
  //    DifferentiableViewMeta on the view tensors.
  //
  // For other ops it's fallthrough kernel since there's no extra
  // work to do.
  //
  // Note [Dream: skip VariableType kernel when requires_grad=false]
  //
  // In an ideal world where we can skip VariableType kernel for inputs
  // with requires_grad=false, instead of a fallthrough kernel, we'll
  // register a kernel shown below to all functional ops as well:
  // torch::Tensor my_functional_op(...) {
  //   {
  //     // Note for every op in VariableType, you need to go through
  //     // `AutoDispatchBelowADInplaceOrView` guard exactly once to add the
  //     // key to TLS excluded set. If you don't go through it at all,
  //     // inplace/view ops called through `at::` inside your backend
  //     // kernel will dispatch to ADInplaceOrView kernels and do a lot
  //     // of extra work.
  //     at::AutoDispatchBelowADInplaceOrView guard;
  //     at::redispatch::my_functional_op(...);
  //   }
  // }
  // But this work is currently blocked since it adds an extra dispatch
  // for all ops and it's non-trivial overhead at model level(a few percents).
  // Thus our current approach takes advantage of the fact every kernel go
  // through VariableType kernel first and pulls the
  // `at::AutoDispatchBelowADInplaceOrView` guard of functional ops
  // up to the `VariableType` kernel. Thus we only add the extra dispatch
  // to view/inplace ops to minimize its perf impact to real models.
  ADInplaceOrView((short)(Undefined.value + 22)),
  // Note [Alias Dispatch Key : Autograd]
  // All backends are oblivious to autograd; autograd is handled as a
  // layer which happens on top of all backends. It inspects the autograd
  // metadata of all inputs, determines what autograd metadata should be
  // constructed by the output, and otherwise defers to the backend to
  // actually do the numeric computation.  Autograd contains
  // the bulk of this logic.

  // Autograd is now an alias dispatch key which by default maps to all
  // backend-specific autograd keys.
  // Backend-specific allow backends to override the default kernel registered
  // to Autograd key as needed.
  // For example, XLA wants to define autograd for einsum directly.
  // Registering a custom autograd implementation at the XLA key won't work
  // because we process Autograd before XLA.  This key has higher priority and
  // gets processed first.  You generally should NOT redispatch after handling
  // autograd here (since that would result in execution of the Autograd
  // operator, which you're trying to skip).  In AutogradXLA implementations,
  // you are responsible for handling autograd yourself, or deferring to other
  // operators which support autograd.

  // Currently we only have backend-specific autograd keys for CPU/CUDA/XLA and
  // reserved user-defined backends. All other in-tree backends share the
  // AutogradOther key. We can add specific autograd key for those backends
  // upon request.
  AutogradOther((short)(Undefined.value + 23)),

  // See [Note: Per-Backend Functionality Dispatch Keys]
  AutogradFunctionality((short)(Undefined.value + 24)),

  // NestedTensor is an example of something that isn't a "real backend"
  // (because it mostly consists of redispatching kernels)
  // but it would like to override autograd functionality in C++.
  // We can handle cases like this by adding an extra functionality key
  // exclusively for handling autograd for NestedTensor.
  // lives out of tree at
  // https://github.com/pytorch/nestedtensor
  AutogradNestedTensor((short)(Undefined.value + 25)),

  Tracer((short)(Undefined.value + 26)),

  // TODO: make Autocast a functionality key
  // Autocasting precedes VariableTypeId, to ensure casts are autograd-exposed
  // and inputs are saved for backward in the post-autocast type.
  AutocastCPU((short)(Undefined.value + 27)),
  AutocastXPU((short)(Undefined.value + 28)),
  AutocastIPU((short)(Undefined.value + 29)),
  AutocastHPU((short)(Undefined.value + 30)),
  AutocastXLA((short)(Undefined.value + 31)),
  // AutocastXLA is only being used for TPUs. XLA GPUs continue to use
  // AutocastCUDA.
  AutocastCUDA((short)(Undefined.value + 32)),
  AutocastPrivateUse1((short)(Undefined.value + 33)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~ WRAPPERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // There are a number of alternative modes which may want to handle before
  // autograd; for example, error checking, tracing, profiling or vmap.  They
  // go here.

  FuncTorchBatched((short)(Undefined.value + 34)), // See Note [Out-of-tree vmap+grad prototype]
  FuncTorchVmapMode((short)(Undefined.value + 35)), // See Note [Out-of-tree vmap+grad prototype]

  // This is the dispatch key for BatchedTensorImpl, which is used to implement
  // batching rules for vmap.
  Batched((short)(Undefined.value + 36)),

  // When we are inside a vmap, all tensors dispatch on this key.
  // See Note: [DispatchKey::VmapMode usage] for more details.
  VmapMode((short)(Undefined.value + 37)),

  FuncTorchGradWrapper((short)(Undefined.value + 38)), // See Note [Out-of-tree vmap+grad prototype]

  // Out-of-core key for Deferred Module Initialization in torchdistx.
  // See https://pytorch.org/torchdistx/latest/deferred_init.html
  DeferredInit((short)(Undefined.value + 39)),

  // Used by Python key logic to know the set of tls on entry to the dispatcher
  // This kernel assumes it is the top-most non-functorch-related DispatchKey.
  // If you add a key above, make sure to update the fallback implementation for
  // this.
  PythonTLSSnapshot((short)(Undefined.value + 40)),

  // This key should be at the very top of the dispatcher
  FuncTorchDynamicLayerFrontMode((short)(Undefined.value + 41)), // See Note [Out-of-tree vmap+grad prototype]

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a single
  // process test.  Use it by creating a TensorImpl with this DispatchKey, and
  // then registering operators to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp for a usage example.
  TESTING_ONLY_GenericWrapper((short)(Undefined.value + 42)),

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a ingle
  // process test.  Use it by toggling the mode on and off via
  // TESTING_ONLY_tls_generic_mode_set_enabled and then registering operators
  // to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp
  // for a usage example
  TESTING_ONLY_GenericMode((short)(Undefined.value + 43)),

  // This key is used for pre-dispatch tracing in make_fx.
  // It has lower priority than the PythonDispatcher key
  // because we use the PythonDispatcher to intercept the key from python,
  // and avoid having to implement it in C++.
  PreDispatch((short)(Undefined.value + 44)),

  // This is a bypass that allows you to skip running the C++ dispatcher
  // entirely
  PythonDispatcher((short)(Undefined.value + 45)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  EndOfFunctionalityKeys((short)(Undefined.value + 46)),

  StartOfDenseBackends((short)(Undefined.value + 47)),
      CPU((short)(Undefined.value + 48)),
          
  CUDA((short)(Undefined.value + 49)),
          
  HIP((short)(Undefined.value + 50)),
          
  XLA((short)(Undefined.value + 51)),
          
  MPS((short)(Undefined.value + 52)),
          
  IPU((short)(Undefined.value + 53)),
          
  XPU((short)(Undefined.value + 54)),
          
  HPU((short)(Undefined.value + 55)),
          
  VE((short)(Undefined.value + 56)),
          
  Lazy((short)(Undefined.value + 57)),
          
  MTIA((short)(Undefined.value + 58)),
          
  PrivateUse1((short)(Undefined.value + 59)),
          
  PrivateUse2((short)(Undefined.value + 60)),
          
  PrivateUse3((short)(Undefined.value + 61)),
          
  Meta((short)(Undefined.value + 62)),
          EndOfDenseBackends((short)(0)),
  StartOfQuantizedBackends((short)(1)),
      QuantizedCPU((short)(2)),
          
  QuantizedCUDA((short)(3)),
          
  QuantizedHIP((short)(4)),
          
  QuantizedXLA((short)(5)),
          
  QuantizedMPS((short)(6)),
          
  QuantizedIPU((short)(7)),
          
  QuantizedXPU((short)(8)),
          
  QuantizedHPU((short)(9)),
          
  QuantizedVE((short)(10)),
          
  QuantizedLazy((short)(11)),
          
  QuantizedMTIA((short)(12)),
          
  QuantizedPrivateUse1((short)(13)),
          
  QuantizedPrivateUse2((short)(14)),
          
  QuantizedPrivateUse3((short)(15)),
          
  QuantizedMeta((short)(16)),
          EndOfQuantizedBackends((short)( QuantizedMeta.value)),
  StartOfSparseBackends((short)( QuantizedMeta.value + 1)),
      SparseCPU((short)( QuantizedMeta.value + 2)),
          
  SparseCUDA((short)( QuantizedMeta.value + 3)),
          
  SparseHIP((short)( QuantizedMeta.value + 4)),
          
  SparseXLA((short)( QuantizedMeta.value + 5)),
          
  SparseMPS((short)( QuantizedMeta.value + 6)),
          
  SparseIPU((short)( QuantizedMeta.value + 7)),
          
  SparseXPU((short)( QuantizedMeta.value + 8)),
          
  SparseHPU((short)( QuantizedMeta.value + 9)),
          
  SparseVE((short)( QuantizedMeta.value + 10)),
          
  SparseLazy((short)( QuantizedMeta.value + 11)),
          
  SparseMTIA((short)( QuantizedMeta.value + 12)),
          
  SparsePrivateUse1((short)( QuantizedMeta.value + 13)),
          
  SparsePrivateUse2((short)( QuantizedMeta.value + 14)),
          
  SparsePrivateUse3((short)( QuantizedMeta.value + 15)),
          
  SparseMeta((short)( QuantizedMeta.value + 16)),
          EndOfSparseBackends((short)( SparseMeta.value)),
  StartOfNestedTensorBackends((short)( SparseMeta.value + 1)),
      NestedTensorCPU((short)( SparseMeta.value + 2)),
          
  NestedTensorCUDA((short)( SparseMeta.value + 3)),
          
  NestedTensorHIP((short)( SparseMeta.value + 4)),
          
  NestedTensorXLA((short)( SparseMeta.value + 5)),
          
  NestedTensorMPS((short)( SparseMeta.value + 6)),
          
  NestedTensorIPU((short)( SparseMeta.value + 7)),
          
  NestedTensorXPU((short)( SparseMeta.value + 8)),
          
  NestedTensorHPU((short)( SparseMeta.value + 9)),
          
  NestedTensorVE((short)( SparseMeta.value + 10)),
          
  NestedTensorLazy((short)( SparseMeta.value + 11)),
          
  NestedTensorMTIA((short)( SparseMeta.value + 12)),
          
  NestedTensorPrivateUse1((short)( SparseMeta.value + 13)),
          
  NestedTensorPrivateUse2((short)( SparseMeta.value + 14)),
          
  NestedTensorPrivateUse3((short)( SparseMeta.value + 15)),
          
  NestedTensorMeta((short)( SparseMeta.value + 16)),
          EndOfNestedTensorBackends((short)( NestedTensorMeta.value)),
  StartOfAutogradFunctionalityBackends((short)( NestedTensorMeta.value + 1)),
      AutogradCPU((short)( NestedTensorMeta.value + 2)),
          
  AutogradCUDA((short)( NestedTensorMeta.value + 3)),
          
  AutogradHIP((short)( NestedTensorMeta.value + 4)),
          
  AutogradXLA((short)( NestedTensorMeta.value + 5)),
          
  AutogradMPS((short)( NestedTensorMeta.value + 6)),
          
  AutogradIPU((short)( NestedTensorMeta.value + 7)),
          
  AutogradXPU((short)( NestedTensorMeta.value + 8)),
          
  AutogradHPU((short)( NestedTensorMeta.value + 9)),
          
  AutogradVE((short)( NestedTensorMeta.value + 10)),
          
  AutogradLazy((short)( NestedTensorMeta.value + 11)),
          
  AutogradMTIA((short)( NestedTensorMeta.value + 12)),
          
  AutogradPrivateUse1((short)( NestedTensorMeta.value + 13)),
          
  AutogradPrivateUse2((short)( NestedTensorMeta.value + 14)),
          
  AutogradPrivateUse3((short)( NestedTensorMeta.value + 15)),
          
  AutogradMeta((short)( NestedTensorMeta.value + 16)),
          EndOfAutogradFunctionalityBackends((short)( AutogradMeta.value)),

      EndOfRuntimeBackendKeys((short)(EndOfAutogradFunctionalityBackends.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~ Alias Dispatch Keys ~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // Note [Alias Dispatch Keys]
  // Alias dispatch keys are synthetic dispatch keys which map to multiple
  // runtime dispatch keys. Alisa keys have precedence, but they are always
  // lower precedence than runtime keys. You can register a kernel to an
  // alias key, the kernel might be populated to the mapped runtime keys
  // during dispatch table computation.
  // If a runtime dispatch key has multiple kernels from alias keys, which
  // kernel wins is done based on the precedence of alias keys (but runtime
  // keys always have precedence over alias keys).
  // Alias keys won't be directly called during runtime.

  // See Note [Alias Dispatch Key : Autograd]
  Autograd((short)(EndOfAutogradFunctionalityBackends.value + 1)),
  CompositeImplicitAutograd((short)(EndOfAutogradFunctionalityBackends.value + 2)), // registered at
  // build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp

  // Note: The alias keyset for FuncTorchBatchedDecomposition is disjoint from
  // all
  // other alias keysets
  // and so precedence order doesn't matter
  FuncTorchBatchedDecomposition((short)(EndOfAutogradFunctionalityBackends.value + 3)), // registered at
  // build/aten/src/ATen/RegisterFuncTorchBatchedDecomposition.cpp
  // Note: The alias keyset for CompositeImplicitAutogradNestedTensor is
  // disjoint from all other alias keysets
  CompositeImplicitAutogradNestedTensor((short)(EndOfAutogradFunctionalityBackends.value + 4)), // registered at
  // build/aten/src/ATen/RegisterCompositeImplicitAutogradNestedTensor.cpp
  CompositeExplicitAutograd((short)(EndOfAutogradFunctionalityBackends.value + 5)), // registered at
  // build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp
  // See Note [CompositeExplicitAutogradNonFunctional Key]
  CompositeExplicitAutogradNonFunctional((short)(EndOfAutogradFunctionalityBackends.value + 6)), // registered at
  // build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp

  // Define an alias key to represent end of alias dispatch keys.
  // If you add new alias keys after Autograd, please also update it here.
  StartOfAliasKeys((short)(Autograd.value)),
  EndOfAliasKeys((short)(CompositeExplicitAutogradNonFunctional.value)), //

  // ~~~~~~~~~~~~~~~~~~~~~~~~~ BC ALIASES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // The aliases exist for backwards compatibility reasons, they shouldn't
  // be used
  CPUTensorId((short)(CPU.value)),
  CUDATensorId((short)(CUDA.value)),
  DefaultBackend((short)(CompositeExplicitAutograd.value)),
  PrivateUse1_PreAutograd((short)(AutogradPrivateUse1.value)),
  PrivateUse2_PreAutograd((short)(AutogradPrivateUse2.value)),
  PrivateUse3_PreAutograd((short)(AutogradPrivateUse3.value)),
  Autocast((short)(AutocastCUDA.value));

    public final short value;
    private DispatchKey(short v) { this.value = v; }
    private DispatchKey(DispatchKey e) { this.value = e.value; }
    public DispatchKey intern() { for (DispatchKey e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Note [Private use DispatchKey]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Private use tensor IDs are preallocated tensor type IDs for use in user
// applications.  Similar to private use fields in HTTP, they can be used
// by end users for experimental or private applications, without needing
// to "standardize" the tensor ID (which would be done by submitting a PR
// to PyTorch to add your type ID).
//
// Private use tensor IDs are appropriate to use if you want to experiment
// with adding a new tensor type (without having to patch PyTorch first) or
// have a private, non-distributed application that needs to make use of a
// new tensor type.  Private use tensor IDs are NOT appropriate to use for
// libraries intended to be distributed to further users: please contact
// the PyTorch developers to get a type ID registered in this case.
//
// We provide two classes of private user tensor id: regular DispatchKeys
// and Autograd DispatchKeys.  DispatchKeys serve the role of ordinary "backend"
// DispatchKeys; if you were adding support for a new type of accelerator, you
// would use a backend DispatchKey, and ideally automatically reuse
// AutogradOther definitions already defined in PyTorch.  AutogradPrivateUse
// DispatchKeys serve as "wrapper" DispatchKeys: they are only necessary for
// tensors that compose multiple internal tensors, and for cases when the
// built-in autograd formulas for operators are not appropriate.

// Check if a DispatchKey is an alias mapping to other runtime keys.
@Namespace("c10") public static native @Cast("const bool") boolean isAliasDispatchKey(DispatchKey k);
@Namespace("c10") public static native @Cast("const bool") boolean isAliasDispatchKey(@Cast("c10::DispatchKey") short k);

// [Note: Per-Backend Functionality Dispatch Keys]
// Check if a DispatchKey is a per-backend functionality key
// Any functionalities that can be customized per-backend should be added here.
// These keys correspond to functionalities that can be customized individually
// per backend. While they only take up one bit in the `DispatchKeySet` bitset,
// they map to (# backends) slots in the operator table.
// Each of these keys also has a separate set of "runtime keys" in the dispatch
// key enum, per backend, which *do* map to the individual operator table slots.
// For example, the "Sparse" key maps to an individual bit in the
// DispatchKeySet, while `SparseCPU`, `SparseCUDA`, etc all map to individual
// slots in the runtime operator table.

@Namespace("c10") public static native @Cast("const bool") boolean isPerBackendFunctionalityKey(DispatchKey k);
@Namespace("c10") public static native @Cast("const bool") boolean isPerBackendFunctionalityKey(@Cast("c10::DispatchKey") short k);

// Note that this includes Undefined in the total count.
// BUT EndOfFunctionalityKeys is its own (placeholder) key.
// e.g. Undefined=0, Dense=1, Sparse=2, EndOfFunctionalityKeys=3.
// In the above example, there are 3 total functionality keys.
@Namespace("c10") @MemberGetter public static native @Cast("const uint8_t") byte num_functionality_keys();

@Namespace("c10") @MemberGetter public static native @Cast("const uint8_t") byte num_backends();

// Note [No More Than 16 Backends]
// Search for this note to find places in the code where the "no more than 16
// backends" invariant is baked in.

@Namespace("c10") public static native @Cast("const uint8_t") byte numPerBackendFunctionalityKeys();

// #if defined(C10_MOBILE_TRIM_DISPATCH_KEYS)
// See [Note: Trimmed Mobile Dispatch Keys]
@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short num_runtime_entries();
// #else
// #endif

// See Note [No More Than 16 Backends]
@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short full_backend_mask();

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(DispatchKey arg0);
@Namespace("c10") public static native String toString(@Cast("c10::DispatchKey") short arg0);
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(BackendComponent arg0);
@Namespace("c10") public static native String toString(@Cast("c10::BackendComponent") byte arg0);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, DispatchKey arg1);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, @Cast("c10::DispatchKey") short arg1);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, BackendComponent arg1);

@Namespace("c10") public static native DispatchKey getAutogradKeyFromBackend(BackendComponent k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short getAutogradKeyFromBackend(@Cast("c10::BackendComponent") byte k);

// Parses a string into a dispatch key.
// If the string cannot be correctly parsed, throws an exception.
@Namespace("c10") public static native DispatchKey parseDispatchKey(@StdString BytePointer k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short parseDispatchKey(@StdString String k);

// These are some convenience identifiers for dispatch keys which are
// shorter to type than their long counterparts.  Note that some of these
// dispatch keys directly correspond to DeviceType; and most APIs that
// accept DispatchKey also accept DeviceType; e.g.,
// torch::dispatch(torch::kCPU, ...) is also valid.
@Namespace("c10") @MemberGetter public static native DispatchKey kAutograd();

// See Note [The Ordering of Per-Backend Dispatch Keys Matters!]
// This function relies on the invariant that the dispatch keys between
// StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend
// in the same order as `BackendComponent`.


@Namespace("c10") public static native DispatchKey toFunctionalityKey(DispatchKey k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short toFunctionalityKey(@Cast("c10::DispatchKey") short k);



// Given (DispatchKey::Dense, BackendComponent::CUDABit), returns
// DispatchKey::CUDA.
// See Note [The Ordering of Per-Backend Dispatch Keys Matters!]
// This function relies on the invariant that the dispatch keys between
// StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend
// in the same order as `BackendComponent`.
@Namespace("c10") public static native DispatchKey toRuntimePerBackendFunctionalityKey(
    DispatchKey functionality_k,
    BackendComponent backend_k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short toRuntimePerBackendFunctionalityKey(
    @Cast("c10::DispatchKey") short functionality_k,
    @Cast("c10::BackendComponent") byte backend_k);

 // namespace c10
// Expose the constant, but not the TYPE (DispatchKey is an implementation
// detail!)
 // namespace torch

// NB: You really shouldn't use this instance; this enum is guaranteed
// to be pretty small so a regular array should be acceptable.
 // namespace std


// Parsed from c10/util/Array.h

// #pragma once

// #include <array>
// #include <utility>

 // namespace guts
 // namespace c10


// Parsed from c10/util/TypeTraits.h

// #pragma once

// #include <c10/util/C++17.h>

/**
 * is_equality_comparable<T> is true_type iff the equality operator is defined
 * for T.
 */

/**
 * is_hashable<T> is true_type iff std::hash is defined for T
 */

/**
 * is_function_type<T> is true_type iff T is a plain function type (i.e.
 * "Result(Args...)")
 */

/**
 * is_instantiation_of<T, I> is true_type iff I is a template instantiation of T
 * (e.g. vector<int> is an instantiation of vector) Example:
 *    is_instantiation_of_t<vector, vector<int>> // true
 *    is_instantiation_of_t<pair, pair<int, string>> // true
 *    is_instantiation_of_t<vector, pair<int, string>> // false
 */
/**
 * strip_class: helper to remove the class type from pointers to {@code operator()}.
 */
 // namespace detail

/**
 * Evaluates to true_type, iff the given class is a Functor
 * (i.e. has a call operator with some set of arguments)
 */

/**
 * lambda_is_stateless<T> is true iff the lambda type T is stateless
 * (i.e. does not have a closure).
 * Example:
 *  auto stateless_lambda = [] (int a) {return a;};
 *  lambda_is_stateless<decltype(stateless_lambda)> // true
 *  auto stateful_lambda = [&] (int a) {return a;};
 *  lambda_is_stateless<decltype(stateful_lambda)> // false
 */
// implementation idea: According to the C++ standard, stateless lambdas are
// convertible to function pointers

// case where LambdaType is not even a functor
// case where LambdaType is a functor
 // namespace detail

/**
 * is_type_condition<C> is true_type iff C<...> is a type trait representing a
 * condition (i.e. has a constexpr static bool ::value member) Example:
 *   is_type_condition<std::is_reference>  // true
 */

/**
 * is_fundamental<T> is true_type iff the lambda type T is a fundamental type
 * (that is, arithmetic type, void, or nullptr_t). Example: is_fundamental<int>
 * // true We define it here to resolve a MSVC bug. See
 * https://github.com/pytorch/pytorch/issues/30932 for details.
 */
 // namespace guts
 // namespace c10


// Parsed from c10/util/TypeList.h

// #pragma once

// #include <c10/util/C++17.h>
// #include <c10/util/TypeTraits.h>
// #include <algorithm>

/**
 * Type holding a list of types for compile time type computations
 */

/**
 * Returns the number of types in a typelist
 * Example:
 *   3  ==  size<typelist<int, int, double>>::value
 */

/**
 * Transforms a list of types into a tuple holding these types.
 * Example:
 *   std::tuple<int, string>  ==  to_tuple_t<typelist<int, string>>
 */

/**
 * Creates a typelist containing the types of a given tuple.
 * Example:
 *   typelist<int, string>  ==  from_tuple_t<std::tuple<int, string>>
 */

/**
 * Concatenates multiple type lists.
 * Example:
 *   typelist<int, string, int>  ==  concat_t<typelist<int, string>,
 * typelist<int>>
 */

/**
 * Filters the types in a type list by a type trait.
 * Examples:
 *   typelist<int&, const string&&>  ==  filter_t<std::is_reference,
 * typelist<void, string, int&, bool, const string&&, int>>
 */

/**
 * Counts how many types in the list fulfill a type trait
 * Examples:
 *   2  ==  count_if<std::is_reference, typelist<void, string, int&, bool, const
 * string&&, int>>
 */

/**
 * Checks if a typelist contains a certain type.
 * Examples:
 *  contains<typelist<int, string>, string> == true_type
 *  contains<typelist<int, string>, double> == false_type
 */
 // namespace detail

/**
 * Returns true iff the type trait is true for all types in the type list
 * Examples:
 *   true   ==  all<std::is_reference, typelist<int&, const float&&, const
 * MyClass&>>::value false  ==  all<std::is_reference, typelist<int&, const
 * float&&, MyClass>>::value
 */

/**
 * Returns true iff the type trait is true for any type in the type list
 * Examples:
 *   true   ==  true_for_any_type<std::is_reference, typelist<int, const
 * float&&, const MyClass>>::value false  ==
 * true_for_any_type<std::is_reference, typelist<int, const float,
 * MyClass>>::value
 */

/**
 * Maps types of a type list using a type trait
 * Example:
 *  typelist<int&, double&, string&>  ==  map_t<std::add_lvalue_reference_t,
 * typelist<int, double, string>>
 */

/**
 * Returns the first element of a type list.
 * Example:
 *   int  ==  head_t<typelist<int, string>>
 */

/**
 * Returns the first element of a type list, or the specified default if the
 * type list is empty. Example: int  ==  head_t<bool, typelist<int, string>>
 *   bool  ==  head_t<bool, typelist<>>
 */

/**
 * Returns the N-th element of a type list.
 * Example:
 * int == element_t<1, typelist<float, int, char>>
 */

/** Base template. */

/** Successful case, we have reached the zero index and can "return" the head
 *  type. */

/** Error case, we have an index but ran out of types! It will only be selected
 *  if {@code Ts...} is actually empty! */

/** Shave off types until we hit the <0, Head, Tail...> or <Index> case. */

/** Convenience alias. */

/**
 * Returns the last element of a type list.
 * Example:
 *   int  ==  last_t<typelist<int, string>>
 */

/**
 * Take/drop a number of arguments from a typelist.
 * Example:
 *   typelist<int, string> == take_t<typelist<int, string, bool>, 2>
 *   typelist<bool> == drop_t<typelist<int, string, bool>, 2>
 */
 // namespace detail

/**
 * Like drop, but returns an empty list rather than an assertion error if {@code num}
 * is larger than the size of the TypeList.
 * Example:
 *   typelist<> == drop_if_nonempty_t<typelist<string, bool>, 2>
 *   typelist<> == drop_if_nonempty_t<typelist<int, string, bool>, 3>
 */

/**
 * Reverses a typelist.
 * Example:
 *   typelist<int, string>  == reverse_t<typelist<string, int>>
 */

/**
 * Find the index of the first type in a typelist fulfilling a type trait
 * condition. Example:
 *
 * 2 == find_if<typelist<char, int, char&, int&>, std::is_reference>::value
 */

/**
 * Maps a list of types into a list of values.
 * Examples:
 *   // Example 1
 *   auto sizes =
 *     map_types_to_values<typelist<int64_t, bool, uint32_t>>(
 *       [] (auto t) { return sizeof(decltype(t)::type); }
 *     );
 *   //  sizes  ==  std::tuple<size_t, size_t, size_t>{8, 1, 4}
 *
 *   // Example 2
 *   auto shared_ptrs =
 *     map_types_to_values<typelist<int, double>>(
 *       [] (auto t) { return make_shared<typename decltype(t)::type>(); }
 *     );
 *   // shared_ptrs == std::tuple<shared_ptr<int>, shared_ptr<double>>()
 */
 // namespace detail

 // namespace typelist
 // namespace guts
 // namespace c10


// Parsed from c10/util/bit_cast.h

// #pragma once

// #include <cstring>
// #include <type_traits>

// Implementations of std::bit_cast() from C++ 20.
//
// This is a less sketchy version of reinterpret_cast.
//
// See https://en.cppreference.com/w/cpp/numeric/bit_cast for more
// information as well as the source of our implementations.

 // namespace c10


// Parsed from c10/core/DispatchKeySet.h

// #pragma once
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/llvmMathExtras.h>
// #include <array>
// #include <ostream>
// Targeting ../FunctionalityOffsetAndMask.java



@Namespace("c10") public static native @ByVal @Cast("std::array<c10::FunctionalityOffsetAndMask,c10::num_functionality_keys>*") FunctionalityOffsetAndMask initializeFunctionalityOffsetsAndMasks();

@Namespace("c10") public static native @Cast("const std::array<c10::FunctionalityOffsetAndMask,c10::num_functionality_keys>*") @ByRef FunctionalityOffsetAndMask offsetsAndMasks();
// Targeting ../DispatchKeySet.java



@Namespace("c10") public static native @StdString BytePointer toString(@ByVal DispatchKeySet arg0);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, @ByVal DispatchKeySet arg1);

@Namespace("c10") public static native int getDispatchTableIndexForDispatchKey(DispatchKey k);
@Namespace("c10") public static native int getDispatchTableIndexForDispatchKey(@Cast("c10::DispatchKey") short k);

// Alias key DispatchKey::Autograd maps to
// (autograd_dispatch_keyset x full_backend_mask)
// NB: keys in this set also get associated with CompositeImplicitAutograd
//
// Note [autograd_dispatch_keyset Does Not Include Backend Bits]
// We don't want to include any backend bits (BackendComponent::CPUBit, etc)
// directly in autograd_dispatch_keyset.
// Why? keysets like autograd_dispatch_keyset are commonly used to remove
// autograd keys from a DispatchKeySet throughout the code base. However, you
// are only allowed to remove functionality bits from a keyset, not backend
// bits. See Note [Removing keys from DispatchKeySet Only Affects Functionality
// Keys] for details. To be consistent and avoid confusion, we're explicitly
// setting up autograd_dispatch_keyset to not have any backend bits.
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autocast_dispatch_keyset();

// See Note [TLS Initialization]
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_included_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_excluded_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset_with_ADInplaceOrView();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet python_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet sparse_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet sparse_csr_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet mkldnn_ks();

// backend dispatch keys that map to DispatchKey::AutogradOther
// NB: keys in this set also get associated with CompositeImplicitAutograd
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autogradother_backends();

// The set of dispatch keys that come after autograd
// n.b. this relies on the fact that AutogradOther is currently the lowest
// Autograd key
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_autograd_keyset();

// The set of dispatch keys that come after ADInplaceOrView
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_ADInplaceOrView_keyset();

// The set of dispatch keys that come after Functionalize
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_func_keyset();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet backend_bitset_mask();
// keyset corresponding to functorch keys that have their own dedicated
// TensorImpl subclass.

// This keyset has:
// (1) the functionality bits corresponding to backends (dense, sparse,
// quantized) (2) all of the backend bits set
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet backend_functionality_keys();

// true if t is a backend dispatch key
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(DispatchKey t);
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(@Cast("c10::DispatchKey") short t);

// Resolve alias dispatch key to DispatchKeySet if applicable
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(@Cast("c10::DispatchKey") short t);

// Resolve alias dispatch key to DispatchKeySet if applicable,
// and chek if k is a part of that set
@Namespace("c10") public static native @Cast("bool") boolean runtimeDispatchKeySetHas(DispatchKey t, DispatchKey k);
@Namespace("c10") public static native @Cast("bool") boolean runtimeDispatchKeySetHas(@Cast("c10::DispatchKey") short t, @Cast("c10::DispatchKey") short k);

// Returns a DispatchKeySet of all backend keys mapped to Autograd dispatch key
// t, DispatchKeySet is empty if t is not alias of DispatchKey::Autograd.
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(@Cast("c10::DispatchKey") short t);

// Returns a DispatchKeySet of autograd related keys mapped to backend.
// for a given backend key, use the associated autograd key.
// for non-backend keys, use AutogradOther as a default.
// Note: it's convenient and fast to return a default here rather than (say)
// returning an optional<DispatchKey>, or throwing. But it makes callers
// responsible for either a) enforcing the invariant that only backend keys
// be passed as arguments, or b) interpreting our return value carefully.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(BackendComponent t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(@Cast("c10::BackendComponent") byte t);

// Returns a DispatchKeySet of autocast related keys mapped to backend.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(BackendComponent t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(@Cast("c10::BackendComponent") byte t);

// returns the "backend" DispatchKey of highest priority in the set.
// This is basically like highestBackendKey(), except that we have some
// "functionality" bits that correspond to backends (Sparse, Quantized)
@Namespace("c10") public static native DispatchKey highestPriorityBackendTypeId(@ByVal DispatchKeySet ks);

// This API exists because we have a use case for checking
// getRuntimeDispatchKeySet(alias).has(DispatchKey::Undefined)
// in OperatorEntry.cpp but we disallow it in has() API.
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(DispatchKey k, DispatchKey alias);
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(@Cast("c10::DispatchKey") short k, @Cast("c10::DispatchKey") short alias);

// Historically, every tensor only had a single DispatchKey, and it was always
// something like CPU, and there wasn't any of this business where TLS
// could cause the DispatchKey of a tensor to change.  But we still have some
// legacy code that is still using DispatchKey for things like instanceof
// checks; if at all possible, refactor the code to stop using DispatchKey in
// those cases.
@Namespace("c10") public static native DispatchKey legacyExtractDispatchKey(@ByVal DispatchKeySet s);

// Given a function type, constructs a function_traits type that drops the first
// parameter type if the first parameter is of type DispatchKeySet. NB:
// DispatchKeySet is currently explicitly hidden from JIT (mainly to avoid
// pushing unnecessary arguments on the stack - see Note [ Plumbing Keys Through
// the Dispatcher] for details). If at any point in the future we need to expose
// this type to JIT, revisit the usage of this type alias.
 // namespace c10


// Parsed from c10/core/Backend.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/Exception.h>

// #include <stdexcept>

/**
 * This legacy enum class defines the set of backends supported by old school,
 * code generated Type-based ATen.  A "backend" in this sense roughly
 * corresponds to the cartesian product of (device type, layout), but restricted
 * only to combinations which we actually have kernels for.  Backend does NOT
 * include dtype.
 *
 * The reason we are sunsetting this enum class is because it doesn't allow for
 * open registration; e.g., if you want to add SparseXLA, you'd have to
 * edit this enum; you wouldn't be able to do it out of tree.  DispatchKey is
 * the replacement for Backend which supports open registration.
 *
 * NB: The concept of 'Backend' here disagrees with the notion of backend
 * exposed to users in torch.backends.  Backend here is something like "CPU"
 * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
 * "CUDNN".
 */
@Namespace("c10") public enum Backend {
  CPU(0),
  CUDA(1),
  HIP(2),
  VE(3),
  FPGA(4),
  IPU(5),
  XPU(6),
  SparseCPU(7),
  SparseCUDA(8),
  SparseCsrCPU(9),
  SparseCsrCUDA(10),
  SparseHIP(11),
  SparseVE(12),
  SparseXPU(13),
  SparsePrivateUse1(14),
  ORT(15),
  XLA(16),
  Vulkan(17),
  Metal(18),
  Meta(19),
  QuantizedCPU(20),
  QuantizedCUDA(21),
  QuantizedXPU(22),
  QuantizedPrivateUse1(23),
  Undefined(24),
  MkldnnCPU(25),
  MPS(26),
  HPU(27),
  Lazy(28),
  MTIA(29),
  PrivateUse1(30),
  NumOptions(31);

    public final int value;
    private Backend(int v) { this.value = v; }
    private Backend(Backend e) { this.value = e.value; }
    public Backend intern() { for (Backend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Backend dispatchKeyToBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::Backend") int dispatchKeyToBackend(@Cast("c10::DispatchKey") short t);

@Namespace("c10") public static native DispatchKey backendToDispatchKey(Backend b);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short backendToDispatchKey(@Cast("c10::Backend") int b);

@Namespace("c10") public static native DeviceType backendToDeviceType(Backend b);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte backendToDeviceType(@Cast("c10::Backend") int b);

// TODO: This probably shouldn't actually be static inline
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(Backend b);
@Namespace("c10") public static native String toString(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparse(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparse(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(@Cast("c10::Backend") int b);

 // namespace c10


// Parsed from c10/core/Layout.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/Exception.h>

// #include <ostream>
@Namespace("c10") public enum Layout {
  Strided((byte)(0)),
  Sparse((byte)(1)),
  SparseCsr((byte)(2)),
  Mkldnn((byte)(3)),
  SparseCsc((byte)(4)),
  SparseBsr((byte)(5)),
  SparseBsc((byte)(6)),
  NumOptions((byte)(7));

    public final byte value;
    private Layout(byte v) { this.value = v; }
    private Layout(Layout e) { this.value = e.value; }
    public Layout intern() { for (Layout e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Layout layout_from_backend(Backend backend);
@Namespace("c10") public static native @Cast("c10::Layout") byte layout_from_backend(@Cast("c10::Backend") int backend);

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @ByVal Layout layout);

 // namespace c10


// Parsed from c10/util/AlignOf.h

//===--- AlignOf.h - Portable calculation of type alignment -----*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the AlignedCharArray and AlignedCharArrayUnion classes.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::AlignOf
// replaced LLVM_ALIGNAS with alignas

// #pragma once

// #include <cstddef>

/** \struct AlignedCharArray
 *  \brief Helper for building an aligned character array type.
 * 
 *  This template is used to explicitly build up a collection of aligned
 *  character array types. We have to build these up using a macro and explicit
 *  specialization to cope with MSVC (at least till 2015) where only an
 *  integer literal can be used to specify an alignment constraint. Once built
 *  up here, we can then begin to indirect between these using normal C++
 *  template parameters. */

// MSVC requires special handling here.
// #ifndef _MSC_VER

// #else // _MSC_VER

/** \brief Create a type with an aligned char buffer. */

// We provide special variations of this template for the most common
// alignments because __declspec(align(...)) doesn't actually work when it is
// a member of a by-value function argument in MSVC, even if the alignment
// request is something reasonably like 8-byte or 16-byte. Note that we can't
// even include the declspec with the union that forces the alignment because
// MSVC warns on the existence of the declspec despite the union member forcing
// proper alignment.

// The rest of these are provided with a __declspec(align(...)) and we simply
// can't pass them by-value as function arguments on MSVC.

// #define AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT(x)
//   template <size_t Size>
//   struct AlignedCharArray<x, Size> {
//     __declspec(align(x)) char buffer[Size];
//   };

// #undef AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT

// #endif // _MSC_VER
 // end namespace detail

/** \brief This union template exposes a suitably aligned and sized character
 *  array member which can hold elements of any of up to ten types.
 * 
 *  These types may be arrays, structs, or any other types. The goal is to
 *  expose a char array buffer member which can be used as suitable storage for
 *  a placement new of any of these types. Support for more than ten types can
 *  be added at the cost of more boilerplate. */
 // end namespace c10


// Parsed from c10/util/SmallVector.h

//===- llvm/ADT/SmallVector.h - 'Normally small' vectors --------*- C++ -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the SmallVector class.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::SmallVector.
// used std::is_trivially_{copy,move}_constructible
// replaced iterator_range constructor with inline Container&& constructor
// replaced LLVM_NODISCARD, LLVM_LIKELY, and LLVM_UNLIKELY with c10 equivalents
// removed LLVM_GSL_OWNER
// added SmallVector::at
// added operator<< for std::ostream
// added C10_API to export SmallVectorBase

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/AlignOf.h>

// #include <algorithm>
// #include <cassert>
// #include <cstddef>
// #include <cstdlib>
// #include <cstring>
// #include <functional>
// #include <initializer_list>
// #include <iterator>
// #include <limits>
// #include <memory>
// #include <new>
// #include <ostream>
// #include <type_traits>
// #include <utility>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif
// Targeting ../IntSizedSmallVectorBase.java



/** Figure out the offset of the first element. */
// Targeting ../SymIntSmallVectorCommon.java


// Targeting ../LongSmallVectorCommon.java


// Targeting ../SymIntSmallVectorBase.java


// Targeting ../LongSmallVectorBase.java



// Define this out-of-line to dissuade the C++ compiler from inlining it.


// Define this out-of-line to dissuade the C++ compiler from inlining it.


// Define this out-of-line to dissuade the C++ compiler from inlining it.


/** SmallVectorTemplateBase<TriviallyCopyable = true> - This is where we put
 *  method implementations that are designed to work with trivially copyable
 *  T's. This allows using memcpy in place of copy/move construction and
 *  skipping destruction. */
// Targeting ../SymIntSmallVectorImpl.java


// Targeting ../LongSmallVectorImpl.java









/** Storage for the SmallVector elements.  This is specialized for the N=0 case
 *  to avoid allocating unnecessary storage. */

/** We need the storage to be properly aligned even for small-size of 0 so that
 *  the pointer math in \a SmallVectorTemplateCommon::getFirstEl() is
 *  well-defined. */

/** Forward declaration of SmallVector so that
 *  calculateSmallVectorDefaultInlinedElements can reference
 *  {@code sizeof(SmallVector<T, 0>)}. */

/** Helper class for calculating the default number of inline elements for
 *  {@code SmallVector<T>}.
 * 
 *  This should be migrated to a constexpr function when our minimum
 *  compiler support is enough for multi-statement constexpr functions. */
// Targeting ../SymDimVector.java


// Targeting ../DimVector.java



/** Given a range of type R, iterate the entire range and return a
 *  SmallVector with elements of the vector.  This is useful, for example,
 *  when you want to iterate a range and then sort the results. */

 // end namespace c10

/** Implement std::swap in terms of SmallVector swap. */

/** Implement std::swap in terms of SmallVector swap. */

 // end namespace std



// Parsed from c10/util/ArrayRef.h

//===--- ArrayRef.h - Array Reference Wrapper -------------------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::ArrayRef.
// removed llvm-specific functionality
// removed some implicit const -> non-const conversions that rely on
// complicated std::enable_if meta-programming
// removed a bunch of slice variants for simplicity...

// #pragma once

// #include <c10/util/C++17.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>

// #include <array>
// #include <iterator>
// #include <vector>
// Targeting ../ArgumentArrayRef.java


// Targeting ../ArgumentDefArrayRef.java


// Targeting ../BFloat16ArrayRef.java


// Targeting ../BlockArrayRef.java


// Targeting ../BoolArrayRef.java


// Targeting ../ByteArrayRef.java


// Targeting ../DimnameArrayRef.java


// Targeting ../DoubleArrayRef.java


// Targeting ../DoubleComplexArrayRef.java


// Targeting ../EnumNameValueArrayRef.java


// Targeting ../FloatArrayRef.java


// Targeting ../FloatComplexArrayRef.java


// Targeting ../FuturePtrArrayRef.java


// Targeting ../HalfArrayRef.java


// Targeting ../IValueArrayRef.java


// Targeting ../IntArrayRef.java


// Targeting ../TagArrayRef.java


// Targeting ../LongArrayRef.java


// Targeting ../LongOptionalArrayRef.java


// Targeting ../NamedValueArrayRef.java


// Targeting ../ScalarArrayRef.java


// Targeting ../ScalarTypeArrayRef.java


// Targeting ../ShortArrayRef.java


// Targeting ../SizeTArrayRef.java


// Targeting ../StrideArrayRef.java


// Targeting ../StringArrayRef.java


// Targeting ../SymIntArrayRef.java


// Targeting ../SymNodeArrayRef.java


// Targeting ../SymbolArrayRef.java


// Targeting ../TensorArrayRef.java


// Targeting ../TensorArgArrayRef.java


// Targeting ../TensorIndexArrayRef.java


// Targeting ../TensorOptionalArrayRef.java


// Targeting ../TypeArrayRef.java


// Targeting ../ValueArrayRef.java



/** \name ArrayRef Convenience constructors
 *  \{
 <p>
 *  Construct an ArrayRef from a single element. */

/** Construct an ArrayRef from a pointer and length. */

/** Construct an ArrayRef from a range. */

/** Construct an ArrayRef from a SmallVector. */

/** Construct an ArrayRef from a SmallVector. */

/** Construct an ArrayRef from a std::vector. */

/** Construct an ArrayRef from a std::array. */

/** Construct an ArrayRef from an ArrayRef (no-op) (const) */

/** Construct an ArrayRef from an ArrayRef (no-op) */

/** Construct an ArrayRef from a C array. */

// WARNING: Template instantiation will NOT be willing to do an implicit
// conversions to get you to an c10::ArrayRef, which is why we need so
// many overloads.

// This alias is deprecated because it doesn't make ownership
// semantics obvious.  Use IntArrayRef instead!
 // namespace c10


// Parsed from c10/core/MemoryFormat.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <ostream>

// Memory format is not the property of a Tensor. It is the way to tell an
// operator how the result should be organized in memory and nothing more. That
// means memory format should never be used as return value for any tensor state
// interrogation functions (internally and externally).
//
// Possible options are:
//  Preserve:
//    If any of the input tensors is in channels_last format, operator output
//    should be in channels_last format
//
//  Contiguous:
//    Regardless of input tensors format, the output should be contiguous
//    Tensor.
//
//  ChannelsLast:
//    Regardless of input tensors format, the output should be in channels_last
//    format.
@Namespace("c10") public enum MemoryFormat {
  Contiguous((byte)(0)),
  Preserve((byte)(1)),
  ChannelsLast((byte)(2)),
  ChannelsLast3d((byte)(3)),
  NumOptions((byte)(4));

    public final byte value;
    private MemoryFormat(byte v) { this.value = v; }
    private MemoryFormat(MemoryFormat e) { this.value = e.value; }
    public MemoryFormat intern() { for (MemoryFormat e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// If you are seeing this, it means that this call site was not checked if
// the memory format could be preserved, and it was switched to old default
// behaviour of contiguous
// #define LEGACY_CONTIGUOUS_MEMORY_FORMAT c10::get_contiguous_memory_format()

@Namespace("c10") public static native MemoryFormat get_contiguous_memory_format();

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal MemoryFormat memory_format);

// Note: Hardcoded the channel last stride indices here to get better
// performance

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

// NOTE:
// Below are Helper functions for is_channels_last_strides_xd.
// 1. Please do not combine these helper functions, each helper function handles
// exactly one case of sizes + memory_format, by doing this, the strides indices
// will be a constant array and we can access it using constant index number,
// the compiler will fully unroll the loop on strides indices to gain a better
// performance.
// 2. No error check in helper function, caller ensures the correctness of the
// input
// 3. All helper functions have similar comments, only 1st helper function is
// commented here.

// Note [Ambiguous is_channels_last_strides_xd]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// The flaw of carrying memory_format implicitly through strides is very hard
// to WAR properly. issue #24090
// Without the history of permutation, we can't infer the memory_format of a
// tensor from the snapshot of its size & stride
// e.g.
//
// 1. We can NOT specify the memory_format of N111 tensor through strides in a
//  meaningful way;
//
// 2. Two path that ended up with identical size/stride
//  N11W contiguous tensor sliced at w-dimension becomes [N,1,1,1]@[W,W,W,W]
//  NC11 channels_last tensor sliced at c-dimension becomes [N,1,1,1]@[C,C,C,C]
//    So if we see a tensor [N,1,1,1]@[X,X,X,X], there's no way for us to infer
//    the memory_format of the original tensor.
//
// Due to the limitations, our temporary WAR `is_channels_last_strides` does the
// best effort to infer whether the original memory_format of a tensor is
// at::MemoryFormat::ChannelsLast. The two objectives of this function (ordered
// by their importance):
//   1. Ensure that normal shape manipulation does not accidentally change the
//      MemoryFormat of an existing tensor.
//   2. Allows user to mark MemoryFormat::ChannelsLast to tensors;
//
// The function does so via checking strides of the tensor, including strides of
// size-1 dimensions. Although conventionally PyTorch implies no restriction on
// trivial stride (stride for size-1 dimension).
//
// Note that this approach is a compromise. We did not solve the problem
// completely. Many cases we will not be able to infer the correct memory
// format.
// The implementation of `is_channels_last_strides` is to serve the objectives:
// MemoryFormat::ChannelsLast has to be explicitly opted-in (no accidental
// conversion); Best effort to maintain the ChannelsLast flag.
//
// Due to the fact that this is not a bulletproof solution, through testing
// (aten/src/ATen/test/memory_format_test.cpp)
//   a. we ensure that the common tasks are supported;
//   a. we identify corner cases where the implementation compromises on.
//
// By the time accumulated permutation is enabled to replace implicit
// memory_format through strides, we should be updating our tests and fix the
// issues in our tests.
//
// We use Channels Last 2d as an example above.
// This is a general problem for all the is_channels_last_strides_xd
// implementation. Please check the helper functions
// (is_channels_last_strides_*d_s*) for more details.

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @Const @ByVal LongArrayRef sizes,
    @Const @ByVal LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @Const @ByVal LongArrayRef sizes,
    @Const @ByVal LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... strides);

 // namespace c10


// Parsed from c10/core/QScheme.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/util/Exception.h>

/**
 * QScheme is an enum that specifies the type of quantization. This has a one
 * to one correspondence with Quantizer
 * Please refer to ATen/quantized/Quantizer.h to see the Quantizers classes.
 * Keep this file in sync with torch/nn/_qscheme.py
 */
@Namespace("c10") public enum QScheme {
  PER_TENSOR_AFFINE((byte)(0)),
  PER_CHANNEL_AFFINE((byte)(1)),
  PER_TENSOR_SYMMETRIC((byte)(2)),
  PER_CHANNEL_SYMMETRIC((byte)(3)),
  PER_CHANNEL_AFFINE_FLOAT_QPARAMS((byte)(4)),
  COMPILE_TIME_NUM_QSCHEMES((byte)(5));

    public final byte value;
    private QScheme(byte v) { this.value = v; }
    private QScheme(QScheme e) { this.value = e.value; }
    public QScheme intern() { for (QScheme e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_NUM_QSCHEMES();

@Namespace("c10") public static native @StdString BytePointer toString(QScheme qscheme);

 // namespace c10


// Parsed from c10/core/Stream.h

// #pragma once

// #include <c10/core/Device.h>

/** An index representing a specific stream.  A StreamId is not independently
 *  meaningful without knowing the Device it is associated with; try to
 *  use Stream rather than StreamId directly.
 * 
 *  StreamIds are opaque; they are assigned by some DeviceType-specific
 *  numbering system which is not visible to the user.  HOWEVER, we
 *  guarantee that StreamId 0 is always a valid stream, and corresponds
 *  to some sort of "default" stream. */
// Targeting ../StreamData3.java


// Targeting ../Stream.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Stream s);

 // namespace c10
 // namespace std


// Parsed from c10/core/OptionalRef.h

// #pragma once

 // namespace c10


// Parsed from c10/util/BFloat16.h

// #pragma once

// Defines the bloat16 type (brain floating-point). This representation uses
// 1 bit for the sign, 8 bits for the exponent and 7 bits for the mantissa.

// #include <c10/macros/Macros.h>
// #include <cmath>
// #include <cstring>

// #if defined(__CUDACC__) && !defined(USE_ROCM)
// #endif

// #if defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
// #endif
@Namespace("c10::detail") public static native float f32_from_bits(@Cast("uint16_t") short src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short bits_from_f32(float src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short round_to_nearest_even(float src);

// Targeting ../BFloat16.java



 // namespace c10

// #include <c10/util/BFloat16-inl.h> // IWYU pragma: keep


// Parsed from c10/util/BFloat16-inl.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/bit_cast.h>

// #include <limits>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

// #if defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
// #endif

/** Constructors */


/** Implicit conversions */


// #if defined(__CUDACC__) && !defined(USE_ROCM)
// #endif

// #if defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
// #endif

// CUDA intrinsics

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

/** Arithmetic */

@Namespace("c10") public static native @ByVal @Name("operator +") BFloat16 add(@Const @ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(@Const @ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByVal @Name("operator *") BFloat16 multiply(@Const @ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByVal @Name("operator /") BFloat16 divide(@Const @ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(@Const @ByRef BFloat16 a);

@Namespace("c10") public static native @ByRef @Name("operator +=") BFloat16 addPut(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator -=") BFloat16 subtractPut(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator *=") BFloat16 multiplyPut(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator /=") BFloat16 dividePut(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator |") BFloat16 or(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator ^") BFloat16 xor(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator &") BFloat16 and(@ByRef BFloat16 a, @Const @ByRef BFloat16 b);

/** Arithmetic with floats */

@Namespace("c10") public static native @Name("operator +") float add(@ByVal BFloat16 a, float b);
@Namespace("c10") public static native @Name("operator -") float subtract(@ByVal BFloat16 a, float b);
@Namespace("c10") public static native @Name("operator *") float multiply(@ByVal BFloat16 a, float b);
@Namespace("c10") public static native @Name("operator /") float divide(@ByVal BFloat16 a, float b);

@Namespace("c10") public static native @Name("operator +") float add(float a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator -") float subtract(float a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator *") float multiply(float a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator /") float divide(float a, @ByVal BFloat16 b);

@Namespace("c10") public static native @ByRef @Name("operator +=") FloatPointer addPut(@ByRef FloatPointer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator +=") FloatBuffer addPut(@ByRef FloatBuffer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator +=") float[] addPut(@ByRef float[] a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatPointer subtractPut(@ByRef FloatPointer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatBuffer subtractPut(@ByRef FloatBuffer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") float[] subtractPut(@ByRef float[] a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatPointer multiplyPut(@ByRef FloatPointer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatBuffer multiplyPut(@ByRef FloatBuffer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") float[] multiplyPut(@ByRef float[] a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatPointer dividePut(@ByRef FloatPointer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatBuffer dividePut(@ByRef FloatBuffer a, @Const @ByRef BFloat16 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") float[] dividePut(@ByRef float[] a, @Const @ByRef BFloat16 b);

/** Arithmetic with doubles */

@Namespace("c10") public static native @Name("operator +") double add(@ByVal BFloat16 a, double b);
@Namespace("c10") public static native @Name("operator -") double subtract(@ByVal BFloat16 a, double b);
@Namespace("c10") public static native @Name("operator *") double multiply(@ByVal BFloat16 a, double b);
@Namespace("c10") public static native @Name("operator /") double divide(@ByVal BFloat16 a, double b);

@Namespace("c10") public static native @Name("operator +") double add(double a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator -") double subtract(double a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator *") double multiply(double a, @ByVal BFloat16 b);
@Namespace("c10") public static native @Name("operator /") double divide(double a, @ByVal BFloat16 b);

/** Arithmetic with ints */

@Namespace("c10") public static native @ByVal @Name("operator +") BFloat16 add(@ByVal BFloat16 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(@ByVal BFloat16 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator *") BFloat16 multiply(@ByVal BFloat16 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator /") BFloat16 divide(@ByVal BFloat16 a, int b);

@Namespace("c10") public static native @ByVal @Name("operator +") BFloat16 add(int a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(int a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator *") BFloat16 multiply(int a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator /") BFloat16 divide(int a, @ByVal BFloat16 b);

//// Arithmetic with int64_t

@Namespace("c10") public static native @ByVal @Name("operator +") BFloat16 add(@ByVal BFloat16 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(@ByVal BFloat16 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator *") BFloat16 multiply(@ByVal BFloat16 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator /") BFloat16 divide(@ByVal BFloat16 a, @Cast("int64_t") long b);

@Namespace("c10") public static native @ByVal @Name("operator +") BFloat16 add(@Cast("int64_t") long a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator -") BFloat16 subtract(@Cast("int64_t") long a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator *") BFloat16 multiply(@Cast("int64_t") long a, @ByVal BFloat16 b);
@Namespace("c10") public static native @ByVal @Name("operator /") BFloat16 divide(@Cast("int64_t") long a, @ByVal BFloat16 b);

// Overloading < and > operators, because std::max and std::min use them.

@Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@ByRef BFloat16 lhs, @ByRef BFloat16 rhs);

@Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@ByRef BFloat16 lhs, @ByRef BFloat16 rhs);

 // namespace c10

 // namespace std



// Parsed from c10/util/TypeSafeSignMath.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <limits>
// #include <type_traits>

// #if C10_CLANG_HAS_WARNING("-Wstring-conversion")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

/** Returns false since we cannot have x < 0 if x is unsigned. */

/** Returns true if a signed variable x < 0 */

/** Returns true if x < 0
 *  NOTE: Will fail on an unsigned custom type
 *        For the most part it's possible to fix this if
 *        the custom type has a constexpr constructor.
 *        However, notably, c10::Half does not :-( */

/** Returns the sign of an unsigned variable x as 0, 1 */

/** Returns the sign of a signed variable x as -1, 0, 1 */

/** Returns the sign of x as -1, 0, 1
 *  NOTE: Will fail on an unsigned custom type
 *        For the most part it's possible to fix this if
 *        the custom type has a constexpr constructor.
 *        However, notably, c10::Half does not :-( */

/** Returns true if a and b are not both negative */

// Suppress sign compare warning when compiling with GCC
// as later does not account for short-circuit rule before
// raising the warning, see https://godbolt.org/z/Tr3Msnz99
// #ifdef __GNUC__
// #pragma GCC diagnostic push
// #pragma GCC diagnostic ignored "-Wsign-compare"
// #endif

/** Returns true if x is greater than the greatest value of the type Limit */

// #ifdef __GNUC__
// #pragma GCC diagnostic pop
// #endif

/** Returns true if x < lowest(Limit). Standard comparison */

/** Returns false since all the limit is signed and therefore includes
 *  negative values but x cannot be negative because it is unsigned */

/** Returns true if x < 0, where 0 is constructed from T.
 *  Limit is not signed, so its lower value is zero */

/** Returns false sign both types are unsigned */

/** Returns true if x is less than the lowest value of type T
 *  NOTE: Will fail on an unsigned custom type
 *        For the most part it's possible to fix this if
 *        the custom type has a constexpr constructor.
 *        However, notably, c10::Half does not : */

 // namespace c10



// Parsed from c10/util/floating_point_utils.h

// #pragma once

// #include <cstdint>

@Namespace("c10::detail") public static native float fp32_from_bits(@Cast("uint32_t") int w);

@Namespace("c10::detail") public static native @Cast("uint32_t") int fp32_to_bits(float f);

 // namespace c10::detail


// Parsed from c10/util/Float8_e4m3fn-inl.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <cstring>
// #include <limits>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

/** Constructors */



/** Implicit conversions */



/** Special values helper */



/** Arithmetic */

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e4m3fn add(@Const @ByRef Float8_e4m3fn a, @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(@Const @ByRef Float8_e4m3fn a, @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e4m3fn multiply(@Const @ByRef Float8_e4m3fn a, @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e4m3fn divide(
    @Const @ByRef Float8_e4m3fn a,
    @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(@Const @ByRef Float8_e4m3fn a);

@Namespace("c10") public static native @ByRef @Name("operator +=") Float8_e4m3fn addPut(
    @ByRef Float8_e4m3fn a,
    @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByRef @Name("operator -=") Float8_e4m3fn subtractPut(
    @ByRef Float8_e4m3fn a,
    @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByRef @Name("operator *=") Float8_e4m3fn multiplyPut(
    @ByRef Float8_e4m3fn a,
    @Const @ByRef Float8_e4m3fn b);

@Namespace("c10") public static native @ByRef @Name("operator /=") Float8_e4m3fn dividePut(
    @ByRef Float8_e4m3fn a,
    @Const @ByRef Float8_e4m3fn b);

/** Arithmetic with floats */

@Namespace("c10") public static native @Name("operator +") float add(@ByVal Float8_e4m3fn a, float b);
@Namespace("c10") public static native @Name("operator -") float subtract(@ByVal Float8_e4m3fn a, float b);
@Namespace("c10") public static native @Name("operator *") float multiply(@ByVal Float8_e4m3fn a, float b);
@Namespace("c10") public static native @Name("operator /") float divide(@ByVal Float8_e4m3fn a, float b);

@Namespace("c10") public static native @Name("operator +") float add(float a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator -") float subtract(float a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator *") float multiply(float a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator /") float divide(float a, @ByVal Float8_e4m3fn b);

@Namespace("c10") public static native @ByRef @Name("operator +=") FloatPointer addPut(@ByRef FloatPointer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator +=") FloatBuffer addPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator +=") float[] addPut(@ByRef float[] a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatPointer subtractPut(@ByRef FloatPointer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatBuffer subtractPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator -=") float[] subtractPut(@ByRef float[] a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatPointer multiplyPut(@ByRef FloatPointer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatBuffer multiplyPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator *=") float[] multiplyPut(@ByRef float[] a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatPointer dividePut(@ByRef FloatPointer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatBuffer dividePut(@ByRef FloatBuffer a, @Const @ByRef Float8_e4m3fn b);
@Namespace("c10") public static native @ByRef @Name("operator /=") float[] dividePut(@ByRef float[] a, @Const @ByRef Float8_e4m3fn b);

/** Arithmetic with doubles */

@Namespace("c10") public static native @Name("operator +") double add(@ByVal Float8_e4m3fn a, double b);
@Namespace("c10") public static native @Name("operator -") double subtract(@ByVal Float8_e4m3fn a, double b);
@Namespace("c10") public static native @Name("operator *") double multiply(@ByVal Float8_e4m3fn a, double b);
@Namespace("c10") public static native @Name("operator /") double divide(@ByVal Float8_e4m3fn a, double b);

@Namespace("c10") public static native @Name("operator +") double add(double a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator -") double subtract(double a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator *") double multiply(double a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @Name("operator /") double divide(double a, @ByVal Float8_e4m3fn b);

/** Arithmetic with ints */

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e4m3fn add(@ByVal Float8_e4m3fn a, int b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(@ByVal Float8_e4m3fn a, int b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e4m3fn multiply(@ByVal Float8_e4m3fn a, int b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e4m3fn divide(@ByVal Float8_e4m3fn a, int b);

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e4m3fn add(int a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(int a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e4m3fn multiply(int a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e4m3fn divide(int a, @ByVal Float8_e4m3fn b);

//// Arithmetic with int64_t

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e4m3fn add(@ByVal Float8_e4m3fn a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(@ByVal Float8_e4m3fn a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e4m3fn multiply(@ByVal Float8_e4m3fn a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e4m3fn divide(@ByVal Float8_e4m3fn a, @Cast("int64_t") long b);

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e4m3fn add(@Cast("int64_t") long a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e4m3fn subtract(@Cast("int64_t") long a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e4m3fn multiply(@Cast("int64_t") long a, @ByVal Float8_e4m3fn b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e4m3fn divide(@Cast("int64_t") long a, @ByVal Float8_e4m3fn b);

/** NOTE: we do not define comparisons directly and instead rely on the implicit
 *  conversion from c10::Float8_e4m3fn to float. */

 // namespace c10

 // namespace std



// Parsed from c10/util/Float8_e4m3fn.h


///
// #pragma once

/** Defines the Float8_e4m3fn type (8-bit floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32.
 *  Binary configuration:
 *  s eeee mmm
 *  1 sign bit
 *  4 exponent bits
 *  3 mantissa bits
 *  bias = 7
 * 
 *  Implementation based on the paper https://arxiv.org/pdf/2209.05433.pdf
 *  and inspired by Half implementation from pytorch/c10/util/Half.h */

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/TypeSafeSignMath.h>
// #include <c10/util/floating_point_utils.h>
// #include <type_traits>

// #if defined(__cplusplus) && (__cplusplus >= 201103L)
// #include <cmath>
// #include <cstdint>
// #elif !defined(__OPENCL_VERSION__)
// #include <math.h>
// #include <stdint.h>
// #endif

// #ifdef _MSC_VER
// #include <intrin.h>
// #endif

// #include <climits>
// #include <cstdint>
// #include <cstring>
// #include <iosfwd>
// #include <limits>
// #include <sstream>
// #include <stdexcept>
// #include <string>
// #include <utility>

// #include <typeinfo> // operator typeid

/*
 * Convert a 8-bit floating-point number in fp8 E4M3FN format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 */
@Namespace("c10::detail") public static native float fp8e4m3fn_to_fp32_value(@Cast("uint8_t") byte input);

/*
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 8-bit floating-point number in fp8 E4M3FN format, in bit representation.
 */
@Namespace("c10::detail") public static native @Cast("uint8_t") byte fp8e4m3fn_from_fp32_value(float f);


// Targeting ../Float8_e4m3fn.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Float8_e4m3fn value);

 // namespace c10

// #include <c10/util/Float8_e4m3fn-inl.h> // IWYU pragma: keep


// Parsed from c10/util/complex_math.h

// #if !defined(C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H)
// #error
//     "c10/util/complex_math.h is not meant to be individually included. Include c10/util/complex.h instead."
// #endif

// Exponential functions

@Namespace("c10_complex_math") public static native @ByVal @Name("exp<float>") FloatComplex exp(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("exp<double>") DoubleComplex exp(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log<float>") FloatComplex log(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log<double>") DoubleComplex log(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log10<float>") FloatComplex log10(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log10<double>") DoubleComplex log10(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log2<float>") FloatComplex log2(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log2<double>") DoubleComplex log2(@Const @ByRef DoubleComplex x);

// Power functions
//
// #if defined(_LIBCPP_VERSION) ||
//     (defined(__GLIBCXX__) && !defined(_GLIBCXX11_USE_C99_COMPLEX))




 // namespace _detail
// #endif

@Namespace("c10_complex_math") public static native @ByVal @Name("sqrt<float>") FloatComplex sqrt(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("sqrt<double>") DoubleComplex sqrt(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float>") FloatComplex pow(
    @Const @ByRef FloatComplex x,
    @Const @ByRef FloatComplex y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double>") DoubleComplex pow(
    @Const @ByRef DoubleComplex x,
    @Const @ByRef DoubleComplex y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float>") FloatComplex pow(
    @Const @ByRef FloatComplex x,
    float y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double>") DoubleComplex pow(
    @Const @ByRef DoubleComplex x,
    double y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float>") FloatComplex pow(
    float x,
    @Const @ByRef FloatComplex y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double>") DoubleComplex pow(
    double x,
    @Const @ByRef DoubleComplex y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double,float>") DoubleComplex pow(@Const @ByRef DoubleComplex x, @Const @ByRef FloatComplex y);
@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float,double>") DoubleComplex pow(@Const @ByRef FloatComplex x, @Const @ByRef DoubleComplex y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double,float>") DoubleComplex pow(@Const @ByRef DoubleComplex x, @Const @ByRef float y);
@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float,double>") DoubleComplex pow(@Const @ByRef FloatComplex x, @Const @ByRef double y);

@Namespace("c10_complex_math") public static native @ByVal @Name("pow<double,float>") DoubleComplex pow(@Const @ByRef double x, @Const @ByRef FloatComplex y);
@Namespace("c10_complex_math") public static native @ByVal @Name("pow<float,double>") DoubleComplex pow(@Const @ByRef float x, @Const @ByRef DoubleComplex y);

// Trigonometric functions

@Namespace("c10_complex_math") public static native @ByVal @Name("sin<float>") FloatComplex sin(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("sin<double>") DoubleComplex sin(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("cos<float>") FloatComplex cos(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("cos<double>") DoubleComplex cos(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("tan<float>") FloatComplex tan(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("tan<double>") DoubleComplex tan(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("asin<float>") FloatComplex asin(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("asin<double>") DoubleComplex asin(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("acos<float>") FloatComplex acos(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("acos<double>") DoubleComplex acos(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("atan<float>") FloatComplex atan(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("atan<double>") DoubleComplex atan(@Const @ByRef DoubleComplex x);

// Hyperbolic functions

@Namespace("c10_complex_math") public static native @ByVal @Name("sinh<float>") FloatComplex sinh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("sinh<double>") DoubleComplex sinh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("cosh<float>") FloatComplex cosh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("cosh<double>") DoubleComplex cosh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("tanh<float>") FloatComplex tanh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("tanh<double>") DoubleComplex tanh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("asinh<float>") FloatComplex asinh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("asinh<double>") DoubleComplex asinh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("acosh<float>") FloatComplex acosh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("acosh<double>") DoubleComplex acosh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("atanh<float>") FloatComplex atanh(@Const @ByRef FloatComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("atanh<double>") DoubleComplex atanh(@Const @ByRef DoubleComplex x);

@Namespace("c10_complex_math") public static native @ByVal @Name("log1p<float>") FloatComplex log1p(@Const @ByRef FloatComplex z);

@Namespace("c10_complex_math") public static native @ByVal @Name("log1p<double>") DoubleComplex log1p(@Const @ByRef DoubleComplex z);

 // namespace c10_complex_math

 // namespace std


// Parsed from c10/util/complex_utils.h

// #if !defined(C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H)
// #error
//     "c10/util/complex_utils.h is not meant to be individually included. Include c10/util/complex.h instead."
// #endif

// #include <limits>

// Extract double from std::complex<double>; is identity otherwise
// TODO: Write in more idiomatic C++17

 // namespace c10

 // namespace std


// Parsed from c10/util/complex.h

// #pragma once

// #include <complex>

// #include <c10/macros/Macros.h>

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// #if C10_CLANG_HAS_WARNING("-Wimplicit-float-conversion")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wfloat-conversion")
// #endif
// Targeting ../DoubleComplex.java


// Targeting ../FloatComplex.java


// Targeting ../HalfComplex.java











 // namespace complex_literals

// Define operators between integral scalars and c10::complex. std::complex does
// not support this when T is a floating-point number. This is useful because it
// saves a lot of "static_cast" when operate a complex and an integer. This
// makes the code both less verbose and potentially more efficient.
// #define COMPLEX_INTEGER_OP_TEMPLATE_CONDITION
//   typename std::enable_if_t<
//       std::is_floating_point<fT>::value && std::is_integral<iT>::value,
//       int> = 0

// #undef COMPLEX_INTEGER_OP_TEMPLATE_CONDITION

 // namespace c10

// std functions
//
// The implementation of these functions also follow the design of C++20

// #if defined(USE_ROCM)
// #else
// #define ROCm_Bug(x) x
// #endif

// #undef ROCm_Bug

// For std::conj, there are other versions of it:
//   constexpr std::complex<float> conj( float z );
//   template< class DoubleOrInteger >
//   constexpr std::complex<double> conj( DoubleOrInteger z );
//   constexpr std::complex<long double> conj( long double z );
// These are not implemented
// TODO(@zasdfgbnm): implement them as c10::conj

// Thrust does not have complex --> complex version of thrust::proj,
// so this function is not implemented at c10 right now.
// TODO(@zasdfgbnm): implement it by ourselves

// There is no c10 version of std::polar, because std::polar always
// returns std::complex. Use c10::polar instead;

 // namespace std

 // namespace c10

// #define C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H
// math functions are included in a separate file
// #include <c10/util/complex_math.h> // IWYU pragma: keep
// utilities for complex types
// #include <c10/util/complex_utils.h> // IWYU pragma: keep
// #undef C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H


// Parsed from c10/util/Half-inl.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/bit_cast.h>

// #include <cstring>
// #include <limits>

// #ifdef __CUDACC__
// #include <cuda_fp16.h>
// #endif

// #ifdef __HIPCC__
// #include <hip/hip_fp16.h>
// #endif

// #if defined(CL_SYCL_LANGUAGE_VERSION)
// #include <CL/sycl.hpp> // for SYCL 1.2.1
// #elif defined(SYCL_LANGUAGE_VERSION)
// #include <sycl/sycl.hpp> // for SYCL 2020
// #endif

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

/** Constructors */



/** Implicit conversions */



// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// #ifdef SYCL_LANGUAGE_VERSION
// #endif

// CUDA intrinsics

// #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)) ||
//     (defined(__clang__) && defined(__CUDA__))

// #endif

/** Arithmetic */

@Namespace("c10") public static native @ByVal @Name("operator +") Half add(@Const @ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(@Const @ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByVal @Name("operator *") Half multiply(@Const @ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByVal @Name("operator /") Half divide(@Const @ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(@Const @ByRef Half a);

@Namespace("c10") public static native @ByRef @Name("operator +=") Half addPut(@ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByRef @Name("operator -=") Half subtractPut(@ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByRef @Name("operator *=") Half multiplyPut(@ByRef Half a, @Const @ByRef Half b);

@Namespace("c10") public static native @ByRef @Name("operator /=") Half dividePut(@ByRef Half a, @Const @ByRef Half b);

/** Arithmetic with floats */

@Namespace("c10") public static native @Name("operator +") float add(@ByVal Half a, float b);
@Namespace("c10") public static native @Name("operator -") float subtract(@ByVal Half a, float b);
@Namespace("c10") public static native @Name("operator *") float multiply(@ByVal Half a, float b);
@Namespace("c10") public static native @Name("operator /") float divide(@ByVal Half a, float b);

@Namespace("c10") public static native @Name("operator +") float add(float a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator -") float subtract(float a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator *") float multiply(float a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator /") float divide(float a, @ByVal Half b);

@Namespace("c10") public static native @ByRef @Name("operator +=") FloatPointer addPut(@ByRef FloatPointer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator +=") FloatBuffer addPut(@ByRef FloatBuffer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator +=") float[] addPut(@ByRef float[] a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatPointer subtractPut(@ByRef FloatPointer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatBuffer subtractPut(@ByRef FloatBuffer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator -=") float[] subtractPut(@ByRef float[] a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatPointer multiplyPut(@ByRef FloatPointer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatBuffer multiplyPut(@ByRef FloatBuffer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator *=") float[] multiplyPut(@ByRef float[] a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatPointer dividePut(@ByRef FloatPointer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatBuffer dividePut(@ByRef FloatBuffer a, @Const @ByRef Half b);
@Namespace("c10") public static native @ByRef @Name("operator /=") float[] dividePut(@ByRef float[] a, @Const @ByRef Half b);

/** Arithmetic with doubles */

@Namespace("c10") public static native @Name("operator +") double add(@ByVal Half a, double b);
@Namespace("c10") public static native @Name("operator -") double subtract(@ByVal Half a, double b);
@Namespace("c10") public static native @Name("operator *") double multiply(@ByVal Half a, double b);
@Namespace("c10") public static native @Name("operator /") double divide(@ByVal Half a, double b);

@Namespace("c10") public static native @Name("operator +") double add(double a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator -") double subtract(double a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator *") double multiply(double a, @ByVal Half b);
@Namespace("c10") public static native @Name("operator /") double divide(double a, @ByVal Half b);

/** Arithmetic with ints */

@Namespace("c10") public static native @ByVal @Name("operator +") Half add(@ByVal Half a, int b);
@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(@ByVal Half a, int b);
@Namespace("c10") public static native @ByVal @Name("operator *") Half multiply(@ByVal Half a, int b);
@Namespace("c10") public static native @ByVal @Name("operator /") Half divide(@ByVal Half a, int b);

@Namespace("c10") public static native @ByVal @Name("operator +") Half add(int a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(int a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator *") Half multiply(int a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator /") Half divide(int a, @ByVal Half b);

//// Arithmetic with int64_t

@Namespace("c10") public static native @ByVal @Name("operator +") Half add(@ByVal Half a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(@ByVal Half a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator *") Half multiply(@ByVal Half a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator /") Half divide(@ByVal Half a, @Cast("int64_t") long b);

@Namespace("c10") public static native @ByVal @Name("operator +") Half add(@Cast("int64_t") long a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator -") Half subtract(@Cast("int64_t") long a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator *") Half multiply(@Cast("int64_t") long a, @ByVal Half b);
@Namespace("c10") public static native @ByVal @Name("operator /") Half divide(@Cast("int64_t") long a, @ByVal Half b);

/** NOTE: we do not define comparisons directly and instead rely on the implicit
 *  conversion from c10::Half to float. */

 // namespace c10

 // namespace std



// Parsed from c10/util/Half.h

// #pragma once

/** Defines the Half type (half-precision floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32, instead of using CUDA half intrinsics.
 *  Most uses of this type within ATen are memory bound, including the
 *  element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
 *  If you are writing a compute bound kernel, you can use the CUDA half
 *  intrinsics directly on the Half type from device code. */

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/TypeSafeSignMath.h>
// #include <c10/util/complex.h>
// #include <c10/util/floating_point_utils.h>
// #include <type_traits>

// #if defined(__cplusplus) && (__cplusplus >= 201103L)
// #include <cmath>
// #include <cstdint>
// #elif !defined(__OPENCL_VERSION__)
// #include <math.h>
// #include <stdint.h>
// #endif

// #ifdef _MSC_VER
// #include <intrin.h>
// #endif

// #include <complex>
// #include <cstdint>
// #include <cstring>
// #include <iosfwd>
// #include <limits>
// #include <sstream>
// #include <stdexcept>
// #include <string>
// #include <utility>

// #ifdef __CUDACC__
// #include <cuda_fp16.h>
// #endif

// #ifdef __HIPCC__
// #include <hip/hip_fp16.h>
// #endif

// #if defined(CL_SYCL_LANGUAGE_VERSION)
// #include <CL/sycl.hpp> // for SYCL 1.2.1
// #elif defined(SYCL_LANGUAGE_VERSION)
// #include <sycl/sycl.hpp> // for SYCL 2020
// #endif

// #include <typeinfo> // operator typeid

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 */
@Namespace("c10::detail") public static native @Cast("uint32_t") int fp16_ieee_to_fp32_bits(@Cast("uint16_t") short h);

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native float fp16_ieee_to_fp32_value(@Cast("uint16_t") short h);

/*
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 16-bit floating-point number in IEEE half-precision format, in bit
 * representation.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native @Cast("uint16_t") short fp16_ieee_from_fp32_value(float f);


// Targeting ../Half.java



// TODO : move to complex.h

// In some versions of MSVC, there will be a compiler error when building.
// C4146: unary minus operator applied to unsigned type, result still unsigned
// C4804: unsafe use of type 'bool' in operation
// It can be addressed by disabling the following warning.
// #ifdef _MSC_VER
// #pragma warning(push)
// #pragma warning(disable : 4146)
// #pragma warning(disable : 4804)
// #pragma warning(disable : 4018)
// #endif

// The overflow checks may involve float to int conversion which may
// trigger precision loss warning. Re-enable the warning once the code
// is fixed. See T58053069.
// #if C10_CLANG_HAS_WARNING("-Wimplicit-float-conversion")
// #endif

// bool can be converted to any type.
// Without specializing on bool, in pytorch_linux_trusty_py2_7_9_build:
// `error: comparison of constant '255' with boolean expression is always false`
// for `f > limit::max()` below

// skip isnan and isinf check for integral types

// #ifdef _MSC_VER
// #pragma warning(pop)
// #endif

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Half value);

 // namespace c10

// #include <c10/util/Half-inl.h> // IWYU pragma: keep


// Parsed from c10/util/Float8_e5m2-inl.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <cstring>
// #include <limits>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

public static final int EXP_WIDTH_FP8 = 5;
public static final int MAN_WIDTH_FP8 = 2;
public static final int EXP_BIAS_FP8 = 15;

/** Constructors */



/** Implicit conversions */



/** Special values helpers */





/** Arithmetic */

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e5m2 add(@Const @ByRef Float8_e5m2 a, @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(@Const @ByRef Float8_e5m2 a, @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e5m2 multiply(@Const @ByRef Float8_e5m2 a, @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e5m2 divide(
    @Const @ByRef Float8_e5m2 a,
    @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(@Const @ByRef Float8_e5m2 a);

@Namespace("c10") public static native @ByRef @Name("operator +=") Float8_e5m2 addPut(
    @ByRef Float8_e5m2 a,
    @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByRef @Name("operator -=") Float8_e5m2 subtractPut(
    @ByRef Float8_e5m2 a,
    @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByRef @Name("operator *=") Float8_e5m2 multiplyPut(
    @ByRef Float8_e5m2 a,
    @Const @ByRef Float8_e5m2 b);

@Namespace("c10") public static native @ByRef @Name("operator /=") Float8_e5m2 dividePut(
    @ByRef Float8_e5m2 a,
    @Const @ByRef Float8_e5m2 b);

/** Arithmetic with floats */

@Namespace("c10") public static native @Name("operator +") float add(@ByVal Float8_e5m2 a, float b);
@Namespace("c10") public static native @Name("operator -") float subtract(@ByVal Float8_e5m2 a, float b);
@Namespace("c10") public static native @Name("operator *") float multiply(@ByVal Float8_e5m2 a, float b);
@Namespace("c10") public static native @Name("operator /") float divide(@ByVal Float8_e5m2 a, float b);

@Namespace("c10") public static native @Name("operator +") float add(float a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator -") float subtract(float a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator *") float multiply(float a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator /") float divide(float a, @ByVal Float8_e5m2 b);

@Namespace("c10") public static native @ByRef @Name("operator +=") FloatPointer addPut(@ByRef FloatPointer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator +=") FloatBuffer addPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator +=") float[] addPut(@ByRef float[] a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatPointer subtractPut(@ByRef FloatPointer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") FloatBuffer subtractPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator -=") float[] subtractPut(@ByRef float[] a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatPointer multiplyPut(@ByRef FloatPointer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") FloatBuffer multiplyPut(@ByRef FloatBuffer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator *=") float[] multiplyPut(@ByRef float[] a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatPointer dividePut(@ByRef FloatPointer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") FloatBuffer dividePut(@ByRef FloatBuffer a, @Const @ByRef Float8_e5m2 b);
@Namespace("c10") public static native @ByRef @Name("operator /=") float[] dividePut(@ByRef float[] a, @Const @ByRef Float8_e5m2 b);

/** Arithmetic with doubles */

@Namespace("c10") public static native @Name("operator +") double add(@ByVal Float8_e5m2 a, double b);
@Namespace("c10") public static native @Name("operator -") double subtract(@ByVal Float8_e5m2 a, double b);
@Namespace("c10") public static native @Name("operator *") double multiply(@ByVal Float8_e5m2 a, double b);
@Namespace("c10") public static native @Name("operator /") double divide(@ByVal Float8_e5m2 a, double b);

@Namespace("c10") public static native @Name("operator +") double add(double a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator -") double subtract(double a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator *") double multiply(double a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @Name("operator /") double divide(double a, @ByVal Float8_e5m2 b);

/** Arithmetic with ints */

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e5m2 add(@ByVal Float8_e5m2 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(@ByVal Float8_e5m2 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e5m2 multiply(@ByVal Float8_e5m2 a, int b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e5m2 divide(@ByVal Float8_e5m2 a, int b);

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e5m2 add(int a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(int a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e5m2 multiply(int a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e5m2 divide(int a, @ByVal Float8_e5m2 b);

//// Arithmetic with int64_t

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e5m2 add(@ByVal Float8_e5m2 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(@ByVal Float8_e5m2 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e5m2 multiply(@ByVal Float8_e5m2 a, @Cast("int64_t") long b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e5m2 divide(@ByVal Float8_e5m2 a, @Cast("int64_t") long b);

@Namespace("c10") public static native @ByVal @Name("operator +") Float8_e5m2 add(@Cast("int64_t") long a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator -") Float8_e5m2 subtract(@Cast("int64_t") long a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator *") Float8_e5m2 multiply(@Cast("int64_t") long a, @ByVal Float8_e5m2 b);
@Namespace("c10") public static native @ByVal @Name("operator /") Float8_e5m2 divide(@Cast("int64_t") long a, @ByVal Float8_e5m2 b);

/** NOTE: we do not define comparisons directly and instead rely on the implicit
 *  conversion from c10::Float8_e5m2 to float. */

 // namespace c10

 // namespace std



// Parsed from c10/util/Float8_e5m2.h


///
// #pragma once

/** Defines the Float8_e5m2 type (8-bit floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32.
 *  Binary configuration:
 *  s eeeee mm
 *  1 sign bit
 *  5 exponent bits
 *  2 mantissa bits
 *  bias = 15
 * 
 *  Implementation based on the paper https://arxiv.org/pdf/2209.05433.pdf
 *  and inspired by Half implementation from pytorch/c10/util/Half.h */

// #include <c10/util/Half.h>

/*
 * Convert a 8-bit floating-point number in fp8 E5M2 format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 */
@Namespace("c10::detail") public static native float fp8e5m2_to_fp32_value(@Cast("uint8_t") byte input);

/*
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 8-bit floating-point number in fp8 E5M2 format, in bit representation.
 */
@Namespace("c10::detail") public static native @Cast("uint8_t") byte fp8e5m2_from_fp32_value(float f);


// Targeting ../Float8_e5m2.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Float8_e5m2 value);

 // namespace c10

// #include <c10/util/Float8_e5m2-inl.h> // IWYU pragma: keep


// Parsed from c10/util/bits.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../bits1x8.java


// Targeting ../bits2x4.java


// Targeting ../bits4x2.java


// Targeting ../bits8.java


// Targeting ../bits16.java



 // namespace c10


// Parsed from c10/util/qint32.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint32.java



 // namespace c10


// Parsed from c10/util/qint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint8.java



 // namespace c10


// Parsed from c10/util/quint2x4.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint2x4.java



 // namespace c10


// Parsed from c10/util/quint4x2.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint4x2.java



 // namespace c10


// Parsed from c10/util/quint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint8.java



 // namespace c10


// Parsed from c10/core/ScalarType.h

// #pragma once

// #include <c10/util/BFloat16.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Float8_e4m3fn.h>
// #include <c10/util/Float8_e5m2.h>
// #include <c10/util/Half.h>
// #include <c10/util/bits.h>
// #include <c10/util/complex.h>
// #include <c10/util/qint32.h>
// #include <c10/util/qint8.h>
// #include <c10/util/quint2x4.h>
// #include <c10/util/quint4x2.h>
// #include <c10/util/quint8.h>

// #include <complex>
// #include <cstdint>
// #include <ostream>

// For the macros below:
// NB: If you want to macro some code for all non-QInt scalar types (i.e. types
// with complete information, you probably want one of the
// AT_FORALL_SCALAR_TYPES / AT_FORALL_SCALAR_TYPES_AND
// macros below, which are designed to behave similarly to the Dispatch macros
// with the same name.

// NB: Order matters for this macro; it is relied upon in
// _promoteTypesLookup and the serialization format.
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(_)
//   _(uint8_t, Byte) /* 0 */
//   _(int8_t, Char) /* 1 */
//   _(int16_t, Short) /* 2 */
//   _(int, Int) /* 3 */
//   _(int64_t, Long) /* 4 */
//   _(at::Half, Half) /* 5 */
//   _(float, Float) /* 6 */
//   _(double, Double) /* 7 */
//   _(c10::complex<c10::Half>, ComplexHalf) /* 8 */
//   _(c10::complex<float>, ComplexFloat) /* 9 */
//   _(c10::complex<double>, ComplexDouble) /* 10 */
//   _(bool, Bool) /* 11 */
//   _(c10::qint8, QInt8) /* 12 */
//   _(c10::quint8, QUInt8) /* 13 */
//   _(c10::qint32, QInt32) /* 14 */
//   _(at::BFloat16, BFloat16) /* 15 */
//   _(c10::quint4x2, QUInt4x2) /* 16 */
//   _(c10::quint2x4, QUInt2x4) /* 17 */
//   _(c10::bits1x8, Bits1x8) /* 18 */
//   _(c10::bits2x4, Bits2x4) /* 19 */
//   _(c10::bits4x2, Bits4x2) /* 20 */
//   _(c10::bits8, Bits8) /* 21 */
//   _(c10::bits16, Bits16) /* 22 */
//   _(c10::Float8_e5m2, Float8_e5m2) /* 23 */
//   _(c10::Float8_e4m3fn, Float8_e4m3fn) /* 24 */

// If you want to support ComplexHalf for real, add ComplexHalf
// into this macro (and change the name).  But beware: convert()
// doesn't work for all the conversions you need...
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_COMPLEX_HALF(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)
//   _(at::Float8_e5m2, Float8_e5m2)
//   _(at::Float8_e4m3fn, Float8_e4m3fn)

// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<c10::Half>, ComplexHalf)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)
//   _(at::Float8_e5m2, Float8_e5m2)
//   _(at::Float8_e4m3fn, Float8_e4m3fn)

@Namespace("c10") public enum ScalarType {
  Byte((byte)(0)), /* 0 */
  Char((byte)(1)), /* 1 */
  Short((byte)(2)), /* 2 */
  Int((byte)(3)), /* 3 */
  Long((byte)(4)), /* 4 */
  Half((byte)(5)), /* 5 */
  Float((byte)(6)), /* 6 */
  Double((byte)(7)), /* 7 */
  ComplexHalf((byte)(8)), /* 8 */
  ComplexFloat((byte)(9)), /* 9 */
  ComplexDouble((byte)(10)), /* 10 */
  Bool((byte)(11)), /* 11 */
  QInt8((byte)(12)), /* 12 */
  QUInt8((byte)(13)), /* 13 */
  QInt32((byte)(14)), /* 14 */
  BFloat16((byte)(15)), /* 15 */
  QUInt4x2((byte)(16)), /* 16 */
  QUInt2x4((byte)(17)), /* 17 */
  Bits1x8((byte)(18)), /* 18 */
  Bits2x4((byte)(19)), /* 19 */
  Bits4x2((byte)(20)), /* 20 */
  Bits8((byte)(21)), /* 21 */
  Bits16((byte)(22)), /* 22 */
  Float8_e5m2((byte)(23)), /* 23 */
  Float8_e4m3fn((byte)(24)),
      Undefined((byte)(25)),
  NumOptions((byte)(26));

    public final byte value;
    private ScalarType(byte v) { this.value = v; }
    private ScalarType(ScalarType e) { this.value = e.value; }
    public ScalarType intern() { for (ScalarType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short NumScalarTypes();

// These are used to map ScalarTypes to C++ types.

// #define SPECIALIZE_ScalarTypeToCPPType(cpp_type, scalar_type)
//   template <>
//   struct ScalarTypeToCPPType<c10::ScalarType::scalar_type> {
//     using type = cpp_type;
// 
//     /* This is a workaround for the CUDA bug which prevents */
//     /* ::detail::ScalarTypeToCType<T>::type being used directly due to */
//     /* ambiguous reference which can't to be resolved. For some reason it */
//     /* can't pick between at::detail and at::cuda::detail. */
//     /* For repro example, please see: */
//     /* https://gist.github.com/izdeby/952ae7cf256ddb740a73776d39a7e7ba */
//     /* TODO: remove once the bug is fixed. */
//     static type t;
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */ /* 17 */ /* 18 */ /* 19 */ /* 20 */ /* 21 */ /* 22 */ /* 23 */ /* 24 */

// #undef SPECIALIZE_ScalarTypeToCPPType

 // namespace impl

// #define SPECIALIZE_CppTypeToScalarType(cpp_type, scalar_type)
//   template <>
//   struct CppTypeToScalarType<cpp_type>
//       : std::
//             integral_constant<c10::ScalarType, c10::ScalarType::scalar_type> {
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */ /* 17 */ /* 18 */ /* 19 */ /* 20 */ /* 21 */ /* 22 */ /* 23 */ /* 24 */

// #undef SPECIALIZE_CppTypeToScalarType

// #define AT_FORALL_INT_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)

// #define AT_FORALL_SCALAR_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)

// #define AT_FORALL_SCALAR_TYPES_AND(SCALARTYPE, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE>::t),
//     SCALARTYPE)

// #define AT_FORALL_SCALAR_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)

// #define AT_FORALL_SCALAR_TYPES_AND3(SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE3>::t),
//     SCALARTYPE3)

// #define AT_FORALL_SCALAR_TYPES_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE3>::t),
//     SCALARTYPE3)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE4>::t),
//     SCALARTYPE4)

// #define AT_FORALL_SCALAR_TYPES_AND5(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, SCALARTYPE5, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE3>::t),
//     SCALARTYPE3)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE4>::t),
//     SCALARTYPE4)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE5>::t),
//     SCALARTYPE5)

// #define AT_FORALL_QINT_TYPES(_)
//   _(c10::qint8, QInt8)
//   _(c10::quint8, QUInt8)
//   _(c10::qint32, QInt32)
//   _(c10::quint4x2, QUInt4x2)
//   _(c10::quint2x4, QUInt2x4)

// #define AT_FORALL_COMPLEX_TYPES(_)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)

// #define DEFINE_CONSTANT(_, name)
//   constexpr ScalarType k##name = ScalarType::name;

@Namespace("c10") @MemberGetter public static native ScalarType kByte(); /* 0 */
  @Namespace("c10") @MemberGetter public static native ScalarType kChar(); /* 1 */
  @Namespace("c10") @MemberGetter public static native ScalarType kShort(); /* 2 */
  @Namespace("c10") @MemberGetter public static native ScalarType kInt(); /* 3 */
  @Namespace("c10") @MemberGetter public static native ScalarType kLong(); /* 4 */
  @Namespace("c10") @MemberGetter public static native ScalarType kHalf(); /* 5 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat(); /* 6 */
  @Namespace("c10") @MemberGetter public static native ScalarType kDouble(); /* 7 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexHalf(); /* 8 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexFloat(); /* 9 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexDouble(); /* 10 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBool(); /* 11 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt8(); /* 12 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt8(); /* 13 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt32(); /* 14 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBFloat16(); /* 15 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt4x2(); /* 16 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt2x4(); /* 17 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBits1x8(); /* 18 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBits2x4(); /* 19 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBits4x2(); /* 20 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBits8(); /* 21 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBits16(); /* 22 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat8_e5m2(); /* 23 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat8_e4m3fn(); /* 24 */
// #undef DEFINE_CONSTANT

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(ScalarType t);

@Namespace("c10") public static native @Cast("size_t") long elementSize(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isIntegralType(ScalarType t, @Cast("bool") boolean includeBool);



@Namespace("c10") public static native @Cast("bool") boolean isFloatingType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isFloat8Type(ScalarType t);
@Namespace("c10") public static native @Cast("bool") boolean isReducedFloatingType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isComplexType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isQIntType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isBitsType(ScalarType t);

@Namespace("c10") public static native ScalarType toQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toUnderlying(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isSignedType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isUnderlying(ScalarType type, ScalarType qtype);

@Namespace("c10") public static native ScalarType toRealValueType(ScalarType t);

@Namespace("c10") public static native ScalarType toComplexType(ScalarType t);

// see tensor_attributes.rst for detailed explanation and examples
// of casting rules.
@Namespace("c10") public static native @Cast("bool") boolean canCast(ScalarType from, ScalarType to);

@Namespace("c10") public static native ScalarType promoteTypes(ScalarType a, ScalarType b);

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    ScalarType scalar_type);

 // namespace c10


// Parsed from c10/util/ExclusivelyOwned.h

// #pragma once

// #include <c10/util/in_place.h>

// See example implementation in TensorBase.h and TensorBody.h.
// Synopsis:
//
// repr_type -- type to use to store an owned T in ExclusivelyOwned.
//
// pointer_type -- pointer-esque type to return from
// ExclusivelyOwned's get() and operator*() methods.
//
// const_pointer_type -- similar to pointer_type, used for the const methods.
//
// static repr_type nullRepr() -- return a null instance of repr_type.
//
// template <class... Args>
// static repr_type createInPlace(Args&&... args) -- used by the in-place
// ExclusivelyOwned constructor.
//
// static repr_type moveToRepr(T&& x) -- move the given x into an
// instance of repr_type. used by the ExclusivelyOwned(T&&)
// constructor.
//
// static void destroyOwned(repr_type x) -- free memory for a
// known-exclusively-owned instance of x. Replaces calling repr_type's
// destructor. Being able to implement this more efficiently than
// repr_type's destructor is the main reason to use ExclusivelyOwned
// for a type.
//
// static T take(repr_type&) -- move out of the given repr_type into an owned T.
//
// static pointer_type getImpl(const repr_type&) -- return a pointer
// to the given repr_type. May take repr_type by value if that is more
// efficient.

/** ExclusivelyOwned is a smart-pointer-like wrapper around an
 *  exclusively-owned instance of some type T that normally has
 *  mandatory reference counting (currently just Tensor). If you have
 *  an isolated piece of code that knows that it has sole ownership of
 *  an object of one of these types (i.e., because you created it
 *  directly or using a factory function) and that object will not
 *  escape from that isolated piece of code, then moving the object
 *  into an ExclusivelyOwned will avoid an atomic reference count
 *  decrement at destruction time.
 * 
 *  If you directly create the Tensor in the first
 *  place, you can use the in_place constructor of ExclusivelyOwned to
 *  additionally avoid doing any stores to initialize the refcount &
 *  weakcount. */

 // namespace c10


// Parsed from c10/util/MaybeOwned.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/in_place.h>

// #include <type_traits>

/** MaybeOwnedTraits<T> describes how to borrow from T.  Here is how we
 *  can implement borrowing from an arbitrary type T using a raw
 *  pointer to const: */

/** It is possible to eliminate the extra layer of indirection for
 *  borrows for some types that we control. For examples, see
 *  intrusive_ptr.h and TensorBody.h. */

// Explicitly enable MaybeOwned<shared_ptr<T>>, rather than allowing
// MaybeOwned to be used for any type right away.
// Targeting ../TensorMaybeOwned.java


// Targeting ../TensorBaseMaybeOwned.java



 // namespace c10


// Parsed from c10/core/SymNodeImpl.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// Targeting ../SymNodeImpl.java



 // namespace c10


// Parsed from c10/core/SymFloat.h

// #pragma once

// #include <c10/core/SymBool.h>
// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/intrusive_ptr.h>

// #include <limits>
// Targeting ../SymFloat.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef SymFloat s);
 // namespace c10


// Parsed from c10/core/SymBool.h

// #pragma once

// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/intrusive_ptr.h>
// Targeting ../SymBool.java



@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef SymBool s);

// #define TORCH_SYM_CHECK(cond, ...)
//   TORCH_CHECK((cond).expect_true(__FILE__, __LINE__), __VA_ARGS__)
// #define TORCH_SYM_INTERNAL_ASSERT(cond, ...)
//   TORCH_INTERNAL_ASSERT((cond).expect_true(__FILE__, __LINE__), __VA_ARGS__)

 // namespace c10


// Parsed from c10/core/SymInt.h

// #pragma once

// #include <c10/core/SymBool.h>
// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <numeric>
// #include <type_traits>
// Targeting ../SymInt.java



/** Sum of a list of SymInt; accumulates into the c10::SymInt expression */

// #define DECLARE_SYMINT_OP_INTONLY(scalar_t, RetTy)
//   C10_API RetTy operator%(const SymInt& a, scalar_t b);
//   C10_API RetTy operator%(scalar_t a, const SymInt& b);

// #define DECLARE_SYMINT_OP(scalar_t, RetTy)
//   C10_API RetTy operator+(const SymInt& a, scalar_t b);
//   C10_API RetTy operator-(const SymInt& a, scalar_t b);
//   C10_API RetTy operator*(const SymInt& a, scalar_t b);
//   C10_API RetTy operator/(const SymInt& a, scalar_t b);
//   C10_API RetTy operator+(scalar_t a, const SymInt& b);
//   C10_API RetTy operator-(scalar_t a, const SymInt& b);
//   C10_API RetTy operator*(scalar_t a, const SymInt& b);
//   C10_API RetTy operator/(scalar_t a, const SymInt& b);
//   C10_API bool operator==(const SymInt& a, scalar_t b);
//   C10_API bool operator!=(const SymInt& a, scalar_t b);
//   C10_API bool operator<(const SymInt& a, scalar_t b);
//   C10_API bool operator<=(const SymInt& a, scalar_t b);
//   C10_API bool operator>(const SymInt& a, scalar_t b);
//   C10_API bool operator>=(const SymInt& a, scalar_t b);
//   C10_API bool operator==(scalar_t a, const SymInt& b);
//   C10_API bool operator!=(scalar_t a, const SymInt& b);
//   C10_API bool operator<(scalar_t a, const SymInt& b);
//   C10_API bool operator<=(scalar_t a, const SymInt& b);
//   C10_API bool operator>(scalar_t a, const SymInt& b);
//   C10_API bool operator>=(scalar_t a, const SymInt& b);

@Namespace("c10") public static native @ByVal @Name("operator %") SymInt mod(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @ByVal @Name("operator %") SymInt mod(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator %") SymInt mod(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @ByVal @Name("operator %") SymInt mod(int a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator +") SymInt add(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymInt multiply(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymInt divide(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @ByVal @Name("operator +") SymInt add(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymInt multiply(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymInt divide(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Const @ByRef SymInt a, @Cast("int64_t") long b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Cast("int64_t") long a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator +") SymInt add(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymInt multiply(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymInt divide(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @ByVal @Name("operator +") SymInt add(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymInt multiply(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymInt divide(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Const @ByRef SymInt a, int b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(int a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(int a, @Const @ByRef SymInt b); // make sure constants work
@Namespace("c10") public static native @ByVal @Name("operator +") SymFloat add(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymFloat subtract(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymFloat multiply(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymFloat divide(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @ByVal @Name("operator +") SymFloat add(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymFloat subtract(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymFloat multiply(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymFloat divide(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Const @ByRef SymInt a, double b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(double a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(double a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator +") SymFloat add(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymFloat subtract(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymFloat multiply(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymFloat divide(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @ByVal @Name("operator +") SymFloat add(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator -") SymFloat subtract(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator *") SymFloat multiply(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @ByVal @Name("operator /") SymFloat divide(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Const @ByRef SymInt a, float b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(float a, @Const @ByRef SymInt b);
  @Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(float a, @Const @ByRef SymInt b); // just for completeness

// On OSX size_t is different than uint64_t so we have to
// define it separately
// #if defined(__APPLE__)
// #endif

// #undef DECLARE_SYMINT_OP

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef SymInt s);
@Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Const @ByRef SymInt s);
 // namespace c10


// Parsed from c10/util/TypeCast.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <c10/util/BFloat16.h>
// #include <c10/util/Float8_e4m3fn.h>
// #include <c10/util/Float8_e5m2.h>
// #include <c10/util/Half.h>

// #include <type_traits>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-float-conversion")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

// Note: deliberately ignores undefined behavior, consistent with NumPy.
// PyTorch's type conversions can cause a variety of undefined behavior,
// including float to integral overflow and signed to unsigned integer overflow.
// Some of this undefined behavior is addressed below.

// Partial template instantiation for casting to uint8.
// Note: Converting from negative float values to unsigned integer types is
// undefined behavior in C++, and current CPU and GPU compilers exhibit
// divergent behavior. Casting from negative float values to signed
// integer types and then to unsigned integer types is not undefined,
// however, so this cast improves the consistency of type conversions
// to uint8 across compilers.
// Further note: Type conversions across compilers still have other undefined
// and divergent behavior.

// Define separately to avoid being inlined and prevent code-size bloat
@Namespace("c10") public static native void report_overflow(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native void report_overflow(String name);

 // namespace c10

// Trigger tests for D25440771. TODO: Remove this line any time you want.


// Parsed from c10/core/Scalar.h

// #pragma once

// #include <stdint.h>
// #include <stdexcept>
// #include <type_traits>
// #include <utility>

// #include <c10/core/OptionalRef.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymInt.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/TypeCast.h>
// #include <c10/util/intrusive_ptr.h>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif
// Targeting ../Scalar.java



// define the scalar.to<int64_t>() specializations
// #define DEFINE_TO(T, name)
//   template <>
//   inline T Scalar::to<T>() const {
//     return to##name();
//   }

  
  
  
  
  
  
  
  
  
  
  
  
  
  
// #undef DEFINE_TO

 // namespace c10



// Parsed from c10/util/IdWrapper.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <cstddef>
// #include <functional>
// #include <utility>

/**
 * This template simplifies generation of simple classes that wrap an id
 * in a typesafe way. Namely, you can use it to create a very lightweight
 * type that only offers equality comparators and hashing. Example:
 *
 *   struct MyIdType final : IdWrapper<MyIdType, uint32_t> {
 *     constexpr explicit MyIdType(uint32_t id): IdWrapper(id) {}
 *   };
 *
 * Then in the global top level namespace:
 *
 *   C10_DEFINE_HASH_FOR_IDWRAPPER(MyIdType);
 *
 * That's it - equality operators and hash functions are automatically defined
 * for you, given the underlying type supports it.
 */

 // namespace c10

// #define C10_DEFINE_HASH_FOR_IDWRAPPER(ClassName)
//   namespace std {
//   template <>
//   struct hash<ClassName> {
//     size_t operator()(ClassName x) const {
//       return hash_value(x);
//     }
//   };
//   }


// Parsed from c10/util/Type.h

// #ifndef C10_UTIL_TYPE_H_
// #define C10_UTIL_TYPE_H_

// #include <cstddef>
// #include <string>
// #ifdef __GXX_RTTI
// #include <typeinfo>
// #endif // __GXX_RTTI

// #include <c10/macros/Macros.h>

/** Utility to demangle a C++ symbol name. */
@Namespace("c10") public static native @StdString BytePointer demangle(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native @StdString String demangle(String name);

/** Returns the printable name of the type. */

 // namespace c10

// #endif // C10_UTIL_TYPE_H_


// Parsed from c10/util/ConstexprCrc.h

// #pragma once

// #include <c10/util/IdWrapper.h>
// #include <c10/util/string_view.h>
// #include <cstddef>
// #include <cstdint>
@Namespace("c10::util::detail") @MemberGetter public static native @Cast("const uint64_t") long crc64_table(int i);
@Namespace("c10::util::detail") @MemberGetter public static native @Cast("const uint64_t*") LongPointer crc64_table();

@Namespace("c10::util::detail") public static native @Cast("const uint64_t") long crc64impl(@Cast("uint64_t") long accumulator, @Cast("const char*") BytePointer data, @Cast("size_t") long size);
@Namespace("c10::util::detail") public static native @Cast("const uint64_t") long crc64impl(@Cast("uint64_t") long accumulator, String data, @Cast("size_t") long size);

// Targeting ../crc64_t.java



// CRC64 with Jones coefficients and an init value of 0.
@Namespace("c10::util") public static native @Const @ByVal crc64_t crc64(@Cast("const char*") BytePointer str, @Cast("size_t") long size);
@Namespace("c10::util") public static native @Const @ByVal crc64_t crc64(String str, @Cast("size_t") long size);

@Namespace("c10::util") public static native @Const @ByVal crc64_t crc64(@StringView BytePointer str);
@Namespace("c10::util") public static native @Const @ByVal crc64_t crc64(@StringView String str);
 // namespace util
 // namespace c10

// Allow usage of crc64_t in std::unordered_set
  


// Parsed from c10/util/TypeIndex.h

// #pragma once

// #include <c10/util/C++17.h>
// #include <c10/util/ConstexprCrc.h>
// #include <c10/util/IdWrapper.h>
// #include <c10/util/string_view.h>
// #include <cinttypes>
// #include <functional>

// TODO Make it work for more compilers

// Intel compiler works
// #if defined(__INTEL_COMPILER)
public static final int C10_TYPENAME_SUPPORTS_CONSTEXPR = 0;
// #define C10_TYPENAME_CONSTEXPR

// Clang works
// #elif defined(__clang__)

// except for NVCC
// #if defined(__CUDACC__)
// #define C10_TYPENAME_CONSTEXPR
// #else
// #define C10_TYPENAME_CONSTEXPR constexpr
// #endif

// Windows works
// #elif defined(_MSC_VER)
// #elif defined(__GNUC__)
// #else
// #define C10_TYPENAME_CONSTEXPR constexpr
// Targeting ../type_index.java



// #if !defined(__clang__) && !defined(_MSC_VER) && defined(__GNUC__) &&
//     __GNUC__ < 5
// Getting __PRETTY_FUNCTION__ at compile time only works with GCC >= 5
// #error "You're running a too old version of GCC. We need GCC 5 or later."
// #endif

// #if defined(__clang__) && __clang_major__ < 4
// Getting __PRETTY_FUNCTION__ at compile time only works with Clang >= 4
// #error "You're running a too old version of Clang. We need Clang 4 or later."
// #endif

@Namespace("c10::util::detail") public static native @StringView BytePointer extract(
    @StringView BytePointer prefix,
    @StringView BytePointer suffix,
    @StringView BytePointer str);
@Namespace("c10::util::detail") public static native @StringView String extract(
    @StringView String prefix,
    @StringView String suffix,
    @StringView String str);

// #if !defined(__CUDA_ARCH__)
// #endif

 // namespace detail

@Namespace("c10::util") public static native @Const @ByVal @Name("get_type_index<std::string>") type_index get_type_index_string();

// #if !defined(TORCH_PEDANTIC)
// Use precomputed hashsum for std::string
// Needed to workaround ambiguity in class name resolution
// into __PRETTY_FUNCION__ when abovementioned class is defined in inlined
// namespace. In multi-ABI C++ library, `std::string` is an alias to
// `std::__cxx11::basic_string<char>` which depending on compiler flags can be
// resolved to `basic_string<char>` either in `std` namespace or in
// `std::__cxx11` one (`__cxx11` is an inline namespace)
// #endif
 // namespace util
 // namespace c10
  


// Parsed from c10/util/flat_hash_map.h

// Taken from
// https://github.com/skarupke/flat_hash_map/blob/2c4687431f978f02a3780e24b8b701d22aa32d9c/flat_hash_map.hpp
// with fixes applied:
// - https://github.com/skarupke/flat_hash_map/pull/25
// - https://github.com/skarupke/flat_hash_map/pull/26
// - replace size_t with uint64_t to fix it for 32bit
// - add "GCC diagnostic" pragma to ignore -Wshadow
// - make sherwood_v3_table::convertible_to_iterator public because GCC5 seems
// to have issues with it otherwise
// - fix compiler warnings in operator templated_iterator<const value_type>

//          Copyright Malte Skarupke 2017.
// Distributed under the Boost Software License, Version 1.0.
//    (See http://www.boost.org/LICENSE_1_0.txt)

// #pragma once

// #include <c10/macros/Macros.h>
// #include <algorithm>
// #include <cmath>
// #include <cstddef>
// #include <cstdint>
// #include <functional>
// #include <iterator>
// #include <stdexcept>
// #include <type_traits>
// #include <utility>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

// #if defined(_MSC_VER) && !defined(__clang__)
// #pragma warning(push)
// #pragma warning(disable : 4624) // destructor was implicitly defined as deleted
// #endif

// #ifdef _MSC_VER
// #define SKA_NOINLINE(...) __declspec(noinline) __VA_ARGS__
// #else
// #define SKA_NOINLINE(...) __VA_ARGS__ __attribute__((noinline))
// #endif
@Namespace("ska::detailv3") @MemberGetter public static native byte min_lookups();
public static final byte min_lookups = min_lookups();

@Namespace("ska::detailv3") public static native byte log2(@Cast("uint64_t") long value);

@Namespace("ska::detailv3") public static native @Cast("uint64_t") long next_power_of_two(@Cast("uint64_t") long i);

// Implementation taken from http://en.cppreference.com/w/cpp/types/void_t
// (it takes CWG1558 into account and also works for older compilers)
 // namespace detailv3

 // end namespace ska

// #if defined(_MSC_VER) && !defined(__clang__)
// #pragma warning(pop)
// #endif


// Parsed from c10/util/irange.h

// Copyright 2004-present Facebook. All Rights Reserved.

// #pragma once

// #include <c10/util/Exception.h>
// #include <c10/util/TypeSafeSignMath.h>

// #include <algorithm>
// #include <iterator>
// #include <limits>
// #include <type_traits>

 // namespace detail

/** Creates an integer range for the half-open interval [begin, end)
 *  If end<=begin, then the range is empty.
 *  The range has the type of the {@code end} integer; {@code begin} integer is
 *  cast to this type. */

/** Creates an integer range for the half-open interval [0, end)
 *  If end<=begin, then the range is empty */

 // namespace c10


// Parsed from c10/util/typeid.h

// #pragma once

// #include <atomic>
// #include <cstdlib>
// #include <memory>
// #include <mutex>
// #include <type_traits>
// #include <vector>

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/IdWrapper.h>
// #include <c10/util/TypeIndex.h>
// #include <c10/util/TypeTraits.h>

// #include <c10/core/ScalarType.h>
// #include <c10/util/irange.h>

/*
 * TypeIdentifier is a small type containing an id.
 * Types must be registered using CAFFE_DECLARE_KNOWN_TYPE() (in their header)
 * and CAFFE_DEFINE_KNOWN_TYPE() (in their .cpp file) for them to have a type
 * id. If a type is registered, you can also create an object containing meta
 * data like constructor, destructor, stringified name, ... about the type by
 * calling TypeMeta::Make<T>. This returns a TypeMeta() object, which is
 * basically just a pointer to the type information, so it's cheap to pass
 * around.
 */

// TODO: This file is still in the caffe2 namespace, despite living
// in the ATen directory.  This is because the macro
// CAFFE_KNOWN_TYPE (and CAFFE_DECLARE_KNOWN_TYPE) defines a template
// specialization, which relies
// on the namespace of TypeMeta matching the namespace where the macro is
// called.  This requires us to fix all of the call-sites, which I want to do
// later.  So the namespace is not fixed at the moment.

// Make at::Half a fundamental type.
 // namespace guts
 // namespace c10
// Targeting ../TypeIdentifier.java



// Allow usage in std::map / std::set
// TODO Disallow this and rather use std::unordered_map/set everywhere
@Namespace("caffe2") public static native @Cast("const bool") @Name("operator <") boolean lessThan(@ByVal TypeIdentifier lhs, @ByVal TypeIdentifier rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeIdentifier typeId);

 // namespace caffe2

  

// This struct holds the actual type information. There will be
// one allocated per type. TypeMeta objects will then point to the struct
// instance for the type they're configured for.

// Mechanism for throwing errors which can't be prevented at compile time
// due to type erasure. E.g. somebody calling TypeMeta::copy() for
// non-copyable type. Right now just throws exception but is implemented
// in .cpp to manage dependencies
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString BytePointer msg);
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString String msg);

/**
 * Placement new function for the type.
 */

/**
 * Typed copy function for classes.
 */

/**
 * A placeholder function for types that do not allow assignment.
 */

/**
 * Destructor for non-fundamental types.
 */

 // namespace detail

//
// note: this is outside TypeMeta bc gcc seems to have trouble
// with scalarTypeItemSizes as a constexpr static member used by
// a public inline instance method
//

// item sizes for TypeMeta::itemsize() fast path
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t") byte scalarTypeItemSizes(int i);
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t*") BytePointer scalarTypeItemSizes();
// Targeting ../TypeMeta.java



// specializations of TypeMeta::_typeMetaData for ScalarType types

// #define DEFINE_SCALAR_METADATA_INSTANCE(T, name)
//   template <>
//   constexpr uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     return static_cast<uint16_t>(ScalarType::name);
//   }
 /* 0 */
   /* 1 */
   /* 2 */
   /* 3 */
   /* 4 */
   /* 5 */
   /* 6 */
   /* 7 */
   /* 8 */
   /* 9 */
   /* 10 */
   /* 11 */
   /* 12 */
   /* 13 */
   /* 14 */
   /* 15 */
   /* 16 */
   /* 17 */
   /* 18 */
   /* 19 */
   /* 20 */
   /* 21 */
   /* 22 */
   /* 23 */
   /* 24 */
// #undef DEFINE_SCALAR_METADATA_INSTANCE





@Namespace("caffe2") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@Const @ByRef TypeMeta lhs, @Const @ByRef TypeMeta rhs);
@Namespace("caffe2") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@Const @ByRef TypeMeta lhs, @Const @ByRef TypeMeta rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeMeta typeMeta);

/**
 * Register unique id for a type so it can be used in TypeMeta context, e.g. be
 * used as a type for Blob or for Tensor elements.
 *
 * CAFFE_KNOWN_TYPE is deprecated; prefer CAFFE_DECLARE_KNOWN_TYPE and
 * CAFFE_DEFINE_KNOWN_TYPE.
 *
 * CAFFE_KNOWN_TYPE does explicit instantiation of TypeIdentifier::Get<T>
 * template function and thus needs to be put in a single translation unit (.cpp
 * file) for a given type T. Other translation units that use type T as a type
 * of the caffe2::Blob or element type of caffe2::Tensor need to depend on the
 * translation unit that contains CAFFE_KNOWN_TYPE declaration via regular
 * linkage dependencies.
 *
 * NOTE: the macro needs to be invoked in ::caffe2 namespace
 */
// Implementation note: in MSVC, we will need to prepend the C10_API
// keyword in order to get things compiled properly. in Linux, gcc seems to
// create attribute ignored error for explicit template instantiations, see
//   http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0537r0.html
//   https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51930
// and as a result, we define these two macros slightly differently.
// #if defined(_MSC_VER) || defined(__clang__)
// #define EXPORT_IF_NOT_GCC C10_EXPORT
// #else
// #define EXPORT_IF_NOT_GCC
// #endif

// CAFFE_KNOWN_TYPE is deprecated! Use CAFFE_DECLARE_KNOWN_TYPE and
// CAFFE_DEFINE_KNOWN_TYPE instead.
// #define CAFFE_KNOWN_TYPE(T)
//   template uint16_t TypeMeta::addTypeMetaData<T>();
//   template <>
//   EXPORT_IF_NOT_GCC uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }

// #define CAFFE_DEFINE_KNOWN_TYPE(T, ident)
//   template uint16_t TypeMeta::addTypeMetaData<T>();
//   namespace detail {
//   EXPORT_IF_NOT_GCC const uint16_t ident##_metadata_index =
//       TypeMeta::addTypeMetaData<T>();
//   } // namespace detail

// Unlike CAFFE_KNOWN_TYPE, CAFFE_DECLARE_KNOWN_TYPE avoids a function
// call to access _typeMetaData in the common case.
// #define CAFFE_DECLARE_KNOWN_TYPE(T, ident)
//   extern template uint16_t TypeMeta::addTypeMetaData<T>();
//   namespace detail {
//   extern C10_API const uint16_t ident##_metadata_index;
//   } /* namespace detail */
//   template <>
//   EXPORT_IF_NOT_GCC C10_ALWAYS_INLINE uint16_t
//   TypeMeta::_typeMetaData<T>() noexcept {
//     return detail::ident##_metadata_index;
//   }

// #define CAFFE_KNOWN_TYPE_NOEXPORT(T)
//   template <>
//   uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }


  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_string_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short uint16_t_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short char_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_unique_ptr_std_mutex_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_unique_ptr_std_atomic_bool_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_vector_int32_t_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_vector_int64_t_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short std_vector_unsigned_long_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short bool_ptr_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short char_ptr_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short int_ptr_metadata_index();
   /* namespace detail */
  

// For some of the compilers, long is defined separately from int32_t and
// int64_t. As a result we will need to actually define them separately.
// It is recommended that one does NOT use long - use int32_t and int64_t
// explicitly. Explicit long type annotation may go away in the future.
// details: This hack works by defining a _guard_long_unique type, which is
// long iff the compiler has a separate long type and is a dummy type otherwise.
// we then allocate a type id to that _guard_long_unique. If the compiler has a
// separate long type, this allocates a type id for long. Otherwise, it
// allocates a type id for the dummy type, which doesn't matter.
 // namespace detail


  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short detail_guard_long_unique_long_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short detail_guard_long_unique_std_vector_long_metadata_index();
   /* namespace detail */
  


  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short float_ptr_metadata_index();
   /* namespace detail */
  

  @Namespace("caffe2::detail") @MemberGetter public static native @Cast("const uint16_t") short at_Half_metadata_index();
   /* namespace detail */
  

 // namespace caffe2


// Parsed from c10/core/ScalarTypeToTypeMeta.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/util/Optional.h>
// #include <c10/util/typeid.h>

// these just expose TypeMeta/ScalarType bridge functions in c10
// TODO move to typeid.h (or codemod away) when TypeMeta et al
// are moved from caffe2 to c10 (see note at top of typeid.h)

/**
 * convert ScalarType enum values to TypeMeta handles
 */
@Namespace("c10") public static native @ByVal TypeMeta scalarTypeToTypeMeta(ScalarType scalar_type);

/**
 * convert TypeMeta handles to ScalarType enum values
 */
@Namespace("c10") public static native ScalarType typeMetaToScalarType(@ByVal TypeMeta dtype);

/**
 * typeMetaToScalarType(), lifted to optional
 */
@Namespace("c10") public static native @ByVal ScalarTypeOptional optTypeMetaToScalarType(
    @ByVal TypeMetaOptional type_meta);

/**
 * convenience: equality across TypeMeta/ScalarType conversion
 */
@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(ScalarType t, @ByVal TypeMeta m);

@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@ByVal TypeMeta m, ScalarType t);

@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(ScalarType t, @ByVal TypeMeta m);

@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@ByVal TypeMeta m, ScalarType t);

 // namespace c10


// Parsed from c10/util/ThreadLocalDebugInfo.h

// #pragma once

// #include <c10/macros/Export.h>

// #include <memory>
// #include <string>

@Namespace("c10") public enum DebugInfoKind {
  PRODUCER_INFO((byte)(0)),
  MOBILE_RUNTIME_INFO((byte)(1)),
  PROFILER_STATE((byte)(2)),
  INFERENCE_CONTEXT((byte)(3)), // for inference usage
  PARAM_COMMS_INFO((byte)(4)),

  TEST_INFO((byte)(5)), // used only in tests
  TEST_INFO_2((byte)(6));// used only in tests

    public final byte value;
    private DebugInfoKind(byte v) { this.value = v; }
    private DebugInfoKind(DebugInfoKind e) { this.value = e.value; }
    public DebugInfoKind intern() { for (DebugInfoKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../DebugInfoBase.java


// Targeting ../ThreadLocalDebugInfo.java


// Targeting ../DebugInfoGuard.java



 // namespace c10


// Parsed from c10/util/UniqueVoidPtr.h

// #pragma once
// #include <memory>

// #include <c10/macros/Macros.h>

// Does not delete anything
@Namespace("c10::detail") public static native void deleteNothing(Pointer arg0);
// Targeting ../UniqueVoidPtr.java



// Note [How UniqueVoidPtr is implemented]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// UniqueVoidPtr solves a common problem for allocators of tensor data, which
// is that the data pointer (e.g., float*) which you are interested in, is not
// the same as the context pointer (e.g., DLManagedTensor) which you need
// to actually deallocate the data.  Under a conventional deleter design, you
// have to store extra context in the deleter itself so that you can actually
// delete the right thing.  Implementing this with standard C++ is somewhat
// error-prone: if you use a std::unique_ptr to manage tensors, the deleter will
// not be called if the data pointer is nullptr, which can cause a leak if the
// context pointer is non-null (and the deleter is responsible for freeing both
// the data pointer and the context pointer).
//
// So, in our reimplementation of unique_ptr, which just store the context
// directly in the unique pointer, and attach the deleter to the context
// pointer itself.  In simple cases, the context pointer is just the pointer
// itself.

@Namespace("c10::detail") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@Const @ByRef UniqueVoidPtr sp, @ByVal @Cast("std::nullptr_t*") PointerPointer arg1);
@Namespace("c10::detail") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@ByVal @Cast("std::nullptr_t*") PointerPointer arg0, @Const @ByRef UniqueVoidPtr sp);
@Namespace("c10::detail") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@Const @ByRef UniqueVoidPtr sp, @ByVal @Cast("std::nullptr_t*") PointerPointer arg1);
@Namespace("c10::detail") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@ByVal @Cast("std::nullptr_t*") PointerPointer arg0, @Const @ByRef UniqueVoidPtr sp);

 // namespace detail
 // namespace c10


// Parsed from c10/core/Allocator.h

// #pragma once

// #include <stddef.h>
// #include <memory>

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>
// #include <c10/util/UniqueVoidPtr.h>
// Targeting ../DataPtr.java



// NB: Device is NOT tested for here; a CUDA nullptr is as much a nullptr as a
// CPU nullptr

@Namespace("c10") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@Cast({"", "c10::DataPtr&&"}) @StdMove DataPtr dp, @ByVal @Cast("std::nullptr_t*") PointerPointer arg1);
@Namespace("c10") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@ByVal @Cast("std::nullptr_t*") PointerPointer arg0, @Cast({"", "c10::DataPtr&&"}) @StdMove DataPtr dp);
@Namespace("c10") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@Cast({"", "c10::DataPtr&&"}) @StdMove DataPtr dp, @ByVal @Cast("std::nullptr_t*") PointerPointer arg1);
@Namespace("c10") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@ByVal @Cast("std::nullptr_t*") PointerPointer arg0, @Cast({"", "c10::DataPtr&&"}) @StdMove DataPtr dp);
// Targeting ../Allocator.java



// This context is used to generate DataPtr which have arbitrary
// std::function deleters associated with them.  In some user facing
// functions, we give a (user-friendly) interface for constructing
// tensors from external data which take an arbitrary std::function
// deleter.  Grep for InefficientStdFunctionContext to find these
// occurrences.
//
// This context is inefficient because we have to do a dynamic
// allocation InefficientStdFunctionContext, on top of the dynamic
// allocation which is implied by std::function itself.

/** Set the allocator for DeviceType {@code t}. The passed in allocator pointer is
 *  expected to have static lifetime; this function does NOT take ownership
 *  of the raw pointer. (The reason for this is to prevent existing pointers
 *  to an allocator of a particular device from being invalidated when
 *  SetAllocator is called.)
 *
 *  Also note that this is not thread-safe, and we assume this function will
 *  only be called during initialization.
 *
 *  The 'priority' flag is introduced when we want to overwrite the default
 *  allocator, since the allocators are set statically. The default priority
 *  is 0, which means the lowest. Only higher or equal priority can overwrite
 *  existing ones.
 */
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc);
@Namespace("c10") public static native Allocator GetAllocator(DeviceType t);
@Namespace("c10") public static native Allocator GetAllocator(@Cast("c10::DeviceType") byte t);

// #define REGISTER_ALLOCATOR(t, f)
//   namespace {
//   static c10::AllocatorRegisterer<t> g_allocator_d(f);
//   }
// Targeting ../MemoryReportingInfoBase.java



@Namespace("c10") public static native @Cast("bool") boolean memoryProfilingEnabled();
@Namespace("c10") public static native void reportMemoryUsageToProfiler(
    Pointer ptr,
    @Cast("int64_t") long alloc_size,
    @Cast("size_t") long total_allocated,
    @Cast("size_t") long total_reserved,
    @ByVal Device device);

@Namespace("c10") public static native void reportOutOfMemoryToProfiler(
    @Cast("int64_t") long alloc_size,
    @Cast("size_t") long total_allocated,
    @Cast("size_t") long total_reserved,
    @ByVal Device device);
// Targeting ../GatheredContext.java



 // namespace c10


// Parsed from c10/util/python_stub.h

// #pragma once


// Parsed from c10/core/StorageImpl.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <c10/core/SymInt.h>
// #include <c10/core/impl/PyObjectSlot.h>

// #include <c10/util/intrusive_ptr.h>
// Targeting ../StorageImpl.java



// Declare StorageImpl create function pointer types.

@Namespace("c10") public static native void SetStorageImplCreate(DeviceType t, @Cast("c10::StorageImplCreateHelper") StorageImplPtr fptr);
@Namespace("c10") public static native void SetStorageImplCreate(@Cast("c10::DeviceType") byte t, @Cast("c10::StorageImplCreateHelper") StorageImplPtr fptr);

@Namespace("c10") public static native @Cast("c10::StorageImplCreateHelper") StorageImplPtr GetStorageImplCreate(DeviceType t);
@Namespace("c10") public static native @Cast("c10::StorageImplCreateHelper") StorageImplPtr GetStorageImplCreate(@Cast("c10::DeviceType") byte t);

 // namespace c10


// Parsed from c10/core/Storage.h

// #pragma once

// #include <c10/core/StorageImpl.h>
// Targeting ../Storage.java



 // namespace c10


// Parsed from c10/core/AutogradState.h

// #pragma once

// #include <c10/macros/Export.h>
// Targeting ../AutogradState.java



 // namespace c10


// Parsed from c10/core/GradMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/macros/Export.h>
// Targeting ../GradMode.java


// Targeting ../AutoGradMode.java


// Targeting ../NoGradGuard.java


// Targeting ../AutoFwGradMode.java



 // namespace c10


// Parsed from c10/util/Registry.h

// #ifndef C10_UTIL_REGISTRY_H_
// #define C10_UTIL_REGISTRY_H_

/**
 * Simple registry implementation that uses static variables to
 * register object creators during program initialization time.
 */

// NB: This Registry works poorly when you have other namespaces.
// Make all macro invocations from inside the at namespace.

// #include <algorithm>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <memory>
// #include <mutex>
// #include <stdexcept>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include <c10/macros/Macros.h>
// #include <c10/util/Type.h>

@Namespace("c10") public enum RegistryPriority {
  REGISTRY_FALLBACK(1),
  REGISTRY_DEFAULT(2),
  REGISTRY_PREFERRED(3);

    public final int value;
    private RegistryPriority(int v) { this.value = v; }
    private RegistryPriority(RegistryPriority e) { this.value = e.value; }
    public RegistryPriority intern() { for (RegistryPriority e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/**
 * \brief A template class that allows one to register classes by keys.
 *
 * The keys are usually a std::string specifying the name, but can be anything
 * that can be used in a std::map.
 *
 * You should most likely not use the Registry class explicitly, but use the
 * helper macros below to declare specific registries as well as registering
 * objects.
 */

/**
 * C10_DECLARE_TYPED_REGISTRY is a macro that expands to a function
 * declaration, as well as creating a convenient typename for its corresponding
 * registerer.
 */
// Note on C10_IMPORT and C10_EXPORT below: we need to explicitly mark DECLARE
// as import and DEFINE as export, because these registry macros will be used
// in downstream shared libraries as well, and one cannot use *_API - the API
// macro will be defined on a per-shared-library basis. Semantically, when one
// declares a typed registry it is always going to be IMPORT, and when one
// defines a registry (which should happen ONLY ONCE and ONLY IN SOURCE FILE),
// the instantiation unit is always going to be exported.
//
// The only unique condition is when in the same file one does DECLARE and
// DEFINE - in Windows compilers, this generates a warning that dllimport and
// dllexport are mixed, but the warning is fine and linker will be properly
// exporting the symbol. Same thing happens in the gflags flag declaration and
// definition caes.
// #define C10_DECLARE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_API ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName();
//   typedef ::c10::Registerer<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>
//       Registerer##RegistryName

// #define TORCH_DECLARE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   TORCH_API ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName();
//   typedef ::c10::Registerer<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>
//       Registerer##RegistryName

// #define C10_DEFINE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry = new ::c10::
//             Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>();
//     return registry;
//   }

// #define C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry =
//             new ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>(
//                 false);
//     return registry;
//   }

// Note(Yangqing): The __VA_ARGS__ below allows one to specify a templated
// creator with comma in its templated arguments.
// #define C10_REGISTER_TYPED_CREATOR(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, priority, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CLASS(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// #define C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       priority,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// C10_DECLARE_REGISTRY and C10_DEFINE_REGISTRY are hard-wired to use
// std::string as the key type, because that is the most commonly used cases.
// #define C10_DECLARE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define TORCH_DECLARE_REGISTRY(RegistryName, ObjectType, ...)
//   TORCH_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY_WITHOUT_WARNING(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DECLARE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define TORCH_DECLARE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   TORCH_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// C10_REGISTER_CREATOR and C10_REGISTER_CLASS are hard-wired to use std::string
// as the key
// type, because that is the most commonly used cases.
// #define C10_REGISTER_CREATOR(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CREATOR(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CREATOR_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

// #define C10_REGISTER_CLASS(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CLASS(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CLASS_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

 // namespace c10

// #endif // C10_UTIL_REGISTRY_H_


// Parsed from c10/util/Flags.h

// #ifndef C10_UTIL_FLAGS_H_
// #define C10_UTIL_FLAGS_H_

/* Commandline flags support for C10.
 *
 * This is a portable commandline flags tool for c10, so we can optionally
 * choose to use gflags or a lightweight custom implementation if gflags is
 * not possible on a certain platform. If you have gflags installed, set the
 * macro C10_USE_GFLAGS will seamlessly route everything to gflags.
 *
 * To define a flag foo of type bool default to true, do the following in the
 * *global* namespace:
 *     C10_DEFINE_bool(foo, true, "An example.");
 *
 * To use it in another .cc file, you can use C10_DECLARE_* as follows:
 *     C10_DECLARE_bool(foo);
 *
 * In both cases, you can then access the flag via FLAGS_foo.
 *
 * It is recommended that you build with gflags. To learn more about the flags
 * usage, refer to the gflags page here:
 *
 * https://gflags.github.io/gflags/
 *
 * Note about Python users / devs: gflags is initiated from a C++ function
 * ParseCommandLineFlags, and is usually done in native binaries in the main
 * function. As Python does not have a modifiable main function, it is usually
 * difficult to change the flags after Python starts. Hence, it is recommended
 * that one sets the default value of the flags to one that's acceptable in
 * general - that will allow Python to run without wrong flags.
 */

// #include <string>

// #include <c10/macros/Macros.h>
// #include <c10/util/Registry.h>
/**
 * Sets the usage message when a commandline tool is called with "--help".
 */
@Namespace("c10") public static native void SetUsageMessage(@StdString BytePointer str);
@Namespace("c10") public static native void SetUsageMessage(@StdString String str);

/**
 * Returns the usage message for the commandline tool set by SetUsageMessage.
 */
@Namespace("c10") public static native @Cast("const char*") BytePointer UsageMessage();

/**
 * Parses the commandline flags.
 *
 * This command parses all the commandline arguments passed in via pargc
 * and argv. Once it is finished, partc and argv will contain the remaining
 * commandline args that c10 does not deal with. Note that following
 * convention, argv[0] contains the binary name and is not parsed.
 */
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntPointer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntBuffer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(int[] pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);

/**
 * Checks if the commandline flags has already been passed.
 */
@Namespace("c10") public static native @Cast("bool") boolean CommandLineFlagsHasBeenParsed();

 // namespace c10

////////////////////////////////////////////////////////////////////////////////
// Below are gflags and non-gflags specific implementations.
// In general, they define the following macros for one to declare (use
// C10_DECLARE) or define (use C10_DEFINE) flags:
// C10_{DECLARE,DEFINE}_{int,int64,double,bool,string}
////////////////////////////////////////////////////////////////////////////////

// #ifdef C10_USE_GFLAGS

////////////////////////////////////////////////////////////////////////////////
// Begin gflags section: most functions are basically rerouted to gflags.
////////////////////////////////////////////////////////////////////////////////
// #include <gflags/gflags.h>

// C10 uses hidden visibility by default. However, in gflags, it only uses
// export on Windows platform (with dllexport) but not on linux/mac (with
// default visibility). As a result, to ensure that we are always exporting
// global variables, we will redefine the GFLAGS_DLL_DEFINE_FLAG macro if we
// are building C10 as a shared library.
// This has to be done after the inclusion of gflags, because some early
// versions of gflags.h (e.g. 2.0 on ubuntu 14.04) directly defines the
// macros, so we need to do definition after gflags is done.
// #ifdef GFLAGS_DLL_DEFINE_FLAG
// #endif // GFLAGS_DLL_DEFINE_FLAG
// #ifdef GFLAGS_DLL_DECLARE_FLAG
// #endif // GFLAGS_DLL_DECLARE_FLAG
// #define GFLAGS_DLL_DEFINE_FLAG C10_EXPORT
// #define GFLAGS_DLL_DECLARE_FLAG C10_IMPORT

// gflags before 2.0 uses namespace google and after 2.1 uses namespace gflags.
// Using GFLAGS_GFLAGS_H_ to capture this change.
// #ifndef GFLAGS_GFLAGS_H_
// #endif // GFLAGS_GFLAGS_H_

// Motivation about the gflags wrapper:
// (1) We would need to make sure that the gflags version and the non-gflags
// version of C10 are going to expose the same flags abstraction. One should
// explicitly use FLAGS_flag_name to access the flags.
// (2) For flag names, it is recommended to start with c10_ to distinguish it
// from regular gflags flags. For example, do
//    C10_DEFINE_BOOL(c10_my_flag, true, "An example");
// to allow one to use FLAGS_c10_my_flag.
// (3) Gflags has a design issue that does not properly expose the global flags,
// if one builds the library with -fvisibility=hidden. The current gflags (as of
// Aug 2018) only deals with the Windows case using dllexport, and not the Linux
// counterparts. As a result, we will explicitly use C10_EXPORT to export the
// flags defined in C10. This is done via a global reference, so the flag
// itself is not duplicated - under the hood it is the same global gflags flag.
// #define C10_GFLAGS_DEF_WRAPPER(type, real_type, name, default_value, help_str)
//   DEFINE_##type(name, default_value, help_str);

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int32, gflags::int32, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int64, gflags::int64, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(double, double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(bool, bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(string, ::fLS::clstring, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_GFLAGS_DECLARE_WRAPPER(type, real_type, name) DECLARE_##type(name);

// #define C10_DECLARE_int(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int32, gflags::int32, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int64, gflags::int64, name)
// #define C10_DECLARE_double(name)
//   C10_GFLAGS_DECLARE_WRAPPER(double, double, name)
// #define C10_DECLARE_bool(name) C10_GFLAGS_DECLARE_WRAPPER(bool, bool, name)
// #define C10_DECLARE_string(name)
//   C10_GFLAGS_DECLARE_WRAPPER(string, ::fLS::clstring, name)
// Targeting ../C10FlagParser.java





 // namespace c10

// The macros are defined outside the c10 namespace. In your code, you should
// write the C10_DEFINE_* and C10_DECLARE_* macros outside any namespace
// as well.

// #define C10_DEFINE_typed_var(type, name, default_value, help_str)
//   C10_EXPORT type FLAGS_##name = default_value;
//   namespace c10 {
//   namespace {
//   class C10FlagParser_##name : public C10FlagParser {
//    public:
//     explicit C10FlagParser_##name(const std::string& content) {
//       success_ = C10FlagParser::Parse<type>(content, &FLAGS_##name);
//     }
//   };
//   }
//   RegistererC10FlagsRegistry g_C10FlagsRegistry_##name(
//       #name,
//       C10FlagsRegistry(),
//       RegistererC10FlagsRegistry::DefaultCreator<C10FlagParser_##name>,
//       "(" #type ", default " #default_value ") " help_str);
//   }

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_DEFINE_typed_var(int, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_DEFINE_typed_var(int64_t, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_DEFINE_typed_var(double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_DEFINE_typed_var(bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_DEFINE_typed_var(std::string, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_DECLARE_typed_var(type, name) C10_API extern type FLAGS_##name

// #define C10_DECLARE_int(name) C10_DECLARE_typed_var(int, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name) C10_DECLARE_typed_var(int64_t, name)
// #define C10_DECLARE_double(name) C10_DECLARE_typed_var(double, name)
// #define C10_DECLARE_bool(name) C10_DECLARE_typed_var(bool, name)
// #define C10_DECLARE_string(name) C10_DECLARE_typed_var(std::string, name)

////////////////////////////////////////////////////////////////////////////////
// End non-gflags section.
////////////////////////////////////////////////////////////////////////////////

// #endif // C10_USE_GFLAGS

// #endif // C10_UTIL_FLAGS_H_


// Parsed from c10/core/impl/LocalDispatchKeySet.h

// #pragma once

// #include <c10/core/DispatchKeySet.h>
// #include <c10/macros/Export.h>

// TLS management for DispatchKeySet (the "local" DispatchKeySet(s))
//
// This manages two thread-local DispatchKeySets:
//
//  - The included type set, which adds a tensor type for consideration
//    in dispatch.  (For example, you might add Profiling to
//    the included type set to turn on profiling on all tensor operations.)
//
//  - The excluded type set, which disqualifies a tensor type from dispatch.
//    (For example, after redispatching on variable, we disqualify
//    Autograd so we don't attempt to handle variable again.)
//    (Exclusion wins over inclusion.)
//
// NB: Originally, I implemented the excluded type set as storing the inverted
// set, but TLS is defined to be zero-initialized, so this doesn't actually work
// (if it's inverted, you want the set to be -1 initialized).
// Targeting ../PODLocalDispatchKeySet.java


// Targeting ../LocalDispatchKeySet.java



// thread_local variables cannot be C10_API on Windows.
// Inlining this seems to break AutoDispatchBelowAutograd on Android.
// #if defined(_MSC_VER) || defined(C10_ANDROID) || defined(C10_IPHONE)
@Namespace("c10::impl") public static native @ByVal LocalDispatchKeySet tls_local_dispatch_key_set();
// #else // defined(_MSC_VER) || defined(C10_ANDROID) || defined(C10_IPHONE)

// #endif // defined(_MSC_VER) || defined(C10_ANDROID) || defined(C10_IPHONE)

// Internal, use ThreadLocalStateGuard

// Targeting ../IncludeDispatchKeyGuard.java


// Targeting ../ForceDispatchKeyGuard.java



// Non-RAII API for manipulating the thread-local dispatch state.
// Please prefer the RAII API.  The non-RAII API may be useful when
// the included/excluded state of a given DispatchKey must span
// many calls from the Python to the C++, so you cannot conveniently
// use an RAII guard.
//
// Example use case:  a Python context manager that includes a certain
// DispatchKey, to ensure ops running under the context manager dispatch
// through that DispatchKey's registered overrides.
//
// The non-RAII API is less efficient than the RAII guards because both the
// getter and setter will do a tls_getaddr lookup (the RAII struct only needs
// one!)

@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_key_excluded(DispatchKey x);
@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_key_excluded(@Cast("c10::DispatchKey") short x);
@Namespace("c10::impl") public static native void tls_set_dispatch_key_excluded(DispatchKey x, @Cast("bool") boolean desired_state);
@Namespace("c10::impl") public static native void tls_set_dispatch_key_excluded(@Cast("c10::DispatchKey") short x, @Cast("bool") boolean desired_state);
@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_key_included(DispatchKey x);
@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_key_included(@Cast("c10::DispatchKey") short x);
@Namespace("c10::impl") public static native void tls_set_dispatch_key_included(DispatchKey x, @Cast("bool") boolean desired_state);
@Namespace("c10::impl") public static native void tls_set_dispatch_key_included(@Cast("c10::DispatchKey") short x, @Cast("bool") boolean desired_state);
@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_keyset_excluded(@ByVal DispatchKeySet ks);
@Namespace("c10::impl") public static native @Cast("bool") boolean tls_is_dispatch_keyset_included(@ByVal DispatchKeySet ks);

 // namespace impl
 // namespace c10


// Parsed from c10/core/InferenceMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Export.h>
// Targeting ../InferenceMode.java


 // namespace c10


// Parsed from c10/core/SymIntArrayRef.h

// #pragma once

// #include <c10/core/SymInt.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

@Namespace("c10") public static native @ByVal LongArrayRef asIntArrayRefUnchecked(@ByVal SymIntArrayRef ar);

// TODO: a SymIntArrayRef containing a heap allocated large negative integer
// can actually technically be converted to an IntArrayRef... but not with
// the non-owning API we have here.  We can't reinterpet cast; we have to
// allocate another buffer and write the integers into it.  If you need it,
// we can do it.  But I don't think you need it.

@Namespace("c10") public static native @ByVal LongArrayRefOptional asIntArrayRefSlowOpt(
    @ByVal SymIntArrayRef ar);



// #define C10_AS_INTARRAYREF_SLOW(a) c10::asIntArrayRefSlow(a, __FILE__, __LINE__)

// Prefer using a more semantic constructor, like
// fromIntArrayRefKnownNonNegative
@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefUnchecked(@ByVal LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefUnchecked(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... array_ref);

@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefKnownNonNegative(@ByVal LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefKnownNonNegative(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... array_ref);

@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefSlow(@ByVal LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntArrayRef fromIntArrayRefSlow(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... array_ref);

 // namespace c10


// Parsed from c10/core/DefaultDtype.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/macros/Export.h>
 // namespace caffe2
@Namespace("c10") public static native void set_default_dtype(@ByVal TypeMeta dtype);
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_dtype();
@Namespace("c10") public static native ScalarType get_default_dtype_as_scalartype();
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_complex_dtype();
 // namespace c10


// Parsed from c10/core/TensorOptions.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/core/DefaultDtype.h>
// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>

// #include <c10/macros/Macros.h>
// #include <c10/util/Optional.h>

// #include <iosfwd>
// #include <utility>

@Namespace("c10") public static native DispatchKey computeDispatchKey(
    @ByVal ScalarTypeOptional dtype,
    @ByVal LayoutOptional layout,
    @ByVal DeviceOptional device);

@Namespace("c10") public static native ScalarType dtype_or_default(@ByVal ScalarTypeOptional dtype);

@Namespace("c10") public static native @ByVal TypeMeta dtype_or_default(
    @ByVal TypeMetaOptional dtype);

@Namespace("c10") public static native Layout layout_or_default(@ByVal LayoutOptional layout);

@Namespace("c10") public static native @ByVal Device device_or_default(@ByVal DeviceOptional device);


///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
@Namespace("c10") public static native @Cast("bool") boolean pinned_memory_or_default(@ByVal BoolOptional pinned_memory);
// Targeting ../TensorOptions.java



// We should aspire to fit in one machine-size word; but a size greater than two
// words is too much.  (We are doing terribly on 32-bit archs, where we require
// three machine size words to store tensor options.  Eek!)

/** Convenience function that returns a {@code TensorOptions} object with the {@code dtype}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions dtype(@ByVal TypeMeta dtype);

// legacy function to support ScalarType
@Namespace("c10") public static native @ByVal TensorOptions dtype(ScalarType dtype);

/** Convenience function that returns a {@code TensorOptions} object with the {@code layout}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions layout(Layout layout);
@Namespace("c10") public static native @ByVal TensorOptions layout(@Cast("c10::Layout") byte layout);

/** Convenience function that returns a {@code TensorOptions} object with the {@code device}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device(@ByVal Device device);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code device} set to CUDA and the {@code device_index} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device_index(short device_index);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code requires_grad} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions requires_grad(@Cast("bool") boolean requires_grad/*=true*/);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code memory_format} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions memory_format(MemoryFormat memory_format);
@Namespace("c10") public static native @ByVal TensorOptions memory_format(@Cast("c10::MemoryFormat") byte memory_format);

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef TensorOptions options);

@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef TensorOptions options);

// This is intended to be a centralized location by which we can determine
// what an appropriate DispatchKey for a tensor is.

@Namespace("c10") public static native Layout dispatchKeyToLayout(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::Layout") byte dispatchKeyToLayout(@Cast("c10::DispatchKey") short dispatch_key);

@Namespace("c10") public static native DeviceType dispatchKeyToDeviceType(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte dispatchKeyToDeviceType(@Cast("c10::DispatchKey") short dispatch_key);

@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(DispatchKey dispatch_key);
@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(@Cast("c10::DispatchKey") short dispatch_key);
@Namespace("c10::detail") public static native @Cast("bool") boolean backend_supports_empty_operator(@Const @ByRef TensorOptions options);

 // namespace detail

 // namespace c10


// Parsed from c10/core/WrapDimMinimal.h

// #pragma once

// #include <c10/core/SymInt.h>
// This template can only be specialized at int64_t and c10::SymInt;
// you'll get linker errors otherwise
 // namespace detail

@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr);

@Namespace("c10") public static native @ByVal SymInt maybe_wrap_dim(
    @ByVal SymInt dim,
    @ByVal SymInt dim_post_expr,
    @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @ByVal SymInt maybe_wrap_dim(
    @ByVal SymInt dim,
    @ByVal SymInt dim_post_expr);

 // namespace c10


// Parsed from c10/core/impl/HermeticPyObjectTLS.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <atomic>
// Targeting ../HermeticPyObjectTLS.java



 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/PyInterpreter.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/macros/Export.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/python_stub.h>
// #include <string>
// #include <vector>

// Forward declarations
 // namespace c10

 // namespace torch

// Actual implementation
// Targeting ../PyInterpreterVTable.java


// Targeting ../PyInterpreter.java



// PyInterpreterStatus describes what the state of its interpreter tag
// is, relative to the thread currently holding the GIL.
@Namespace("c10::impl") public enum PyInterpreterStatus {
  // We just allocated the Tensor, it hasn't escaped to other threads,
  // we know that it definitely hasn't been tagged to be associated
  // with an interpreter.
  DEFINITELY_UNINITIALIZED(0),
  // We queried the interpreter field and it looked uninitialized.  But
  // another thread may have raced with us to tag it with some other
  // interpreter id.  So we will have to do a CEX to make sure we can
  // actually nab it.
  MAYBE_UNINITIALIZED(1),
  // We queried the interpreter field and it was tagged to belong to us.
  // This means we have sole write access (as we hold the GIL for this
  // interpreter)
  TAGGED_BY_US(2),
  // Someone else tagged this.  We can't use this TensorImpl from Python.
  TAGGED_BY_OTHER(3);

    public final int value;
    private PyInterpreterStatus(int v) { this.value = v; }
    private PyInterpreterStatus(PyInterpreterStatus e) { this.value = e.value; }
    public PyInterpreterStatus intern() { for (PyInterpreterStatus e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/PyObjectSlot.h

// #pragma once

// #include <c10/core/impl/HermeticPyObjectTLS.h>
// #include <c10/core/impl/PyInterpreter.h>
// #include <c10/util/Optional.h>
// #include <c10/util/python_stub.h>

// #include <atomic>

 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/SizesAndStrides.h

// #pragma once

// #include <algorithm>
// #include <cstdint>

// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/SmallVector.h>

public static final int C10_SIZES_AND_STRIDES_MAX_INLINE_SIZE = 5;
// Targeting ../SizesAndStrides.java



 // namespace impl
 // namespace c10


// Parsed from c10/util/DimVector.h

// #pragma once

// #include <c10/core/SymInt.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/util/SmallVector.h>
// #include <cstdint>

@Namespace("c10") @MemberGetter public static native @Cast("const size_t") long kDimVectorStaticSize();

/** A container for sizes or strides */

 // namespace c10


// Parsed from c10/util/Logging.h

// #ifndef C10_UTIL_LOGGING_H_
// #define C10_UTIL_LOGGING_H_

// #include <climits>
// #include <exception>
// #include <functional>
// #include <limits>
// #include <sstream>

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/StringUtil.h>

// CAFFE2_LOG_THRESHOLD is a compile time flag that would allow us to turn off
// logging at compile time so no logging message below that level is produced
// at all. The value should be between INT_MIN and CAFFE_FATAL.
// #ifndef CAFFE2_LOG_THRESHOLD
// If we have not defined the compile time log threshold, we keep all the
// log cases.
public static native @MemberGetter int CAFFE2_LOG_THRESHOLD();
public static final int CAFFE2_LOG_THRESHOLD = CAFFE2_LOG_THRESHOLD();
// #endif // CAFFE2_LOG_THRESHOLD

// Below are different implementations for glog and non-glog cases.
// #ifdef C10_USE_GLOG
// #include <c10/util/logging_is_google_glog.h>
// #else // !C10_USE_GLOG
// #include <c10/util/logging_is_not_google_glog.h>
// #endif // C10_USE_GLOG




// Some versions of GLOG support less-spammy version of LOG_EVERY_MS. If it's
// not available - just short-circuit to the always working one one.
// We define the C10_ name to avoid confusing other files
// #ifdef LOG_EVERY_MS
// #define C10_LOG_EVERY_MS(severity, ms) LOG_EVERY_MS(severity, ms)
// #else
// #define C10_LOG_EVERY_MS(severity, ms) LOG(severity)
// #endif

// Same for LOG_FIRST_N
// #ifdef LOG_FIRST_N
// #define C10_LOG_FIRST_N(severity, n) LOG_FIRST_N(severity, n)
// #else
// #define C10_LOG_FIRST_N(severity, n) LOG(severity)
// #endif

// Same for LOG_EVERY_N
// #ifdef LOG_EVERY_N
// #define C10_LOG_EVERY_N(severity, n) LOG_EVERY_N(severity, n)
// #else
// #define C10_LOG_EVERY_N(severity, n) LOG(severity)
// #endif

// Functions that we use for initialization.
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") PointerPointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") @ByPtrPtr BytePointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntBuffer argc, @Cast("char**") @ByPtrPtr ByteBuffer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(int[] argc, @Cast("char**") @ByPtrPtr byte[] argv);
@Namespace("c10") public static native void UpdateLoggingLevelsFromFlags();

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3);

@Namespace("c10") public static native @Cast("const bool") boolean IsUsingGoogleLogging();

/**
 * A utility to allow one to show log info to stderr after the program starts.
 *
 * This is similar to calling GLOG's --logtostderr, or setting caffe2_log_level
 * to smaller than INFO. You are recommended to only use this in a few sparse
 * cases, such as when you want to write a tutorial or something. Normally, use
 * the commandline flags to set the log level.
 */
@Namespace("c10") public static native void ShowLogInfoToStderr();

@Namespace("c10") public static native void SetStackTraceFetcher(@ByVal StringSupplier fetcher);

// #define CAFFE_ENFORCE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_FINITE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceFiniteNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_WITH_CALLER(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__), this);
//     }
//   } while (false)

// #define CAFFE_THROW(...)
//   ::c10::ThrowEnforceNotMet(__FILE__, __LINE__, "", ::c10::str(__VA_ARGS__))

/**
 * Rich logging messages
 *
 * CAFFE_ENFORCE_THAT can be used with one of the "checker functions" that
 * capture input argument values and add it to the exception message. E.g.
 * {@code CAFFE_ENFORCE_THAT(Equals(foo(x), bar(y)), "Optional additional message")}
 * would evaluate both foo and bar only once and if the results are not equal -
 * include them in the exception message.
 *
 * Some of the basic checker functions like Equals or Greater are already
 * defined below. Other header might define customized checkers by adding
 * functions to caffe2::enforce_detail namespace. For example:
 *
 *   namespace caffe2 { namespace enforce_detail {
 *   inline EnforceFailMessage IsVector(const vector<int64_t>& shape) {
 *     if (shape.size() == 1) { return EnforceOK(); }
 *     return c10::str("Shape ", shape, " is not a vector");
 *   }
 *   }}
 *
 * With further usages like {@code CAFFE_ENFORCE_THAT(IsVector(Input(0).dims()))}
 *
 * Convenient wrappers for binary operations like CAFFE_ENFORCE_EQ are provided
 * too. Please use them instead of TORCH_CHECK_EQ and friends for failures in
 * user-provided input.
 */

// GCC7 is getting an internal compiler error on the new
// implementation, so keep the old one (which evaluates the error
// message eagerly and therefore is undesirable for general use
// compared to the new one) around for it.
// #if defined(__GNUG__) && __GNUC__ <= 7 && !defined(__clang__)

// #define CAFFE_ENFORCE_THAT_IMPL(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, lhs, rhs, __FILE__, __LINE__, expr, nullptr, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, (lhs), (rhs), __FILE__, __LINE__, expr, this, ##__VA_ARGS__)

// #else

// #define CAFFE_ENFORCE_THAT_IMPL(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op,
//       (lhs),
//       (rhs),
//       __FILE__,
//       __LINE__,
//       expr,
//       nullptr,
//       [&](const auto& arg1, const auto& arg2) {
//         return ::c10::enforce_detail::enforceFailMsgImpl(
//             arg1, arg2, ##__VA_ARGS__);
//       })

// #define CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op,
//       (lhs),
//       (rhs),
//       __FILE__,
//       __LINE__,
//       expr,
//       this,
//       [&](const auto& arg1, const auto& arg2) {
//         return ::c10::enforce_detail::enforceFailMsgImpl(
//             arg1, arg2, ##__VA_ARGS__);
//       })
// #endif

 // namespace enforce_detail

// #define CAFFE_ENFORCE_THAT(cmp, op, lhs, rhs, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, lhs, rhs, #lhs " " #op " " #rhs, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater<void>(), >, x, y, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater<void>(), >, x, y, ##__VA_ARGS__)

/**
 * Very lightweight logging for the first time API usage. It's beneficial for
 * tracking of individual functionality usage in larger applications.
 *
 * In order to ensure light-weightedness of logging, we utilize static variable
 * trick - LogAPIUsage will be invoked only once and further invocations will
 * just do an atomic check.
 *
 * Example:
 *   // Logs caller info with an arbitrary text event, if there is a usage.
 *   C10_LOG_API_USAGE_ONCE("my_api");
 */
// #define C10_LOG_API_USAGE_ONCE(...)
//   C10_UNUSED static bool C10_ANONYMOUS_VARIABLE(logFlag) =
//       ::c10::detail::LogAPIUsageFakeReturn(__VA_ARGS__);

// API usage logging capabilities
@Namespace("c10") public static native void SetAPIUsageLogger(@ByVal StringConsumer logger);
@Namespace("c10") public static native void LogAPIUsage(@StdString BytePointer context);
@Namespace("c10") public static native void LogAPIUsage(@StdString String context);

@Namespace("c10") public static native void SetAPIUsageMetadataLogger(
    @ByVal MetadataLogger logger);
@Namespace("c10") public static native void LogAPIUsageMetadata(
    @StdString BytePointer context,
    @Const @ByRef StringStringMap metadata_map);
@Namespace("c10") public static native void LogAPIUsageMetadata(
    @StdString String context,
    @Const @ByRef StringStringMap metadata_map);
// Targeting ../DDPLoggingData.java



@Namespace("c10") public static native void SetPyTorchDDPUsageLogger(
    @ByVal DDPLogger logger);
@Namespace("c10") public static native void LogPyTorchDDPUsage(@Const @ByRef DDPLoggingData ddpData);
// Return value is needed to do the static variable initialization trick
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString BytePointer context);
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString String context);
 // namespace detail

// Initializes the c10 logger.
@Namespace("c10") public static native void initLogging();

 // namespace c10

// #endif // C10_UTIL_LOGGING_H_


// Parsed from c10/util/accumulate.h

// Copyright 2004-present Facebook. All Rights Reserved.

// #pragma once

// #include <c10/util/ArrayRef.h>

// #include <iterator>
// #include <numeric>
// #include <type_traits>

/** Sum of a list of integers; accumulates into the int64_t datatype */

/** Sum of integer elements referred to by iterators; accumulates into the
 *  int64_t datatype */

/** Product of a list of integers; accumulates into the int64_t datatype */

/** Product of integer elements referred to by iterators; accumulates into the
 *  int64_t datatype */

/** Return product of all dimensions starting from k
 *  Returns 1 if k>=dims.size() */

/** Product of all dims up to k (not including dims[k])
 *  Throws an error if k>dims.size() */

/** Product of all dims between k and l (including dims[k] and excluding
 *  dims[l]) k and l may be supplied in either order */

 // namespace c10


// Parsed from c10/util/safe_numerics.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>

// #include <iterator>
// #include <numeric>
// #include <type_traits>

// GCC has __builtin_mul_overflow from before it supported __has_builtin
// #ifdef _MSC_VER
// #define C10_HAS_BUILTIN_OVERFLOW() (0)
// #include <c10/util/llvmMathExtras.h>
// #include <intrin.h>
// #else
// #define C10_HAS_BUILTIN_OVERFLOW() (1)
// #endif

@Namespace("c10") public static native @Cast("bool") boolean add_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") LongPointer out);
@Namespace("c10") public static native @Cast("bool") boolean add_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") LongBuffer out);
@Namespace("c10") public static native @Cast("bool") boolean add_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") long[] out);

@Namespace("c10") public static native @Cast("bool") boolean mul_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") LongPointer out);
@Namespace("c10") public static native @Cast("bool") boolean mul_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") LongBuffer out);
@Namespace("c10") public static native @Cast("bool") boolean mul_overflows(@Cast("uint64_t") long a, @Cast("uint64_t") long b, @Cast("uint64_t*") long[] out);

 // namespace c10


// Parsed from c10/core/TensorImpl.h

// #pragma once

// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/InferenceMode.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/SymBool.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/core/impl/PyObjectSlot.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/Optional.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/irange.h>
// #include <c10/util/safe_numerics.h>
// #include <c10/util/typeid.h>

// #include <algorithm>
// #include <atomic>
// #include <limits>
// #include <memory>
// #include <type_traits>
// #include <utility>

// A global boolean variable to control whether we free memory when a Tensor
// is shrunk to a smaller size. As a result, a Tensor is always going to
// keep the memory allocated for its maximum capacity reshaped to so far.
//
// This parameter is respected "upper-case" methods which call Resize()
// (e.g., CopyFrom, ResizeLike); it is NOT respected by Tensor::resize_
// or ShrinkTo, both of which guarantee to never to free memory.


// Since we can have high variance in blob memory allocated across different
// inputs in the same run, we will shrink the blob only if the memory gain
// is larger than this flag in bytes.  This only applies to functions which
// respect caffe2_keep_on_shrink.


// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif
 // namespace at

/**
 * A utility function to convert vector<int> to vector<int64_t>.
 */
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector ToVectorint64_t(@Const @ByRef IntArrayRef src);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector ToVectorint64_t(@ByRef @Cast({"jint*", "c10::ArrayRef<jint>", "std::vector<jint>&"}) @StdVector("jint") int... src);

/**
 * Return product of all dimensions starting from k
 */
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// Product of all dims up to k (not including dims[k])
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// Product of all dims between k and l (not including dims[k] and dims[l])
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// Wrap around axis_index if it is negative, s.t., -1 is the last dim
@Namespace("c10") public static native int canonical_axis_index_(int axis_index, int ndims);
// Targeting ../PlacementDeleteContext.java


// Targeting ../AutogradMetaInterface.java


// Targeting ../AutogradMetaFactory.java



@Namespace("c10::impl") public static native void SetAutogradMetaFactory(AutogradMetaFactory factory);
@Namespace("c10::impl") public static native AutogradMetaFactory GetAutogradMetaFactory();
// Targeting ../AutogradMetaFactoryRegisterer.java




// Targeting ../NamedTensorMetaInterface.java


// Targeting ../BackendMeta.java


// Targeting ../SymbolicShapeMeta.java


// Targeting ../VariableVersion.java



// Forward declaration of TensorImpl needed for forward declaration of
// C10_TensorImpl_Size_Check_Dummy_Class

/**
 * NOTE: Some TensorImpl methods are small and not overridden in the
 * PyTorch codebase itself, but may theoretically need to be
 * overridden by third-party TensorImpl subclasses. This macro allows
 * users that need maximum performance and don't need these extension
 * points to disable them with a build-time flag. (In particular,
 * XLA's XLATensorImpl currently overrides these methods, so we can't
 * enable this flag by default.)
 */
// #ifdef C10_DISABLE_TENSORIMPL_EXTENSIBILITY
// #define TENSORIMPL_MAYBE_VIRTUAL
// #else
// #define TENSORIMPL_MAYBE_VIRTUAL virtual
// Targeting ../TensorImpl.java



// Note [TensorImpl size constraints]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Changed the size of TensorImpl?  If the size went down, good for
// you!  Adjust the documentation below and the expected size.
// Did it go up?  Read on...
//
// Struct size matters.  In some production systems at Facebook, we have
// 400M live tensors during a training run.  Do the math: every 64-bit
// word you add to Tensor is an extra 3.2 gigabytes in RAM.
//
// If you are a Facebook employee, you can check if the run in question
// has tipped you over the point using the command here:
// https://fburl.com/q5enpv98
//
// For reference, we OOMed at 160 bytes (20 words) per TensorImpl.
// This is not counting overhead from strides out-of-line allocation and
// StorageImpl space and this is from before we inlined sizes and strides
// directly into TensorImpl as SmallVectors.
//
// Our memory usage on 32-bit systems is suboptimal, but we're not checking
// for it at the moment (to help avoid rage inducing cycles when the
// 32-bit number is wrong).
//
// Current breakdown:
//
//    vtable pointer
//    strong refcount           TODO: pack these into one word
//    weak refcount
//    storage pointer
//    autograd metadata pointer
//    named tensor metadata pointer
//    version counter pointer
//    PyObjectSlot
//    SizesAndStrides size/pointer
//    SizesAndStrides sizes (pre-allocated 0)
//    SizesAndStrides sizes (pre-allocated 1)
//    SizesAndStrides sizes (pre-allocated 2)
//    SizesAndStrides sizes (pre-allocated 3)
//    SizesAndStrides sizes (pre-allocated 4)
//    SizesAndStrides strides (pre-allocated 0)
//    SizesAndStrides strides (pre-allocated 1)
//    SizesAndStrides strides (pre-allocated 2)
//    SizesAndStrides strides (pre-allocated 3)
//    SizesAndStrides strides (pre-allocated 4)
//    storage offset
//    numel
//    data type, device, is_contiguous, storage_access_should_throw_, bitfields
//    DispatchKeySet
//

// Various preprocessor macros we use to check that the
// TensorImpl size hasn't changed unexpectedly. We undef
// these later.
// #ifndef __NVCC__
public static final int C10_NVCC = 0;
// #else
// #endif

// #ifndef __CUDA_VER_MAJOR__
public static final int C10_CUDA_VERSION_MAJOR = 0;
// #else
// #endif

// #ifndef CUDA_VERSION
public static final int C10_CUDA_VERSION = 0;
// #else
// #endif

// #ifndef __clang_major__
public static final int C10_CLANG_MAJOR_VERSION = 0;
// #else
// #endif

// #ifndef __GNUC__
public static final int C10_GCC_VERSION = 0;
// #else
// #endif

// #ifndef __GNUC_MINOR__
public static final int C10_GCC_VERSION_MINOR = 0;
// #else
// #endif

// We use a templatized class to both contain the logic of checking the sizes
// as well as to provide compile-time information that might be useful in
// figuring out why sizes may have changed.
// All the compile time information is given by the template fields that are
// always printed by the compiler when the static_assert fails.

// We use a class to encapsulate size-checking logic with
// templates to capture sizes and flags. We call this within
// a static assert to prove there is no run-time behaviour.
// Since the methods we call return either true or fail their
// own static_asserts, we should never see the error messages
// below. We have to provide it though for c++ <17.

// Clean up after ourselves
// #undef C10_NVCC
// #undef C10_CUDA_VERSION_MAJOR
// #undef C10_CUDA_VERSION
// #undef C10_CLANG_MAJOR_VERSION
// #undef C10_GCC_VERSION
// #undef C10_GCC_VERSION_MINOR

 // namespace c10



// Parsed from c10/core/UndefinedTensorImpl.h

// #pragma once

// #include <c10/core/TensorImpl.h>
// Targeting ../UndefinedTensorImpl.java



 // namespace c10


// Parsed from ATen/core/CheckMemoryFormat.h

// #include <c10/core/TensorOptions.h>

@Namespace("c10::impl") public static native @ByVal MemoryFormatOptional check_tensor_options_and_extract_memory_format(
    @Const @ByRef TensorOptions options,
    @ByVal MemoryFormatOptional memory_format);

 // namespace impl namespace c10


// Parsed from c10/core/GeneratorImpl.h

// #pragma once

// #include <stdint.h>
// #include <mutex>

// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/macros/Export.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/python_stub.h>

/**
 * Note [Generator]
 * ~~~~~~~~~~~~~~~~
 * A Pseudo Random Number Generator (PRNG) is an engine that uses an algorithm
 * to generate a seemingly random sequence of numbers, that may be later be used
 * in creating a random distribution. Such an engine almost always maintains a
 * state and requires a seed to start off the creation of random numbers. Often
 * times, users have found it beneficial to be able to explicitly create,
 * retain, and destroy PRNG states and also be able to have control over the
 * seed value.
 *
 * A Generator in ATen gives users the ability to read, write and modify a PRNG
 * engine. For instance, it does so by letting users seed a PRNG engine, fork
 * the state of the engine, etc.
 *
 * By default, there is one generator per device, and a device's generator is
 * lazily created. A user can use the torch.Generator() api to create their own
 * generator. Currently torch.Generator() can only create a CPUGeneratorImpl.
 */

/**
 * Note [Acquire lock when using random generators]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Generator and its derived classes are NOT thread-safe. Please note that most
 * of the places where we have inserted locking for generators are historically
 * based, and we haven't actually checked that everything is truly thread safe
 * (and it probably isn't). Please use the public mutex_ when using any methods
 * from these classes, except for the read-only methods. You can learn about the
 * usage by looking into the unittests (aten/src/ATen/cpu_generator_test.cpp)
 * and other places where we have used lock_guard.
 *
 * TODO: Look into changing the threading semantics of Generators in ATen (e.g.,
 * making them non-thread safe and instead making the generator state
 * splittable, to accommodate forks into other threads).
 */

// The default seed is selected to be a large number
// with good distribution of 0s and 1s in bit representation
@Namespace("c10") @MemberGetter public static native @Cast("const uint64_t") long default_rng_seed_val();
// Targeting ../GeneratorImpl.java



@Namespace("c10::detail") public static native @Cast("uint64_t") long getNonDeterministicRandom(@Cast("bool") boolean is_cuda/*=false*/);
@Namespace("c10::detail") public static native @Cast("uint64_t") long getNonDeterministicRandom();

 // namespace detail

 // namespace c10


// Parsed from ATen/core/Generator.h

// #pragma once

// #include <mutex>
// #include <deque>
// #include <atomic>
// #include <typeinfo>
// #include <utility>
// #include <cstddef>
// #include <cstdint>

// #include <c10/util/Exception.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>

// For the record I don't think this is a correct pimpl idiom.
// Including Impl header in interface header defeats the purpose
// because you can't change Impl private members without forcing
// everything that included the interface to rebuild.
// Impl should be forward-declared in the interface header instead.
// #include <c10/core/GeneratorImpl.h>
// Targeting ../Generator.java



@Namespace("at") public static native @ByVal @Name("make_generator<at::CPUGeneratorImpl>") Generator make_generator_cpu();
@Namespace("at") public static native @ByVal @Name("make_generator<at::CPUGeneratorImpl,uint64_t>") Generator make_generator_cpu(@Cast("uint64_t&&") long seed_in);

/**
 * Utility function to static cast input Generator* to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Utility function used in tensor implementations, which
 * supplies the default generator to tensors, if an input generator
 * is not supplied. The input Generator* is also static casted to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Helper function for checking the validity of new random generator
 * state. Right now following conditions are checked:
 *
 * - The new state tensor must be a torch.ByteTensor
 * - Data of the new state tensor must be contiguous
 */
@Namespace("at::detail") public static native void check_rng_state(@Const @ByRef TensorImpl new_state);

 // namespace detail

 // namespace at


// Parsed from ATen/core/symbol.h

// #pragma once
// #include <c10/macros/Export.h>
// #include <cstdint>
// #include <functional>  // For std::hash
// #include <string>

// 'prim' symbols are synthetic operators that occur only in the IR
// and don't have corresponding implementations in ATen.

// 'onnx' symbols correspond to ONNX operators.  Their semantics
// are defined in https://github.com/onnx/onnx/blob/master/docs/Operators.md
// The particular version we are targeting is specified by '_onnx_opset_version'
// in torch.onnx.symbolic_helper
//
// In general, most ONNX operators won't get an entry here, because they
// are handled from the Python end.  However, you may occasionally need
// to intern an ONNX symbol here so that you can conveniently write an
// optimization on ONNX operations.

// 'attr' symbols are attribute keys.  They are shared between both ONNX and ATen
// operators (you disambiguate their meaning by looking at the operator itself).
// In general, you only need to define attribute keys that are used by
// onnx or prim; ATen attributes are automatically generated in FORALL_ATTR_BASE_SYMBOLS.

// Note [Symbol allocation]
// ~~~~~~~~~~~~~~~~~~~~~~~~
//
//  1. Symbol namespace is split up into namespaces.
//
//  2. The intended access pattern for built-in symbols is onnx::MatMul
//  in the c10 namespace (this is a Symbol).
//

// Built-in constant definition strategy:
// - Enum is the most convenient way to generate a contiguous sequence
//   of numbers for an identifier.
// - However, an enum gives you a fresh type.  We want onnx::MatMul to
//   be type Symbol, not some random enum type!
// - Therefore, after using enums to generate the sequence of integers,
//   we then declare constexpr Symbols to get everything the actual Symbol
//   type we want.  Symbols must be constexpr to be valid to be "case"ed on.


// Targeting ../Symbol.java



@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@ByVal Symbol lhs, @ByVal Symbol rhs);











 // namespace c10

// make symbol behave like an integer in hash tables



// Parsed from ATen/core/Dimname.h

// #pragma once

// #include <ATen/core/symbol.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <ostream>

@Namespace("at") public enum NameType { BASIC((byte)(0)), WILDCARD((byte)(1));

    public final byte value;
    private NameType(byte v) { this.value = v; }
    private NameType(NameType e) { this.value = e.value; }
    public NameType intern() { for (NameType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Dimname.java



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Dimname dimname);

@Namespace("at") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

@Namespace("at") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

 // namespace at


// Parsed from ATen/core/NamedTensor.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/C++17.h>
// Targeting ../NamedTensorMeta.java


// Targeting ../NamesMode.java


// Targeting ../NoNamesGuard.java






// Sets the names of `tensor` to be `names`.
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @ByVal DimnameListOptional names);
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kMaxNamedTensorDim();



// Some helper functions on TensorImpl. Useful for working with names in TH.
// XXX: Ideally these would exist as methods on TensorImpl
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @ByVal DimnameListOptional names, @Cast("bool") boolean validate_names);
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);



// Returns true if the tensor's names exist and are not all 'None'.
// Returns false if the tensor's names don't exist (were not allocated),
// or if all names are 'None'.
// We treat not-allocated-names the same as allocated names that are all 'None'.
@Namespace("at::impl") public static native @Cast("bool") boolean has_names(@Const TensorImpl impl);

// Returns the names of the tensor's dimensions.
// Unnamed tensors are treated as having 'None' in all dimension; this method
// would return a DimnameList of all 'None's for an unnamed tensor.
@Namespace("at::impl") public static native @ByVal DimnameArrayRef get_names(@Const TensorImpl impl);

// This is more of an implementation detail; one should use impl::get_names /
// Tensor::names() whenever possible because it provides a cleaner API.
// Returns the names of the tensor if they have been allocated; returns nullopt
// instead if the haven't been. The names of a tensor are not allocated if a
// tensor is constructed with names=None.
@Namespace("at::impl") public static native @ByVal DimnameListOptional get_opt_names(@Const TensorImpl impl);

 // namespace impl

 // namespace at


// Parsed from ATen/core/QuantizerBase.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/core/QScheme.h>
// #include <c10/util/intrusive_ptr.h>
// Targeting ../QTensorImpl.java


// Targeting ../Quantizer.java



 // namespace at


// Parsed from ATen/core/TensorAccessor.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>
// #include <stdint.h>
// #include <cstddef>

// The PtrTraits argument to the TensorAccessor/GenericPackedTensorAccessor
// is used to enable the __restrict__ keyword/modifier for the data
// passed to cuda.

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// TensorAccessorBase and TensorAccessor are used for both CPU and CUDA tensors.
// For CUDA tensors it is used in device code (only). This means that we restrict ourselves
// to functions and types available there (e.g. IntArrayRef isn't).

// The PtrTraits argument is only relevant to cuda to support `__restrict__` pointers.

// The `TensorAccessor` is typically instantiated for CPU `Tensor`s using
// `Tensor.accessor<T, N>()`.
// For CUDA `Tensor`s, `GenericPackedTensorAccessor` is used on the host and only
// indexing on the device uses `TensorAccessor`s.


// GenericPackedTensorAccessorBase and GenericPackedTensorAccessor are used on for CUDA `Tensor`s on the host
// and as
// In contrast to `TensorAccessor`s, they copy the strides and sizes on instantiation (on the host)
// in order to transfer them on the device when calling kernels.
// On the device, indexing of multidimensional tensors gives to `TensorAccessor`s.
// Use RestrictPtrTraits as PtrTraits if you want the tensor's data pointer to be marked as __restrict__.
// Instantiation from data, sizes, strides is only needed on the host and std::copy isn't available
// on the device, so those functions are host only.


// Can't put this directly into the macro function args because of commas
// #define AT_X GenericPackedTensorAccessor<T, N, PtrTraits, index_t>

// Old name for `GenericPackedTensorAccessor`
// #undef AT_X
 // namespace at


// Parsed from c10/util/ExclusivelyOwnedTensorTraits.h

// #pragma once

// #include <c10/core/TensorImpl.h>

// #include <utility>
// Shared ExclusivelyOwnedTraits implementation between caffe2::Tensor and
// at::TensorBase.
 // namespace c10


// Parsed from ATen/StorageUtils.h

// #pragma once

// #include <c10/core/Storage.h>
// #include <c10/core/StorageImpl.h>
// #include <c10/util/intrusive_ptr.h>

// Here we define a series of utils to create/manipulate ATen backed
// c10 storage implementations.

/**
 * Create a new shared memory storage impl managed by file descriptor
 *
 * @param size  size in bytes
 */
@Namespace("at") public static native @ByVal StorageImplPtr new_shm_fd_storage(@Cast("size_t") long size);

/**
 * Copy src to dst
 * Caller must guarantee the validness of the storage objects
 * during the entire copy process, esp. when it's async.
 *
 * This can probably live in c10 namespace later if needed,
 * but for now keep it in at to keep implementation simple.
 *
 * @param dst  dst tensor
 * @param src  src tensor
 * @param non_blocking  (default false) whether this operation blocks caller
 */
@Namespace("at") public static native void storage_copy(
    @ByRef Storage dst,
    @Cast({"", "c10::Storage&&"}) @StdMove Storage src,
    @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native void storage_copy(
    @ByRef Storage dst,
    @Cast({"", "c10::Storage&&"}) @StdMove Storage src);

/**
 * In place change the storage to shm based.
 *
 * This is only applicable to CPU tensors not already shared.
 * Otherwise, it's a no op to mirror the THP tensor behavior:
 * https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html
 *
 * @param t  a tensor
 */
@Namespace("at") public static native void share_memory_(@ByRef TensorBase t);

 // namespace at


// Parsed from ATen/core/TensorBase.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ExclusivelyOwned.h>
// #include <c10/util/ExclusivelyOwnedTensorTraits.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>

// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <ATen/core/TensorAccessor.h>
// #include <ATen/StorageUtils.h>


 // namespace torch::autograd

// Convert Tensor to TensorBase without any need to include Tensor.h
@Namespace("at") public static native @Const @ByRef TensorBase get_tensor_base(@Const @ByRef Tensor t);
@Namespace("at::impl") public static native @Cast("bool") boolean variable_excluded_from_dispatch();


// Targeting ../TensorBase.java








// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

@Namespace("at") public static native DispatchKey legacyExtractDispatchKey(@Const @ByRef TensorBase t);

 // namespace at
 // namespace c10





 // namespace symint

 // namespace at


// Parsed from ATen/MethodOperators.h

// #pragma once

// @generated by torchgen/gen.py from MethodOperators.h

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// Forward declarations of any types needed in the operator signatures.
// We can't directly include these classes because it will cause circular include dependencies.
// This file is included by TensorBody.h, which defines the Tensor class.
// #include <ATen/core/ATen_fwd.h>

// #include <ATen/ops/_addmm_activation_ops.h>
// #include <ATen/ops/_autocast_to_full_precision_ops.h>
// #include <ATen/ops/_autocast_to_reduced_precision_ops.h>
// #include <ATen/ops/_backward_ops.h>
// #include <ATen/ops/_coalesced_ops.h>
// #include <ATen/ops/_conj_ops.h>
// #include <ATen/ops/_conj_physical_ops.h>
// #include <ATen/ops/_dimI_ops.h>
// #include <ATen/ops/_dimV_ops.h>
// #include <ATen/ops/_fw_primal_ops.h>
// #include <ATen/ops/_indices_ops.h>
// #include <ATen/ops/_is_all_true_ops.h>
// #include <ATen/ops/_is_any_true_ops.h>
// #include <ATen/ops/_is_zerotensor_ops.h>
// #include <ATen/ops/_neg_view_ops.h>
// #include <ATen/ops/_nested_tensor_size_ops.h>
// #include <ATen/ops/_nested_tensor_storage_offsets_ops.h>
// #include <ATen/ops/_nested_tensor_strides_ops.h>
// #include <ATen/ops/_nnz_ops.h>
// #include <ATen/ops/_reshape_alias_ops.h>
// #include <ATen/ops/_sparse_mask_projection_ops.h>
// #include <ATen/ops/_to_dense_ops.h>
// #include <ATen/ops/_to_sparse_bsc_ops.h>
// #include <ATen/ops/_to_sparse_bsr_ops.h>
// #include <ATen/ops/_to_sparse_csc_ops.h>
// #include <ATen/ops/_to_sparse_csr_ops.h>
// #include <ATen/ops/_to_sparse_ops.h>
// #include <ATen/ops/_values_ops.h>
// #include <ATen/ops/_version_ops.h>
// #include <ATen/ops/abs_ops.h>
// #include <ATen/ops/absolute_ops.h>
// #include <ATen/ops/acos_ops.h>
// #include <ATen/ops/acosh_ops.h>
// #include <ATen/ops/add_ops.h>
// #include <ATen/ops/addbmm_ops.h>
// #include <ATen/ops/addcdiv_ops.h>
// #include <ATen/ops/addcmul_ops.h>
// #include <ATen/ops/addmm_ops.h>
// #include <ATen/ops/addmv_ops.h>
// #include <ATen/ops/addr_ops.h>
// #include <ATen/ops/adjoint_ops.h>
// #include <ATen/ops/alias_ops.h>
// #include <ATen/ops/align_as_ops.h>
// #include <ATen/ops/align_to_ops.h>
// #include <ATen/ops/all_ops.h>
// #include <ATen/ops/allclose_ops.h>
// #include <ATen/ops/amax_ops.h>
// #include <ATen/ops/amin_ops.h>
// #include <ATen/ops/aminmax_ops.h>
// #include <ATen/ops/and_ops.h>
// #include <ATen/ops/angle_ops.h>
// #include <ATen/ops/any_ops.h>
// #include <ATen/ops/arccos_ops.h>
// #include <ATen/ops/arccosh_ops.h>
// #include <ATen/ops/arcsin_ops.h>
// #include <ATen/ops/arcsinh_ops.h>
// #include <ATen/ops/arctan2_ops.h>
// #include <ATen/ops/arctan_ops.h>
// #include <ATen/ops/arctanh_ops.h>
// #include <ATen/ops/argmax_ops.h>
// #include <ATen/ops/argmin_ops.h>
// #include <ATen/ops/argsort_ops.h>
// #include <ATen/ops/argwhere_ops.h>
// #include <ATen/ops/as_strided_ops.h>
// #include <ATen/ops/as_strided_scatter_ops.h>
// #include <ATen/ops/asin_ops.h>
// #include <ATen/ops/asinh_ops.h>
// #include <ATen/ops/atan2_ops.h>
// #include <ATen/ops/atan_ops.h>
// #include <ATen/ops/atanh_ops.h>
// #include <ATen/ops/baddbmm_ops.h>
// #include <ATen/ops/bernoulli_ops.h>
// #include <ATen/ops/bincount_ops.h>
// #include <ATen/ops/bitwise_and_ops.h>
// #include <ATen/ops/bitwise_left_shift_ops.h>
// #include <ATen/ops/bitwise_not_ops.h>
// #include <ATen/ops/bitwise_or_ops.h>
// #include <ATen/ops/bitwise_right_shift_ops.h>
// #include <ATen/ops/bitwise_xor_ops.h>
// #include <ATen/ops/bmm_ops.h>
// #include <ATen/ops/broadcast_to_ops.h>
// #include <ATen/ops/cauchy_ops.h>
// #include <ATen/ops/ccol_indices_ops.h>
// #include <ATen/ops/ceil_ops.h>
// #include <ATen/ops/chalf_ops.h>
// #include <ATen/ops/cholesky_inverse_ops.h>
// #include <ATen/ops/cholesky_ops.h>
// #include <ATen/ops/cholesky_solve_ops.h>
// #include <ATen/ops/chunk_ops.h>
// #include <ATen/ops/clamp_max_ops.h>
// #include <ATen/ops/clamp_min_ops.h>
// #include <ATen/ops/clamp_ops.h>
// #include <ATen/ops/clip_ops.h>
// #include <ATen/ops/clone_ops.h>
// #include <ATen/ops/coalesce_ops.h>
// #include <ATen/ops/col_indices_ops.h>
// #include <ATen/ops/conj_ops.h>
// #include <ATen/ops/conj_physical_ops.h>
// #include <ATen/ops/contiguous_ops.h>
// #include <ATen/ops/copy_ops.h>
// #include <ATen/ops/copysign_ops.h>
// #include <ATen/ops/corrcoef_ops.h>
// #include <ATen/ops/cos_ops.h>
// #include <ATen/ops/cosh_ops.h>
// #include <ATen/ops/count_nonzero_ops.h>
// #include <ATen/ops/cov_ops.h>
// #include <ATen/ops/cross_ops.h>
// #include <ATen/ops/crow_indices_ops.h>
// #include <ATen/ops/cummax_ops.h>
// #include <ATen/ops/cummin_ops.h>
// #include <ATen/ops/cumprod_ops.h>
// #include <ATen/ops/cumsum_ops.h>
// #include <ATen/ops/data_ops.h>
// #include <ATen/ops/deg2rad_ops.h>
// #include <ATen/ops/dense_dim_ops.h>
// #include <ATen/ops/dequantize_ops.h>
// #include <ATen/ops/det_ops.h>
// #include <ATen/ops/detach_ops.h>
// #include <ATen/ops/diag_embed_ops.h>
// #include <ATen/ops/diag_ops.h>
// #include <ATen/ops/diagflat_ops.h>
// #include <ATen/ops/diagonal_ops.h>
// #include <ATen/ops/diagonal_scatter_ops.h>
// #include <ATen/ops/diff_ops.h>
// #include <ATen/ops/digamma_ops.h>
// #include <ATen/ops/dist_ops.h>
// #include <ATen/ops/div_ops.h>
// #include <ATen/ops/divide_ops.h>
// #include <ATen/ops/dot_ops.h>
// #include <ATen/ops/dsplit_ops.h>
// #include <ATen/ops/eq_ops.h>
// #include <ATen/ops/equal_ops.h>
// #include <ATen/ops/erf_ops.h>
// #include <ATen/ops/erfc_ops.h>
// #include <ATen/ops/erfinv_ops.h>
// #include <ATen/ops/exp2_ops.h>
// #include <ATen/ops/exp_ops.h>
// #include <ATen/ops/expand_as_ops.h>
// #include <ATen/ops/expand_ops.h>
// #include <ATen/ops/expm1_ops.h>
// #include <ATen/ops/exponential_ops.h>
// #include <ATen/ops/fill_diagonal_ops.h>
// #include <ATen/ops/fill_ops.h>
// #include <ATen/ops/fix_ops.h>
// #include <ATen/ops/flatten_ops.h>
// #include <ATen/ops/flip_ops.h>
// #include <ATen/ops/fliplr_ops.h>
// #include <ATen/ops/flipud_ops.h>
// #include <ATen/ops/float_power_ops.h>
// #include <ATen/ops/floor_divide_ops.h>
// #include <ATen/ops/floor_ops.h>
// #include <ATen/ops/fmax_ops.h>
// #include <ATen/ops/fmin_ops.h>
// #include <ATen/ops/fmod_ops.h>
// #include <ATen/ops/frac_ops.h>
// #include <ATen/ops/frexp_ops.h>
// #include <ATen/ops/gather_ops.h>
// #include <ATen/ops/gcd_ops.h>
// #include <ATen/ops/ge_ops.h>
// #include <ATen/ops/geometric_ops.h>
// #include <ATen/ops/geqrf_ops.h>
// #include <ATen/ops/ger_ops.h>
// #include <ATen/ops/greater_equal_ops.h>
// #include <ATen/ops/greater_ops.h>
// #include <ATen/ops/gt_ops.h>
// #include <ATen/ops/hardshrink_backward_ops.h>
// #include <ATen/ops/hardshrink_ops.h>
// #include <ATen/ops/heaviside_ops.h>
// #include <ATen/ops/histc_ops.h>
// #include <ATen/ops/histogram_ops.h>
// #include <ATen/ops/hsplit_ops.h>
// #include <ATen/ops/hypot_ops.h>
// #include <ATen/ops/i0_ops.h>
// #include <ATen/ops/igamma_ops.h>
// #include <ATen/ops/igammac_ops.h>
// #include <ATen/ops/index_add_ops.h>
// #include <ATen/ops/index_copy_ops.h>
// #include <ATen/ops/index_fill_ops.h>
// #include <ATen/ops/index_ops.h>
// #include <ATen/ops/index_put_ops.h>
// #include <ATen/ops/index_reduce_ops.h>
// #include <ATen/ops/index_select_ops.h>
// #include <ATen/ops/indices_ops.h>
// #include <ATen/ops/inner_ops.h>
// #include <ATen/ops/int_repr_ops.h>
// #include <ATen/ops/inverse_ops.h>
// #include <ATen/ops/is_coalesced_ops.h>
// #include <ATen/ops/is_complex_ops.h>
// #include <ATen/ops/is_conj_ops.h>
// #include <ATen/ops/is_distributed_ops.h>
// #include <ATen/ops/is_floating_point_ops.h>
// #include <ATen/ops/is_inference_ops.h>
// #include <ATen/ops/is_leaf_ops.h>
// #include <ATen/ops/is_neg_ops.h>
// #include <ATen/ops/is_nonzero_ops.h>
// #include <ATen/ops/is_pinned_ops.h>
// #include <ATen/ops/is_same_size_ops.h>
// #include <ATen/ops/is_set_to_ops.h>
// #include <ATen/ops/is_signed_ops.h>
// #include <ATen/ops/isclose_ops.h>
// #include <ATen/ops/isfinite_ops.h>
// #include <ATen/ops/isinf_ops.h>
// #include <ATen/ops/isnan_ops.h>
// #include <ATen/ops/isneginf_ops.h>
// #include <ATen/ops/isposinf_ops.h>
// #include <ATen/ops/isreal_ops.h>
// #include <ATen/ops/istft_ops.h>
// #include <ATen/ops/item_ops.h>
// #include <ATen/ops/kron_ops.h>
// #include <ATen/ops/kthvalue_ops.h>
// #include <ATen/ops/lcm_ops.h>
// #include <ATen/ops/ldexp_ops.h>
// #include <ATen/ops/le_ops.h>
// #include <ATen/ops/lerp_ops.h>
// #include <ATen/ops/less_equal_ops.h>
// #include <ATen/ops/less_ops.h>
// #include <ATen/ops/lgamma_ops.h>
// #include <ATen/ops/log10_ops.h>
// #include <ATen/ops/log1p_ops.h>
// #include <ATen/ops/log2_ops.h>
// #include <ATen/ops/log_normal_ops.h>
// #include <ATen/ops/log_ops.h>
// #include <ATen/ops/log_softmax_ops.h>
// #include <ATen/ops/logaddexp2_ops.h>
// #include <ATen/ops/logaddexp_ops.h>
// #include <ATen/ops/logcumsumexp_ops.h>
// #include <ATen/ops/logdet_ops.h>
// #include <ATen/ops/logical_and_ops.h>
// #include <ATen/ops/logical_not_ops.h>
// #include <ATen/ops/logical_or_ops.h>
// #include <ATen/ops/logical_xor_ops.h>
// #include <ATen/ops/logit_ops.h>
// #include <ATen/ops/logsumexp_ops.h>
// #include <ATen/ops/lshift_ops.h>
// #include <ATen/ops/lt_ops.h>
// #include <ATen/ops/lu_solve_ops.h>
// #include <ATen/ops/mH_ops.h>
// #include <ATen/ops/mT_ops.h>
// #include <ATen/ops/masked_fill_ops.h>
// #include <ATen/ops/masked_scatter_ops.h>
// #include <ATen/ops/masked_select_ops.h>
// #include <ATen/ops/matmul_ops.h>
// #include <ATen/ops/matrix_H_ops.h>
// #include <ATen/ops/matrix_exp_ops.h>
// #include <ATen/ops/matrix_power_ops.h>
// #include <ATen/ops/max_ops.h>
// #include <ATen/ops/maximum_ops.h>
// #include <ATen/ops/mean_ops.h>
// #include <ATen/ops/median_ops.h>
// #include <ATen/ops/min_ops.h>
// #include <ATen/ops/minimum_ops.h>
// #include <ATen/ops/mm_ops.h>
// #include <ATen/ops/mode_ops.h>
// #include <ATen/ops/moveaxis_ops.h>
// #include <ATen/ops/movedim_ops.h>
// #include <ATen/ops/msort_ops.h>
// #include <ATen/ops/mul_ops.h>
// #include <ATen/ops/multinomial_ops.h>
// #include <ATen/ops/multiply_ops.h>
// #include <ATen/ops/mv_ops.h>
// #include <ATen/ops/mvlgamma_ops.h>
// #include <ATen/ops/nan_to_num_ops.h>
// #include <ATen/ops/nanmean_ops.h>
// #include <ATen/ops/nanmedian_ops.h>
// #include <ATen/ops/nanquantile_ops.h>
// #include <ATen/ops/nansum_ops.h>
// #include <ATen/ops/narrow_copy_ops.h>
// #include <ATen/ops/narrow_ops.h>
// #include <ATen/ops/ne_ops.h>
// #include <ATen/ops/neg_ops.h>
// #include <ATen/ops/negative_ops.h>
// #include <ATen/ops/new_empty_ops.h>
// #include <ATen/ops/new_empty_strided_ops.h>
// #include <ATen/ops/new_full_ops.h>
// #include <ATen/ops/new_ones_ops.h>
// #include <ATen/ops/new_zeros_ops.h>
// #include <ATen/ops/nextafter_ops.h>
// #include <ATen/ops/nonzero_numpy_ops.h>
// #include <ATen/ops/nonzero_ops.h>
// #include <ATen/ops/nonzero_static_ops.h>
// #include <ATen/ops/norm_ops.h>
// #include <ATen/ops/normal_ops.h>
// #include <ATen/ops/not_equal_ops.h>
// #include <ATen/ops/numpy_T_ops.h>
// #include <ATen/ops/or_ops.h>
// #include <ATen/ops/orgqr_ops.h>
// #include <ATen/ops/ormqr_ops.h>
// #include <ATen/ops/outer_ops.h>
// #include <ATen/ops/output_nr_ops.h>
// #include <ATen/ops/permute_ops.h>
// #include <ATen/ops/pin_memory_ops.h>
// #include <ATen/ops/pinverse_ops.h>
// #include <ATen/ops/polygamma_ops.h>
// #include <ATen/ops/positive_ops.h>
// #include <ATen/ops/pow_ops.h>
// #include <ATen/ops/prelu_ops.h>
// #include <ATen/ops/prod_ops.h>
// #include <ATen/ops/put_ops.h>
// #include <ATen/ops/q_per_channel_axis_ops.h>
// #include <ATen/ops/q_per_channel_scales_ops.h>
// #include <ATen/ops/q_per_channel_zero_points_ops.h>
// #include <ATen/ops/q_scale_ops.h>
// #include <ATen/ops/q_zero_point_ops.h>
// #include <ATen/ops/qr_ops.h>
// #include <ATen/ops/qscheme_ops.h>
// #include <ATen/ops/quantile_ops.h>
// #include <ATen/ops/rad2deg_ops.h>
// #include <ATen/ops/random_ops.h>
// #include <ATen/ops/ravel_ops.h>
// #include <ATen/ops/reciprocal_ops.h>
// #include <ATen/ops/record_stream_ops.h>
// #include <ATen/ops/refine_names_ops.h>
// #include <ATen/ops/relu_ops.h>
// #include <ATen/ops/remainder_ops.h>
// #include <ATen/ops/rename_ops.h>
// #include <ATen/ops/renorm_ops.h>
// #include <ATen/ops/repeat_interleave_ops.h>
// #include <ATen/ops/repeat_ops.h>
// #include <ATen/ops/requires_grad_ops.h>
// #include <ATen/ops/reshape_as_ops.h>
// #include <ATen/ops/reshape_ops.h>
// #include <ATen/ops/resize_as_ops.h>
// #include <ATen/ops/resize_as_sparse_ops.h>
// #include <ATen/ops/resize_ops.h>
// #include <ATen/ops/resolve_conj_ops.h>
// #include <ATen/ops/resolve_neg_ops.h>
// #include <ATen/ops/retain_grad_ops.h>
// #include <ATen/ops/retains_grad_ops.h>
// #include <ATen/ops/roll_ops.h>
// #include <ATen/ops/rot90_ops.h>
// #include <ATen/ops/round_ops.h>
// #include <ATen/ops/row_indices_ops.h>
// #include <ATen/ops/rshift_ops.h>
// #include <ATen/ops/rsqrt_ops.h>
// #include <ATen/ops/scatter_add_ops.h>
// #include <ATen/ops/scatter_ops.h>
// #include <ATen/ops/scatter_reduce_ops.h>
// #include <ATen/ops/select_ops.h>
// #include <ATen/ops/select_scatter_ops.h>
// #include <ATen/ops/set_data_ops.h>
// #include <ATen/ops/set_ops.h>
// #include <ATen/ops/sgn_ops.h>
// #include <ATen/ops/sigmoid_ops.h>
// #include <ATen/ops/sign_ops.h>
// #include <ATen/ops/signbit_ops.h>
// #include <ATen/ops/sin_ops.h>
// #include <ATen/ops/sinc_ops.h>
// #include <ATen/ops/sinh_ops.h>
// #include <ATen/ops/size_ops.h>
// #include <ATen/ops/slice_ops.h>
// #include <ATen/ops/slice_scatter_ops.h>
// #include <ATen/ops/slogdet_ops.h>
// #include <ATen/ops/smm_ops.h>
// #include <ATen/ops/softmax_ops.h>
// #include <ATen/ops/sort_ops.h>
// #include <ATen/ops/sparse_dim_ops.h>
// #include <ATen/ops/sparse_mask_ops.h>
// #include <ATen/ops/sparse_resize_and_clear_ops.h>
// #include <ATen/ops/sparse_resize_ops.h>
// #include <ATen/ops/split_ops.h>
// #include <ATen/ops/split_with_sizes_ops.h>
// #include <ATen/ops/sqrt_ops.h>
// #include <ATen/ops/square_ops.h>
// #include <ATen/ops/squeeze_ops.h>
// #include <ATen/ops/sspaddmm_ops.h>
// #include <ATen/ops/std_ops.h>
// #include <ATen/ops/stft_ops.h>
// #include <ATen/ops/stride_ops.h>
// #include <ATen/ops/sub_ops.h>
// #include <ATen/ops/subtract_ops.h>
// #include <ATen/ops/sum_ops.h>
// #include <ATen/ops/sum_to_size_ops.h>
// #include <ATen/ops/svd_ops.h>
// #include <ATen/ops/swapaxes_ops.h>
// #include <ATen/ops/swapdims_ops.h>
// #include <ATen/ops/t_ops.h>
// #include <ATen/ops/take_along_dim_ops.h>
// #include <ATen/ops/take_ops.h>
// #include <ATen/ops/tan_ops.h>
// #include <ATen/ops/tanh_ops.h>
// #include <ATen/ops/tensor_split_ops.h>
// #include <ATen/ops/tile_ops.h>
// #include <ATen/ops/to_dense_ops.h>
// #include <ATen/ops/to_mkldnn_ops.h>
// #include <ATen/ops/to_ops.h>
// #include <ATen/ops/to_padded_tensor_ops.h>
// #include <ATen/ops/to_sparse_bsc_ops.h>
// #include <ATen/ops/to_sparse_bsr_ops.h>
// #include <ATen/ops/to_sparse_csc_ops.h>
// #include <ATen/ops/to_sparse_csr_ops.h>
// #include <ATen/ops/to_sparse_ops.h>
// #include <ATen/ops/topk_ops.h>
// #include <ATen/ops/trace_ops.h>
// #include <ATen/ops/transpose_ops.h>
// #include <ATen/ops/triangular_solve_ops.h>
// #include <ATen/ops/tril_ops.h>
// #include <ATen/ops/triu_ops.h>
// #include <ATen/ops/true_divide_ops.h>
// #include <ATen/ops/trunc_ops.h>
// #include <ATen/ops/type_as_ops.h>
// #include <ATen/ops/unbind_ops.h>
// #include <ATen/ops/unflatten_ops.h>
// #include <ATen/ops/unfold_ops.h>
// #include <ATen/ops/uniform_ops.h>
// #include <ATen/ops/unsafe_chunk_ops.h>
// #include <ATen/ops/unsafe_split_ops.h>
// #include <ATen/ops/unsafe_split_with_sizes_ops.h>
// #include <ATen/ops/unsqueeze_ops.h>
// #include <ATen/ops/values_ops.h>
// #include <ATen/ops/var_ops.h>
// #include <ATen/ops/vdot_ops.h>
// #include <ATen/ops/view_as_ops.h>
// #include <ATen/ops/view_ops.h>
// #include <ATen/ops/vsplit_ops.h>
// #include <ATen/ops/where_ops.h>
// #include <ATen/ops/xlogy_ops.h>
// #include <ATen/ops/xor_ops.h>
// #include <ATen/ops/zero_ops.h>

 // namespace _ops
 // namespace at


// Parsed from ATen/core/TensorBody.h

// #pragma once

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/QScheme.h>
// #include <c10/core/Stream.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ExclusivelyOwned.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/OptionalArrayRef.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/macros/Export.h>
// #include <ATen/core/CheckMemoryFormat.h>
// #include <ATen/core/DeprecatedTypePropertiesRegistry.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <c10/core/SymInt.h>
// #include <ATen/core/TensorAccessor.h>
// #include <ATen/core/TensorBase.h>


// #include <ATen/MethodOperators.h>

 // namespace at
 // namespace indexing
 // namespace at

 // namespace torch::autograd
// Targeting ../Tensor.java


// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

 // namespace at

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()


// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()


// aten::data(Tensor self) -> Tensor


// aten::is_leaf(Tensor self) -> bool


// aten::output_nr(Tensor self) -> int


// aten::_version(Tensor self) -> int


// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)


// aten::retain_grad(Tensor(a!) self) -> ()


// aten::retains_grad(Tensor self) -> bool


// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)


// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)


// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)


// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)


// aten::align_as(Tensor self, Tensor other) -> Tensor


// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::abs(Tensor self) -> Tensor


// aten::abs_(Tensor(a!) self) -> Tensor(a!)


// aten::absolute(Tensor self) -> Tensor


// aten::absolute_(Tensor(a!) self) -> Tensor(a!)


// aten::angle(Tensor self) -> Tensor


// aten::sgn(Tensor self) -> Tensor


// aten::sgn_(Tensor(a!) self) -> Tensor(a!)


// aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor


// aten::_conj(Tensor(a) self) -> Tensor(a)


// aten::conj(Tensor(a) self) -> Tensor(a)


// aten::_conj_physical(Tensor self) -> Tensor


// aten::conj_physical(Tensor self) -> Tensor


// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)


// aten::resolve_conj(Tensor(a) self) -> Tensor(a)


// aten::resolve_neg(Tensor(a) self) -> Tensor(a)


// aten::_neg_view(Tensor(a) self) -> Tensor(a)


// aten::acos(Tensor self) -> Tensor


// aten::acos_(Tensor(a!) self) -> Tensor(a!)


// aten::arccos(Tensor self) -> Tensor


// aten::arccos_(Tensor(a!) self) -> Tensor(a!)


// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::_is_all_true(Tensor self) -> Tensor


// aten::_is_any_true(Tensor self) -> Tensor


// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool


// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::acosh(Tensor self) -> Tensor


// aten::acosh_(Tensor(a!) self) -> Tensor(a!)


// aten::arccosh(Tensor self) -> Tensor


// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)


// aten::asinh(Tensor self) -> Tensor


// aten::asinh_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsinh(Tensor self) -> Tensor


// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)


// aten::atanh(Tensor self) -> Tensor


// aten::atanh_(Tensor(a!) self) -> Tensor(a!)


// aten::arctanh(Tensor self) -> Tensor


// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)


// aten::asin(Tensor self) -> Tensor


// aten::asin_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsin(Tensor self) -> Tensor


// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)


// aten::atan(Tensor self) -> Tensor


// aten::atan_(Tensor(a!) self) -> Tensor(a!)


// aten::arctan(Tensor self) -> Tensor


// aten::arctan_(Tensor(a!) self) -> Tensor(a!)


// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor


// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor


// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor


// aten::bitwise_not(Tensor self) -> Tensor


// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)


// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor


// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor


// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::logical_not(Tensor self) -> Tensor


// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)


// aten::logical_xor(Tensor self, Tensor other) -> Tensor


// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_and(Tensor self, Tensor other) -> Tensor


// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_or(Tensor self, Tensor other) -> Tensor


// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bmm(Tensor self, Tensor mat2) -> Tensor


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::ceil(Tensor self) -> Tensor


// aten::ceil_(Tensor(a!) self) -> Tensor(a!)


// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]


// aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]


// aten::tensor_split.tensor_indices_or_sections(Tensor(a -> *) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]


// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::clamp_max(Tensor self, Scalar max) -> Tensor


// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor


// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)


// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)


// aten::clamp_min(Tensor self, Scalar min) -> Tensor


// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor


// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)


// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)


// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)


// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)


// aten::cos(Tensor self) -> Tensor


// aten::cos_(Tensor(a!) self) -> Tensor(a!)


// aten::cosh(Tensor self) -> Tensor


// aten::cosh_(Tensor(a!) self) -> Tensor(a!)


// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor


// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor


// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor


// aten::corrcoef(Tensor self) -> Tensor


// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor


// aten::diagflat(Tensor self, int offset=0) -> Tensor


// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)


// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)


// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)


// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor


// aten::div.Tensor(Tensor self, Tensor other) -> Tensor


// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::div.Scalar(Tensor self, Scalar other) -> Tensor


// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::dot(Tensor self, Tensor tensor) -> Tensor


// aten::vdot(Tensor self, Tensor other) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::erf(Tensor self) -> Tensor


// aten::erf_(Tensor(a!) self) -> Tensor(a!)


// aten::erfc(Tensor self) -> Tensor


// aten::erfc_(Tensor(a!) self) -> Tensor(a!)


// aten::exp(Tensor self) -> Tensor


// aten::exp_(Tensor(a!) self) -> Tensor(a!)


// aten::exp2(Tensor self) -> Tensor


// aten::exp2_(Tensor(a!) self) -> Tensor(a!)


// aten::expm1(Tensor self) -> Tensor


// aten::expm1_(Tensor(a!) self) -> Tensor(a!)


// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)


// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)


// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)


// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)


// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -> Tensor(a)


// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -> Tensor(a)


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -> Tensor(a)


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -> Tensor(a)


// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)


// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)


// aten::floor(Tensor self) -> Tensor


// aten::floor_(Tensor(a!) self) -> Tensor(a!)


// aten::floor_divide(Tensor self, Tensor other) -> Tensor


// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::frac(Tensor self) -> Tensor


// aten::frac_(Tensor(a!) self) -> Tensor(a!)


// aten::gcd(Tensor self, Tensor other) -> Tensor


// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lcm(Tensor self, Tensor other) -> Tensor


// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor


// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor


// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor


// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)


// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor


// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor


// aten::isnan(Tensor self) -> Tensor


// aten::is_distributed(Tensor self) -> bool


// aten::is_floating_point(Tensor self) -> bool


// aten::is_complex(Tensor self) -> bool


// aten::is_conj(Tensor self) -> bool


// aten::_is_zerotensor(Tensor self) -> bool


// aten::is_neg(Tensor self) -> bool


// aten::isreal(Tensor self) -> Tensor


// aten::is_nonzero(Tensor self) -> bool


// aten::is_same_size(Tensor self, Tensor other) -> bool


// aten::is_signed(Tensor self) -> bool


// aten::is_inference(Tensor self) -> bool


// aten::kron(Tensor self, Tensor other) -> Tensor


// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor


// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)


// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::log(Tensor self) -> Tensor


// aten::log_(Tensor(a!) self) -> Tensor(a!)


// aten::log10(Tensor self) -> Tensor


// aten::log10_(Tensor(a!) self) -> Tensor(a!)


// aten::log1p(Tensor self) -> Tensor


// aten::log1p_(Tensor(a!) self) -> Tensor(a!)


// aten::log2(Tensor self) -> Tensor


// aten::log2_(Tensor(a!) self) -> Tensor(a!)


// aten::logaddexp(Tensor self, Tensor other) -> Tensor


// aten::logaddexp2(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor


// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::logcumsumexp(Tensor self, int dim) -> Tensor


// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor


// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor


// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::matmul(Tensor self, Tensor other) -> Tensor


// aten::matrix_power(Tensor self, int n) -> Tensor


// aten::matrix_exp(Tensor self) -> Tensor


// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)


// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::median(Tensor self) -> Tensor


// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian(Tensor self) -> Tensor


// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mm(Tensor self, Tensor mat2) -> Tensor


// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor


// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor


// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor


// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor


// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::mv(Tensor self, Tensor vec) -> Tensor


// aten::mvlgamma(Tensor self, int p) -> Tensor


// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)


// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)


// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::numpy_T(Tensor(a) self) -> Tensor(a)


// aten::matrix_H(Tensor(a) self) -> Tensor(a)


// aten::mT(Tensor(a) self) -> Tensor(a)


// aten::mH(Tensor(a) self) -> Tensor(a)


// aten::adjoint(Tensor(a) self) -> Tensor(a)


// aten::is_pinned(Tensor self, Device? device=None) -> bool


// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)


// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor


// aten::rad2deg(Tensor self) -> Tensor


// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)


// aten::deg2rad(Tensor self) -> Tensor


// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)


// aten::ravel(Tensor(a) self) -> Tensor(a)


// aten::reciprocal(Tensor self) -> Tensor


// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)


// aten::neg(Tensor self) -> Tensor


// aten::neg_(Tensor(a!) self) -> Tensor(a!)


// aten::negative(Tensor self) -> Tensor


// aten::negative_(Tensor(a!) self) -> Tensor(a!)


// aten::repeat(Tensor self, SymInt[] repeats) -> Tensor


// aten::repeat(Tensor self, SymInt[] repeats) -> Tensor


// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)


// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::round(Tensor self) -> Tensor


// aten::round_(Tensor(a!) self) -> Tensor(a!)


// aten::round.decimals(Tensor self, *, int decimals) -> Tensor


// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)


// aten::relu(Tensor self) -> Tensor


// aten::relu_(Tensor(a!) self) -> Tensor(a!)


// aten::prelu(Tensor self, Tensor weight) -> Tensor


// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor


// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor


// aten::rsqrt(Tensor self) -> Tensor


// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)


// aten::sigmoid(Tensor self) -> Tensor


// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)


// aten::logit(Tensor self, float? eps=None) -> Tensor


// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)


// aten::sin(Tensor self) -> Tensor


// aten::sin_(Tensor(a!) self) -> Tensor(a!)


// aten::sinc(Tensor self) -> Tensor


// aten::sinc_(Tensor(a!) self) -> Tensor(a!)


// aten::sinh(Tensor self) -> Tensor


// aten::sinh_(Tensor(a!) self) -> Tensor(a!)


// aten::detach(Tensor(a) self) -> Tensor(a)


// aten::detach_(Tensor(a!) self) -> Tensor(a!)


// aten::size.Dimname(Tensor self, Dimname dim) -> int


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor


// aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -> Tensor


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor


// aten::smm(Tensor self, Tensor mat2) -> Tensor


// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]


// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::vsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::vsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::squeeze(Tensor(a) self) -> Tensor(a)


// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)


// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)


// aten::squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)


// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)


// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::squeeze_.dims(Tensor(a!) self, int[] dim) -> Tensor(a!)


// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)


// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor


// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor


// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor


// aten::stride.Dimname(Tensor self, Dimname dim) -> int


// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum_to_size(Tensor self, SymInt[] size) -> Tensor


// aten::sum_to_size(Tensor self, SymInt[] size) -> Tensor


// aten::sqrt(Tensor self) -> Tensor


// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::square(Tensor self) -> Tensor


// aten::square_(Tensor(a!) self) -> Tensor(a!)


// aten::std(Tensor self, bool unbiased=True) -> Tensor


// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor


// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor


// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::t(Tensor(a) self) -> Tensor(a)


// aten::t_(Tensor(a!) self) -> Tensor(a!)


// aten::tan(Tensor self) -> Tensor


// aten::tan_(Tensor(a!) self) -> Tensor(a!)


// aten::tanh(Tensor self) -> Tensor


// aten::tanh_(Tensor(a!) self) -> Tensor(a!)


// aten::tile(Tensor self, SymInt[] dims) -> Tensor


// aten::tile(Tensor self, SymInt[] dims) -> Tensor


// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)


// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::flip(Tensor self, int[] dims) -> Tensor


// aten::fliplr(Tensor self) -> Tensor


// aten::flipud(Tensor self) -> Tensor


// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor


// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor


// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor


// aten::_nested_tensor_size(Tensor self) -> Tensor


// aten::_nested_tensor_strides(Tensor self) -> Tensor


// aten::_nested_tensor_storage_offsets(Tensor self) -> Tensor


// aten::trunc(Tensor self) -> Tensor


// aten::trunc_(Tensor(a!) self) -> Tensor(a!)


// aten::fix(Tensor self) -> Tensor


// aten::fix_(Tensor(a!) self) -> Tensor(a!)


// aten::type_as(Tensor self, Tensor other) -> Tensor


// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)


// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::var(Tensor self, bool unbiased=True) -> Tensor


// aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor


// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor


// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor


// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor


// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor


// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor


// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor


// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)


// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor


// aten::positive(Tensor(a) self) -> Tensor(a)


// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)


// aten::zero_(Tensor(a!) self) -> Tensor(a!)


// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::heaviside(Tensor self, Tensor values) -> Tensor


// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)


// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -> Tensor


// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor


// aten::_sparse_mask_projection(Tensor self, Tensor mask, bool accumulate_matches=False) -> Tensor


// aten::to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -> Tensor


// aten::_to_dense(Tensor self, ScalarType? dtype=None, bool? masked_grad=None) -> Tensor


// aten::sparse_dim(Tensor self) -> int


// aten::_dimI(Tensor self) -> int


// aten::dense_dim(Tensor self) -> int


// aten::_dimV(Tensor self) -> int


// aten::_nnz(Tensor self) -> int


// aten::coalesce(Tensor(a) self) -> Tensor(a)


// aten::is_coalesced(Tensor self) -> bool


// aten::_indices(Tensor(a) self) -> Tensor(a)


// aten::_values(Tensor(a) self) -> Tensor(a)


// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)


// aten::indices(Tensor(a) self) -> Tensor(a)


// aten::values(Tensor(a) self) -> Tensor(a)


// aten::crow_indices(Tensor(a) self) -> Tensor(a)


// aten::col_indices(Tensor(a) self) -> Tensor(a)


// aten::ccol_indices(Tensor(a) self) -> Tensor(a)


// aten::row_indices(Tensor(a) self) -> Tensor(a)


// aten::unbind.int(Tensor(a -> *) self, int dim=0) -> Tensor(a)[]


// aten::unbind.Dimname(Tensor(a -> *) self, Dimname dim) -> Tensor(a)[]


// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor


// aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor


// aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor


// aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor


// aten::to_sparse_csr(Tensor self, int? dense_dim=None) -> Tensor


// aten::_to_sparse_csr(Tensor self, int? dense_dim=None) -> Tensor


// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor


// aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor


// aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::_to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::dequantize.self(Tensor self) -> Tensor


// aten::q_scale(Tensor self) -> float


// aten::q_zero_point(Tensor self) -> int


// aten::q_per_channel_scales(Tensor self) -> Tensor


// aten::q_per_channel_zero_points(Tensor self) -> Tensor


// aten::q_per_channel_axis(Tensor self) -> int


// aten::int_repr(Tensor self) -> Tensor


// aten::qscheme(Tensor self) -> QScheme


// aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -> Tensor(a)


// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::item(Tensor self) -> Scalar


// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)


// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)


// aten::set_(Tensor(a!) self) -> Tensor(a!)


// aten::is_set_to(Tensor self, Tensor tensor) -> bool


// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)


// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor


// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)


// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor


// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)


// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor


// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)


// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)


// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor


// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor(a!)


// aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor


// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor


// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)


// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor


// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor


// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor


// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor


// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)


// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor


// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)


// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor


// aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor(a!)


// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::digamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)


// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)


// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)


// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)


// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)


// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)


// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)


// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)


// aten::diag(Tensor self, int diagonal=0) -> Tensor


// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor


// aten::triu(Tensor self, int diagonal=0) -> Tensor


// aten::tril(Tensor self, int diagonal=0) -> Tensor


// aten::trace(Tensor self) -> Tensor


// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor


// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::le.Scalar(Tensor self, Scalar other) -> Tensor


// aten::le.Tensor(Tensor self, Tensor other) -> Tensor


// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::take(Tensor self, Tensor index) -> Tensor


// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor


// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor


// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor


// aten::masked_select(Tensor self, Tensor mask) -> Tensor


// aten::nonzero(Tensor self) -> Tensor


// aten::nonzero_static(Tensor self, *, int size, int fill_value=-1) -> Tensor


// aten::nonzero_numpy(Tensor self) -> Tensor[]


// aten::argwhere(Tensor self) -> Tensor


// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)


// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)


// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)


// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)


// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::cholesky(Tensor self, bool upper=False) -> Tensor


// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor


// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor


// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)


// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)


// aten::orgqr(Tensor self, Tensor input2) -> Tensor


// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor


// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor


// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor


// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lgamma(Tensor self) -> Tensor


// aten::digamma(Tensor self) -> Tensor


// aten::polygamma(int n, Tensor self) -> Tensor


// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)


// aten::erfinv(Tensor self) -> Tensor


// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)


// aten::i0(Tensor self) -> Tensor


// aten::i0_(Tensor(a!) self) -> Tensor(a!)


// aten::sign(Tensor self) -> Tensor


// aten::sign_(Tensor(a!) self) -> Tensor(a!)


// aten::signbit(Tensor self) -> Tensor


// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor


// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::atan2(Tensor self, Tensor other) -> Tensor


// aten::arctan2(Tensor self, Tensor other) -> Tensor


// aten::arctan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor


// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor


// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor


// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor


// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor


// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::hypot(Tensor self, Tensor other) -> Tensor


// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igamma(Tensor self, Tensor other) -> Tensor


// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igammac(Tensor self, Tensor other) -> Tensor


// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::nextafter(Tensor self, Tensor other) -> Tensor


// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor


// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor


// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::min(Tensor self) -> Tensor


// aten::fmin(Tensor self, Tensor other) -> Tensor


// aten::max(Tensor self) -> Tensor


// aten::fmax(Tensor self, Tensor other) -> Tensor


// aten::maximum(Tensor self, Tensor other) -> Tensor


// aten::max.other(Tensor self, Tensor other) -> Tensor


// aten::minimum(Tensor self, Tensor other) -> Tensor


// aten::min.other(Tensor self, Tensor other) -> Tensor


// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::msort(Tensor self) -> Tensor


// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor


// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor


// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor


// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)


// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)


// aten::all(Tensor self) -> Tensor


// aten::any(Tensor self) -> Tensor


// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor


// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)


// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)


// aten::equal(Tensor self, Tensor other) -> bool


// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)


// aten::alias(Tensor(a) self) -> Tensor(a)


// aten::isfinite(Tensor self) -> Tensor


// aten::isinf(Tensor self) -> Tensor


// aten::record_stream(Tensor(a!) self, Stream s) -> ()


// aten::isposinf(Tensor self) -> Tensor


// aten::isneginf(Tensor self) -> Tensor


// aten::det(Tensor self) -> Tensor


// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)


// aten::logdet(Tensor self) -> Tensor


// aten::inverse(Tensor self) -> Tensor


// aten::inner(Tensor self, Tensor other) -> Tensor


// aten::outer(Tensor self, Tensor vec2) -> Tensor


// aten::ger(Tensor self, Tensor vec2) -> Tensor


// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor


// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor

 // namespace at
 // namespace c10




 // namespace at


// Parsed from ATen/core/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>
// #include <c10/util/Exception.h>

// Use to convert a TensorBase (that may be undefined) to an at::Tensor
// without bumping refcount.





 // namespace at


// Parsed from ATen/Tensor.h

// #pragma once

// #include <ATen/core/Tensor.h>


// Parsed from torch/csrc/autograd/function_hook.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <torch/csrc/Export.h>
// #include <string>
// #include <vector>
// Targeting ../CompiledNodeArgs.java


// Targeting ../SwapSavedVariables.java


 // namespace torch::dynamo::autograd

// A hook that's called on gradients
// Targeting ../FunctionPreHook.java


// Targeting ../FunctionPostHook.java


// Targeting ../PostAccumulateGradHook.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/cpp_hook.h

// #pragma once
// #include <torch/csrc/autograd/function_hook.h>
// #include <functional>
// #include <memory>

 // namespace autograd
 // namespace torch


// Parsed from c10/util/hash.h

// #pragma once

// #include <functional>
// #include <iomanip>
// #include <sstream>
// #include <vector>

// #include <c10/util/ArrayRef.h>
// #include <c10/util/complex.h>

// NOTE: hash_combine and SHA1 hashing is based on implementation from Boost
//
// Boost Software License - Version 1.0 - August 17th, 2003
//
// Permission is hereby granted, free of charge, to any person or organization
// obtaining a copy of the software and accompanying documentation covered by
// this license (the "Software") to use, reproduce, display, distribute,
// execute, and transmit the Software, and to prepare derivative works of the
// Software, and to permit third-parties to whom the Software is furnished to
// do so, all subject to the following:
//
// The copyright notices in the Software and this entire statement, including
// the above license grant, this restriction and the following disclaimer,
// must be included in all copies of the Software, in whole or in part, and
// all derivative works of the Software, unless such copies or derivative
// works are solely in the form of machine-executable object code generated by
// a source language processor.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
// SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
// FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
// ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
// DEALINGS IN THE SOFTWARE.

@Namespace("c10") public static native @Cast("size_t") long hash_combine(@Cast("size_t") long seed, @Cast("size_t") long value);

// Creates the SHA1 hash of a string. A 160-bit hash.
// Based on the implementation in Boost (see notice above).
// Note that SHA1 hashes are no longer considered cryptographically
//   secure, but are the standard hash for generating unique ids.
// Usage:
//   // Let 'code' be a std::string
//   c10::sha1 sha1_hash{code};
//   const auto hash_code = sha1_hash.str();
// TODO: Compare vs OpenSSL and/or CryptoPP implementations

////////////////////////////////////////////////////////////////////////////////
// c10::hash implementation
////////////////////////////////////////////////////////////////////////////////

// Use template argument deduction to shorten calls to c10::hash

// Use SFINAE to dispatch to std::hash if possible, cast enum types to int
// automatically, and fall back to T::hash otherwise. NOTE: C++14 added support
// for hashing enum types to the standard, and some compilers implement it even
// when C++14 flags aren't specified. This is why we have to disable this
// overload if T is an enum type (and use the one below in this case).

 // namespace _hash_detail

// Hasher struct

// Specialization for std::tuple

// Specialization for std::vector

 // namespace _hash_detail

// Use this function to actually hash multiple things in one line.
// Dispatches to c10::hash, so it can hash containers.
// Example:
//
// static size_t hash(const MyStruct& s) {
//   return get_hash(s.member1, s.member2, s.member3);
// }

// Specialization for c10::complex

 // namespace c10


// Parsed from torch/csrc/autograd/edge.h

// #pragma once

// #include <cstdint>
// #include <functional>
// #include <memory>

// #include <c10/util/hash.h>
// Targeting ../Edge.java


 // namespace autograd
 // namespace torch

// The idiomatic way of enabling use of a custom type as the key of hash
// containers in C++11. This method removes the requirement of having to pass
// a custom hasher to std::unordered_{map, set}.
// See http://en.cppreference.com/w/cpp/utility/hash for more information.
 // namespace std


// Parsed from torch/csrc/autograd/forward_grad.h

// #pragma once

// #include <ATen/core/Tensor.h>
// #include <unordered_set>

// [ Using ForwardGrad ]
// ForwardGrad needs to be a shared_ptr to satisfy constraints of its inner
// design. But this shared_ptr must be uniquely associated with the object that
// stores it (as of writing, either AutogradMeta or SavedVariable). This object
// is called the "owning object" in the discussions below. This owning object
// must call `ForwardGrad::clear()` when it is destroyed to ensure that the
// ForwardGrad is properly de-allocated.

// This file contains two classes that are used to store forward AD gradients
// and ensure that they are scoped properly. Because forward AD runs
// concurrently with the evaluation of the function, we need a mechanism to
// separate different forward AD invocations and be able to compute the right
// gradients. We model such invocations as levels here. The particular scoping
// issue mentioned above has two main drivers:
//   - Ensure that we can conveniently use forward AD within a high level API
//   without
//     leaking the forward AD states outside.
//   - Ensure that we can keep the level that we expose to the user API simple
//   (an integer
//     that represents the nesting depth) while avoiding confusions when the
//     level index is re-used.

// The important external APIs from this file are:
//   - ForwardADLevel::get_next_idx() that can be used to enter a new level and
//   get its index
//   - ForwardADLevel::release_idx() that can be used to exit a given level.
//   - ForwardGrad() can be used to store a given forward gradient that will
//   handle the level
//     tracking automatically.

// The basic implementation strategy is as follows:
// Every tensor has a ForwardGrad, maintaining a map from levels to tangents.
// ForwardGrad is responsible for registering itself to the appropriate
// ForwardADLevel when a new tangent is added to it via ForwardGrad::set_value
// and to un-register itself from this same level if that tangent is removed via
// ForwardGrad::reset. The ForwardADLevel is created when a new level is entered
// via ForwardADLevel::get_next_idx. A reference to the new ForwardADLevel is
// stored into a global (for the whole process) vector that ensure it can be
// accessed via ForwardADLevel::get_by_idx. This reference is deleted when the
// index is released by the user when calling ForwardADLevel::release_idx. When
// it is destructed, the ForwardADLevel is responsible for clearing all the
// tangents for its level stored in all the ForwardGrad that registered with it.
//
// This process-wide level design, compared to a thread local one, allows us to
// use very simple user facing handle for the level (an int) while enabling
// cross-thread forward AD. The only required synchronization for the user is
// when entering and exiting the levels. Some discussion on alternative design
// is in https://github.com/pytorch/pytorch/pull/49097#discussion_r543716453 and
// can be refined in the future.

// Correctness of concurrency:
// Each class uses its own lock when reading or modifying internal storages.
// This allows in particular to safely remove tangents from ForwardGrad when the
// ForwardADLevel is being exited. We ensure no deadlock by ensuring that a
// methods never calls into another class's method while the local class's lock
// is held except in one single case: calling from ForwardADLevel's destructor
// into ForwardGrad::reset with update_level=false.

// The lifetime of these objects is as follows:
// The ForwardADLevel can be in three states:
//      - Initialized: where one of its reference is held by the global vector
//      and there may be more
//        references held by temporary variables in ForwardGrad's methods.
//      - About to be destructed: where "release_idx" has been called and the
//      only reason for the
//        ForwardADLevel not to be destructed right away is that some methods in
//        ForwardGrad have owning reference to it. This is done so that a
//        ForwardADLevel can never be destructed when a ForwardGrad is
//        registered with it and in the process of adding something to its
//        internal state.
//      - Being destructed: Here the ForwardADLevel is not referenced anymore
//      and can be safely reset
//        all of the ForwardGrad. Note that we can have more than one reset
//        being called here (which is ok) but we are guaranteed that there is at
//        least one.
// The ForwardGrad is simpler as there is no intermediary state and no special
// destructor for. The logic to unregister it from the different ForwardADLevel
// is done when the owning object (AutogradMeta or SavedVariable) is being
// destroyed.

// Other considered design:
// To avoid having the ForwardGrad::clear, we considered storing weak_ptr inside
// the ForwardADLevel. While this would work, it would mean that the set inside
// the ForwardADLevel would only grow unless we do an expensive linear scan to
// remove all the dangling weak pointers. Hence this approach was not used.

// Data structures in this file are optimized for this maximum number of levels.
// The number of levels corresponds to the degree of the gradient being
// computed using forward AD and we don't expect more than second order
// gradients to be common.
public static final int EXPECTED_MAX_LEVEL = 2;
// Targeting ../ForwardADLevel.java


// Targeting ../ForwardGrad.java



 // namespace autograd
 // namespace torch


// Parsed from ATen/NamedTensor.h

// #include <ATen/core/NamedTensor.h>


// Parsed from ATen/core/ivalue_to.h

// #pragma once

// #include <string>
 // namespace at
// Determine the return type of `IValue::to() const &`. It's a const
// reference when possible and a copy otherwise. It is in this
// separate header so that List can use it as well.

 // namespace detail
 // namespace c10


// Parsed from ATen/core/qualified_name.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/StringUtil.h>
// #include <c10/util/irange.h>
// #include <string>
// Targeting ../QualifiedName.java


 // namespace c10
 // namespace std


// Parsed from ATen/core/type_ptr.h

// #pragma once

// #include <memory>
// #include <type_traits>

// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// Targeting ../SingletonTypePtr.java


// Targeting ../AnyTypePtr.java


// Targeting ../AnyEnumTypePtr.java


// Targeting ../NumberTypePtr.java


// Targeting ../FloatTypePtr.java


// Targeting ../ComplexTypePtr.java


// Targeting ../IntTypePtr.java


// Targeting ../BoolTypePtr.java


// Targeting ../StringTypePtr.java


// Targeting ../StorageTypePtr.java


// Targeting ../NoneTypePtr.java


// Targeting ../GeneratorTypePtr.java


// Targeting ../QuantizerTypePtr.java


// Targeting ../QSchemeTypePtr.java


// Targeting ../DeviceObjTypePtr.java


// Targeting ../StreamObjTypePtr.java


// Targeting ../CapsuleTypePtr.java


// Targeting ../PyObjectTypePtr.java


// Targeting ../LayoutTypePtr.java


// Targeting ../ScalarTypeTypePtr.java


// Targeting ../AnyListTypePtr.java


// Targeting ../AnyTupleTypePtr.java


// Targeting ../AnyClassTypePtr.java



 // namespace c10


// Parsed from ATen/core/jit_type_base.h

// #pragma once

// #include <functional>
// #include <memory>
// #include <string>
// #include <utility>

// #include <ATen/core/qualified_name.h>
// #include <ATen/core/type_ptr.h>
// #include <c10/core/SymInt.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymBool.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #define C10_FORALL_TYPES(_)
//   _(AnyType)
//   _(EnumType)
//   _(AnyEnumType)
//   _(TensorType)
//   _(StorageType)
//   _(TupleType)
//   _(ListType)
//   _(DictType)
//   _(NumberType)
//   _(FloatType)
//   _(ComplexType)
//   _(FutureType)
//   _(AwaitType)
//   _(RRefType)
//   _(IntType)
//   _(NoneType)
//   _(StringType)
//   _(GeneratorType)
//   _(QuantizerType)
//   _(BoolType)
//   _(OptionalType)
//   _(VarType)
//   _(DeviceObjType)
//   _(StreamObjType)
//   _(FunctionType)
//   _(ClassType)
//   _(PyObjectType)
//   _(CapsuleType)
//   _(InterfaceType)
//   _(QSchemeType)
//   _(ScalarTypeType)
//   _(LayoutType)
//   _(MemoryFormatType)
//   _(AnyListType)
//   _(AnyTupleType)
//   _(AnyClassType)
//   _(SymIntType)
//   _(SymFloatType)
//   _(SymBoolType)
//   _(UnionType)
//   _(DynamicType)

@Namespace("c10") public enum TypeKind {
  AnyType(0),
  EnumType(1),
  AnyEnumType(2),
  TensorType(3),
  StorageType(4),
  TupleType(5),
  ListType(6),
  DictType(7),
  NumberType(8),
  FloatType(9),
  ComplexType(10),
  FutureType(11),
  AwaitType(12),
  RRefType(13),
  IntType(14),
  NoneType(15),
  StringType(16),
  GeneratorType(17),
  QuantizerType(18),
  BoolType(19),
  OptionalType(20),
  VarType(21),
  DeviceObjType(22),
  StreamObjType(23),
  FunctionType(24),
  ClassType(25),
  PyObjectType(26),
  CapsuleType(27),
  InterfaceType(28),
  QSchemeType(29),
  ScalarTypeType(30),
  LayoutType(31),
  MemoryFormatType(32),
  AnyListType(33),
  AnyTupleType(34),
  AnyClassType(35),
  SymIntType(36),
  SymFloatType(37),
  SymBoolType(38),
  UnionType(39),
  DynamicType(40);

    public final int value;
    private TypeKind(int v) { this.value = v; }
    private TypeKind(TypeKind e) { this.value = e.value; }
    public TypeKind intern() { for (TypeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @Cast("const char*") BytePointer typeKindToString(TypeKind kind);
@Namespace("c10") public static native String typeKindToString(@Cast("c10::TypeKind") int kind);

// Use this to customize how a Type is printed using `annotation_str()`. If
// c10::nullopt is returned, `annotation_str()` falls through to its default
// implementation.
 // namespace detail
// #define TORCH_DECLARE_SINGLETON(Type)
//   struct Type;
//   namespace detail {
//   template <> struct IsSingletonType<Type> : public std::integral_constant<bool, true> {};
//   }
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

// Targeting ../Type.java



// Explicitly enable MaybeOwned<shared_ptr<T>>, rather than allowing
// MaybeOwned to be used for any type right away.
// Targeting ../SharedType.java






@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Type lhs, @Const @ByRef Type rhs);
// Targeting ../NamedType.java



 // namespace c10
 // namespace std


// Parsed from ATen/core/DimVector.h

// #pragma once
// #include <c10/util/DimVector.h>

// Re-declaring 'DimVector' type and size inside 'at' namespace.
// This is done to avoid modifying every use into their 'c10'
// equivalent.

 // namespace at


// Parsed from ATen/core/blob.h

// #pragma once

// #include <cstddef>
// #include <sstream>
// #include <type_traits>
// #include <typeinfo>
// #include <vector>

// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/typeid.h>
// #include <c10/macros/Macros.h>
// Targeting ../Blob.java



@Namespace("caffe2") public static native void swap(@ByRef Blob lhs, @ByRef Blob rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Blob v);

 // namespace caffe2


// Parsed from ATen/core/custom_class.h

// #pragma once

// #include <typeindex>
// #include <memory>

// #include <c10/macros/Export.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>

@Namespace("c10") public static native @SharedPtr("c10::ClassType") @ByVal ClassType getCustomClassTypeImpl(@ByRef @Cast("std::type_index*") Pointer tindex);




// Parsed from ATen/core/dynamic_type.h

// #pragma once

// #include <memory>
// #include <type_traits>

// #include <ATen/core/jit_type_base.h>
// #include <c10/util/Optional.h>
// #define DYNAMIC_TYPE_BIT(x) (1 << x)

@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicCovariantTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicAnyTypeBit();

@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicNoneTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicIntTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicFloatTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicComplexTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicListTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicTupleTypeBit();
@Namespace("c10") @MemberGetter public static native @Cast("const c10::DynamicTypeBits") int kDynamicClassTypeBit();

// #define FORALL_DYNAMIC_TYPES(_)
//   _(Tensor, DYNAMIC_TYPE_BIT(0), 1)
//   _(None, kDynamicNoneTypeBit, 1)
//   _(Bool, DYNAMIC_TYPE_BIT(2), 1)
//   _(Int, kDynamicIntTypeBit, 1)
//   _(Float, kDynamicFloatTypeBit, 1)
//   _(Complex, kDynamicComplexTypeBit, 1)
//   _(Number,
//     (kDynamicIntTypeBit | kDynamicFloatTypeBit | kDynamicComplexTypeBit),
//     1)
//   _(String, DYNAMIC_TYPE_BIT(6), 1)
//   _(List, kDynamicListTypeBit, 0)
//   _(Tuple, (kDynamicTupleTypeBit | kDynamicCovariantTypeBit), 0)
//   _(Dict, DYNAMIC_TYPE_BIT(9), 0)
//   _(Class, kDynamicClassTypeBit, 0)
//   _(Optional,
//     (DYNAMIC_TYPE_BIT(11) | kDynamicNoneTypeBit | kDynamicCovariantTypeBit),
//     0)
//   _(AnyList, (kDynamicListTypeBit | kDynamicAnyTypeBit), 1)
//   _(AnyTuple,
//     (kDynamicTupleTypeBit | kDynamicCovariantTypeBit | kDynamicAnyTypeBit),
//     1)
//   _(DeviceObj, DYNAMIC_TYPE_BIT(12), 1)
//   _(StreamObj, DYNAMIC_TYPE_BIT(13), 1)
//   _(Capsule, DYNAMIC_TYPE_BIT(14), 1)
//   _(Generator, DYNAMIC_TYPE_BIT(15), 1)
//   _(Storage, DYNAMIC_TYPE_BIT(16), 1)
//   _(Var, DYNAMIC_TYPE_BIT(17), 0)
//   _(AnyClass, (kDynamicClassTypeBit | kDynamicAnyTypeBit), 1)
//   _(QScheme, DYNAMIC_TYPE_BIT(18), 1)
//   _(Quantizer, DYNAMIC_TYPE_BIT(19), 1)
//   _(AnyEnum, DYNAMIC_TYPE_BIT(20), 1)
//   _(RRef, DYNAMIC_TYPE_BIT(21), 0)
//   _(Future, DYNAMIC_TYPE_BIT(22), 0)
//   _(Await, DYNAMIC_TYPE_BIT(23), 0)
//   _(Any, 0xffffffff, 1)

// #define FORALL_DYNAMIC_TYPES_FAKE(_)
//   _(ScalarType, kDynamicIntTypeBit, 1)
//   _(Layout, kDynamicIntTypeBit, 1)
//   _(SymInt, kDynamicIntTypeBit, 1)
//   _(MemoryFormat, kDynamicIntTypeBit, 1)

// #define FORWARD_DECL_TYPE(NAME, _, __) struct NAME ## Type;
// #undef FORWARD_DECL_TYPE

/**
 * DynamicType is designed as a low dependency type system for TorchScript. The
 * existing JIT types are used for both compilation and runtime, which makes
 * sense for server contexts because we often compile and run the model in
 * the same process, however this doesn't hold for mobile devices where we
 * always compiles a model ahead of time, therefore there will be dependencies
 * which are not needed, but built with mobile runtime causing binary size
 * bloat, by design. Every basic type like Int, Bool or String will bring their
 * vtable, typeinfo, constructor, destructor and even more data from their
 * specializations for STL types to the binary causing a long tail bloat.
 *
 * The core problem is about the complexity to implement and maintain a single
 * type system for both analysis and execution purposes. Although they should
 * have the exactly same semantics, in practice implement a unified abstraction
 * adds conceptual and representational overhead for both sides of the world.
 *
 * To address the issues, DynamicType implements a minimal subset of JIT types
 * and uses a generic algorithm to test all subtyping relations. To achieve
 * this, we assign each dynamic type a single integer tag to represent its
 * semantics. More specifically, a dynamic type is defined as a set of "control
 * bits" and "data bits", where control bits describe the special behavior when
 * testing a type and data bits map to identity of each nominal type. We use bit
 * operations to perform all the tests.
 *
 * For example, a "covariant bit" is a control bit used to describe if a type
 * is covariant, right now the most used one is tuple type, and in addition to
 * the control bit, tuple type's data bit is the 8th bit from the LSB. Control
 * bits start from MSB and data bits start from LSB.
 *
 * If two types are equal, then they are subtype of each other, also if the bits
 * from one type tag is subset of the other tag, it automatically becomes a
 * subtype of the other. This simplifies the subtyping logic a lot, and over the
 * long term it is possible to adopt this scheme on the server side as well.
 * Special cases can be added but they generally should not take too much code
 * size.
 *
 * DynamicType may or may not inherit from c10::Type because it's not the core
 * requirement of DynamicType to interface with existing JIT types, but we might
 * want to inherit from c10::Type to reduce the migration cost.
 */



// #define DYNAMIC_TYPE_TAG_VALUE(NAME, _, IS_BASE_TYPE)
//   template <>
//   struct TORCH_API DynamicTypeTrait<NAME##Type> {
//     C10_ERASE static auto tagValue() {
//       return DynamicType::Tag::NAME;
//     }
//     static constexpr bool isBaseType = IS_BASE_TYPE;
//     template <typename T = const DynamicTypePtr&>
//     static std::enable_if_t<isBaseType, T> getBaseType() {
//       static auto type = detail::makeBaseType(tagValue());
//       return type;
//     }
//   }; // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10 // namespace c10
// #undef DYNAMIC_TYPE_TAG_VALUE

 // namespace c10


// Parsed from ATen/core/type_factory.h

// #pragma once

// #include <type_traits>
// #include <unordered_map>

// #include <ATen/core/dynamic_type.h>
// #include <ATen/core/jit_type_base.h>
// #include <c10/macros/Macros.h>

// Helper functions for constructing DynamicTypes inline.

 // namespace c10


// Parsed from c10/util/order_preserving_flat_hash_map.h

// Taken from
// https://github.com/skarupke/flat_hash_map/blob/2c4687431f978f02a3780e24b8b701d22aa32d9c/flat_hash_map.hpp
// with fixes applied:
// - https://github.com/skarupke/flat_hash_map/pull/25
// - https://github.com/skarupke/flat_hash_map/pull/26
// - replace size_t with uint64_t to fix it for 32bit
// - add "GCC diagnostic" pragma to ignore -Wshadow
// - make sherwood_v3_table::convertible_to_iterator public because GCC5 seems
// to have issues with it otherwise
// - fix compiler warnings in operator templated_iterator<const value_type>

//          Copyright Malte Skarupke 2017.
// Distributed under the Boost Software License, Version 1.0.
//    (See http://www.boost.org/LICENSE_1_0.txt)

// Modified to maintain insertion and deletion order through a doubly-linked
// list

// #pragma once

// #include <c10/util/C++17.h>
// #include <algorithm>
// #include <cmath>
// #include <cstddef>
// #include <cstdint>
// #include <functional>
// #include <iterator>
// #include <type_traits>
// #include <utility>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

// #ifdef _MSC_VER
// #define SKA_NOINLINE(...) __declspec(noinline) __VA_ARGS__
// #else
// #define SKA_NOINLINE(...) __VA_ARGS__ __attribute__((noinline))
// #endif



// Implementation taken from http://en.cppreference.com/w/cpp/types/void_t
// (it takes CWG1558 into account and also works for older compilers)
 // namespace detailv3

 // namespace ska_ordered



// Parsed from ATen/core/Dict_inl.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/hash.h>


@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Type type);





















































// Parsed from ATen/core/Dict.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/macros/Export.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/order_preserving_flat_hash_map.h>
// #include <c10/util/Optional.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/jit_type_base.h>



// Targeting ../GenericDictEntryRef.java


// Targeting ../GenericDictIterator.java



// Targeting ../GenericDict.java


// Targeting ../StringGenericListDict.java


// GenericDict is how IValue stores dicts. It is, however, not part of the
// public API. Kernels should use Dicts with concrete Key, Value types instead
// (maybe except for some internal prim ops).





// #include <ATen/core/Dict_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/functional.h

// #pragma once

// #include <vector>
// #include <c10/util/ArrayRef.h>

// The passed in function must take T by value (T), or by
// const reference (const T&); taking T by non-const reference
// will result in an error like:
//
//    error: no type named 'type' in 'class std::result_of<foobar::__lambda(T)>'
//
// No explicit template parameters are required.

// Overload for explicit function and ArrayRef

// C++ forbids taking an address of a constructor, so here's a workaround...
// Overload for constructor (R) application

 // namespace c10


// Parsed from ATen/core/jit_type.h

// #pragma once

// #include <ATen/core/custom_class.h>
// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/symbol.h>
// #include <ATen/core/type_factory.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/Optional.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymBool.h>
// #include <c10/core/Device.h>

// #include <array>
// #include <memory>
// #include <ostream>
// #include <sstream>
// #include <type_traits>
// #include <utility>
 // namespace jit
 // namespace torch




@Namespace("c10") public static native @Cast("bool") boolean is_contiguous_strides(
    @Const @ByVal LongArrayRef sizes,
    @Const @ByVal LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_contiguous_strides(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... strides);
// Targeting ../AnyType.java



// Shim for compatibility with code that uses TypePtr.
@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Type.TypePtr typePtr);

@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Type lhs, @Const @ByRef Type rhs);
// Targeting ../AwaitSingleElementType.java


// Targeting ../ListSingleElementType.java


// Targeting ../RRefSingleElementType.java


// Targeting ../FutureSingleElementType.java


// Targeting ../UnionType.java


// Targeting ../OptionalType.java




// Targeting ../Stride.java




// Targeting ../ShapeSymbol.java




// Targeting ../SymbolicShape.java


@Namespace("c10::detail") public static native @Cast("bool") boolean isComplete(@Const @ByRef Stride s);

// Targeting ../LongVaryingShape.java


// Targeting ../StrideVaryingShape.java


// TODO: investigate making this SingletonOrSharedTypePtr<TensorType>
// Targeting ../TensorType.java


// Targeting ../ListType.java


// Targeting ../DictType.java


// Targeting ../FutureType.java


// Targeting ../AwaitType.java


// Targeting ../RRefType.java



// Any should never appear in a named type like a class, namedtuple or
// interface. If it does, then dynamic type information will be lost in the
// Pickler, leading to hard-to-track-down bugs that will only occur
// after saving or loading a model. This is because we rely on the
// static types in named types to reconstruct type tags of loaded
// values. Lifting this restriction requires solving the serialization
// problem first.
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    @Cast("const char*") BytePointer what,
    @StdString BytePointer attrname,
    @Const @ByRef Type.TypePtr attrtype);
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    String what,
    @StdString String attrname,
    @Const @ByRef Type.TypePtr attrtype);
// Targeting ../TupleType.java



// the common supertype of all Enums, only used in operator registraion.
// EnumType <: AnyEnumType for all Enums
// Targeting ../AnyEnumType.java


// Targeting ../NumberType.java


// Targeting ../FloatType.java


// Targeting ../ComplexType.java



// We need to introduce `SymIntType` to represent the `SymInt` type
// used in function schemas e.g. `aten::narrow_copy(... SymInt length)
// `SymInt` will be used to enable tracing arithmetic operations on
// dimension values. Please see [SymInt.h] for more information
// Targeting ../SymIntType.java


// Targeting ../SymFloatType.java


// Targeting ../SymBoolType.java


// Targeting ../IntType.java


// Targeting ../BoolType.java


// Targeting ../StringType.java


// Targeting ../StorageType.java


// Targeting ../FunctionType.java


// Targeting ../NoneType.java


// Targeting ../GeneratorType.java


// Targeting ../QuantizerType.java


// Targeting ../QSchemeType.java


// Targeting ../DeviceObjType.java


// Targeting ../StreamObjType.java


// This type represents a type variable, used in FunctionSchema
// Targeting ../CapsuleType.java


// Targeting ../PyObjectType.java



@Namespace("c10") public enum TypeVerbosity {
  None(0),
  Type(1),
  TypeAndStride(2),
  Full(3),
  Symbolic(4),
  Default(Full.value);

    public final int value;
    private TypeVerbosity(int v) { this.value = v; }
    private TypeVerbosity(TypeVerbosity e) { this.value = e.value; }
    public TypeVerbosity intern() { for (TypeVerbosity e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native TypeVerbosity type_verbosity();

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Type t);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef SymbolicShape s);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef ShapeSymbol s);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer os, @Const @ByRef Stride s);
// what is the type, ignoring extra size/shape information?
// e.g. Tensor(2x3) -> Dynamic, and Tuple(Tensor(2x3),...) -> Tuple(Dynamic,...)

// `unshapedType` is used to remove Tensor subtypes. We treat all Tensor
// subtypes as simply "Tensor"; we also create a new version of any
// container types in which internal Tensors have undergone the same
// operation. This is used for type comparisons between two Tensor types
// (`unshapedType` means that we don't falsely return `false` for e.g.
// Tensors of different dimensions). It's also used in the alias
// analysis pass.
// Be careful with calls because this can be very slow. If calling this
// on a graph, use `EraseShapeInformation` in shape_analysis.h
@Namespace("c10") public static native @ByVal Type.TypePtr unshapedType(@Const @ByRef Type.TypePtr type);




@Namespace("c10") public static native @ByVal ScalarTypeOptional tryScalarTypeFromJitType(@Const @ByRef Type type);

@Namespace("c10") public static native ScalarType scalarTypeFromJitType(@Const @ByRef Type type);

// Attempt to find the correct supertype of the two types `t1` and `t2`.
// If no supertype is found, then nullopt will be returned if
// `default_to_union` is false, and `Union[t1, t2]` will be returned
// if it is true. If `t1 == t2`, or `t1` is a type refinement of `t2`,
// then `t2` will be returned (and vice versa).
//
// Two different tensortypes will return dynamic.
//
// Currently we chose not to support returning a NumberType for
// two types from the set of {FloatType, IntType, ComplexType}, because
// there is a lack of operator support for NumberType.
//
// If `type_hint` is an `InterfaceType`, then we can use that as a
// potential supertype for `ClassType`s in the list. Otherwise, we have
// no way to find and use some common interface type
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @ByRef Type.TypePtr t1,
    @Const @ByRef Type.TypePtr t2,
    @Cast("bool") boolean default_to_union/*=false*/,
    @ByVal(nullValue = "c10::TypePtr(nullptr)") Type.TypePtr type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @ByRef Type.TypePtr t1,
    @Const @ByRef Type.TypePtr t2);

@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not,
    @Cast("bool") boolean default_to_union/*=false*/,
    @ByVal(nullValue = "c10::TypePtr(nullptr)") Type.TypePtr type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeVector elements,
    @Cast("std::ostream*") @ByRef Pointer why_not,
    @Cast("bool") boolean default_to_union/*=false*/,
    @ByVal(nullValue = "c10::TypePtr(nullptr)") Type.TypePtr type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeVector elements,
    @Cast("std::ostream*") @ByRef Pointer why_not);
 // namespace detail
// Targeting ../MatchTypeReturn.java



// attempt to match the type variables in formal to actual, adding them to type_env.
// If no match is possible this returns a MatchTypeReturn with r.success() == false
// and a r.reason() that describes why it could not match.
// note: It is possible to successfully match a formal, but for type variables
// in the formal to still not be defined. In particular, None matches Optional[T]
// but does not define the value of T.
@Namespace("c10") public static native @ByVal MatchTypeReturn matchTypeVariables(@Const @ByRef Type.TypePtr formal, @Const @ByRef Type.TypePtr actual, @ByRef TypeEnv type_env);

// replace type variables appearing in `type` with the values in
// `type_env`. Returns nullptr if a variable used in `type`
// does not appear in `type_env`
@Namespace("c10") public static native @ByVal Type.TypePtr tryEvalTypeVariables(@Const @ByRef Type.TypePtr type, @ByRef TypeEnv type_env);

@Namespace("c10") public static native @Cast("bool") boolean elementTypeCanBeInferredFromMembers(@Const @ByRef Type.TypePtr elem_type);
// Targeting ../InterfaceType.java


// Targeting ../LayoutEnumerationType.java


// Targeting ../ScalarTypeEnumerationType.java


// Targeting ../MemoryFormattEnumerationType.java



// WARNING: These enumeration types below DO NOT actually get parsed out
// from the logical schema strings, instead they are mapped as ints.  To
// observe these types, use real_type() instead of type() on Argument
// Targeting ../ScalarTypeType.java


// Targeting ../MemoryFormatType.java


// Targeting ../LayoutType.java


 // namespace detail

// the common supertype of all lists,
// List[T] <: AnyList for all T
// Targeting ../AnyListType.java



// the common supertype of all tuples,
// Tuple[T...] <: AnyTuple for all T
// Targeting ../AnyTupleType.java



// the common supertype of all classes,
// ClassType <: AnyClassType for all classes
// Targeting ../AnyClassType.java








// Targeting ../InferredType.java



@Namespace("c10") public static native @Cast("bool") boolean containsAnyType(@Const @ByRef Type.TypePtr type);

 // namespace c10


// Parsed from ATen/core/rref_interface.h

// #pragma once

// #include <c10/util/intrusive_ptr.h>
// #include <ATen/core/type_ptr.h>
// Targeting ../RRefInterface.java






// Parsed from c10/core/impl/DeviceGuardImplInterface.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/DeviceType.h>
// #include <c10/core/Stream.h>
// #include <c10/util/Exception.h>

// Just for C10_ANONYMOUS_VARIABLE
// #include <c10/util/Registry.h>

// #include <atomic>

// Forward declaration

/**
 * Flags defining the behavior of events.
 *
 * PYTORCH_DEFAULT and BACKEND_DEFAULT are valid for all backends. The
 * BACKEND_DEFAULT is what a particular backend would select if no
 * flags were given. PYTORCH_DEFAULT is the PyTorch's framework default
 * choice for events on that backend, which may not be the same. For example,
 * when PyTorch creates a CUDA event it sets the flag
 * CUDA_EVENT_DISABLING_TIMING by default to improve performance.
 *
 * The mapping of PYTORCH_DEFAULT and BACKEND_DEFAULT is done by each
 * backend implementation. Backend-specific flags, like CUDA_EVENT_DEFAULT,
 * should map one-to-one with actual event flags for those backends.
 */
@Namespace("c10") public enum EventFlag {
  PYTORCH_DEFAULT(0),
  BACKEND_DEFAULT(1),
  // CUDA flags
  CUDA_EVENT_DEFAULT(2),
  CUDA_EVENT_DISABLE_TIMING(3), // PyTorch-default for CUDA
  // HIP flags
  HIP_EVENT_DEFAULT(4),
  HIP_EVENT_DISABLE_TIMING(5), // PyTorch-default for HIP
  // FOR TESTING ONLY
  INVALID(6);

    public final int value;
    private EventFlag(int v) { this.value = v; }
    private EventFlag(EventFlag e) { this.value = e.value; }
    public EventFlag intern() { for (EventFlag e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../DeviceGuardImplInterface.java



// A no-op device guard impl that doesn't do anything interesting.  Useful
// for devices that don't actually have a concept of device index.  Prominent
// examples are CPU and Meta.

// The registry is NON-owning.  Each stored pointer is std::atomic so
// that under all interleavings of registry calls the structure is
// race-free.  This doesn't cost us anything on reads in X86.  (An
// unsynchronized implementation probably is OK too, but I didn't want
// to prove that we never read from device_guard_impl_registry at the
// same time some registration is occurring.  Shiver.)
//
// I'd like this registry to be valid even at program destruction time
// (in case someone uses a DeviceGuard in a destructor to do some cleanup
// in the CUDA API.)  Since there are no direct accesses of the underlying
// owning objects which I can use to enforce initialization order (unlike
// in a Meyer singleton), it implies that you must *leak* objects when
// putting them in the registry.  This is done by deleting the destructor
// on DeviceGuardImplInterface.

// Targeting ../DeviceGuardImplRegistrar.java



// #define C10_REGISTER_GUARD_IMPL(DevType, DeviceGuardImpl)
//   static ::c10::impl::DeviceGuardImplRegistrar C10_ANONYMOUS_VARIABLE(
//       g_##DeviceType)(::c10::DeviceType::DevType, new DeviceGuardImpl());

@Namespace("c10::impl") public static native @Const DeviceGuardImplInterface getDeviceGuardImpl(DeviceType type);
@Namespace("c10::impl") public static native @Const DeviceGuardImplInterface getDeviceGuardImpl(@Cast("c10::DeviceType") byte type);

@Namespace("c10::impl") public static native @Cast("bool") boolean hasDeviceGuardImpl(DeviceType type);
@Namespace("c10::impl") public static native @Cast("bool") boolean hasDeviceGuardImpl(@Cast("c10::DeviceType") byte type);

 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/VirtualGuardImpl.h

// #pragma once

// #include <c10/core/impl/DeviceGuardImplInterface.h>

/**
 * An implementation of DeviceGuardImplInterface which delegates
 * to virtual dispatch on the DeviceGuardImpl registry.
 */

 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/InlineDeviceGuard.h

// #pragma once

// This file provides implementations of InlineDeviceGuard and
// InlineOptionalDeviceGuard.

// #include <c10/core/Device.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/core/impl/VirtualGuardImpl.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Optional.h>

/**
 * A DeviceGuard is an RAII class that sets a device to some value
 * on construction, and resets the device to its original value on
 * destruction.
 *
 * InlineDeviceGuard is a helper class for implementing DeviceGuards.
 * It is templated over a DeviceGuardImpl (anything that implements
 * DeviceGuardImplInterface).  There are two primary ways to instantiate
 * InlineDeviceGuard:
 *
 *  - With a concrete implementation of DeviceGuardImpl, e.g., CUDAGuardImpl.
 *    This is the best way to use InlineDeviceGuard, as all calls are
 *    devirtualized, giving you code as efficient as straight line
 *    calls to cudaGetDevice/cudaSetDevice.
 *
 *  - With VirtualGuardImpl, which does a virtual dispatch to a DeviceGuardImpl
 *    retrieved from a DeviceType registry.  We have explicitly instantiated
 *    InlineDeviceGuard this way as c10::DeviceGuard.
 *
 * If you are in a hurry, you can use InlineDeviceGuard directly:
 *
 *    using CUDAGuard = impl::InlineDeviceGuard<CUDAGuardImpl>;
 *
 * However, you can provide a better user experience if you explicitly write a
 * wrapper class that itself contains the template instantiation:
 *
 *    class CUDAGuard {
 *    public:
 *      // ... the API ...
 *    private:
 *      impl::InlineDeviceGuard<CUDAGuardImpl> guard_;
 *    }
 *
 * The wrapper class provides a good place to write documentation, and helps
 * avoid weird template instantiation errors when a user incorrectly uses the
 * class.
 *
 * If you need to test this class, consider instantiating it with FakeGuardImpl.
 */

/**
 * A OptionalDeviceGuard is an RAII class that sets a device to some value on
 * initialization, and resets the device to its original value on destruction.
 *
 * InlineOptionalDeviceGuard is a helper class for implementing
 * OptionalDeviceGuards.  See guidance in InlineDeviceGuard on how to
 * use this.  See OptionalDeviceGuard for user-oriented usage notes.
 */

 // namespace impl
 // namespace c10


// Parsed from c10/core/DeviceGuard.h

// #pragma once

// #include <c10/core/impl/InlineDeviceGuard.h>

/** RAII guard that sets a certain default device in its constructor, and
 *  changes it back to the device that was originally active upon destruction.
 * 
 *  The device is always reset to the one that was active at the time of
 *  construction of the guard. Even if you {@code set_device} after construction, the
 *  destructor will still reset the device to the one that was active at
 *  construction time.
 * 
 *  This device guard does NOT have an uninitialized state; it is guaranteed
 *  to reset a device on exit.  If you are in a situation where you *might*
 *  want to setup a guard (i.e., are looking for the moral equivalent
 *  of optional<DeviceGuard>), see OptionalDeviceGuard. */
// Targeting ../OptionalDeviceGuard.java



// Note [Whither the DeviceGuard boilerplate]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Design note: in principle, we could avoid these wrappers using:
//
// using DeviceGuard = impl::InlineDeviceGuard<impl::VirtualGuardImpl>;
// using OptionalDeviceGuard =
// impl::InlineOptionalDeviceGuard<impl::VirtualGuardImpl>;
//
// But the error messages are worse, and our users can't just look at the
// header file to find out what's going on.  Furthermore, for specializations
// like CUDAStreamGuard, it can be profitable to replace some interfaces with
// refined types (e.g., return CUDAStream instead of Stream).  So, we eat
// the boilerplate and write out the API explicitly.

 // namespace c10


// Parsed from c10/core/impl/InlineEvent.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/Stream.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/Exception.h>

 // namespace impl
 // namespace c10


// Parsed from c10/core/Event.h

// #pragma once

// #include <c10/core/impl/InlineEvent.h>
// #include <c10/core/impl/VirtualGuardImpl.h>

/**
 * A backend-generic movable, not copyable, not thread-safe event.
 *
 * The design of this event follows that of CUDA and HIP events. These events
 * are recorded and waited on by streams and can be rerecorded to,
 * each rerecording essentially creating a new version of the event.
 * For example, if (in CPU time), stream X is asked to record E,
 * stream Y waits on E, and stream X is asked to record E again, then Y will
 * wait for X to finish the first call to record and not the second, because
 * it's waiting on the first version of event E, not the second.
 * Querying an event only returns the status of its most recent version.
 *
 * Backend-generic events are implemented by this class and
 * impl::InlineEvent. In addition to these events there are also
 * some backend-specific events, like ATen's CUDAEvent. Each of these
 * classes has its own use.
 *
 * impl::InlineEvent<...> or a backend-specific event should be
 * preferred when the backend is known at compile time and known to
 * be compiled. Backend-specific events may have additional functionality.
 *
 * This Event should be used if a particular backend may not be available,
 * or the backend required is not known at compile time.
 *
 * These generic events are built on top of DeviceGuardImpls, analogous
 * to DeviceGuard and InlineDeviceGuard. The name "DeviceGuardImpls,"
 * is no longer entirely accurate, as these classes implement the
 * backend-specific logic for a generic backend interface.
 *
 * See DeviceGuardImplInterface.h for a list of all supported flags.
 */

 // namespace c10


// Parsed from c10/core/impl/InlineStreamGuard.h

// #pragma once

// #include <c10/core/impl/InlineDeviceGuard.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/irange.h>

/**
 * A StreamGuard is an RAII class that changes the current device
 * to the device corresponding to some stream, and changes the
 * default stream on that device to be this stream.
 *
 * InlineStreamGuard is a helper class for implementing StreamGuards.
 * See InlineDeviceGuard for guidance on how to use this class.
 */

/**
 * An OptionalStreamGuard is an RAII class that sets a device to some value on
 * initialization, and resets the device to its original value on destruction.
 * See InlineOptionalDeviceGuard for more guidance on how to use this class.
 */

 // namespace impl
 // namespace c10


// Parsed from c10/core/StreamGuard.h

// #pragma once

// #include <c10/core/impl/InlineStreamGuard.h>

/**
 * A StreamGuard is an RAII class that changes the current device
 * to the device corresponding to some stream, and changes the
 * default stream on that device to be this stream.
 *
 * Use of StreamGuard is HIGHLY discouraged in operator definitions.  In
 * a single operator, you probably don't know enough about the global
 * state of the world to profitably decide how to set streams.  Let
 * the caller handle this appropriately, and just use the current stream
 * in your operator code.
 *
 * This StreamGuard does NOT have an uninitialized state; it is guaranteed
 * to reset the stream and device on exit.  If you are in a situation
 * where you *might* want to setup a stream guard, see OptionalStreamGuard.
 */

/**
 * An OptionalStreamGuard is an RAII class that sets a device to some value on
 * initialization, and resets the device to its original value on destruction.
 * See OptionalDeviceGuard for more guidance on how to use this class.
 */

/**
 * A MultiStreamGuard is an RAII class that sets the current streams of a set of
 * devices all at once, and resets them to their original values on destruction.
 */

 // namespace c10


// Parsed from c10/util/FunctionRef.h

//===- llvm/ADT/STLExtras.h - Useful STL related functions ------*- C++ -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file contains some templates that are useful if you are working with the
// STL at all.
//
// No library is required when using these functions.
//
//===----------------------------------------------------------------------===//

// c10: modified from llvm::function_ref
// c10: added more SFINAE to enable use in overloaded functions

// #pragma once

// #include <cstdint>
// #include <type_traits>
// #include <utility>

/** An efficient, type-erasing, non-owning reference to a callable. This is
 *  intended for use as the type of a function parameter that is not used
 *  after the function in question returns.
 * 
 *  This class does not own the callable, so it is not in general safe to store
 *  a function_ref. */

 // namespace c10


// Parsed from c10/util/intrusive_ptr.h

// #pragma once

// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// #include <atomic>
// #include <climits>
// #include <memory>

@Namespace("c10::raw::weak_intrusive_ptr") public static native void incref(@Cast("c10::intrusive_ptr_target*") Pointer self);


// Targeting ../DontIncreaseRefcount.java


 // namespace raw
/**
 * intrusive_ptr<T> is an alternative to shared_ptr<T> that has better
 * performance because it does the refcounting intrusively
 * (i.e. in a member of the object itself).
 * Your class T needs to inherit from intrusive_ptr_target to allow it to be
 * used in an intrusive_ptr<T>. Your class's constructor should not allow
 *{@code this} to escape to other threads or create an intrusive_ptr from {@code this}.
 */

// Note [Stack allocated intrusive_ptr_target safety]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// A well known problem with std::enable_shared_from_this is that it
// allows you to create a std::shared_ptr from a stack allocated object,
// which is totally bogus because the object will die once you return
// from the stack.  In intrusive_ptr, we can detect that this has occurred,
// because we set the refcount/weakcount of objects which inherit from
// intrusive_ptr_target to zero, *unless* we can prove that the object
// was dynamically allocated (e.g., via make_intrusive).
//
// Thus, whenever you transmute a T* into a intrusive_ptr<T>, we check
// and make sure that the refcount isn't zero (or, a more subtle
// test for weak_intrusive_ptr<T>, for which the refcount may validly
// be zero, but the weak refcount better not be zero), because that
// tells us if the object was allocated by us.  If it wasn't, no
// intrusive_ptr for you!

// NOLINTNEXTLINE(cppcoreguidelines-virtual-class-destructor)

// Increment needs to be acquire-release to make use_count() and
// unique() reliable.
@Namespace("c10::detail") public static native @Cast("size_t") long atomic_refcount_increment(@Cast("std::atomic<size_t>*") @ByRef LongPointer refcount);

// weak_use_count() is only used for testing, so we don't need it to
// be reliable. Relaxed should be fine.
@Namespace("c10::detail") public static native @Cast("size_t") long atomic_weakcount_increment(@Cast("std::atomic<size_t>*") @ByRef LongPointer weakcount);

// Both decrements need to be acquire-release for correctness. See
// e.g. std::shared_ptr implementation.
@Namespace("c10::detail") public static native @Cast("size_t") long atomic_refcount_decrement(@Cast("std::atomic<size_t>*") @ByRef LongPointer refcount);

@Namespace("c10::detail") public static native @Cast("size_t") long atomic_weakcount_decrement(@Cast("std::atomic<size_t>*") @ByRef LongPointer weakcount);


// Targeting ../TuplePtr.java


// Targeting ../FuturePtr.java


// Targeting ../ConstantStringPtr.java


// Targeting ../GeneratorImplPtr.java


// Targeting ../QuantizerPtr.java


// Targeting ../AwaitPtr.java


// Targeting ../RRefInterfacePtr.java


// Targeting ../PyObjectHolderPtr.java


// Targeting ../EnumHolderPtr.java


// Targeting ../TensorImplPtr.java


// Targeting ../TreeRef.java


// Targeting ../StorageImplPtr.java


// Targeting ../SymNode.java


// Targeting ../BackendMetaRef.java



// To allow intrusive_ptr inside std::map or std::set, we need operator<
// Targeting ../WeakStorage.java



// To allow weak_intrusive_ptr inside std::map or std::set, we need operator<

// Alias for documentary purposes, to more easily distinguish
// weak raw intrusive pointers from intrusive pointers.

// This namespace provides some methods for working with
// raw pointers that subclass intrusive_ptr_target.  They are not provided
// as methods on intrusive_ptr_target, because ideally you would not need these
// methods at all (use smart pointers), but if you are dealing with legacy code
// that still needs to pass around raw pointers, you may find these quite
// useful.
//
// An important usage note: some functions are only valid if you have a
// strong raw pointer to the object, while others are only valid if you
// have a weak raw pointer to the object.  ONLY call intrusive_ptr namespace
// functions on strong pointers, and weak_intrusive_ptr namespace functions
// on weak pointers.  If you mix it up, you may get an assert failure.

// WARNING: Unlike the reclaim() API, it is NOT valid to pass
// NullType::singleton to this function

// WARNING: Unlike the reclaim() API, it is NOT valid to pass
// NullType::singleton to this function
@Namespace("c10::raw::intrusive_ptr") public static native void decref(@Cast("c10::intrusive_ptr_target*") Pointer self);

@Namespace("c10::raw::intrusive_ptr") public static native @Cast("size_t") long use_count(@Cast("c10::intrusive_ptr_target*") Pointer self);

 // namespace intrusive_ptr

// This gives the STRONG refcount of a WEAK pointer

 // namespace weak_intrusive_ptr

 // namespace raw

 // namespace c10
// To allow intrusive_ptr and weak_intrusive_ptr inside std::unordered_map or
// std::unordered_set, we need std::hash
 // namespace std


// Parsed from ATen/core/ivalue_inl.h

// #pragma once

// #include <condition_variable>
// #include <memory>
// #include <type_traits>
// #include <utility>

// #include <ATen/core/Dict.h>
// #include <ATen/core/List.h>
// #include <ATen/core/IListRef.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/qualified_name.h>
// #include <ATen/core/rref_interface.h>
// #include <ATen/core/symbol.h>
// #include <c10/core/DeviceGuard.h>
// #include <c10/core/Event.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Stream.h>
// #include <c10/core/StreamGuard.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/FunctionRef.h>
// #include <c10/util/Logging.h>
// #include <c10/util/hash.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/irange.h>
 // namespace jit
@Namespace("torch") public static native @Cast("bool") boolean isCustomClass(@Const @ByRef IValue v);
 // namespace torch

// For custom class __init__ registration, we need to pass in a function
// that looks like this: [](IValue x, args...)

// However, make_boxed_from_unboxed_functor.h automatically sets the input types
// of the function by introspecting the types of the functor (which is IValue in
// this case). However, we need the type it binds to be Foo.

// Instead, we pass in a lambda [](ivalue_holder<CurClass> x, args...) from
// which getTypePtr can recover the original class pointer.










































@Namespace("c10::ivalue") public static native void checkCustomClassType(@Const ClassType expected_type, @Const Type actual_type);
// Targeting ../ConstantString.java


// Targeting ../TupleElements.java


// Targeting ../Tuple.java



// Targeting ../Future.java


// Targeting ../Await.java



// Input is a list of Futures with the same target type.
// Output is a Future to the List of completed Futures.
@Namespace("c10") public static native @ByVal FuturePtr collectAll(
    @ByVal FuturePtrList srcs);
// Input is a List of Futures with the same target type.
// Output is a Future that will be updated with a seen value.
@Namespace("c10") public static native @ByVal FuturePtr collectAny(
    @ByVal FuturePtrList srcs);

// User-defined object.
// Targeting ../PyObjectHolder.java


// Targeting ../EnumHolder.java



// #undef TORCH_FORALL_TAGS

 // namespace detail



// note: when adding a DEFINE_TO case here you should also add a
// toX method to IValue. These named methods are much more discoverable
// than the to templated function.

// #define DEFINE_TO(T, method_name)
//   template <>
//   inline T IValue::to<T>()&& {
//     return static_cast<T>(std::move(*this).method_name());
//   }
//   template <>
//   inline c10::detail::ivalue_to_const_ref_overload_return<T>::type IValue::to<T>() const& {
//     typedef c10::detail::ivalue_to_const_ref_overload_return<T>::type return_type;
//     return static_cast<return_type>(this->method_name());
//   }


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

// generic_to<T> converts an IValue from a generic list or generic dict
// to a concrete list/dict type likelike List<T>, Dict<...> or optional<T>.
// Note that in the case of lists, this only works for IValue-based lists,
// i.e. not for int64_t, double, ...
// generic_to<T> is an implementation detail of IValue::to<T> and not
// supposed to be called directly.
// The _fake_type<T> parameter allows us to overload
// based on the return type.




 // namespace detail
 // namespace detail





































































































 // namespace detail

 // namespace ivalue



 // namespace c10


// Parsed from ATen/core/ivalue.h

// #pragma once

// #include <ATen/core/DimVector.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/blob.h>
// #include <ATen/core/custom_class.h>
// #include <ATen/core/ivalue_to.h>
// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/type_factory.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymBool.h>
// #include <c10/macros/Export.h>
// #include <c10/util/C++17.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/intrusive_ptr.h>
// #include <typeindex>
// #include <unordered_map>
// #include <unordered_set>
// #include <utility>
// Targeting ../CustomClassHolder.java


 // namespace jit
 // namespace torch

@Namespace("c10") public static native @Cast("bool") boolean _fastEqualsForContainer(@Const @ByRef IValue lhs, @Const @ByRef IValue rhs);

@Namespace("c10") public static native Function checkObjectSortSchema(
    @Const @SharedPtr("c10::ClassType") @ByRef ClassType t,
    @Cast("std::stringstream*") @ByRef Pointer why_not);

// A comparator that checks ordering of two IValues of same type.



// We need a ComplexHolder because currently the payloads in the Union
// only take 64 bits. Since ComplexDouble takes up 128 bits, and is too big
// to fit in the IValue directly, we indirect complex numbers through an intrusive
// pointer to ComplexHolder (which contains a c10::complex).

// Similar to ComplexHolder, for StreamData3

 // namespace ivalue

// This is an owning wrapper for a c10::optional<std::vector<T>>
// that can be implicitly converted to a (non-owning) optional<ArrayRef<T>>.
// Its purpose is to be used in generated code to keep the vector alive
// either until the end of a statement (as a temporary), or as a saved arg
// in autograd.

// Capsule is an internal implementation detail of custom C++ classes. We
// define it as an owning wrapper for
// c10::intrusive_ptr<torch::CustomClassHolder> This wrapper is here to serve as
// an abstraction of the type erased custom class object pointer. It also allow
// pybind11 to treat this as a standalone class to register as a separate type
// caster, instead of a custom pointer holder which the pointer holder type
// caster try to "unwrap" it automatically.

// IValue is the generic tagged union used by the interpreter to hold
// all value types.
// It is a 16-byte object with an 8-byte payload and an 8-byte tag.
// The tag is currently 4 bytes to determine the type, and 1 byte
// to mark whether that type is a subtype of c10::intrusive_ptr_target and needs
// retain/release calls.


///
///
///
///
///
// #define TORCH_FORALL_TAGS(_)
//   _(None)
//   _(Tensor)
//   _(Storage)
//   _(Double)
//   _(ComplexDouble)
//   _(Int)
//   _(SymInt)
//   _(SymFloat)
//   _(SymBool)
//   _(Bool)
//   _(Tuple)
//   _(String)
//   _(Blob)
//   _(GenericList)
//   _(GenericDict)
//   _(Future)
//   _(Await)
//   _(Device)
//   _(Stream)
//   _(Object)
//   _(PyObject)
//   _(Uninitialized)
//   _(Capsule)
//   _(RRef)
//   _(Quantizer)
//   _(Generator)
//   _(Enum)
// Targeting ../IValue.java


// Targeting ../WeakIValue.java


// Targeting ../StrongTypePtr.java


// Targeting ../WeakTypePtr.java


// Targeting ../WeakOrStrongCompilationUnit.java


// Targeting ../WeakOrStrongTypePtr.java




 // namespace c10

// #include <ATen/core/ivalue_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/List_inl.h

// #pragma once

// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/ivalue.h>

























@Namespace("c10::impl") public static native void swap(@ByRef(true) DoubleComplexElementReference lhs, @ByRef(true) DoubleComplexElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) BooleanElementReference lhs, @ByRef(true) BooleanElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) LongElementReference lhs, @ByRef(true) LongElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) DoubleElementReference lhs, @ByRef(true) DoubleElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) TensorOptionalElementReference lhs, @ByRef(true) TensorOptionalElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) TensorElementReference lhs, @ByRef(true) TensorElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) FuturePtrElementReference lhs, @ByRef(true) FuturePtrElementReference rhs);

@Namespace("c10::impl") public static native void swap(@ByRef(true) GenericElementReference lhs, @ByRef(true) GenericElementReference rhs);





 // namespace impl



























































@Namespace("c10::impl") public static native @Const IValue ptr_to_first_element(@Const @ByRef GenericList list);





// Parsed from ATen/core/List.h

// #pragma once

// #include <ATen/core/ivalue_to.h>
// #include <ATen/core/jit_type_base.h>
// #include <c10/macros/Macros.h>
// #include <c10/macros/Export.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <vector>



// There is no to() overload for c10::optional<std::string>.
// Targeting ../DoubleComplexElementReference.java


// Targeting ../BooleanElementReference.java


// Targeting ../LongElementReference.java


// Targeting ../DoubleElementReference.java


// Targeting ../TensorOptionalElementReference.java


// Targeting ../TensorElementReference.java


// Targeting ../FuturePtrElementReference.java


// Targeting ../GenericElementReference.java


// Targeting ../DoubleComplexListIterator.java


// Targeting ../BooleanListIterator.java


// Targeting ../LongListIterator.java


// Targeting ../DoubleListIterator.java


// Targeting ../TensorOptionalListIterator.java


// Targeting ../TensorListIterator.java


// Targeting ../FuturePtrListIterator.java


// Targeting ../GenericListIterator.java



// Targeting ../DoubleComplexList.java


// Targeting ../BooleanList.java


// Targeting ../LongList.java


// Targeting ../DoubleList.java


// Targeting ../TensorOptionalList.java


// Targeting ../TensorList.java


// Targeting ../FuturePtrList.java


// Targeting ../GenericList.java


// GenericList is how IValue stores lists. It is, however, not part of the
// public API. Kernels should use Lists with concrete types instead
// (maybe except for some internal prim ops).





// #include <ATen/core/List_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/IListRef_inl.h

// #pragma once

// #include <ATen/core/List.h>
// #include <ATen/core/Tensor.h>


/*
 * Specializations of `IListRefTagImplBase` that implement the default
 * implementation for `IListRefTag::Unboxed`.
 */

/*
 * Specializations of `IListRefTagImplBase` that implement the default
 * implementation for `IListRefTag::Boxed`.
 */

/*
 * Specializations of `IListRefTagImplBase` that implement the default
 * implementation for `IListRefTag::Materialized`.
 */

/*
 * [Note: ITensorListRef]
 * Specializations necessary for `IListRef<at::Tensor>` type.
 *
 * Since the default implementations are usually done with supporting
 * `Tensor` in mind, we only have to inherit from the base implementations.
 */

/*
 * [Note: IOptTensorListRef]
 * Specializations necessary for `IListRef<at::OptionalTensorRef>` type.
 *
 * We can't get an `at::OptionalTensorRef` directly from an instance of
 * `List<optional<Tensor>>` (the type that corresponds to the boxed world).
 *
 * So, the default implementation won't help us. Thus, we have to implement
 * this method ourselves.
 */

 // namespace detail
 // namespace c10

// [Note: ITensorListRef]
// [Note: IOptTensorListRef]

 // namespace at


// Parsed from ATen/core/IListRef.h

// #pragma once

// #include <ATen/core/ivalue_to.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <functional>
// #include <initializer_list>
// #include <iterator>
// #include <type_traits>

/*
 * [Note: IListRef]
 * Wrapper around different API containers (e.g. boxed and unboxed).
 *
 * What is it?
 * ===========
 * It is a tagged union of both boxed and unboxed API containers.
 * Working implementations:
 *
 * - `IListRef<at::Tensor>`
 * - `IListRef<at::OptionalTensorRef>`
 *
 * Note that `IListRef` is a view type. Meaning that it won't own the
 * tensors it holds. It's intended to be used only as argument parameters.
 * Specifically, where these 2 worlds overlap.
 *
 * What is this for?
 * =================
 * Historically, PyTorch has maintained 2 different APIs: the unboxed
 * (called from C++ API and Python eager mode) and boxed APIs (called
 * from the TorchScript JIT, mobile interpreter, and boxed fallbacks).
 *
 * Calling unboxed kernels from the boxed "world" and vice-versa may
 * result in non-negligible overhead. Lists are one of those types:
 *
 * - Boxed world: `c10::List`
 * - Unboxed world: `c10::ArrayRef`
 *
 * In this context, `c10::IListRef` solves this problem by wrapping those
 * 2 container types, so that we don't need to convert from one to
 * the other.
 *
 * (see https://github.com/pytorch/pytorch/issues/66328)
 *
 * What does it do?
 * ================
 * This container wraps around the different tagged containers
 * (currently, only boxed and unboxed), without incurring in extra
 * overhead for converting from one to another. It does so while
 * exposing usual container methods, which dispatch to corresponding
 * implementations.
 *
 * While it works with different container types, it introduces
 * overhead for repeatedly calling member functions (since those will
 * get dispatched, again). Therefore, you should only use it to iterate
 * through the list up to one time. If you need to do more complex things,
 * call `materialize()` first.
 *
 * Adding support for a new Tag
 * ============================
 * Suppose we want to add a new tag: `Chest`. Here are the steps
 * we would have to go through:
 *
 * 1. Add a line for it in the macro `TORCH_ILISTREF_FORALL_TAGS`.
 *
 *   #define TORCH_ILISTREF_FORALL_TAGS(_, ...) \
 *     ...
 *     _(Chest, ##__VA_ARGS__)
 *
 * 2. Add type aliases, union members, and constructors.
 *
 *   template <typename T>
 *   class IListRef {
 *     ...
 *     using chest_type =
 *       typename detail::IListRefTagImpl<T, IListRefTag::Chest>::list_type;
 *     ...
 *     IListRef(...) : tag_(IListRefTag::Chest) {
 *       ...
 *     }
 *     ...
 *     union Payload {
 *       ...
 *       chest_type chest;
 *       ...
 *     };
 *     ...
 *   };
 *
 * 3. Add a default implementation for it (in 'IListRef_inl.h'). It's
 *    preferable to make the default implementation work for `T = Tensor`
 *    (both `Unboxed` and `Boxed` do it).
 *
 *   template <typename T, typename ListElemT>
 *   class IListRefTagImplBase<IListRefTag::Chest, T, ListElemT> {
 *    public:
 *     using elem_type = ListElemT;
 *     using list_type = ChestContainer<elem_type>;
 *
 *     static const list_type& unwrap(const IListRef<T>& ilist) { ... }
 *
 *     static typename list_type::const_iterator& unwrap(
 *         IListRefIterator<T>& it) { ... }
 *
 *     static const typename list_type::const_iterator& unwrap(
 *         const IListRefIterator<T>& it) { ... }
 *
 *     static IListRefConstRef<T> iterator_get(
 *         const typename list_type::const_iterator& it) { ... }
 *   }
 *
 * 4. Add an specialization for each of the already supported types.
 *    Finally, for consistency, add them to the tracking list.
 *    (see [Note: IListRefTagImpl Specializations])
 *
 *   template <>
 *   class IListRefTagImpl<IListRefTag::Chest, at::Tensor>
 *       : public IListRefTagImplBase<IListRefTag::Chest, at::Tensor> {};
 *
 * Adding support for a new Type
 * =============================
 * Suppose we want to add support for a new type: `Matrix`.
 * Here are the steps we would have to go through:
 *
 * 1. Add an specialization for each of the existing tags.
 *    For consistency, add them to the tracking list.
 *    (see [Note: IListRefTagImpl Specializations])
 *
 *   template <>
 *   class IListRefTagImpl<IListRefTag::Unboxed, Matrix>
 *       : public IListRefTagImplBase<IListRefTag::Unboxed, Matrix> {};
 *
 *   template <>
 *   class IListRefTagImpl<Matrix, IListRefTag::Boxed>
 *       : public IListRefTagImplBase<IListRefTag::Boxed, Matrix> {};
 *
 * Common Problems
 * ===============
 * 1. One of `IListRef(Iterator)` methods are failing to compile.
 *
 *     That may be happening because the container type you added
 *     is not compatible with the code written for that method. If
 *     that's true, then you might have to transform that code into
 *     a static method call (see `List::operator[]` method).
 *
 * 2. Can't make `IListRefIterator<T>::operator*` return a const-reference.
 *
 *    First, keep in mind that we assume that boxed containers will
 *    have to deal with `IValue` (e.g. `c10::List`). In this context,
 *    what may be happening is that `IValue` doesn't store internally
 *    your type `T`. Instead, it constructs a type new `T` everytime
 *    you try to get `T` for it (see `IListRef<at::OptinalTensorRef>`).
 */

/*
 * Applies arbitrary macros to each `IListRefTag`.
 */
// #define TORCH_ILISTREF_FORALL_TAGS(_, ...)
//   _(Unboxed, ##__VA_ARGS__)
//   _(Boxed, ##__VA_ARGS__)
//   _(Materialized, ##__VA_ARGS__)

/*
 * Defines a "switch-case" for `TAG`. Inside, it executes `BODY`,
 * while bringing to scope:
 *
 * - `ImplT`: the implementation class for `TAG`
 * - `this_`: the result of unwrapping `this`
 */
// #define TORCH_ILISTREF_UNWRAP_CASE(TAG, BODY)
//   case c10::IListRefTag::TAG: {
//     using ImplT = c10::detail::IListRefTagImpl<IListRefTag::TAG, T>;
//     auto& this_ = ImplT::unwrap(*this);
//     BODY
//   } break;

/*
 * Dispatches the unwrap call, depending on `TAG`, followed by
 * the execution of `BODY`. It aborts if `TAG` is not a `IListRefTag`.
 *
 * This macro is useful because it allows us to handle different
 * types (that correspond to different tags) to be implemented
 * only once. We can do it even when the implementation of the
 * different tags aren't syntatically the same, by dispatching
 * it to a function (e.g. `ImplT::<dispatch-function>(this_)`).
 */
// #define TORCH_ILISTREF_UNWRAP(TAG, BODY)
//   switch (TAG) {
//     TORCH_ILISTREF_FORALL_TAGS(TORCH_ILISTREF_UNWRAP_CASE, BODY)
//     break;
//     default:
//       TORCH_INTERNAL_ASSERT(false, "invalid IListRef tag.");
//   }

@Namespace("c10") public enum IListRefTag {
  TORCH_ILISTREF_FORALL_TAGS(0),DEFINE_TAG(1),
      None(2);

    public final int value;
    private IListRefTag(int v) { this.value = v; }
    private IListRefTag(IListRefTag e) { this.value = e.value; }
    public IListRefTag intern() { for (IListRefTag e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
/*
 * Type alias that specifies whether we return a reference or a copy of `T`.
 *
 * What is this for?
 * =================
 * Since values in the boxed world are represented by an `IValue`, we also
 * depend on whether it can be converted to a const-reference (`Tensor`) or
 * has to create a new copy of `T` (`OptionalTensorRef`).
 */

/*
 * Interface that implements key functions for each `IListRefTag` type.
 *
 * What is this for?
 * =================
 * Given an `IListRef(Iterator)<T>`, some methods have to be implemented
 * differently for each `TAG`. Therefore, the methods inside this class
 * are used as dispatch targets for the different `IListRefTag` values.
 *
 * You should create an specialization of this class for each possible
 * combination of `IListRefTag` type (except `None`) and element types
 * (e.g. `Tensor`).
 *
 * What does it do?
 * ================
 * 1. defines static methods to be used as dispatch targets by both
 *    `IListRef<T>` and `IListRefIterator<T>` (see the implementation of
 *    `IListRefTagImplBase`).
 *
 * 2. defines the `elem_type` and `list_type` aliases that will be
 *    used in the definition of `IListRef<T>`. In general, we should do
 *    so by inheriting from `IListRefTagImplBase<TAG, T, ListElemT>`.
 *
 * [Note: IListRefTagImpl Specialization]
 * ======================================
 * For `IListRef(Iterator)<at::Tensor>`:
 * - <IListRefTag::Unboxed, at::Tensor>
 * - <IListRefTag::Boxed, at::Tensor>
 * - <IListRefTag::Materialized, at::Tensor>
 *
 * For `IListRef(Iterator)<at::OptionalTensorRef>`:
 * - <IListRefTag::Unboxed, at::OptionalTensorRef>
 * - <IListRefTag::Boxed, at::OptionalTensorRef>
 * - <IListRefTag::Materialized, at::OptionalTensorRef>
 */

/*
 * Base implementation of `IListRefTagImpl<TAG, T>` methods.
 *
 * What is this for?
 * =================
 * This should make adding specializations for new types easier. For
 * example, one should be able to add a new type just by making its
 * `IListRefTagImpl` specialization inherit from `IListRefTagImplBase`.
 *
 * You should create a partial specialization for this class only if
 * you introduce a new `IListRefTag`. The idea being that there is one
 * default implementation for each possible value of `IListRefTag`.
 *
 * What does it do?
 * ================
 * 1. defines `elem_type` as an alias to `ListElemT`.
 *
 * 1. defines `list_type` as an alias to the default container type
 *    that will hold a collection of `elem_type`. The idea being that
 *    all types tagged as `TAG` will have `list_type` as its container,
 *    with different `elem_type`.
 *
 * 3. defines the default implementation for each of the methods that
 *    are supposed to be defined on `IListRefTagImpl` specializations.
 *
 * 4. inheriting from `IListRefTagImplBase<TAG, T, ListElemT>` also means
 *    that the payload of the type `IListRef<T>` will be of type `list_type`
 *    when it is tagged as `TAG`.
 */

/*
 * Materialized container for `IListRef<T>`.
 *
 * What is this for?
 * =================
 * Container that groups `T` references together. This exchanges the
 * overhead of every method call from `IListRef<T>` for a dynamic allocation.
 *
 * You should use this container instead of `IListRef<T>` if:
 *
 *   - You are going to iterate the list more than once
 *   - You need to repeatedly access arbitrary elements (using `operator[]`)
 * What does it do?

 * ================
 * Removes the reference (&) from the type, and wraps it into a
 * `std::reference_wrapper`. If `IListRefConstRef<T>` is not a
 * reference type, then it's left unchanged.
 */

 // namespace detail

/*
 * Iterator for `IListRef<T>`.
 *
 * What is it?
 * ===========
 * Currently, a `std::bidirectional_iterator` that wraps the iterator
 * types defined for each of the `IListRefTag`.
 *
 * One should be able to use it, as if it were the unwrapped
 * iterators themselves.

 * What does it do?
 * ================
 * Similarly to `IListRef<T>`, this is a wrapper class. Specifically, it
 * wraps each container's `const_iterator` type alias. So, for example,
 * given that the container for `IListRefTag::Boxed` is `c10::List`, this
 * iterator will wrap a `c10::List::const_iterator`.
 *
 * [Note: MSVC Iterator Debug]
 * ===========================
 * MSVC `vector<T>::iterator` implementation (used in the boxed variant)
 * makes it so this union's destructor, copy-constructor (assignment), and
 * move-constructor (assignment) are implicitly deleted.
 *
 * Therefore, we need to explicitly define them as needed. Follows a list
 * of places where these are needed and their reason:
 *
 *   - `Payload` destructor:
 *     it is deleted only if the macro `_ITERATOR_DEBUG_LEVEL` is set to 2.
 *
 *   - `IListRefIterator` destructor:
 *     same as above. However, we need to explicitly call the variant
 *     destructor explicitly.
 *
 *   - `IListRefIterator` copy-constructor:
 *     it is deleted only if the macro `_ITERATOR_DEBUG_LEVEL` is different
 *     than 0.
 */

/*
 * See [Note: IListRef]
 */

 // namespace c10

// #include <ATen/core/IListRef_inl.h>


// Parsed from ATen/WrapDimUtils.h

// #pragma once

// #include <ATen/core/IListRef.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/irange.h>

// if dim_post_expr is 0 and wrap_scalar is true, then dim must be in the
// range [-1, 0]. This is a special case for scalar tensors and manifests in
// e.g. torch.sum(scalar_tensor, 0) Otherwise, dim should be in the range
// [-dim_post_expr, dim_post_expr-1].

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, TensorImpl tensor);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorVector tensors);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

// Given an array of dimensions `dims` of length `ndims`, this function "Wraps"
// each dim in-place for a tensor of rank `dim_post_expr`, allowing dims to be
// specified using negative indices.
//
// Additionally, if `wrap_scalar` is true then scalar tensors with rank 0, will
// allow dimensions in the range [-1, 0]. Otherwise, an IndexError is raised for
// dimensions not in the range [-dim_post_expr, dim_post_expr).
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongPointer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongPointer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongBuffer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongBuffer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") long[] dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") long[] dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);

// Given a contiguous container of dimensions `dims`, this function "Wraps"
// each dim in-place for a tensor of rank `dim_post_expr`, allowing dims to be
// specified using negative indices.
//
// Additionally, if `wrap_scalar` is true then scalar tensors with rank 0, will
// allow dimensions in the range [-1, 0]. Otherwise, an IndexError is raised for
// dimensions not in the range [-dim_post_expr, dim_post_expr).

// previously, size [0] tensors were the only possible empty tensors; thus, it
// wasn't possible to cat empty tensors unless all the other tensors were
// 1-dimensional, so we allowed these tensors to be "skipped" (both for wrap
// dimension behavior and dimension size checking). We maintain this behavior
// for backwards compatibility, but only for this specific size (i.e. other
// empty sizes are not skipped).

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim_symint(
    @Cast("int64_t") long dim,
    @StdVector SymIntVector tensor_sizes);

// wrap negative dims in a vector
@Namespace("at") public static native void wrap_all_dims(
    @Cast("std::vector<int64_t>*") @ByRef LongVector dims_to_wrap,
    @Cast("int64_t") long tensor_total_dims);

 // namespace at


// Parsed from ATen/TensorNames.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// Targeting ../TensorName.java


// Targeting ../TensorNames.java



 // namespace namedinference
 // namespace at


// Parsed from ATen/NamedTensorUtils.h

// #pragma once
// #include <ATen/NamedTensor.h>
// #include <ATen/TensorNames.h>
// #include <ATen/WrapDimUtilsMulti.h>

// #include <ATen/core/DimVector.h>
// #include <ATen/core/Tensor.h>
// #include <functional>

@Namespace("at") public static native @Cast("bool") boolean has_names(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast("bool") boolean has_names(@ByVal TensorVector tensors);

// Converts dim to an positional index. Errors if `dim` cannot be used to
// refer to any dimension of tensor.
@Namespace("at") public static native @Cast("int64_t") long dimname_to_position(@Const @ByRef Tensor tensor, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector dimnames_to_positions(
    @Const @ByRef Tensor tensor,
    @ByVal DimnameArrayRef dims);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector dimnames_to_positions(
    @Const @ByRef Tensor tensor,
    @ByVal DimnameVector dims);

// Unifies two DimnameList to produce a third. This is useful for implementing
// the named inference rule for binary broadcasting operations like add.
//
// There are three main constraints:
// 1) Check matching: Names must match positionally from the right.
// 2) Check misaligned: If a name `n` is in `names`, then it must appear at
//    the same index from the right in other.
// 3) The output names are obtained by unifying the names individually from the
// right.
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameArrayRef names,
    @ByVal DimnameArrayRef other,
    @Cast("const char*") BytePointer action/*="broadcast"*/);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameArrayRef names,
    @ByVal DimnameArrayRef other);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameVector names,
    @ByVal DimnameVector other,
    String action/*="broadcast"*/);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameVector names,
    @ByVal DimnameVector other);

@Namespace("at") public static native void reportNYIDimnameOverload(@Cast("const char*") BytePointer op_name);
@Namespace("at") public static native void reportNYIDimnameOverload(String op_name);

// [NOTE] Writing name inference rules
//
// Operators that support named tensors are either composed of operations that
// support named tensors or implement some name inference rule. An op that
// implements its own name inference rule generally looks like the following:
//
// Tensor op(...) {
//   perform_shape_checks(...);
//   # (1)
//   auto maybe_outnames = compute_outnames(...);
//   auto result = [&]() {
//     NoNamesGuard guard;
//     return op_impl(...);
//   }();
//   # (2)
//   propagate_names_if_nonempty(result, maybe_outnames);
//
// Each op has (1) a compute outnames step and (2) a propagate names step.
//
// compute_outnames is responsible for checking that input names match and
// determining what the output names should be. It returns either:
// - {} (if the inputs tensors are all unnamed)
// - non-empty outnames.
//
// propagate_names_if_nonempty propagates the outnames if they exist to the
// result tensors.
//
// The {} case is an optimization; if the user does not use named tensors they
// pay no perf cost for it.


// Propagates `names` to `result` if `names` is not empty.
// `names` can be empty; see [NOTE] Writing name inference rules
// If `names` is not empty, `names.size()` should equal `result.dim()`.
// When in doubt, use this overload instead of the others.
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameVector maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameVector maybe_names);

// Propagates `names` to `result`. Only use this if we are certain that there
// are names to propagate (that names is not empty).
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameVector names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameVector names);

// Propagates all names from src to result.
@Namespace("at::namedinference") public static native void propagate_names(@Const @ByRef Tensor result, @Const @ByRef Tensor src);

// Propagates all names except for those at the excluded_idxs.
@Namespace("at::namedinference") public static native void propagate_names_except(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal LongArrayRef excluded_idxs);
@Namespace("at::namedinference") public static native void propagate_names_except(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... excluded_idxs);

// Used for reduction ops that have a `keepdim` arg.
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal LongArrayRef excluded_idxs,
    @Cast("bool") boolean keepdim);
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] excluded_idxs,
    @Cast("bool") boolean keepdim);

@Namespace("at::namedinference") public static native void propagate_names_for_expand(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_broadcast_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    @Cast("const char*") BytePointer op_name);
@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    String op_name);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_matmul_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cdist_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_bmm_outnames(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_squeeze_outnames(@Const @ByRef Tensor tensor);
@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_squeeze_outnames(
    @Const @ByRef Tensor tensor,
    long dims);



// TensorImpl* overloads for Legacy TH/THC code. Use these sparingly.

@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameVector maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameVector maybe_names);

@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameVector names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameVector names);

@Namespace("at::namedinference") public static native void propagate_names(TensorImpl result, TensorImpl src);

@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names);
@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameVector names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameVector names);

@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names);
@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameVector names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameVector names);

@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @Const @ByRef TensorBase src);

// result = m1 @ m2 + bias
@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmm(
    @Const @ByRef Tensor m1,
    @Const @ByRef Tensor m2,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmv(
    @Const @ByRef Tensor mat,
    @Const @ByRef Tensor vec,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native void check_names_for_dot(TensorImpl vec1, TensorImpl vec2);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_baddbmm_outnames(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @Cast("bool") boolean are_names_equal(TensorImpl self, TensorImpl other);

 // namespace namedinference

 // namespace at


// Parsed from ATen/core/VariableHooksInterface.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <ATen/core/Tensor.h>

// A little explanation about why this file exists at all.  We have
// a few methods on Tensor class which require access to reified access to
// AutogradMeta.  In open source, this isn't a big deal: we just access
// torch/csrc/autograd/variable.h from aten/src/ATen/core/Tensor.cpp and
// we can put the definitions inline.  This is because everything gets balled
// into a single dynamic library in the end.
//
// However, inside our Facebook internal version of our build system, we
// have a split between aten and torch/csrc.  So we cannot simply just
// cross this boundary.  "Now wait," you might say, "Why don't we just
// merge the libraries inside Facebook".  Well, the problem is that there
// are some downstream applications which are at binary size limit, and
// incorporating all of the extra code from libtorch would push them
// over (admarket/adreview/service:adreviewservice, see also
// https://github.com/pytorch/pytorch/pull/29299)  So if you want to do that,
// we have to fix all of the services like this.
//
// I didn't want to block eliminating Tensor-Variable on this work, so I
// had to introduce another dynamic dispatch to get to the variable
// implementations (which live in torch/csrc/autograd/variable.cpp, FYI).
//
// I also considered using our existing dynamic dispatch mechanism, c10
// dispatcher, to do this.  However, (1) some of the functions on Tensor
// have weird signatures that are not supported by autograd, and (2)
// see this bug https://github.com/pytorch/pytorch/issues/30102


// Targeting ../VariableHooksInterface.java



@Namespace("at::impl") public static native void SetVariableHooks(VariableHooksInterface hooks);
@Namespace("at::impl") public static native VariableHooksInterface GetVariableHooks();
@Namespace("at::impl") public static native @Cast("bool") boolean HasVariableHooks();

 // namespace at::impl


// Parsed from torch/csrc/autograd/variable.h

// #pragma once

// #include <torch/csrc/utils/python_stub.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/cpp_hook.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/forward_grad.h>
// #include <torch/csrc/autograd/function_hook.h>

// #include <ATen/NamedTensorUtils.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/VariableHooksInterface.h>
// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <mutex>
// #include <stdexcept>
// #include <string>
// #include <utility>
// #include <vector>

/** {@code Variable} is exactly the same as {@code Tensor} (i.e. we have {@code using Variable =
 *  at::Tensor}). This means you can perform all the usual mathematical and
 *  other operations you can perform on {@code Tensor}s also on {@code Variable}s.
 * 
 *  The only reason we are keeping the {@code Variable} class is backward
 *  compatibility with external user's legacy C++ frontend code. Our intention
 *  is to eliminate the {@code Variable} class in the near future. */

 // namespace autograd
 // namespace torch

// The following are all internal APIs and should not be shown in libtorch docs.
// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS
// ... #endif`

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

/** Check if this type is supported by the autograd engine.
 *  If you change this, update the doc at the top of the
 *  torch/autograd/__init__.py file and
 *  "test_set_requires_grad_only_for_continuous_types" in test/test_autograd.py */
@Namespace("torch::autograd") public static native @Cast("bool") boolean isDifferentiableType(ScalarType t);

/**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                                 Variable
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  A {@code Variable} augments a {@code Tensor} with the ability to interact in our
 *  autograd machinery. Conceptually, {@code Variable}s travel along {@code Edge}s between
 *  {@code Node}s in the autograd graph. A {@code Variable} can either be a leaf, like a
 *  weight in a neural network, or an interior variable, when it is the result
 *  of an operation between variables. Every {@code Variable} also stores another
 *  {@code Variable} called its {@code grad} (gradient). If the variable is a leaf, its
 *  gradient will be accumulated into this variable.
 * 
 *  Every Tensor is a Variable, but sometimes we colloquially refer to Variables
 *  that don't require gradients as Tensors (since none of the autograd
 *  machinery for Variables applies).  Historically, Variables and Tensors
 *  were separate concepts, but now they are exactly the same (i.e. we have
 *  {@code using Variable = at::Tensor}).
 * 
 *                               Gradient Edges
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Furthermore, {@code Variable}s have the notion of a {@code gradient_edge}, which is the
 *  edge in the autograd graph that connects the variable to a particular input
 *  of the gradient function that will be invoked with the variable during the
 *  backward pass. More precisely, this gradient function can be one of two
 *  things:
 *  1. A {@code grad_fn}, if the variable is in the interior of the graph. This is the
 *     gradient of the function that produced the variable.
 *  2. A {@code grad_accumulator}, if the variable is a leaf, which accumulates a
 *     scalar gradient value into its {@code grad} variable.
 * 
 *                                Versioning
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Another major feature of {@code Variable}s are *versions*. Versions are
 *  incremented when an in-place mutation of a variable occurs. Versions are
 *  useful when constructing {@code SavedVariable}s, which take a snapshot of a
 *  {@code Variable} at a certain version. You can retrieve a {@code Variable}'s version
 *  through its {@code current_version()} method.
 * 
 *                                  Views
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  It is possible for a  {@code Variable} to be a *view* of another {@code Variable}, in
 *  which case it tracks that {@code Variable}'s data and autograd history. Beyond
 *  construction, the interface of a view is identical to that of a regular
 *  {@code Variable}. You can determine whether {@code Variable} is in fact a view by
 *  probing its {@code is_view()} method. Note that the *view* semantics are only
 *  meaningful for {@code Variable} relations that are relevant to autograd.
 *  See NOTE [ Autograd View Variables ] for more details.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */

// Private-ish functions for manipulating variables; we don't want to put them
// on Tensor proper

// WARNING: This may return a nullptr.  If you require AutogradMeta to return
// a materialized structure, use materialize_autograd_meta instead.
@Namespace("torch::autograd::impl") public static native AutogradMeta get_autograd_meta(@Const @ByRef TensorBase arg0);

// WARNING: This will return a nullptr if the Tensor is not a view.
@Namespace("torch::autograd::impl") public static native DifferentiableViewMeta get_view_autograd_meta(@Const @ByRef TensorBase arg0);

// Returns the current autograd meta, materializing it if it was previously
// none.  This counts as a *mutating* operation, so do not call it on
// "read-only" operators; in particular, this is NOT thread safe
@Namespace("torch::autograd::impl") public static native AutogradMeta materialize_autograd_meta(@Const @ByRef TensorBase arg0);

/** Set the gradient accumulator of the {@code Variable}. This is only applicable to
 *  leaf variables. Interior variables should call {@code set_gradient_edge()}. */

/** Attempts to get a pointer to the gradient accumulator of the {@code Variable},
 *  if it still exists. If the gradient accumulator function has been
 *  destroyed, returns a {@code nullptr}. */
@Namespace("torch::autograd::impl") public static native @SharedPtr Node try_get_grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Gets the gradient accumulator of the {@code Variable} if it has one, or else
 *  create one on the fly and return it. */
@Namespace("torch::autograd::impl") public static native @SharedPtr Node grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Returns the "canonical" gradient edge of this {@code Variable}, i.e. either the
 *  gradient function if this is an interior {@code Variable}, or the gradient
 *  accumulator otherwise. If the {@code Variable} is interior, the returned {@code Edge}
 *  will store the input index of the {@code Node} to which this variable is
 *  connected in its {@code input_nr} field. For leaves, the {@code input_nr} is always
 *  zero. Note that {@code set_gradient_edge} and {@code gradient_edge} are not
 *  symmetric. You must use {@code set_gradient_edge} to set the {@code grad_fn} and
 *  {@code set_grad_accumulator} to set the accumulator. */
@Namespace("torch::autograd::impl") public static native @ByVal Edge gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Set the gradient edge -- i.e. {@code grad_fn} and {@code input_nr} -- of the
 *  {@code Variable}.
 *  NOTE: This will always set the {@code grad_fn}, even if this is a leaf variable,
 *  and never the {@code grad_accumulator}. For the latter, use
 *  {@code set_grad_accumulator}. This allows late construction of an interior
 *  {@code Variable}. */

///
@Namespace("torch::autograd::impl") public static native void set_gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge edge);

// Autograd Graph Interaction
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Update the {@code grad_fn} of an existing Variable. Called after in-place
 *  modifications.
 * 
 *  For View Variables:
 *  Called after in-place modifications. Modifies the grad_fn of the base
 *  Variable. */
@Namespace("torch::autograd::impl") public static native void rebase_history(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge gradient_edge);

/** Gets the raw gradient function pointer, whatever it currently is. */
@Namespace("torch::autograd::impl") public static native Node grad_fn_unsafe(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Increments the version count of this {@code Variable}. */
@Namespace("torch::autograd::impl") public static native void bump_version(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
@Namespace("torch::autograd::impl") public static native void set_version_counter(
    @Cast("const torch::autograd::Variable*") @ByRef Tensor arg0,
    @Const @ByRef VariableVersion version_counter);

/** Retrieves this {@code Variable}s version counter. */
@Namespace("torch::autograd::impl") public static native @Const @ByRef VariableVersion version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

@Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString BytePointer name);
@Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString String name);

@Namespace("torch::autograd::impl") public static native void add_hook(
    @Const @ByRef TensorBase arg0,
    @UniquePtr @Cast({"", "std::unique_ptr<torch::autograd::FunctionPreHook>&&"}) FunctionPreHook hook);
@Namespace("torch::autograd::impl") public static native @ByRef FunctionPreHookVector hooks(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
@Namespace("torch::autograd::impl") public static native void clear_hooks(@Const @ByRef TensorBase arg0);

@Namespace("torch::autograd::impl") public static native void set_post_acc_grad_hooks(
    @Const @ByRef TensorBase arg0,
    @UniquePtr PostAccumulateGradHook dict);
@Namespace("torch::autograd::impl") public static native @UniquePtr PostAccumulateGradHook post_acc_grad_hooks(
    @Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

@Namespace("torch::autograd::impl") public static native void create_cpp_hook(
    @Const @ByRef TensorBase arg0,
    @Cast("bool") boolean is_retains_grad_hooks/*=false*/);
@Namespace("torch::autograd::impl") public static native void create_cpp_hook(
    @Const @ByRef TensorBase arg0);

// Targeting ../AutogradMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     DifferentiableViewMeta
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** NOTE [ Autograd View Variables ]
 * 
 *  Many operations return Variable that shares storage with an input Variable.
 *  The returned Variable is called a **view** Variable on the input **base**
 *  Variable.
 * 
 *  In PyTorch, we have two types of views: differentiable views, and
 *  non-differentiable views. In either type, to support proper version
 *  checking, the base and view Variables must always share the same
 *  version_counter.
 * 
 * 
 *  Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  This class allows to track both forward and backward AD differentiable
 *  views. These views can have different base as non-differentiable view for
 *  forward and backward mode AD are not the same.
 * 
 *  Most function are either both forward and backward differentiable views (for
 *  example: view, select, narrow, transpose, etc) or both not forward and not
 *  backward differentiable views (for example: indices, values, eq, lt, etc).
 *  But there are also functions that are forward but not backward
 *  differentiable views (only detach for now) or functions that are backward
 *  but not forward differentiable view (only make_dual and unpack dual for
 *  now).
 * 
 *  A concrete example of two views with different bases is as follow:
 * 
 *      # Have:
 *      #   dual is a dual Tensor that is neither a forward or backward view
 *      detached_dual = dual.detach()
 *      view = detached_dual.view_as(dual)
 *      # The forward base of view is dual
 *      # The backward base of view is detached_dual
 * 
 *  - Backward Mode View
 *  Differentiable views are the view variables where you want gradients to flow
 *  back to the base variables. Out-of-place operations on views are quite
 *  straightforward, but in-place ones are very tricky. Even if the base
 *  variable may not require grad when we create the view, we still need to
 *  track the view relation because future in-place ops may require back-proping
 *  through it. For example, we need to support
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- should return an all ones
 *      tensor
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      view = base[1]
 *      base.copy_(var)
 *      torch.autograd.grad(view.sum(), var)  <- should return a tensor with
 *                                               var[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *  - Forward Mode View
 *  Forward differentiable views follow the same semantic as backward ones but
 *  show up differently as they are computed along with the forward evaluation.
 *  The hard examples above are thus very similar
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      # Now, base is a dual Tensor
 *      _, fw_grad = fwAD.unpack_dual(base) <- fw_grad should be a tensor with
 *                                               fw_grad[1] filled with all ones
 *                                               and zeros everywhere else
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      view = base[1]
 *      base.copy_(var)
 *      _, fw_grad = fwAD.unpack_dual(view) <- fw_grad should be an all ones
 *      tensor
 * 
 *  See Note [Forward Grad View/inplace] for more details on how we handle these
 *  hard cases.
 * 
 * 
 *  DifferentiableViewMeta is created to support gradient tracking of
 *  such **in-place** operations. In particular,
 *    + if an in-place op is done on base, the grad_fn field of the view may
 *      become stale. So accesses should always go through grad_fn(), which
 *      reconstructs an updated grad_fn if the version_counter has incremented.
 *      All other fields are always valid.
 *    + if an in-place op is done on view, in rebase_history() of view, which is
 *      called after every in-place op in VariableType.cpp, the grad_fn of base
 *      is updated.
 *    + if a single autograd Node returns multiple differentiable views, if any
 *      output is modified by an inplace operation, the autograd engine will
 *      make an equivalent graph (corresponding to the view operations) without
 *      using equivalent graph, where each output is treated as if it were
 *      produced by a distinct view operation. This discards the original (e.g.,
 *      user provided) grad_fn. If the provided grad_fn does more than the
 *      backward of the view, then the DifferentiableViewMeta must be created
 *      with creation_meta= CreationMeta::MULTI_OUTPUT_NODE to prevent the
 *      engine from ignoring the provided grad_fn.
 * 
 *  Interaction with GradMode:
 *  The particular case that we consider here is:
 * 
 *      # Have:
 *      #   base.requires_grad = True or False
 *      with torch.no_grad():
 *          view = base[1]
 *      base.requires_grad_()
 *      view.copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- what should it return?
 * 
 *  Given that this particular code example is ambiguous and can easily be
 *  replace by either moving both inside the no_grad block or both outside, we
 *  explicitly forbid it. For now, it is deprecated by a warning. This is
 *  achieved by setting creation_meta=CreationMeta::NO_GRAD_MODE for all
 *  differentiable views created in no_grad mode.
 * 
 *  See Note [View + Inplace update for base tensor]
 *  and Note [View + Inplace update for view tensor] for the details how
 *  autograd handles inplace update with view ops.
 * 
 *  Non-Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  In certain cases, although function outputs share storage with inputs, they
 *  will **never** require gradient history tracking. Instead of registering the
 *  view relation via DifferentiableViewMeta in autograd, the views will be
 *  using usual AutogradMeta and just share the version counters with the base
 *  Variables.
 *  Such views include:
 *    1. Views created from .detach()
 *    2. Views that are non-differentiable by its nature.
 *       E.g., {@code sparse_tensor.indices()} is a integral view on a (possibly)
 *       floating point tensor.
 *       See top of {@code derivatives.yaml} on how to specify that outputs of a
 *       function are non-differentiable.
 *  These are called non-differentiable views as the gradients do not flow
 *  through the view relation.
 * 
 *  Relevant logic for both differentiable and non-differentiable views is
 *  implemented in make_variable_(non_)differentiable_view below, and
 *  wrap_output of gen_variable_type.py.
 <p>
 *  NOTE [ View + Inplace detection ]
 * 
 *  We want to detect views followed by inplace as they are often forbidden to
 *  ensure correctness of the computed gradients. But since we want to only
 *  notify the user when both happen, we tag the DifferentiableViewMeta when the
 *  view is created via the {@code make_variable_*_view()} functions. This tag is then
 *  checked by the {@code check_inplace()} function from {@code VariableTypeUtils.h} that
 *  should be called before every inplace operation and to detect cases where
 *  other views are modified and this one is rebased by side effect, we also
 *  check in the {@code VariableHooks::grad_fn()}.
 <p>
 *  Flag that gives more information about when this view was created:
 *  - IN_CUSTOM_FUNCTION should be set when the view is created inside a custom
 *    autograd Function is returned.
 *  - NO_GRAD_MODE should be set when a view in created when GradMode is
 *  disabled
 *  - MULTI_OUTPUT_NODE should be set when a Node created by codegen code
 *  returns
 *    multiple differentiable views
 *  - Inference_MODE should be set when a view of normal tensor is created in
 *  InferenceMode.
 *  - DEFAULT is for all other cases */
@Namespace("torch::autograd") public enum CreationMeta {
  DEFAULT((byte)(0)),
  IN_CUSTOM_FUNCTION((byte)(1)),
  MULTI_OUTPUT_NODE((byte)(2)),
  NO_GRAD_MODE((byte)(3)),
  INFERENCE_MODE((byte)(4));

    public final byte value;
    private CreationMeta(byte v) { this.value = v; }
    private CreationMeta(CreationMeta e) { this.value = e.value; }
    public CreationMeta intern() { for (CreationMeta e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Handles correctly propagating CreationMeta when a new view is created from a
 *  previous view. In general, we don't want the new view to be _less_
 *  restrictive than the previous view (it's okay to be _more_ restrictive). A
 *  CreationMeta value of DEFAULT is currently the least restrictive, as the
 *  behavior for all other CreationMeta values is to error out for in-place ops.
 *  A CreationMeta value of INFERENCE_MODE is currently the most restrictive, so
 *  it takes precedence in propagation. If this changes, the logic here will
 *  need to be updated to properly handle the new semantics. */
@Namespace("torch::autograd") public static native CreationMeta propagate_creation_meta(
    CreationMeta prev_view_creation_meta,
    CreationMeta new_view_creation_meta);
@Namespace("torch::autograd") public static native @Cast("torch::autograd::CreationMeta") byte propagate_creation_meta(
    @Cast("torch::autograd::CreationMeta") byte prev_view_creation_meta,
    @Cast("torch::autograd::CreationMeta") byte new_view_creation_meta);

/** Unified function to handle error checking when rebase happens
 *  indirect=true means that the caller is not doing the inplace, but the
 *  inplace happened somewhere else. */
@Namespace("torch::autograd") public static native void handle_view_on_rebase(
    DifferentiableViewMeta diff_view_meta,
    @Cast("bool") boolean indirect/*=false*/);
@Namespace("torch::autograd") public static native void handle_view_on_rebase(
    DifferentiableViewMeta diff_view_meta);
// Targeting ../DifferentiableViewMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                        Variable Implementation
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// Factory Functions
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Creates a {@code Variable} that is a *view* of another (*base*) variable.
 *  The {@code gradient_edge} is an optional (gradient_function, input_number) pair.
 *  {@code is_differentiable} is a bool that specifies whether this view is
 *  differentiable, i.e., whether the relation should be tracked by autograd.
 *  See NOTE [ Autograd View Variables ] for details.
 <p>
 *  NOTE: {@code allow_tensor_metadata_change} is set to true by default, because
 *  there are a lot of call sites to these factory functions that need to change
 *  the variable's size or storage afterwards, and they don't expect the
 *  original tensor (where the variable is created from) to be updated. Setting
 *  {@code allow_tensor_metadata_change_} to false by default would unnecessarily
 *  prevent those changes from happening and is undesirable. */

// See NOTE [ Autograd View Variables ] for details.
// Differentiable view. Track history with DifferentiableViewMeta.
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta);

// See NOTE [ Autograd View Variables ] for details.
// Non-differentiable view. Just share version counter.

///
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying
 *  {@code TensorImpl}. {@code requires_grad} should be set only for leaves, and determines
 *  whether the {@code Variable} will accumulate gradients. NOTE: {@code data} must *not* be
 *  a {@code Variable} already. Its dynamic type *must* be {@code Tensor}.
 * 
 *  TODO: Eliminate this function as much as possible, as it can be expressed
 *  more clearly as detach() or a no-op in most call sites (especially when
 *  there is only one use of the variable). */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @Cast("bool") boolean requires_grad/*=false*/,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying
 *  {@code TensorImpl}. {@code gradient_edge} should be a (function, input_nr) pair
 *  specifying the function in the autograd graph, and what particular input of
 *  that function, this variable is connected to. */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge);

@Namespace("torch::autograd::utils") public static native @Cast("bool") boolean has_same_meta(@Cast("const torch::autograd::Variable*") @ByRef Tensor base, @Cast("const torch::autograd::Variable*") @ByRef Tensor other);

 // namespace utils
 // namespace autograd
 // namespace torch

// #endif /* DOXYGEN_SHOULD_SKIP_THIS */


// Parsed from torch/csrc/autograd/autograd.h

// #pragma once

// #include <torch/csrc/autograd/variable.h>

/** Computes the sum of gradients of given tensors with respect to graph leaves.
 * 
 *  The graph is differentiated using the chain rule. If any of {@code }tensors{@code }
 *  are non-scalar (i.e. their data has more than one element) and require
 *  gradient, then the Jacobian-vector product would be computed, in this case
 *  the function additionally requires specifying {@code grad_tensors}. It should be a
 *  sequence of matching length, that contains the "vector" in the
 *  Jacobian-vector product, usually the gradient of the differentiated function
 *  w.r.t. corresponding tensors
 *  ({@code torch::Tensor()} is an acceptable value for all tensors that don't need
 *  gradient tensors).
 * 
 *  This function accumulates gradients in the leaves - you might need to zero
 *  them before calling it.
 * 
 *  @param tensors Tensors of which the derivative will be computed.
 *  @param grad_tensors The "vector" in the Jacobian-vector product, usually
 *  gradients
 *      w.r.t. each element of corresponding tensors. {@code torch::Tensor()} values
 *      can be specified for scalar Tensors or ones that don't require grad. If
 *      a {@code torch::Tensor()} value would be acceptable for all grad_tensors, then
 *      this argument is optional.
 *  @param retain_graph If {@code false}, the graph used to compute the grad will be
 *  freed.
 *      Note that in nearly all cases setting this option to {@code true} is not
 *      needed and often can be worked around in a much more efficient way.
 *      Defaults to the value of {@code create_graph}.
 *  @param create_graph If {@code true}, graph of the derivative will be constructed,
 *  allowing
 *      to compute higher order derivative products. Defaults to {@code false}.
 *  @param inputs Inputs w.r.t. which the gradient will be accumulated into
 *      {@code at::Tensor::grad}. All other Tensors will be ignored. If not provided,
 *      the gradient is accumulated into all the leaf Tensors that were used to
 *      compute param {@code tensors}. */
//      When inputs are provided and a given input is not a leaf,
//      the current implementation will call its grad_fn (even though it is not
//      strictly needed to get this gradients). It is an implementation detail
//      on which the user should not rely. See
//      https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for
//      more details.

///
///
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensors,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector grad_tensors/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector inputs/*={}*/);
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensors);

/** Computes and returns the sum of gradients of outputs with respect to the
 *  inputs.
 * 
 *  {@code }grad_outputs{@code } should be a sequence of length matching {@code }output{@code }
 *  containing the "vector" in Jacobian-vector product, usually the pre-computed
 *  gradients w.r.t. each of the outputs. If an output doesn't require_grad,
 *  then the gradient can be {@code }torch::Tensor(){@code }).
 * 
 *  @param outputs outputs of the differentiated function.
 *  @param inputs Inputs w.r.t. which the gradient will be
 *      returned (and not accumulated into {@code }at::Tensor::grad{@code }).
 *  @param grad_outputs The "vector" in the Jacobian-vector product.
 *      Usually gradients w.r.t. each output. {@code torch::Tensor()} values can be
 *      specified for scalar Tensors or ones that don't require grad. If a
 *      {@code torch::Tensor()} value would be acceptable for all grad_tensors, then
 *      this argument is optional. Default: {@code {}}.
 *  @param retain_graph If {@code }false{@code }, the graph used to compute the grad
 *      will be freed. Note that in nearly all cases setting this option to
 *      {@code }true{@code } is not needed and often can be worked around in a much more
 *      efficient way. Defaults to the value of {@code }create_graph{@code }.
 *  @param create_graph If {@code }true{@code }, graph of the derivative will
 *      be constructed, allowing to compute higher order derivative products.
 *      Default: {@code }false{@code }.
 *  @param allow_unused If {@code }false{@code }, specifying inputs that were not
 *      used when computing outputs (and therefore their grad is always zero)
 *      is an error. Defaults to {@code }false{@code }. */
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector inputs,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector grad_outputs/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast("bool") boolean allow_unused/*=false*/);
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector inputs);

/** Creates a new dual level and returns its index. This level index should then
 *  be used to call into the other functions below. This API supports entering a
 *  new level before the previous one is exited. We call them nested forward AD
 *  levels. These can be used to compute higher order derivatives. */
@Namespace("torch::autograd::forward_ad") public static native @Cast("uint64_t") long enter_dual_level();

/** Exits the given level. This will clear up all the gradients from this level
 *  and all dual Tensors that had gradients for this level will become regular
 *  Tensors again. This function can only be used to exit the innermost nesting
 *  level and so exiting must happen in reverse order compared to the entering
 *  that was done with the function above. */
@Namespace("torch::autograd::forward_ad") public static native void exit_dual_level(@Cast("uint64_t") long level);

 // namespace forward_ad
 // namespace autograd
 // namespace torch


// Parsed from ATen/core/alias_info.h

// #pragma once
// #include <unordered_set>
// #include <vector>
// #include <ATen/core/symbol.h>
// #include <c10/util/Exception.h>
// #include <c10/util/hash.h>
// Targeting ../AliasInfo.java



@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef AliasInfo lhs, @Const @ByRef AliasInfo rhs);

// this does match the way things are represented in the schema
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef AliasInfo aliasInfo);
 // namespace c10



// Parsed from ATen/core/operator_name.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/string_view.h>
// #include <string>
// #include <utility>
// #include <ostream>
// Targeting ../OperatorName.java



// Non-owning view of an OperatorName.  Unlike OperatorName, most of
// its functions are constexpr, so it can be used for compile time
// computations

@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef OperatorName lhs, @Const @ByRef OperatorName rhs);

@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef OperatorName lhs, @Const @ByRef OperatorName rhs);

@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef OperatorName opName);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, @Const @ByRef OperatorName arg1);

 // namespace c10



// Parsed from ATen/core/dispatch/OperatorOptions.h

// #pragma once

// #include <cstdint>

@Namespace("c10") public enum AliasAnalysisKind {
  INTERNAL_SPECIAL_CASE((byte)(0)),
  CONSERVATIVE((byte)(1)), // The most conservative alias analysis type, assumes
                // side-effects. This is the default analysis.
  FROM_SCHEMA((byte)(2)),
  PURE_FUNCTION((byte)(3));

    public final byte value;
    private AliasAnalysisKind(byte v) { this.value = v; }
    private AliasAnalysisKind(AliasAnalysisKind e) { this.value = e.value; }
    public AliasAnalysisKind intern() { for (AliasAnalysisKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #if !defined(_MSC_VER)
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(AliasAnalysisKind aliasAnalysisKind);

 // namespace c10


// Parsed from ATen/core/function_schema.h

// #pragma once

// #include <c10/util/StringUtil.h>
// #include <c10/util/string_view.h>
// #include <c10/util/irange.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/symbol.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/alias_info.h>
// #include <ATen/core/operator_name.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <unordered_map>

// schema as used in the compiler for resolving function calls and reporting
// errors. These objects should be constructed from C10 schema once those
// are available.

@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Argument lhs, @Const @ByRef Argument rhs);
// Targeting ../Argument.java



@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Argument lhs, @Const @ByRef Argument rhs);

@Namespace("c10") public enum SchemaArgType { input(0), output(1);

    public final int value;
    private SchemaArgType(int v) { this.value = v; }
    private SchemaArgType(SchemaArgType e) { this.value = e.value; }
    public SchemaArgType intern() { for (SchemaArgType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../SchemaArgument.java



@Namespace("c10") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef FunctionSchema lhs, @Const @ByRef FunctionSchema rhs);
// Targeting ../FunctionSchema.java



@Namespace("c10") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef FunctionSchema lhs, @Const @ByRef FunctionSchema rhs);

// print out Argument, which is compatible with FunctionSchema parser
// full format: Type(alias)? name=default_value
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Argument arg);

@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef FunctionSchema schema);

@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef FunctionSchema schema);

 // namespace c10
 // namespace std


// #include <ATen/core/function_schema_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/function_schema_inl.h

// #pragma once
// #include <ostream>
// #include <sstream>

// note: windows build doesn't find symbols in operator files unless
// this is a header file

@Namespace("c10") public static native @Cast("size_t") long findFirstOutArg(@StdVector Argument args);



















// covariant subtyping of list of Arguments
@Namespace("c10") public static native @Cast("bool") boolean isSubtypeOfList(
    @ByVal ArgumentArrayRef child,
    @ByVal ArgumentArrayRef parent,
    @Cast("std::ostream*") Pointer why_not);



 // namespace c10


// Parsed from ATen/core/op_registration/infer_schema.h

// #pragma once

/**
 * This file contains functionality to take a C++ function and infer its
 * c10::FunctionSchema.
 */

// #include <ATen/core/function_schema.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Metaprogramming.h>
// Targeting ../ArgumentDef.java



/** Checks the static C++ types {@code Types} for correctness to catch common error cases. */

/** Creates a vector of {@code ArgumentDef} from a list of C++ types that are specified
 *  as template arguments. */

/** Creates a vector of {@code ArgumentDef} from a list of C++ types that are specified
 *  as a tuple (i.e. in the way c10 kernels return values).
 *  It can be a tuple<A, B, C> if there's three output arguments with types A, B, C.
 *  It can be an empty tuple<>, or void for kernels that don't return anything.
 *  It can be a single type A (i.e. no tuple) for the case where a kernel just
 *  returns one value. */


@Namespace("c10::detail::infer_schema") public static native @ByVal FunctionSchema make_function_schema(@ByVal ArgumentDefArrayRef arguments, @ByVal ArgumentDefArrayRef returns);

/** Creates a {@code FunctionSchema} object from a {@code FunctionTraits} type for a
 *  function. Flattens std::tuple returns into multiple return types */

/** Creates a {@code FunctionSchema} object from a {@code FunctionTraits} type for a
 *  function. Preserves std::tuple returns as a Tuple return type */




@Namespace("c10") public static native @ByVal StringOptional findSchemaDifferences(@Const @ByRef FunctionSchema inferred, @Const @ByRef FunctionSchema specified);




// Parsed from ATen/record_function.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <ATen/core/operator_name.h>
// #include <c10/macros/Export.h>
// #include <c10/util/Optional.h>
// #include <c10/util/SmallVector.h>
// #include <c10/util/variant.h>

// #include <array>
// #include <atomic>
// #include <functional>
// #include <memory>


// Kind of record function scope;
@Namespace("at") public enum RecordScope {
  // c10/ATen ops, autograd nodes
  FUNCTION((byte)(0)),
  // Functions/nodes called from the autograd
  BACKWARD_FUNCTION((byte)(1)),
  // TorchScript functions, methods
  TORCHSCRIPT_FUNCTION((byte)(2)),
  // Kernel Function dtype Tag
  KERNEL_FUNCTION_DTYPE((byte)(3)),
  // Torchbind custom class,
  CUSTOM_CLASS((byte)(4)),
  // Generic Build Feature
  BUILD_FEATURE((byte)(5)),
  // Kernel Function dtype Tag
  LITE_INTERPRETER((byte)(6)),
  // User defined scope (e.g. with record_function())
  USER_SCOPE((byte)(7)),
  // Scopes for static runtime, a specialized TorchScript interpreter
  STATIC_RUNTIME_OP((byte)(8)),
  STATIC_RUNTIME_MODEL((byte)(9)),
  NUM_SCOPES((byte)(10));// must be the last in the list

    public final byte value;
    private RecordScope(byte v) { this.value = v; }
    private RecordScope(RecordScope e) { this.value = e.value; }
    public RecordScope intern() { for (RecordScope e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

 // namespace at
 // namespace std

// Soft limit on the number of callbacks to use;
@Namespace("at") @MemberGetter public static native @Cast("const std::size_t") long kSoftLimitCallbacks();

// An abstract base class for various observer contexts that can be attached to
// the RecordFunction.

//
// PyTorch callbacks/observers API:
//

/**
 * RecordFunctionCallback represents a pair of callbacks to be used with
 * RecordFunction, members:
 *   start, end - the callbacks to run when entering and exiting the scope;
 *     optionally, the start callback may return an ObserverContext which will
 *     be passed to the end callback, use appropriate constructor accordingly.
 *   needs_inputs - whether the callbacks need the inputs passed from the
 * observed function/range; NOTE: passing the inputs incurs an additional
 * overhead; sampling_probability - if not 1.0, then the callback is
 * probabilistically sampled to run; NOTE: start and end callbacks always run as
 * a pair and are sampled together; scopes - types of scopes to execute the
 * callbacks on (see RecordScope); passing empty set means the callbacks will be
 * executed for all possible scope types should_run - optional function that
 * returns whether this callback should run; overwrites the effect of setting
 * sampling_probability
 */

// Notes:
//  - two types of callbacks are provided: thread local and global
//     - thread local callbacks are added/removed only for the given thread
//       and are stored locally for each thread and separately from the list
//       of the global callbacks
//     - global callbacks are stored in a single per process list and are
//       invoked by every RecordFunction, in addition to the thread local
//       callbacks specific to the given thread
//  - we allow the added callbacks to be sampled, by specifying a sampling
//    probability for each callback pair, if the start callback is
//    not picked to run, the corresponding end callback won't be called
//  - a typical use case for the global callbacks is passive monitoring
//    in the background (e.g. fleet-wide monitoring), without focusing on
//    the specific piece of code
//  - in contrast, thread local callbacks are enabled locally, on demand,
//    for the specific piece of code (range) and are not sampled
//  - a typical use case for thread local callbacks is profiler and code
//    execution tracer
//  - note, thread local callbacks are automatically propagated with
//    ThreadLocalState across JIT continuations and async tasks (at::launch)

@Namespace("at") @MemberGetter public static native @Cast("const at::CallbackHandle") long INVALID_CALLBACK_HANDLE();
// Targeting ../RecordFunctionCallbacksEntry.java



// Holds pairs (callbacks, unique_id)
// Targeting ../RecordFunction.java



@Namespace("at") public static native @ByVal @Cast("at::StepCallbacks*") Pointer getStepCallbacks(RecordScope scope);
@Namespace("at") public static native @ByVal @Cast("at::StepCallbacks*") Pointer getStepCallbacks(@Cast("at::RecordScope") byte scope);

@Namespace("at") public static native @ByVal @Cast("c10::optional<at::StepCallbacks>*") Pointer getStepCallbacksUnlessEmpty(
    RecordScope scope);
@Namespace("at") public static native @ByVal @Cast("c10::optional<at::StepCallbacks>*") Pointer getStepCallbacksUnlessEmpty(
    @Cast("at::RecordScope") byte scope);

 // namespace detail

// optional argument - function's seq_no
// #define RECORD_FUNCTION_WITH_SCOPE(scope, fn, inputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     ::at::detail::record_function_with_scope(
//         guard, fn, inputs, ##__VA_ARGS__);
//   }

// #define RECORD_FUNCTION_WITH_SCOPE_INPUTS_OUTPUTS(
//     scope, fn, inputs, outputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     if (guard.needsInputs()) {
//       guard.before(fn, inputs, ##__VA_ARGS__);
//     } else {
//       guard.before(fn, ##__VA_ARGS__);
//     }
//     if (guard.needsOutputs()) {
//       guard.setOutputs(outputs);
//     }
//   }

// #define RECORD_FUNCTION(fn, inputs, ...)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::FUNCTION, fn, inputs, ##__VA_ARGS__)

// #define RECORD_TORCHSCRIPT_FUNCTION(mn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(at::RecordScope::TORCHSCRIPT_FUNCTION, mn, inputs)

// #define RECORD_FUNCTION_WITH_INPUTS_OUTPUTS(fn, inputs, outputs, ...)
//   RECORD_FUNCTION_WITH_SCOPE_INPUTS_OUTPUTS(
//       at::RecordScope::FUNCTION, fn, inputs, outputs, ##__VA_ARGS__)

// Custom user scopes in C++; similar to Python's 'with record_function("..."):'
// #define RECORD_USER_SCOPE(fn)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::USER_SCOPE, fn, c10::ArrayRef<const c10::IValue>{})

// RECORD_USER_SCOPE with inputs
// #define RECORD_USER_SCOPE_WITH_INPUTS(fn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(at::RecordScope::USER_SCOPE, fn, inputs)

// Helper macro to pass in debug handle that is used to
// post process events
// #define RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//     scope, fn, debug_handle, inputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     ::at::detail::record_function_with_scope_and_debug_handle(
//         guard, fn, debug_handle, inputs, ##__VA_ARGS__);
//   }

// Helper macros to record LITE INTERPETER scope events with debug handles
// #define RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(
//     fn, debug_handle, inputs)
//   RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//       at::RecordScope::LITE_INTERPRETER, fn, debug_handle, inputs)

// Bookend to the RECORD_FUNCTION macros.  Use this after the kernel
// launch to let the profiler bind the outputs to the op that produced
// them.  Note that guard is declared by RECORD_FUNCTION so this macro
// needs to be called from the same scope as RECORD_FUNCTION
// #define RECORD_OUTPUTS(outputs)
//   if (guard.needsOutputs()) {
//     guard.setOutputs(
//         std::vector<c10::IValue>(outputs.begin(), outputs.end()));
//   }

/**
 * addThreadLocalCallback adds a thread local callback to run with
 * RecordFunction, returns handle to use with removeThreadLocalCallback
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addThreadLocalCallback(@ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * hasThreadLocalCallbacks returns whether there're callbacks registered
 * with addThreadLocalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasThreadLocalCallbacks();

/**
 * clearThreadLocalCallbacks removes all thread local callbacks
 */
@Namespace("at") public static native void clearThreadLocalCallbacks();

/**
 * addGlobalCallback adds a global callback to run with RecordFunction:
 *
 * only during the program initialization
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addGlobalCallback(@ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * removeCallback removes a callback given the handle returned by
 * addThreadLocalCallback or addGlobalCallback;
 *
 * no other code can run simultaneously
 */
@Namespace("at") public static native void removeCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Prevent the given callback from executing. If handle is invalid,
 * does nothing.
 */
@Namespace("at") public static native void disableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Allow the given callback, previously disabled with disableCallback, to
 * execute again. If handle is invalid, does nothing.
 */
@Namespace("at") public static native void reenableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * hasGlobalCallbacks returns whether there're global callbacks
 * registered with pushGlobalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasGlobalCallbacks();

/**
 * clearGlobalCallbacks removes all global callbacks
 */
@Namespace("at") public static native void clearGlobalCallbacks();

// for both thread local and global callbacks
@Namespace("at") public static native @Cast("bool") boolean hasCallbacks();
@Namespace("at") public static native void clearCallbacks();

/**
 * enableRecordFunction enables RecordFunction thread locally
 */
@Namespace("at") public static native void enableRecordFunction(@Cast("bool") boolean enable/*=true*/);
@Namespace("at") public static native void enableRecordFunction();

/**
 * isRecordFunctionEnabled returns whether RecordFunction
 * is enabled thread locally
 */
@Namespace("at") public static native @Cast("bool") boolean isRecordFunctionEnabled();
// Targeting ../RecordFunctionGuard.java


// Targeting ../DisableRecordFunctionGuard.java


// Targeting ../RecordFunctionTLS.java



@Namespace("at") public static native @Const @ByRef RecordFunctionTLS get_record_function_tls_();

@Namespace("at") public static native void set_record_function_tls_(@Const @ByRef RecordFunctionTLS tls);

@Namespace("at") public static native void set_record_function_seed_for_testing(@Cast("uint32_t") int seed);

 // namespace at


// Parsed from ATen/core/op_registration/op_allowlist.h

// #pragma once

// TODO: unify to C10_MOBILE. In theory this header could be used in OSS.
// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #endif

/**
 * This header implements functionality to build PyTorch with only a certain
 * set of operators (+ dependencies) included.
 *
 * - Build with -DTORCH_OPERATOR_WHITELIST="aten::add;aten::sub" and only these
 *   two ops will be included in your build.  The allowlist records operators
 *   only, no overloads; if you include aten::add, all overloads of aten::add
 *   will be included.
 *
 * Internally, this is done by removing the operator registration calls
 * using compile time programming, and the linker will then prune all
 * operator functions that weren't registered.
 * See Note [Selective build] for more details
 *
 * WARNING: The allowlist mechanism doesn't work for all ways you could go about
 * registering an operator.  If the dispatch key / operator name is not
 * sufficiently obvious at compile time, then the allowlisting mechanism
 * will fail (and the operator will be included in the binary anyway).
 */

// #include <c10/util/string_view.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/macros/Macros.h>


// #if defined(ENABLE_RECORD_KERNEL_FUNCTION_DTYPE)
// #include <ATen/record_function.h>
// #endif

@Namespace("c10::impl") public static native @Cast("const bool") boolean allowlist_contains(@StringView BytePointer allowlist, @StringView BytePointer item);
@Namespace("c10::impl") public static native @Cast("const bool") boolean allowlist_contains(@StringView String allowlist, @StringView String item);  // Forward Declare

/**
 * In selective build mode returns true/false depending on whether a build
 * feature is available or not.
 *
 * In instrumenting mode (tracing mode), always returns true, and doesn't
 * trigger any side effects.
 */
@Namespace("c10::impl") public static native @Cast("const bool") boolean is_build_feature_available(@Cast("const char*") BytePointer name);
@Namespace("c10::impl") public static native @Cast("const bool") boolean is_build_feature_available(String name);



/**
 * Use BUILD_FEATURE_REQUIRED macro in user-code.
 *
 * In selective build mode becomes a no-op if the build feature passed
 * in is available. If not available, throws an exception (c10::Error).
 * The compiler is able to perform dead code elimination for code
 * following this method if the build feature is not available.
 *
 * In instrumenting mode (tracing mode), registers (as a side effect)
 * the presence of this specific build feature being triggered.
 */
// #if !defined(ENABLE_RECORD_KERNEL_FUNCTION_DTYPE)  // selective build mode

// #if defined(TORCH_BUILD_FEATURE_ALLOWLIST)
// #define BUILD_FEATURE_REQUIRED(NAME)
//   if (!c10::impl::is_build_feature_available(NAME)) {
//     ::c10::impl::build_feature_required_feature_not_available(NAME);
//   }
// #else  // Everything trivially selected
// #define BUILD_FEATURE_REQUIRED(NAME)

// #endif

// #else  // trace mode
// #define BUILD_FEATURE_REQUIRED(NAME)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::BUILD_FEATURE,
//       std::string(NAME),
//       {});
// #endif

// Use this macro, and not is_build_feature_available
// #define BUILD_FEATURE_AVAILABLE(NAME) ::c10::impl::is_build_feature_available(NAME)

// returns true iff allowlist contains item
// allowlist_contains("a;bc;d", "bc") == true

// Returns true iff the given op name is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_check(@StringView BytePointer op_name);
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_check(@StringView String op_name);

// Returns true iff the given schema string is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean schema_allowlist_check(@StringView BytePointer schema);
@Namespace("c10::impl") public static native @Cast("const bool") boolean schema_allowlist_check(@StringView String schema);

// Returns true iff the given custom class name is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean custom_class_allowlist_check(@StringView BytePointer custom_class_name);
@Namespace("c10::impl") public static native @Cast("const bool") boolean custom_class_allowlist_check(@StringView String custom_class_name);

// schema_allowlist_check() implicitly depends on a macro, TORCH_OPERATOR_WHITELIST.
// Add this API to pass arbitrary allowlist.
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_contains_name_in_schema(@StringView BytePointer allowlist, @StringView BytePointer schema);
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_contains_name_in_schema(@StringView String allowlist, @StringView String schema);

// Returns true iff the given dispatch key is on the allowlist
// and should be registered.  When we turn this on, the list of valid
// mobile dispatch keys is hard coded (but you need to make sure
// that you have the correct set of dispatch keys for this).
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(DispatchKey arg0);
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(@Cast("c10::DispatchKey") short arg0);

 // namespace impl
 // namespace c10


// Parsed from c10/util/either.h

// Originally taken from
// https://github.com/cryfs/cryfs/blob/14ad22570ddacef22d5ff139cdff68a54fc8234d/src/cpp-utils/either.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Optional.h>
/**
 * either<A, B> is a tagged union that holds either an object of type A
 * or an object of type B.
 */
 // namespace c10


// Parsed from torch/csrc/jit/frontend/function_schema_parser.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/either.h>
// #include <string>


@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString BytePointer schema);
@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString String schema);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString BytePointer name);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString String name);

 // namespace jit
 // namespace torch


// Parsed from c10/core/CompileTimeFunctionPointer.h

// #pragma once

// #include <c10/util/TypeTraits.h>

/**
 * Represent a function pointer as a C++ type.
 * This allows using the function pointer as a type
 * in a template and calling it from inside the template
 * allows the compiler to inline the call because it
 * knows the function pointer at compile time.
 *
 * Example 1:
 *  int add(int a, int b) {return a + b;}
 *  using Add = TORCH_FN_TYPE(add);
 *  template<class Func> struct Executor {
 *    int execute(int a, int b) {
 *      return Func::func_ptr()(a, b);
 *    }
 *  };
 *  Executor<Add> executor;
 *  EXPECT_EQ(3, executor.execute(1, 2));
 *
 * Example 2:
 *  int add(int a, int b) {return a + b;}
 *  template<class Func> int execute(Func, int a, int b) {
 *    return Func::func_ptr()(a, b);
 *  }
 *  EXPECT_EQ(3, execute(TORCH_FN(add), 1, 2));
 */

 // namespace c10

// #define TORCH_FN_TYPE(func)
//   ::c10::CompileTimeFunctionPointer<
//       std::remove_pointer_t<std::remove_reference_t<decltype(func)>>,
//       func>
// #define TORCH_FN(func) TORCH_FN_TYPE(func)()


// Parsed from ATen/core/boxing/OperatorKernel.h

// #pragma once
// #include <c10/util/intrusive_ptr.h>
// Targeting ../OperatorKernel.java



  // namespace c10


// Parsed from ATen/core/boxing/BoxedKernel.h

// #pragma once

// #include <ATen/core/boxing/OperatorKernel.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/intrusive_ptr.h>

// This kernel implements the behavior of falling through to the next available
// registered dispatch key.  The implementation of this function is FAST; it is
// no overhead to fallthrough to the next key.  See cpp file for some more
// implementation notes; notably, this does NOT actually go through the
// boxing/unboxing codepath.
@Namespace("c10") public static native void fallthrough_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);

// Note [Ambiguity in AutogradOther kernel]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// This error-reporting kernel is registered to the AutogradOther entry in the
// dispatch table when there is both a CompositeImplicitAutograd kernel and a
// backend kernel for ANY backend that maps to AutogradOther.  To see why
// this is necessary in the AutogradOther case, it's helpful to first see
// why everything works out fine for a backend that has a reserved Autograd
// entry (see rule 2.2 in [Note] DispatchTable computation):
//
//    CPU   AutogradCPU
//    reg?  registers with...
//    -------------------------------------------------
//    y     Autograd registration takes precedence
//          over CompositeImplicitAutograd.
//          This is good, because the CPU specific backend
//          implementation is more specialized and typically better;
//          if we used the composite, we would bypass it.
//          (NB: the Autograd key is guaranteed to exist because
//          the autograd codegen requires it!)
//
//    n     CompositeImplicitAutograd takes precedence.
//          This is also good, because the Autograd
//          registration (if it exists) would try to redispatch
//          to the (non-existent) CPU implementation; by
//          using the composite, we ensure the operator
//          actually works.
//
// As you can see, when we have a specific Autograd key (AutogradCPU), we can
// decide whether or not to use the CompositeImplicitAutograd kernel or the
// Autograd kernel based on whether or not the backend kernel exists.
//
// However, for AutogradOther (which is the catchall autograd kernel for
// everything that doesn't have a specific Autograd key), we can't do this
// trick because there isn't any unique backend to peek at to disambiguate;
// if there are some backends that have implementations they prefer Autograd,
// but unimplemented backends would prefer CompositeImplicitAutograd.  Rather
// than arbitrarily pick one or the other, we just register a kernel that raises
// an error and let the user decide how to proceed.
@Namespace("c10") public static native void ambiguous_autogradother_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);

// Note [named_not_supported_kernel]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// This kernel implements reporting an error message saying that named tensor is
// not supported.  This kernel doesn't rely on the Stack, and so it is special
// cased in the dispatcher to be triggered before we attempt boxing (so we can
// give a good error message in cases when boxing is not supported).  When
// boxing is universally supported this can be removed.
@Namespace("c10") public static native void named_not_supported_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);

/**
 * BoxedKernel is similar to a std::function storing a boxed kernel.
 */

  // namespace c10

// #include <ATen/core/boxing/BoxedKernel_impl.h>


// Parsed from ATen/core/boxing/BoxedKernel_impl.h

// #pragma once






























  // namespace c10


// Parsed from ATen/core/stack.h

// #pragma once

// #include <type_traits>

// #include <ATen/core/ivalue.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/irange.h>

// TODO move this to c10 namespace
// Targeting ../Operation.java



// An operation with N inputs and M outputs pops the last N inputs off
// the stack and pushes its M inputs onto the stack
// before: <other stack items> I0, I1, ... IN <- stack.back()
// after: <other stack items> O0, O1, ... OM
// operations are defined this way so that ownership of inputs can be
// transferred to the operation and it can incrementally drop ownership of
// tensors when they become unneeded. For large operations, like 'run an entire
// subgraph', this functionality is very important for minimizing gpu memory
// usage return value is the relative 'offset' to jump to for the next
// operation:
//   pc += 1 + offset
// so a return value of 0 goes to the next instruction

// treat the last N elements of the stack as a list, looking up
// element i
@Namespace("torch::jit") public static native @ByRef IValue peek(@ByRef IValueVector stack, @Cast("size_t") long i, @Cast("size_t") long N);
// treat the last N elements of the stack as a list, looking up the
// slice starting at index i and having length len
@Namespace("torch::jit") public static native @ByVal IValueArrayRef peekSlice(
    @Const @ByRef IValueVector stack,
    @Cast("size_t") long i,
    @Cast("size_t") long len,
    @Cast("size_t") long N);
@Namespace("torch::jit") public static native @ByVal IValueArrayRef last(@Const @ByRef IValueVector stack, @Cast("size_t") long N);
@Namespace("torch::jit") public static native void drop(@ByRef IValueVector stack, @Cast("size_t") long n);
@Namespace("torch::jit") public static native @ByVal IValue pop(@ByRef IValueVector stack);
@Namespace("torch::jit") public static native @ByVal IValueVector pop(@ByRef IValueVector stack, @Cast("size_t") long n);

// variadic pop:
// int64_t a; at::Tensor b;
// pop(stack, a, b);
// equivalent to:
// b = pop(stack).toTensor();
// a = pop(stack).toInt();

@Namespace("torch::jit") public static native void push_one(@ByRef IValueVector stack, @ByVal TensorOptions options);

// The packer here is carefully written not to make any unnecessary
// copies.

// pack takes the return values of aten functions pushes them onto the stack

 // namespace jit
 // namespace torch


// Parsed from ATen/core/boxing/impl/boxing.h

// #pragma once

// This file contains boxing (not unboxing) logic,
// i.e. how to make a vector<IValue> from a set of concrete arguments.

// #include <ATen/core/ivalue.h>
// #include <ATen/core/stack.h>
// #include <c10/core/TensorOptions.h>

// #include <ATen/core/boxing/BoxedKernel.h>

// #include <c10/util/Metaprogramming.h>

//
// utils
//

// is_mutable_tensor_ref

// is_tuple_of_mutable_tensor_refs
//

// has_ivalue_to<T> tests the presence/absence of instance method IValue::to<T>()
//

//
// boxing predicates
//

// A boxable arg type is one that IValue has a constructor for.

// an unboxable result is one that can be extracted from an IValue

//
// boxArgs - utility for pushing unboxed args onto IValue stack
//



// torch::jit::push pushes 4 values for a TensorOptions; this needs to
// be kept in sync.


// NOTE: this could probably be simplified with C++17 fold expressions.

@Namespace("c10::impl") public static native void boxToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer dest, @ByVal TensorOptions options, @ByRef IntPointer lastIdx);
@Namespace("c10::impl") public static native void boxToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer dest, @ByVal TensorOptions options, @ByRef IntBuffer lastIdx);
@Namespace("c10::impl") public static native void boxToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer dest, @ByVal TensorOptions options, @ByRef int[] lastIdx);

@Namespace("c10::impl") public static native void boxArgsToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer arg0, @ByRef IntPointer arg1);
@Namespace("c10::impl") public static native void boxArgsToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer arg0, @ByRef IntBuffer arg1);
@Namespace("c10::impl") public static native void boxArgsToStack(@Cast("c10::impl::IValueAlignedStorage*") Pointer arg0, @ByRef int[] arg1);

//
// PopResult is a helper class whose specializations handle popping single and
// multiple return values, respectively.
//

//
// BoxedKernelWrapper
//
// For a given function type FT, BoxedKernelWrapper<FT> implements
// a `call` method that
// - takes a boxed kernel and unboxed arguments as specified by FT,
// - calls `boxArgs` to box the arguments
// - calls the boxed kernel
// - unboxes and returns the result
//
// The partial specializations below handle various cases: in
// particular, not all types appearing in op signatures are supported,
// and ops returning references have nonstandard wrapper implementations.
//

// 1. The base specialization of BoxedKernelWrapper should never be instantiated.
// A "no call method defined on BoxedKernelWrapper" compile error means that
// an op signature has failed to trigger any of the partial specializations
// that follow this one.
//

//
// 2. Supported signatures, other than those involving non-const Tensor refs -
// i.e., "functional" ops.
//

//
// 3. in-place ops take a single non-const Tensor reference
// as their first argument, and return it.
//
// Note: all signatures matching this pattern are assumed to be for such ops.
// Because of this, the generated BoxedKernelWrapper specializations simply
// return the in-place argument.
//

//
// 3.5. In-process migration to make in-place ops take and return
// const references instead.

//
// 4. out of place ops that take a single non-const Tensor reference as their
// final argument, and also return it.
//
// Note: all signatures matching this pattern are assumed to be for such ops.
// This assumption permits the generated BoxedKernelWrapper specializations to simply
// return out arguments.
//

//
// 5. out of place ops that take multiple non-const Tensor references as their
// final arguments, and return them in a std::tuple.
//
// Note: all signatures matching this pattern are assumed to be for such ops.
// This assumption permits the generated BoxedKernelWrapper specializations to simply
// return the out arguments.
//

 // impl
 // c10


// Parsed from ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h

// #pragma once

// #include <ATen/core/boxing/OperatorKernel.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/stack.h>
// #include <c10/util/TypeList.h>
// #include <ATen/core/IListRef.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/Metaprogramming.h>

// #include <utility> // TODO Instead of this, move torch::jit::Stack to the c10 namespace.

/*
 * [Note: Argument forwarding in the dispatcher]
 *
 * The dispatcher uses a somewhat unusual way to forward arguments through several layers of
 * wrapper functions. This can be confusing because an experienced C++ programmer would look at this
 * and think "oh this is supposed to be forwarding a universal reference but the && is missing. This is a bug.".
 * It is not a bug. The common way in C++ to forward arguments is to use universal references:
 *
 * > template<class T> void func(T&& arg) { func2(std::forward<T>(arg)); }
 *
 * but that relies on inferring the correct reference type (i.e. value vs & vs &&) from the argument.
 * In our case, we cannot rely on the argument as supplied by the caller, because that could infer a
 * different reference type than was used in the kernel function. The correct reference type
 * is dictated by the kernel signature and must be identical since we cast function pointers
 * through void* pointers and mismatches would be UB. So we need a forwarding pattern that determines
 * the reference type to use by looking at the explicitly supplied operator signature, not by looking at
 * the argument we're calling it with.
 *
 * What does std::forward do, exactly?
 * ------------------------------------
 * std::forward<T>(t) is a way to cast t to the reference type supplied in T.
 * Let's assume decay_t<T> == U and T is either U or some reference of U.
 *  - std::forward<T&>(t) will return U&, no matter what kind of reference t is.
 *  - std::forward<T&&>(t) will return U&&, no matter what kind of reference t is.
 *  - std::forward<T>(t) will return U&& (not U!), no matter what kind of reference t is.
 *
 * For universal references, that means that in the following function
 * > template<class T> void func(T&& arg) { func2(std::forward<T>(arg)); }
 *
 *  - when called with arg being a rvalue reference or non-reference value, T gets inferred to be
 *    a non-reference U, and std::forward<T>(t) will return U&&, correctly moving the argument.
 *  - when called with arg behind a lvalue reference, T gets inferred to be U& because that's the only
 *    way to match the signature (in C++, a type that is (T&)&& will collapse to T&).
 *    That means std::forward<T>(t) will return U& and the value will not be moved but passed on as
 *    a lvalue reference.
 *
 * How do we use that?
 * ------------------------------------
 * But std::forward can also be used outside of the common "universal forwarding" pattern to change
 * reference types. So instead of following the common C++ pattern, we notice what
 * std::forward<T>() actually does, and that is it takes a value and changes its reference to the
 * type of reference passed in as T. If we don't infer T but explicitly specify it, we can use this
 * to forward based on an explicitly specified reference type instead of the inferred argument type.
 *
 * This is why many of the dispatcher functions look like
 * > template<class T> func(T t) { func2<T>(std::forward<T>(t)); }
 * instead of the common
 * > template<class T> func(T&& t) { func2(std::forward<T>(t)); }
 *
 * and are expected to be called by explicitly specifying the template parameters in a way that matches
 * the expected operator signature at each call site.
 */
  // supported_primitive_arg_types defines which primitive types we allow in
  // kernel functions as arguments or returns.
  // Additionally, we support lists, dicts and optionals containing these types.

  // We have an unboxed functor in hand that takes C++ arguments, and
  // we're building a boxed functor wrapper for it that takes IValues.
  // So "outside" is boxed and "inside" is unboxed.
  //
  // So a valid input type is one that our boxed functor wrapper can
  // unbox from an IValue into a C++ value.
  //
  // Whereas a valid output type is one that our wrapper can recieve
  // as a C++ value from the unboxed functor, and box into an IValue.

  //
  // assert_is_valid_input_type
  // checks that T can be unboxed from an IValue into a C++ value.
  //

  // The following specialisations of assert_is_valid_input_type are technically not
  // necessary since we would hit the base case and show an error message
  // there if they didn't exist, but we can show a better error message
  // in some common error scenarios.

  //
  // assert_is_valid_output_type
  //

  // The following specialisations of assert_is_valid_output_type are technically not
  // necessary since we would hit the base case and show an error message
  // there if they didn't exist, but we can show a better error message
  // in some common error scenarios.

  // ivalue_to_arg

  // The following two specializations take advantage of specialized
  // `toTensor()` overloads on IValue to avoid copying.

  // return_to_ivalue

  // Special case to allow kernels to return `Tensor&`.
  // TODO Delete this once kernels don't do that anymore

  // wrap_kernel_functor_unboxed_

  // This specialization is for kernels with a first argument that is NOT of type DispatchKeySet
  // This includes kernels with 0 arguments.

  // This specialization is for kernels with a first argument of type DispatchKeySet

  // call_functor_with_args_from_stack

  // push_outputs

  // make_boxed_from_unboxed_functor
 // namespace impl

 // namespace c10



// Parsed from ATen/core/boxing/impl/WrapFunctionIntoFunctor.h

// #pragma once

// #include <c10/core/CompileTimeFunctionPointer.h>
  

  // WrapFunctionIntoFunctor: Wraps a compile time function pointer into a kernel functor.
  // Since it is a compile time function pointer, many compilers can inline it
  // into the wrapper and you don't get any performance overhead for wrapping.





// Parsed from ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h

// #pragma once

// #include <c10/util/TypeTraits.h>
  

  // WrapFunctionIntoRuntimeFunctor: Wraps any runtime functor into a functor that
  // inherits from c10::OperatorKernel, so it can be used as a c10 kernel.
  // This can, for example, be used for lambdas, functors or even function pointers.
  // In the case of function pointers, since it is a runtime function pointer,
  // there is an overhead for calling it whenever the kernel is invoked.





// Parsed from ATen/core/boxing/KernelFunction.h

// #pragma once

// #include <ATen/core/ATen_fwd.h>
// #include <ATen/core/boxing/BoxedKernel.h>
// #include <ATen/core/stack.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/TypeList.h> // TODO Instead of this, move torch::jit::Stack to the c10 namespace.
// Targeting ../KernelFunction.java





// #include <ATen/core/boxing/KernelFunction_impl.h>


// Parsed from ATen/core/boxing/KernelFunction_impl.h

// #include <ATen/core/boxing/impl/boxing.h>
// #include <ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h>
// #include <ATen/core/boxing/impl/WrapFunctionIntoFunctor.h>
// #include <ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h>

















// This template requires you to explicitly specify the argument you want to
// forward; it doesn't work if you try to deduce it
// NB: keep this in sync with cloneWithRealTypes in function_schema.cpp
































// Parsed from ATen/core/dispatch/CppSignature.h

// #pragma once

// #include <typeindex>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/Type.h>
// Targeting ../CppSignature.java



@Namespace("c10::impl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef CppSignature lhs, @Const @ByRef CppSignature rhs);





// Parsed from ATen/core/dispatch/RegistrationHandleRAII.h

// #pragma once

// #include <functional>
// Targeting ../RegistrationHandleRAII.java






// Parsed from ATen/core/ATenOpList.h

// #pragma once

// #include <c10/macros/Export.h>


// check if an op is a custom op (i.e. did not come from native_functions.yaml)
@Namespace("at") public static native @Cast("bool") boolean is_custom_op(@Const @ByRef OperatorName opName);



// Parsed from ATen/core/op_registration/op_registration.h

// #pragma once

/**
 * Include this file if you want to register operators. It includes all
 * functionality needed to do so for you.
 */

// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/CompileTimeFunctionPointer.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/core/op_registration/infer_schema.h>
// #if defined(EXPOSE_C2_OPS) || !defined(CAFFE2_IS_XPLAT_BUILD)
// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #endif
// #include <ATen/core/ATenOpList.h>
// The first argument of the schema might be of type DispatchKeySet, in which case we remove it.
// We do this because every argument in a function schema is expected to be convertable
// to an ivalue, but DispatchKeySet is not a type we want the jit to be aware of.
// See Note [Plumbing Keys Through The Dispatcher]

// Targeting ../RegisterOperators.java



 // namespace c10
  // Old-style API



// Parsed from ATen/core/enum_tag.h

// #pragma once

// @generated by torchgen/gen.py from enum_tag.h
    // Enum of valid tags obtained from the entries in tags.yaml
    @Namespace("at") public enum Tag {
        core(0),
        data_dependent_output(1),
        dynamic_output_shape(2),
        generated(3),
        inplace_view(4),
        nondeterministic_bitwise(5),
        nondeterministic_seeded(6),
        pointwise(7),
        view_copy(8);

        public final int value;
        private Tag(int v) { this.value = v; }
        private Tag(Tag e) { this.value = e.value; }
        public Tag intern() { for (Tag e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }



// Parsed from ATen/core/function.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/Exception.h>
// #include <c10/util/FunctionRef.h>

@Namespace("at") public static native void launch(@ByVal Func func);



@Namespace("torch::jit") public static native void preoptimizeGraph(@SharedPtr("torch::jit::Graph") @ByRef Graph graph, @Cast("bool") boolean disable_autocast/*=false*/);
@Namespace("torch::jit") public static native void preoptimizeGraph(@SharedPtr("torch::jit::Graph") @ByRef Graph graph);
// Targeting ../Function.java


 // namespace jit
 // namespace torch


// Parsed from ATen/core/class_type.h

// #pragma once

// #include <memory>

// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type_base.h>
// #include <c10/util/Optional.h>
 // namespace jit
 // namespace torch

// This enumerator represents the 'kind' of an attribute - a buffer, a parameter, or neither.
// This state is mutually exclusive. Buffers and Parameters can only appear on modules.
@Namespace("c10") public enum AttributeKind {
  BUFFER(0),
  PARAMETER(1),
  REGULAR_ATTRIBUTE(2);

    public final int value;
    private AttributeKind(int v) { this.value = v; }
    private AttributeKind(AttributeKind e) { this.value = e.value; }
    public AttributeKind intern() { for (AttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ClassAttribute.java



/**
 * User Defined Types
 */
// Targeting ../ClassType.java






// Parsed from torch/library.h


///
///
///
///
///
///
///
///
///
// #pragma once

/** \file
 * 
 *  This header provides an API for extending PyTorch's core library
 *  of operators with user defined operators and data types.  This
 *  API can be used in a few ways:
 * 
 *  * You can define new custom operators and classes with TORCH_LIBRARY(),
 *    making them available for use in both eager Python as well as in
 *    TorchScript. This API is modeled off of pybind11's {@code PYBIND11_MODULE}
 *    macro, as the provided functionality is similar (pybind11 lets you bind
 *    C++ to Python only; {@code torch/library.h} lets you bind C++ simultaneously to
 *    Python and TorchScript).
 * 
 *  * You can override existing operators with TORCH_LIBRARY_IMPL(),
 *    providing a new implementation for these operators for a custom
 *    backend (e.g., XLA).  When you pass operators with tensors of your custom
 *    backend, your overridden implementations will be called instead
 *    of the standard implementations.
 * 
 *  * You can use both capabilities at the same time, allowing you
 *    to write custom operators that register CPU/CUDA/Autograd
 *    implementations without having to write the boilerplate
 *    conditionals yourself.
 * 
 *  For a tutorial style introduction to the library API, check
 *  out the [Extending TorchScript with Custom C++
 *  Operators](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html)
 *  tutorial.
 * 
 *  <pre>{@code
 *  // Define a library whose operators live in the namespace 'myops'.
 *  // You must define all of the operators for this library in
 *  // this namespace.
 *  TORCH_LIBRARY(myops, m) {
 *    // Define a operator with exactly one implementation for all backends.
 *    m.def("add(Tensor self, Tensor other) -> Tensor", &add_impl);
 * 
 *    // Define a schema for an operator, but provide no implementation
 *    // (use this syntax if you want to use the dispatcher)
 *    m.def("mul(Tensor self, Tensor other) -> Tensor");
 * 
 *    // Provide an implementation for a defined operator (you can
 *    // provide multiple; one per backend).  The dispatcher takes care of
 *    // calling the correct implementation depending on if we get a CPU
 *    // tensor or a CUDA tensor
 *    m.impl("mul", torch::kCPU, &mul_cpu_impl);
 *    m.impl("mul", torch::kCUDA, &mul_cuda_impl);
 *  }
 * 
 *  // Define implementations for operators for a non-standard backend,
 *  // e.g., XLA (valid values are entries of DispatchKey).  This can
 *  // be used to define operators in a different file than the initial
 *  // TORCH_LIBRARY definition (e.g., if it is in an external library)
 *  TORCH_LIBRARY_IMPL(myops, XLA, m) {
 *    m.impl("mul", &mul_xla_impl);
 *  }
 *  }</pre> */

// #include <ATen/core/op_registration/infer_schema.h>
// #include <ATen/core/op_registration/op_allowlist.h>
// #include <c10/core/DispatchKey.h>
// #include <torch/csrc/jit/frontend/function_schema_parser.h>

// Just for inferFunctionSchemaFromFunctor
// #include <ATen/core/enum_tag.h>
// #include <ATen/core/op_registration/op_registration.h>

// #if defined C10_MOBILE
/**
 * The NoInferSchemaTag is a type name used to indicate that this call to the
 * CppFunction constructor should not trigger schema inference from functor.
 * Schema inference from functor utilizes template meta-programming, and is
 * costly from a size perspective. Ideally, one would expect that the schema
 * inference would require very little binary size since most of the
 * computation can be done by the compiler at build time, but that isn't
 * necessarily the case.
 *
 * Schema inference is elided only for mobile use-cases where we don't need
 * the additional runtime cost or size overhead on client devices.
 *
 */
// #endif

// For multipy/torchdeploy use case
@Namespace("torch") public enum _RegisterOrVerify { REGISTER(0), VERIFY(1);

    public final int value;
    private _RegisterOrVerify(int v) { this.value = v; }
    private _RegisterOrVerify(_RegisterOrVerify e) { this.value = e.value; }
    public _RegisterOrVerify intern() { for (_RegisterOrVerify e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../CppFunction.java



/** \defgroup torch-dispatch-overloads torch::dispatch overloads
 <p>
 *  Create a torch::CppFunction which is associated with a specific
 *  dispatch key.  torch::CppFunctions that are tagged with a
 *  c10::DispatchKey don't get invoked unless the dispatcher determines
 *  that this particular c10::DispatchKey is the one that should be
 *  dispatched to.
 * 
 *  This function is generally not used directly, instead, prefer using
 *  TORCH_LIBRARY_IMPL(), which will implicitly set the c10::DispatchKey
 *  for all registration calls inside of its body.
 * 
 *  \ingroup torch-dispatch-overloads */

/** Convenience overload of dispatch() which accepts c10::DeviceType
 * 
 *  \ingroup torch-dispatch-overloads */

/** \defgroup torch-schema-overloads torch::schema overloads
 <p>
 *  Construct a c10::FunctionSchema from a string, with an explicitly
 *  specified c10::AliasAnalysisKind.  Ordinarily, schemas are simply
 *  passed in as strings, but if you need to specify a custom alias
 *  analysis, you can replace the string with a call to this function.
 * 
 *  <pre>{@code
 *  // Default alias analysis (FROM_SCHEMA)
 *  m.def("def3(Tensor self) -> Tensor");
 *  // Pure function alias analysis
 *  m.def(torch::schema("def3(Tensor self) -> Tensor",
 *  c10::AliasAnalysisKind::PURE_FUNCTION));
 *  }</pre>
 * 
 *  \ingroup torch-schema-overloads */

///
@Namespace("torch") public static native @ByVal FunctionSchema schema(@Cast("const char*") BytePointer str, AliasAnalysisKind k);
@Namespace("torch") public static native @ByVal FunctionSchema schema(String str, @Cast("c10::AliasAnalysisKind") byte k);

/** Function schemas can be directly constructed from string literals.
 * 
 *  \ingroup torch-schema-overloads */

///
///
@Namespace("torch") public static native @ByVal FunctionSchema schema(@Cast("const char*") BytePointer s);
@Namespace("torch") public static native @ByVal FunctionSchema schema(String s);

/** \private
 * 
 *  Already constructed function schemas are accepted if they are
 *  rvalues.
 * 
 *  \ingroup torch-schema-overloads */
@Namespace("torch") public static native @ByRef(true) FunctionSchema schema(@ByRef(true) FunctionSchema s);





 // namespace detail

// Note [Selective build]
// ~~~~~~~~~~~~~~~~~~~~~~
// In some settings, especially mobile, it is important to avoid compiling any
// references to functions that you aren't actually going to use, so that they
// can be eliminated by the linker.  We call this capability "selective build".
//
// A very easy way to implement selective build which results in a lot of
// boilerplate is to just add ifdef's around every registration call, but this
// means you have to write a lot of extra lines of code at every registration
// site, and it also means you have to define some munging scheme to map
// operators to macros.
//
// Instead of doing this, we have a different mechanism centered around the
// concept of a SelectiveStr.  A selective name is like a const char* string,
// except it also carries at compile time a boolean saying whether or not a
// registration should actually happen or not.  We then have extra overloads
// which bypass registration entirely if a selective name is disabled.  We do a
// constexpr test to see if a operator should be enabled or not; this is
// currently implemented in ATen/core/op_registration/op_allowlist.h

// dummy class for non selected custom torchbind classes
// Targeting ../DisabledStr.java


// Targeting ../EnabledStr.java



// #define TORCH_SELECTIVE_CLASS(n)
//   torch::detail::SelectiveStr<c10::impl::custom_class_allowlist_check(n)>(n)
// #define TORCH_SELECTIVE_NAME(n)
//   torch::detail::SelectiveStr<c10::impl::op_allowlist_check(n)>(n)
// #define TORCH_SELECTIVE_SCHEMA(n)
//   torch::detail::SelectiveStr<c10::impl::schema_allowlist_check(n)>(n)


// Targeting ../Library.java



 // namespace detail

 // namespace torch

// NB: The EXACT NAMING of the initializer functions (e.g.,
// TORCH_LIBRARY_init_aten) matters for the code analyzer;
// see the regexes at tools/code_analyzer/run_analyzer.sh

/** Macro for defining a function that will be run at static
 *  initialization time to define a library of operators in the
 *  namespace {@code ns} (must be a valid C++ identifier, no quotes).
 *  Use this macro when you want to define a new set of custom operators
 *  that do not already exist in PyTorch.
 * 
 *  Example usage:
 * 
 *  <pre>{@code
 *  TORCH_LIBRARY(myops, m) {
 *    // m is a torch::Library; methods on it will define
 *    // operators in the myops namespace
 *    m.def("add", add_impl);
 *  }
 *  }</pre>
 * 
 *  The {@code m} argument is bound to a torch::Library that is used to
 *  register operators.  There may only be one TORCH_LIBRARY()
 *  for any given namespace. */

///
// #define TORCH_LIBRARY(ns, m)
//   static void TORCH_LIBRARY_init_##ns(torch::Library&);
//   static const torch::detail::TorchLibraryInit TORCH_LIBRARY_static_init_##ns(
//       torch::Library::DEF,
//       &TORCH_LIBRARY_init_##ns,
//       #ns,
//       c10::nullopt,
//       __FILE__,
//       __LINE__);
//   void TORCH_LIBRARY_init_##ns(torch::Library& m)

/** \private
 * 
 *  This macro is a version of TORCH_LIBRARY() that doesn't enforce that there
 *  is only one library (it is a "fragment").  This is used inside the
 *  PerOpRegistration.cpp file, as well as in places where all op registrations
 *  within the same namespace cannot be easily put into one macro block
 *  (this is mostly the case for custom ops in fbcode that were ported from
 *  the old API) */

///
// #define TORCH_LIBRARY_FRAGMENT(ns, m) _TORCH_LIBRARY_FRAGMENT(ns, m, C10_UID)

/** \private
 * 
 *  The above macro requires an extra unique identifier (uid) to prevent
 *  variable name collisions This can happen if TORCH_LIBRARY_FRAGMENT is called
 *  multiple times with the same namespace in the same translation unit. Note
 *  that the TORCH_LIBRARY variant doesn't run into this problem, because it
 *  enforces that it can only be called once for a given namespace. */

///
///
///
///
///
///
// #define _TORCH_LIBRARY_FRAGMENT(ns, m, uid)
//   static void C10_CONCATENATE(
//       TORCH_LIBRARY_FRAGMENT_init_##ns##_, uid)(torch::Library&);
//   static const torch::detail::TorchLibraryInit C10_CONCATENATE(
//       TORCH_LIBRARY_FRAGMENT_static_init_##ns##_, uid)(
//       torch::Library::FRAGMENT,
//       &C10_CONCATENATE(TORCH_LIBRARY_FRAGMENT_init_##ns##_, uid),
//       #ns,
//       c10::nullopt,
//       __FILE__,
//       __LINE__);
//   void C10_CONCATENATE(
//       TORCH_LIBRARY_FRAGMENT_init_##ns##_, uid)(torch::Library & m)

/** Macro for defining a function that will be run at static
 *  initialization time to define operator overrides for dispatch key
 *  {@code k} (must be an unqualified enum member of c10::DispatchKey) in
 *  namespace {@code ns} (must be a valid C++ identifer, no quotes).  Use this
 *  macro when you want to implement a preexisting set of custom
 *  operators on a new dispatch key (e.g., you want to provide CUDA
 *  implementations of already existing operators).  One common usage
 *  pattern is to use TORCH_LIBRARY() to define schema for all new
 *  operators you want to define, and then use several
 *  TORCH_LIBRARY_IMPL() blocks to provide implementations of the
 *  operator for CPU, CUDA and Autograd.
 * 
 *  In some cases, you need to define something that applies to all namespaces,
 *  not just one namespace (usually a fallback).  In that case, use the reserved
 *  namespace _, e.g.,
 * 
 *  <pre>{@code
 *  TORCH_LIBRARY_IMPL(_, XLA, m) {
 *     m.fallback(xla_fallback);
 *  }
 *  }</pre>
 * 
 *  Example usage:
 * 
 *  <pre>{@code
 *  TORCH_LIBRARY_IMPL(myops, CPU, m) {
 *    // m is a torch::Library; methods on it will define
 *    // CPU implementations of operators in the myops namespace.
 *    // It is NOT valid to call torch::Library::def()
 *    // in this context.
 *    m.impl("add", add_cpu_impl);
 *  }
 *  }</pre>
 * 
 *  If {@code }add_cpu_impl{@code } is an overloaded function, use a
 *  {@code }static_cast{@code } to specify which overload you want
 *  (by providing the full type).
 *  */
// NB: if the dispatch key is not whitelisted, we simply omit the Library
// call entirely

///
// #define TORCH_LIBRARY_IMPL(ns, k, m) _TORCH_LIBRARY_IMPL(ns, k, m, C10_UID)

/** \private
 * 
 *  The above macro requires an extra unique identifier (uid) to prevent
 *  variable name collisions. This can happen if TORCH_LIBRARY_IMPL is called
 *  multiple times with the same namespace and dispatch key in the same
 *  translation unit. */
// #define _TORCH_LIBRARY_IMPL(ns, k, m, uid)
//   static void C10_CONCATENATE(
//       TORCH_LIBRARY_IMPL_init_##ns##_##k##_, uid)(torch::Library&);
//   static const torch::detail::TorchLibraryInit C10_CONCATENATE(
//       TORCH_LIBRARY_IMPL_static_init_##ns##_##k##_, uid)(
//       torch::Library::IMPL,
//       (c10::impl::dispatch_key_allowlist_check(c10::DispatchKey::k)
//            ? &C10_CONCATENATE(TORCH_LIBRARY_IMPL_init_##ns##_##k##_, uid)
//            : [](torch::Library&) -> void {}),
//       #ns,
//       c10::make_optional(c10::DispatchKey::k),
//       __FILE__,
//       __LINE__);
//   void C10_CONCATENATE(
//       TORCH_LIBRARY_IMPL_init_##ns##_##k##_, uid)(torch::Library & m)

// These are variants of the macros above which are to be used for testing (they
// don't setup the static initializer, so you can control the visibility of
// the allocated library yourself).
//
// DO NOT use these in production code, they are NOT understood by the
// code analyzer and will be incorrectly analyzed in those situations.

/** \private */
// #define MAKE_TORCH_LIBRARY(ns)
//   torch::Library(torch::Library::DEF, #ns, c10::nullopt, __FILE__, __LINE__)
/** \private */
// #define MAKE_TORCH_LIBRARY_IMPL(ns, k)
//   torch::Library(
//       torch::Library::IMPL,
//       #ns,
//       c10::make_optional(c10::DispatchKey::k),
//       __FILE__,
//       __LINE__)

// Make the custom class API visible, so it is available from
// torch::Library.

// #include <torch/custom_class.h>


// Parsed from torch/csrc/autograd/autograd_not_implemented_fallback.h

// #pragma once

// #include <torch/library.h>

// Default DispatchKey::Autograd fallback for built-in operators.
// Can be registered for custom operators.
@Namespace("torch::autograd") public static native @ByVal CppFunction autogradNotImplementedFallback();

// Default DispatchKey::AdInplaceOrView fallback for built-in operators
// Can be registered for custom operators.
@Namespace("torch::autograd") public static native @ByVal CppFunction autogradNotImplementedInplaceOrViewFallback();

// Default DispatchKey::Autograd fallback for all other operators (i.e. custom
// operators)
@Namespace("torch::autograd") public static native @ByVal CppFunction basicAutogradNotImplementedFallback();

@Namespace("torch::autograd") public enum AutogradFallbackMode {
  Nothing(0), // Fallback is a redispatch
  Warn(1), // Fallback raises a warning if backward is called
  Error(2);// Fallback raises an error if backward is called

    public final int value;
    private AutogradFallbackMode(int v) { this.value = v; }
    private AutogradFallbackMode(AutogradFallbackMode e) { this.value = e.value; }
    public AutogradFallbackMode intern() { for (AutogradFallbackMode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Change the behavior of "basicAutogradNotImplementedFallback"
// In Python this is:
// - torch._C._set_autograd_fallback_mode(str) -> None
// - torch._C._get_autograd_fallback_mode() -> str
@Namespace("torch::autograd") public static native void setAutogradFallbackMode(AutogradFallbackMode mode);
@Namespace("torch::autograd") public static native void setAutogradFallbackMode(@Cast("torch::autograd::AutogradFallbackMode") int mode);
@Namespace("torch::autograd") public static native AutogradFallbackMode getAutogradFallbackMode();

 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/anomaly_mode.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <memory>
// #include <string>
// Targeting ../AnomalyMode.java


// Targeting ../DetectAnomalyGuard.java


// Targeting ../AnomalyMetadata.java



 // namespace autograd
 // namespace torch


// Parsed from ATen/core/grad_mode.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/core/GradMode.h>



// Parsed from torch/csrc/autograd/grad_mode.h

// #pragma once

// #include <ATen/core/grad_mode.h>
// #include <torch/csrc/Export.h>

 // namespace autograd
 // namespace torch


// Parsed from ATen/FuncTorchTLS.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <memory>
// Targeting ../FuncTorchTLSBase.java



// returns deepcopy of the functorch tls
@Namespace("at::functorch") public static native @UniquePtr FuncTorchTLSBase getCopyOfFuncTorchTLS();

// sets the functorch tls. always does a deep copy.
@Namespace("at::functorch") public static native void setFuncTorchTLS(
    @Const @SharedPtr("const at::functorch::FuncTorchTLSBase") @ByRef FuncTorchTLSBase state);

// get a mutable reference to the functorch tls
@Namespace("at::functorch") public static native @UniquePtr FuncTorchTLSBase functorchTLSAccessor();

 // namespace functorch
 // namespace at


// Parsed from c10/core/SafePyObject.h

// #pragma once

// #include <c10/core/impl/PyInterpreter.h>
// #include <c10/macros/Export.h>
// #include <c10/util/python_stub.h>
// Targeting ../SafePyObject.java


// Targeting ../SafePyHandle.java



 // namespace c10


// Parsed from ATen/PythonTorchFunctionTLS.h

// #pragma once

// #include <c10/core/SafePyObject.h>
// #include <c10/macros/Macros.h>

@Namespace("at::impl") public enum TorchFunctionDisabledState { ENABLED(0), SUBCLASSES_DISABLED(1), ALL_DISABLED(2);

    public final int value;
    private TorchFunctionDisabledState(int v) { this.value = v; }
    private TorchFunctionDisabledState(TorchFunctionDisabledState e) { this.value = e.value; }
    public TorchFunctionDisabledState intern() { for (TorchFunctionDisabledState e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../PythonTorchFunctionTLS.java



@Namespace("at::impl") public static native @Cast("bool") boolean torch_function_mode_enabled();

 // namespace impl
 // namespace at


// Parsed from ATen/SavedTensorHooks.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <c10/util/Optional.h>
// #include <c10/util/python_stub.h>
// #include <stack>
// #include <string>

// #include <utility>
// Targeting ../SavedTensorDefaultHooksTLS.java




// Targeting ../SavedTensorDefaultHooks.java



 // namespace at


// Parsed from ATen/ThreadLocalPythonObjects.h

// #pragma once

// #include <c10/core/SafePyObject.h>
// #include <c10/macros/Macros.h>
// #include <unordered_map>
// Targeting ../ThreadLocalPythonObjects.java



 // namespace impl
 // namespace at


// Parsed from c10/core/impl/PythonDispatcherTLS.h

// #pragma once

// #include <c10/core/impl/PyInterpreter.h>
// #include <c10/macros/Export.h>
// Targeting ../PythonDispatcherTLS.java


// Targeting ../DisablePythonDispatcher.java



 // namespace impl
 // namespace c10


// Parsed from c10/core/impl/TorchDispatchModeTLS.h

// #pragma once

// #include <c10/core/SafePyObject.h>
// #include <c10/macros/Export.h>
// Targeting ../TorchDispatchModeTLS.java



@Namespace("c10::impl") public static native @Cast("bool") boolean dispatch_mode_enabled();

 // namespace impl
 // namespace c10


// Parsed from ATen/ThreadLocalState.h

// #pragma once

// #include <stack>

// #include <c10/core/InferenceMode.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>

// #include <ATen/FuncTorchTLS.h>
// #include <ATen/PythonTorchFunctionTLS.h>
// #include <ATen/SavedTensorHooks.h>
// #include <ATen/ThreadLocalPythonObjects.h>
// #include <ATen/record_function.h>
// #include <c10/core/impl/PythonDispatcherTLS.h>
// #include <c10/core/impl/TorchDispatchModeTLS.h>
// Targeting ../ThreadLocalState.java


// Targeting ../ThreadLocalStateGuard.java



 // namespace at


// Parsed from c10/util/ThreadLocal.h

// #pragma once

// #include <c10/macros/Macros.h>

/**
 * Android versions with libgnustl incorrectly handle thread_local C++
 * qualifier with composite types. NDK up to r17 version is affected.
 *
 * (A fix landed on Jun 4 2018:
 * https://android-review.googlesource.com/c/toolchain/gcc/+/683601)
 *
 * In such cases, use c10::ThreadLocal<T> wrapper
 * which is {@code pthread_*} based with smart pointer semantics.
 *
 * In addition, convenient macro C10_DEFINE_TLS_static is available.
 * To define static TLS variable of type std::string, do the following
 * <pre>{@code
 *  C10_DEFINE_TLS_static(std::string, str_tls_);
 *  ///////
 *  {
 *    *str_tls_ = "abc";
 *    assert(str_tls_->length(), 3);
 *  }
 * }</pre>
 *
 * (see c10/test/util/ThreadLocal_test.cpp for more examples)
 */
// #if !defined(C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE)

// #if defined(C10_ANDROID) && defined(__GLIBCXX__) && __GLIBCXX__ < 20180604
// #define C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE
// #endif // defined(C10_ANDROID) && defined(__GLIBCXX__) && __GLIBCXX__ < 20180604

// #endif // !defined(C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE)

// #if defined(C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE)
// #include <c10/util/Exception.h>
// #include <errno.h>
// #include <pthread.h>
// #include <memory>

/**
 * \brief Temporary thread_local C++ qualifier replacement for Android
 * based on {@code pthread_*}.
 * To be used with composite types that provide default ctor.
 */

 // namespace c10

// #define C10_DEFINE_TLS_static(Type, Name) static ::c10::ThreadLocal<Type> Name

// #define C10_DECLARE_TLS_class_static(Class, Type, Name)
//   static ::c10::ThreadLocal<Type> Name

// #define C10_DEFINE_TLS_class_static(Class, Type, Name)
//   ::c10::ThreadLocal<Type> Class::Name

// #else // defined(C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE)

/**
 * \brief Default thread_local implementation for non-Android cases.
 * To be used with composite types that provide default ctor.
 */

 // namespace c10

// #define C10_DEFINE_TLS_static(Type, Name)
//   static ::c10::ThreadLocal<Type> Name([]() {
//     static thread_local Type var;
//     return &var;
//   })

// #define C10_DECLARE_TLS_class_static(Class, Type, Name)
//   static ::c10::ThreadLocal<Type> Name

// #define C10_DEFINE_TLS_class_static(Class, Type, Name)
//   ::c10::ThreadLocal<Type> Class::Name([]() {
//     static thread_local Type var;
//     return &var;
//   })

// #endif // defined(C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE)


// Parsed from torch/csrc/autograd/input_buffer.h

// #pragma once

// The InputBuffer class accumulates a list of Variables for use by a
// function. It implements logic to avoid modifying the passed
// values in-place (adding an input twice will accumulate the result).
// This behaviour is needed and used only in backward graphs.

// #include <memory>
// #include <utility>
// #include <vector>

// #include <c10/core/Stream.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/autograd/variable.h>

 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/utils/warnings.h

// #pragma once
// #include <c10/util/Exception.h>

// #include <mutex>
// #include <vector>

// Warning handler for multi-threaded contexts. Gather warnings from
// all threads into a single queue, then process together at the end
// in the main thread.

 // namespace utils
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/graph_task.h

// #pragma once
// #include <ATen/ThreadLocalState.h>
// #include <ATen/core/Tensor.h>
// #include <c10/util/ThreadLocal.h>
// #include <torch/csrc/autograd/input_buffer.h>
// #include <torch/csrc/autograd/utils/warnings.h>
// #include <vector>

@Namespace("torch::autograd") @MemberGetter public static native int NO_DEVICE();
public static final int NO_DEVICE = NO_DEVICE();
@Namespace("torch::autograd") @MemberGetter public static native int CPU_DEVICE();
public static final int CPU_DEVICE = CPU_DEVICE();



// GraphTask holds metadata needed for a single execution of backward()

// The guard that sets and restores current_graph_task.


@Namespace("torch::autograd") public static native @Const NodeSet get_current_graph_task_nodes_in_graph();
@Namespace("torch::autograd") public static native @Cast("bool") boolean get_current_graph_task_keep_graph();
@Namespace("torch::autograd") public static native @Cast("torch::autograd::Node**") @StdVector PointerPointer get_current_graph_task_execution_order();
@Namespace("torch::autograd") public static native int get_current_graph_task_id();


 // namespace autograd
 // namespace torch


// Parsed from ATen/core/MT19937RNGEngine.h

// #pragma once

// #include <c10/util/irange.h>

// define constants like M_PI and C keywords for MSVC
// #ifdef _MSC_VER
// #ifndef _USE_MATH_DEFINES
// #define _USE_MATH_DEFINES
// #endif
// #include <math.h>
// #endif

// #include <array>
// #include <cmath>
// #include <cstdint>

@Namespace("at") @MemberGetter public static native int MERSENNE_STATE_N();
@Namespace("at") @MemberGetter public static native int MERSENNE_STATE_M();
@Namespace("at") @MemberGetter public static native @Cast("const uint32_t") int MATRIX_A();
@Namespace("at") @MemberGetter public static native @Cast("const uint32_t") int UMASK();
@Namespace("at") @MemberGetter public static native @Cast("const uint32_t") int LMASK();
// Targeting ../mt19937_data_pod.java


// Targeting ../mt19937_engine.java



 // namespace at


// Parsed from ATen/CPUGeneratorImpl.h

// #pragma once

// #include <ATen/core/Generator.h>
// #include <ATen/core/MT19937RNGEngine.h>
// #include <c10/core/GeneratorImpl.h>
// #include <c10/util/Optional.h>
// Targeting ../CPUGeneratorImpl.java



@Namespace("at::detail") public static native @Const @ByRef Generator getDefaultCPUGenerator();
@Namespace("at::detail") public static native @ByVal Generator createCPUGenerator(@Cast("uint64_t") long seed_val/*=c10::default_rng_seed_val*/);
@Namespace("at::detail") public static native @ByVal Generator createCPUGenerator();

 // namespace detail

 // namespace at


// Parsed from ATen/LinalgBackend.h

// #pragma once

// #include <c10/util/Exception.h>

// #include <ostream>
// #include <string>

@Namespace("at") public enum LinalgBackend { Default((byte)(0)), Cusolver((byte)(1)), Magma((byte)(2));

    public final byte value;
    private LinalgBackend(byte v) { this.value = v; }
    private LinalgBackend(LinalgBackend e) { this.value = e.value; }
    public LinalgBackend intern() { for (LinalgBackend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public static native @StdString BytePointer LinalgBackendToString(LinalgBackend backend);
@Namespace("at") public static native @StdString String LinalgBackendToString(@Cast("at::LinalgBackend") byte backend);

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    LinalgBackend backend);

 // namespace at


// Parsed from ATen/core/ATenGeneral.h

// #pragma once

// #include <c10/macros/Macros.h>


// Parsed from ATen/core/LegacyTypeDispatch.h

// #pragma once

// The legacy mechanism for dispatching operators in ATen is a Type
// object, which is essentially a giant virtual dispatch table
// for every operation we support dynamically dispatching over.
//
// This has been deprecated in favor of ATenDispatch, and in the future,
// c10 dispatcher.
// TODO: Clean up what remains here

// #include <c10/core/impl/LocalDispatchKeySet.h>
// Targeting ../AutoDispatchBelowAutograd.java


// Targeting ../AutoNonVariableTypeMode.java


// Targeting ../AutoDispatchSkipFunctionalize.java


// Targeting ../AutoDispatchBelowADInplaceOrView.java


 // namespace at


// Parsed from ATen/detail/CUDAHooksInterface.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/Registry.h>

// #include <cstddef>
// #include <functional>
// #include <memory>

// Forward-declares at::Context, at::Generator and at::cuda::NVRTC
 // namespace cuda
 // namespace at

// NB: Class must live in `at` due to limitations of Registry.h.

// #ifdef _MSC_VER
@Namespace("at") @MemberGetter public static native @Cast("const char*") BytePointer CUDA_HELP();
// #else
// Targeting ../CUDAHooksInterface.java


// Targeting ../CUDAHooksArgs.java


// #define REGISTER_CUDA_HOOKS(clsname)
//   C10_REGISTER_CLASS(CUDAHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef CUDAHooksInterface getCUDAHooks();
 // namespace detail
 // namespace at


// Parsed from ATen/detail/HIPHooksInterface.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <ATen/core/Generator.h>
// #include <c10/util/Exception.h>

// #include <c10/util/Registry.h>

// #include <cstddef>
// #include <functional>
// #include <memory>


// NB: Class must live in `at` due to limitations of Registry.h.
// Targeting ../HIPHooksInterface.java


// Targeting ../HIPHooksArgs.java


// #define REGISTER_HIP_HOOKS(clsname)
//   C10_REGISTER_CLASS(HIPHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef HIPHooksInterface getHIPHooks();

 // namespace detail
 // namespace at


// Parsed from ATen/detail/MPSHooksInterface.h

//  Copyright © 2022 Apple Inc.

// #pragma once

// #include <c10/core/Allocator.h>
// #include <ATen/core/Generator.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Registry.h>

// #include <cstddef>
// #include <functional>

// Targeting ../MPSHooksInterface.java


// Targeting ../MPSHooksArgs.java


// #define REGISTER_MPS_HOOKS(clsname)
//   C10_REGISTER_CLASS(MPSHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef MPSHooksInterface getMPSHooks();

 // namespace detail
 // namespace at


// Parsed from ATen/detail/MTIAHooksInterface.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>

// #include <c10/util/Registry.h>

// #include <cstddef>
// #include <functional>
// #include <memory>

// Targeting ../DLDevice_.java



@Namespace("at") @MemberGetter public static native @Cast("const char*") BytePointer MTIA_HELP();
// Targeting ../MTIAHooksInterface.java


// #define REGISTER_MTIA_HOOKS(clsname)
//   C10_REGISTER_CLASS(MTIAHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef MTIAHooksInterface getMTIAHooks();
 // namespace detail
 // namespace at


// Parsed from ATen/detail/ORTHooksInterface.h

// #pragma once

// #include <c10/util/Exception.h>
// #include <c10/util/Registry.h>

@MemberGetter public static native @Cast("const char*") BytePointer ORT_HELP();
// Targeting ../ORTHooksInterface.java


// Targeting ../ORTHooksArgs.java


// #define REGISTER_ORT_HOOKS(clsname)
//   C10_REGISTER_CLASS(ORTHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef ORTHooksInterface getORTHooks();
 // namespace detail

 // namespace at


// Parsed from ATen/detail/PrivateUse1HooksInterface.h

// #pragma once

// #include <ATen/core/Generator.h>
// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>
// Targeting ../PrivateUse1HooksInterface.java


// Targeting ../PrivateUse1HooksArgs.java



@Namespace("at") public static native void RegisterPrivateUse1HooksInterface(PrivateUse1HooksInterface hook_);

@Namespace("at") public static native PrivateUse1HooksInterface GetPrivateUse1HooksInterface();




// Parsed from ATen/detail/XPUHooksInterface.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>

// #include <c10/util/Registry.h>

// #include <cstddef>
// #include <functional>
// #include <memory>


// We use forward declaration here instead of #include <ATen/dlpack.h> to avoid
// leaking DLPack implementation detail to every project that includes `ATen/Context.h`, which in turn
// would lead to a conflict when linked with another project using DLPack (for example TVM)

@Namespace("at") @MemberGetter public static native @Cast("const char*") BytePointer XPU_HELP();
// Targeting ../XPUHooksInterface.java


// Targeting ../XPUHooksArgs.java


// #define REGISTER_XPU_HOOKS(clsname)
//   C10_REGISTER_CLASS(XPUHooksRegistry, clsname, clsname)
@Namespace("at::detail") public static native @Const @ByRef XPUHooksInterface getXPUHooks();
 // namespace detail
 // namespace at


// Parsed from c10/core/QEngine.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>

/**
 * QEngine is an enum that is used to select the engine to run quantized ops.
 * Keep this enum in sync with get_qengine_id() in
 * torch/backends/quantized/__init__.py
 */
@Namespace("c10") public enum QEngine {
  NoQEngine((byte)(0)),
  FBGEMM((byte)(1)),
  QNNPACK((byte)(2)),
  ONEDNN((byte)(3)),
  X86((byte)(4));

    public final byte value;
    private QEngine(byte v) { this.value = v; }
    private QEngine(QEngine e) { this.value = e.value; }
    public QEngine intern() { for (QEngine e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @StdString BytePointer toString(QEngine qengine);

 // namespace c10


// Parsed from c10/util/CallOnce.h

// #pragma once

// #include <atomic>
// #include <mutex>
// #include <utility>

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>

// custom c10 call_once implementation to avoid the deadlock in std::call_once.
// The implementation here is a simplified version from folly and likely much
// much higher memory footprint.

 // namespace c10


// Parsed from c10/util/env.h

// #pragma once

// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <cstring>
// Reads an environment variable and returns
// - optional<true>,              if set equal to "1"
// - optional<false>,             if set equal to "0"
// - nullopt,   otherwise
//
// NB:
// Issues a warning if the value of the environment variable is not 0 or 1.
@Namespace("c10::utils") public static native @ByVal BoolOptional check_env(@Cast("const char*") BytePointer name);
@Namespace("c10::utils") public static native @ByVal BoolOptional check_env(String name);
 // namespace utils
 // namespace c10


// Parsed from ATen/Context.h

// #pragma once

// #include <ATen/CPUGeneratorImpl.h>
// #include <ATen/LinalgBackend.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/LegacyTypeDispatch.h>
// #include <ATen/detail/CUDAHooksInterface.h>
// #include <ATen/detail/HIPHooksInterface.h>
// #include <ATen/detail/MPSHooksInterface.h>
// #include <ATen/detail/MTIAHooksInterface.h>
// #include <ATen/detail/ORTHooksInterface.h>
// #include <ATen/detail/PrivateUse1HooksInterface.h>
// #include <ATen/detail/XPUHooksInterface.h>
// #include <c10/core/QEngine.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/CallOnce.h>
// #include <c10/util/Exception.h>
// #include <c10/util/env.h>
// #include <c10/util/irange.h>

// #include <cstdint>
// #include <memory>
// #include <mutex>

@Namespace("at") public enum Float32MatmulPrecision { HIGHEST(0), HIGH(1), MEDIUM(2);

    public final int value;
    private Float32MatmulPrecision(int v) { this.value = v; }
    private Float32MatmulPrecision(Float32MatmulPrecision e) { this.value = e.value; }
    public Float32MatmulPrecision intern() { for (Float32MatmulPrecision e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Context.java



@Namespace("at") public static native @ByRef Context globalContext();

@Namespace("at") public static native void init();

@Namespace("at") public static native Allocator getCPUAllocator();

@Namespace("at") public static native @Cast("bool") boolean hasCUDA();

@Namespace("at") public static native @Cast("bool") boolean hasMTIA();

@Namespace("at") public static native @Cast("bool") boolean hasHIP();

@Namespace("at") public static native @Cast("bool") boolean hasIPU();

@Namespace("at") public static native @Cast("bool") boolean hasXLA();

@Namespace("at") public static native @Cast("bool") boolean hasMPS();

@Namespace("at") public static native @Cast("bool") boolean hasORT();

@Namespace("at") public static native @Cast("bool") boolean hasXPU();

// Despite its name, this function returns the number of *CUDA* GPUs.
@Namespace("at") public static native @Cast("size_t") long getNumGPUs();

@Namespace("at") public static native @Cast("bool") boolean hasOpenMP();

@Namespace("at") public static native @Cast("bool") boolean hasMKL();

@Namespace("at") public static native @Cast("bool") boolean hasLAPACK();

@Namespace("at") public static native @Cast("bool") boolean hasMAGMA();

@Namespace("at") public static native @Cast("bool") boolean hasMKLDNN();

@Namespace("at") public static native void manual_seed(@Cast("uint64_t") long seed);
// Targeting ../NoTF32Guard.java



// #ifdef USE_ROCM
// #endif

 // namespace at


// Parsed from ATen/DeviceGuard.h

// #pragma once

// #include <ATen/core/IListRef.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/DeviceGuard.h>
// #include <c10/core/ScalarType.h> // TensorList whyyyyy

// Are you here because you're wondering why DeviceGuard(tensor) no
// longer works?  For code organization reasons, we have temporarily(?)
// removed this constructor from DeviceGuard.  The new way to
// spell it is:
//
//    OptionalDeviceGuard guard(device_of(tensor));

/** Return the Device of a Tensor, if the Tensor is defined. */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef Tensor t);

@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef TensorOptional t);

/** Return the Device of a TensorList, if the list is non-empty and
 *  the first Tensor is defined.  (This function implicitly assumes
 *  that all tensors in the list have the same device.) */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@ByVal TensorArrayRef t);
@Namespace("at") public static native @ByVal DeviceOptional device_of(@ByVal TensorVector t);

 // namespace at


// Parsed from ATen/DimVector.h

// #pragma once
// #include <ATen/core/DimVector.h>


// Parsed from ATen/EmptyTensor.h

// #pragma once
// #include <ATen/core/TensorBase.h>

@Namespace("at::detail") public static native void check_size_nonnegative(@ByVal LongArrayRef size);
@Namespace("at::detail") public static native void check_size_nonnegative(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);

@Namespace("at::detail") public static native void check_size_nonnegative(@ByVal SymIntArrayRef size);

@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal LongArrayRef sizes,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal LongArrayRef sizes,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytesContiguous(
    @ByVal SymIntArrayRef sizes,
    @Const @ByRef SymInt itemsize,
    @Const @ByRef(nullValue = "c10::SymInt(0)") SymInt storage_offset);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytesContiguous(
    @ByVal SymIntArrayRef sizes,
    @Const @ByRef SymInt itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytes(
    @ByVal SymIntArrayRef sizes,
    @ByVal SymIntArrayRef strides,
    @Const @ByRef SymInt itemsize,
    @Const @ByRef(nullValue = "c10::SymInt(0)") SymInt storage_offset);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytes(
    @ByVal SymIntArrayRef sizes,
    @ByVal SymIntArrayRef strides,
    @Const @ByRef SymInt itemsize);

@Namespace("at::detail") public static native @ByVal TensorBase empty_generic(
    @ByVal LongArrayRef size,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_generic(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_generic(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_generic(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_generic(
    @ByVal SymIntArrayRef size,
    @ByVal SymIntArrayRef stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal LongArrayRef size,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal LongArrayRef size,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal LongArrayRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(@ByVal LongArrayRef size, @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal LongArrayRef size,
    ScalarType dtype,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal LongArrayRef size,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    ScalarType dtype,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal LongArrayRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_symint_meta(
    @ByVal SymIntArrayRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(@ByVal LongArrayRef size, @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(@ByVal LongArrayRef size, @ByVal LongArrayRef stride, ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal LongArrayRef size,
    @ByVal LongArrayRef stride,
    @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntArrayRef size,
    @ByVal SymIntArrayRef stride,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntArrayRef size,
    @ByVal SymIntArrayRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntArrayRef size,
    @ByVal SymIntArrayRef stride,
    @Const @ByRef TensorOptions options);

 // namespace detail
 // namespace at


// Parsed from ATen/TensorGeometry.h

// #pragma once

// #include <ATen/core/TensorBase.h>
// #include <c10/core/WrapDimMinimal.h>

// Return if the tensor geometry represented by `sizes` and `strides` is
// contiguous Although we cache is_contiguous in tensor now, this is till useful
// because it allows checking if a particular geometry is contiguous without
// explicitly constructing a tensor, e.g., when you want to choose a kernel
// strategy based on whether a subgeometry is contiguous.
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal LongArrayRef sizes, @ByVal LongArrayRef strides);
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... strides);
// Targeting ../TensorGeometry.java



 // namespace at


// Parsed from ATen/core/Formatting.h

// #pragma once

// #include <ostream>
// #include <string>

// #include <c10/core/Scalar.h>
// #include <ATen/core/Tensor.h>
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, Backend b);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Cast("c10::Backend") int b);
@Namespace("c10") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Scalar s);
@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Scalar s);

@Namespace("at") public static native @Cast("std::ostream*") @ByRef Pointer print(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long linesize);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Tensor t);
@Namespace("at") public static native void print(@Const @ByRef Tensor t, @Cast("int64_t") long linesize/*=80*/);
@Namespace("at") public static native void print(@Const @ByRef Tensor t);



// Parsed from ATen/Formatting.h

// #include <ATen/core/Formatting.h>


// Parsed from ATen/Utils.h

// #pragma once

// #include <ATen/EmptyTensor.h>
// #include <ATen/Formatting.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/StorageImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/irange.h>

// #include <algorithm>
// #include <memory>
// #include <numeric>
// #include <sstream>
// #include <typeinfo>

// #define AT_DISALLOW_COPY_AND_ASSIGN(TypeName)
//   TypeName(const TypeName&) = delete;
//   void operator=(const TypeName&) = delete

@Namespace("at") public static native int _crash_if_asan(int arg0);

// Converts a TensorList (i.e. ArrayRef<Tensor> to vector of TensorImpl*)
// NB: This is ONLY used by legacy TH bindings, and ONLY used by cat.
// Once cat is ported entirely to ATen this can be deleted!
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(
    @ByVal TensorArrayRef tensors,
    @Cast("const char*") BytePointer name,
    int pos,
    DeviceType device_type,
    ScalarType scalar_type);
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(
    @ByVal TensorVector tensors,
    String name,
    int pos,
    @Cast("c10::DeviceType") byte device_type,
    ScalarType scalar_type);
 // namespace detail

 // namespace at


// Parsed from ATen/TensorUtils.h

// #pragma once

// #include <ATen/DimVector.h>
// #include <ATen/EmptyTensor.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/Utils.h>

// #include <utility>

// These functions are NOT in Utils.h, because this file has a dep on Tensor.h

// #define TORCH_CHECK_TENSOR_ALL(cond, ...)
//   TORCH_CHECK((cond)._is_all_true().item<bool>(), __VA_ARGS__);
// Targeting ../TensorArg.java


// Targeting ../TensorGeometryArg.java



// A string describing which function did checks on its input
// arguments.
// TODO: Consider generalizing this into a call stack.

// The undefined convention: singular operators assume their arguments
// are defined, but functions which take multiple tensors will
// implicitly filter out undefined tensors (to make it easier to perform
// tests which should apply if the tensor is defined, and should not
// otherwise.)
//
// NB: This means that the n-ary operators take lists of TensorArg,
// not TensorGeometryArg, because the Tensor to TensorGeometry
// conversion will blow up if you have undefined tensors.

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal TensorGeometryArg t);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef Tensor tensor,
    @Cast("const char*") BytePointer name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef Tensor tensor,
    String name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t, @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t, @Cast("int64_t") long dim);
// NB: this is an inclusive-exclusive range
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal LongArrayRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal SymIntArrayRef sizes);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal SymIntArrayRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @ByVal SymInt size);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @ByVal SymInt size);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);

@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkScalarType(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t, ScalarType s);
@Namespace("at") public static native void checkScalarType(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t, ScalarType s);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeVector l);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameSize(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameSize(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef t);

// FixMe: does TensorArg slow things down?
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorVector t,
    @ByVal Backend backend);

@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);
@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorVector tensors,
    @ByVal DeviceType device_type);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef Tensor t, Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @Const @ByRef Tensor t, @Cast("c10::Layout") byte layout);

@Namespace("at") public static native void checkLayout(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal Layout layout);
@Namespace("at") public static native void checkLayout(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorVector tensors,
    @ByVal Layout layout);

// Methods for getting data_ptr if tensor is defined
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef Tensor tensor);
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef TensorArg tensor);

@Namespace("at") public static native void check_dim_size(
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_size,
    @Cast("int64_t") long size);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal LongArrayRef sizes);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal LongArrayRef oldshape,
    @ByVal LongArrayRef oldstride,
    @ByVal LongArrayRef newshape);
@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] oldstride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... newshape);

@Namespace("at::detail") public static native @ByVal SymDimVectorOptional computeStride(
    @ByVal SymIntArrayRef oldshape,
    @ByVal SymIntArrayRef oldstride,
    @ByVal SymIntArrayRef newshape);

@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal LongArrayRef oldshape,
    @ByVal LongArrayRef oldstride,
    @Const @ByRef DimVector newshape);
@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] oldstride,
    @Const @ByRef DimVector newshape);

 // namespace detail
 // namespace at


// Parsed from ATen/TracerMode.h

// #pragma once

// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Export.h>
// #include <c10/macros/Macros.h>

// NOTE [Tracing Mode Switches]
//
// Historically, tracing function was controlled by two switches:
//
// - `AutoDispatchBelowADInplaceOrView` guard
//
//    Tracing function used to be script-generated inside `VariableType_*.cpp`
//    kernels, sharing the same `Autograd` dispatch key with autograd function.
//    Therefore, before tracing function was moved out of VariableType,
//    `AutoDispatchBelowADInplaceOrView` guard can also disable tracing as a
//    side effect of disabling `Autograd` dispatching.
//
// - `setTracingState()` API in `torch/csrc/jit/frontend/tracer.h`
//
//    It stores tracing data in a `TracingState` object in TLS. If the
//    `TracingState` object in TLS is `null`, then tracing is paused.
//
//    The `TracingState` object is created in `tracer::trace()` - the main
//    entrance of tracing function. It's temporarily set to `null` inside
//    generated VariableType (now TraceType) to bypass tracing for intermediate
//    ops (ops being called by other ops). After the intermediate op call
//    finishes it's set back to the original `TracingState` object.
//
//    The `TracingState` obect in TLS can also be read/written via its Python
//    binding in `python_tracer.cpp`, and `get/setTracingState()` C++ APIs,
//    which are also exposed as `TORCH_API`.
//
// Two new switches were introduced since tracing function was moved out of
// VariableType:
//
// - `tracer::impl::set_dispatch_enabled()` API
//
//    Unlike the special `Autograd` dispatch key which is included in dispatch
//    key set by default, `Tracer` dispatch key is off by default. The
//    dispatching switch can be toggled via this new API.
//
// - `tracer::impl::NoTracerDispatchMode` guard
//
//    It's used to cover the old semantics of `AutoDispatchBelowADInplaceOrView`
//    after tracing was moved out of VariableType.
//
// Before tracing function was moved out of VariableType, tracing was enabled
// when the following conditions are satisfied:
//
//    1) `TracingState` object in TLS != null;
//       - Either inside the execution scope of `tracer::trace()`, or
//       - Eagerly called `setTracingState()` with non-null object.
//    2) Not inside `AutoDispatchBelowADInplaceOrView` scope;
//
// After:
//
//    1) `TracingState` object in TLS != null;
//    2) Has called `tracer::impl::set_dispatch_enabled(true)`;
//    3) Not inside `tracer::impl::NonDispatchGuard` scope;
//
// [TODOs]
//
// - `setTracingState()` v.s. `tracer::impl::set_dispatch_enabled()`
//
//   Currently `set_dispatch_enabled()` is set/unset inside `setTracingState()`
//   to keep the semantics exactly the same as before - it's confusing to keep
//   both switches, though. We should consider simplifying/limiting the exposed
//   `setTracingState()` Python/C++ APIs (and other APIs calling it) so that
//   these two can be unified.
//
// - `AutoDispatchBelowADInplaceOrView` v.s.
// `tracer::impl::NoTracerDispatchMode`
//
//   We don't need to always set both guards together to keep semantics
//   unchanged. For the follow use cases of `AutoDispatchBelowADInplaceOrView`
//   we don't need set the new tracer guard:
//
//   * Script-generated VariableType kernels. The guard is not necessary as
//     tracing is already disabled explicitly by `setTracingState(null)` in
//     generated TraceType kernels - we could keep it as is or use the new guard
//     instead.
//
//   * Custom ops. Will be handled by fallback kernel for `Tracer`.
//
//   * Functions that are not likely to be called in tracing context (no python
//     binding / not an operator), e.g.: all mobile forward() wrappers, test
//     binaries, and etc.
//
//   * Where new threads are spawned, e.g.: ATen/native/ConvolutionMM2d.cpp.
//     It's not necessary as tracing is off by default.
//
//   For the rest of cases we might need have both:
//
//   * Functions that might be reachable from eager mode python (especially
//     factory methods), e.g.:
//     `internal_new_from_data()` in `torch/csrc/utils/tensor_new.cpp`.
//     Without the new guard it will add `aten::empty` to the traced graph.
//
//   * Some manually maintained functions, e.g.:
//     `torch/csrc/autograd/VariableTypeManual.cpp`.
//     Set the new guard if it's not obvious whether `setTracingState(null)`
//     has been called before it reaches the `AutoDispatchBelowADInplaceOrView`
//     guard.
//
//   We might need tweak the usage of the new guard to optimize/fix things.
//   It should only affect the correctness of tracing function, because the
//   guard is essentially no-op when the master `setTracingState()` switch is
//   off.
// TODO: move this from `at::` to `jit::torch::` after
// `aten/src/ATen/cpp_custom_type_hack.h` is removed.

@Namespace("at::tracer::impl") public static native @Cast("bool") boolean is_dispatch_enabled();

@Namespace("at::tracer::impl") public static native void set_dispatch_enabled(@Cast("bool") boolean enabled);

 // namespace impl
 // namespace tracer
 // namespace at


// Parsed from ATen/core/Reduction.h

// #pragma once

// NB: Keep this in sync with Reduction class in torch/nn/_reduction.py
// These constants control the reduction behavior of loss functions.
// Ideally, this would be a scoped enum, but jit doesn't support that
@Namespace("at::Reduction") public enum Reduction {
  None(0),             // Do not reduce
  Mean(1),             // (Possibly weighted) mean of losses
  Sum(2),              // Sum losses
  END(3);

    public final int value;
    private Reduction(int v) { this.value = v; }
    private Reduction(Reduction e) { this.value = e.value; }
    public Reduction intern() { for (Reduction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
 // namespace Reduction
 // namespace at


// Parsed from ATen/ops/abs.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/abs_ops.h>


// aten::abs(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor abs(@Const @ByRef Tensor self);

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_(@ByRef Tensor self);

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/absolute.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/absolute_ops.h>


// aten::absolute(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor absolute(@Const @ByRef Tensor self);

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/acos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/acos_ops.h>


// aten::acos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acos(@Const @ByRef Tensor self);

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_(@ByRef Tensor self);

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/acosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/acosh_ops.h>


// aten::acosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acosh(@Const @ByRef Tensor self);

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_(@ByRef Tensor self);

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/adaptive_avg_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool1d_ops.h>


// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);




// Parsed from ATen/ops/adaptive_avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool2d_ops.h>


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/adaptive_avg_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool3d_ops.h>


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/adaptive_avg_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool3d_backward_ops.h>


// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);




// Parsed from ATen/ops/adaptive_max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool1d_ops.h>


// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);




// Parsed from ATen/ops/adaptive_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool2d_ops.h>


// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);
// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);




// Parsed from ATen/ops/adaptive_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool2d_backward_ops.h>


// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/adaptive_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool3d_ops.h>


// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);
// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);




// Parsed from ATen/ops/adaptive_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool3d_backward_ops.h>


// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/add_ops.h>


// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/addbmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addbmm_ops.h>


// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);




// Parsed from ATen/ops/addcdiv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addcdiv_ops.h>


// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);




// Parsed from ATen/ops/addcmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addcmul_ops.h>


// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);




// Parsed from ATen/ops/addmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addmm_ops.h>


// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/addmv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addmv_ops.h>


// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/addr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addr_ops.h>


// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);
// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/adjoint.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adjoint_ops.h>


// aten::adjoint(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor adjoint(@Const @ByRef Tensor self);




// Parsed from ATen/ops/affine_grid_generator.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/affine_grid_generator_ops.h>


// aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean align_corners);


// aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_symint(@Const @ByRef Tensor theta, @ByVal SymIntArrayRef size, @Cast("bool") boolean align_corners);


// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @ByVal LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean align_corners);


// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_outf(@Const @ByRef Tensor theta, @ByVal LongArrayRef size, @Cast("bool") boolean align_corners, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_outf(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean align_corners, @ByRef Tensor out);


// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_symint_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @ByVal SymIntArrayRef size, @Cast("bool") boolean align_corners);


// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_symint_outf(@Const @ByRef Tensor theta, @ByVal SymIntArrayRef size, @Cast("bool") boolean align_corners, @ByRef Tensor out);





// Parsed from ATen/ops/affine_grid_generator_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/affine_grid_generator_backward_ops.h>


// aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean align_corners);


// aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntArrayRef size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/alias.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alias_ops.h>


// aten::alias(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor alias(@Const @ByRef Tensor self);




// Parsed from ATen/ops/alias_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alias_copy_ops.h>


// aten::alias_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor alias_copy(@Const @ByRef Tensor self);

// aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alias_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alias_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/align_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_as_ops.h>






// Parsed from ATen/ops/align_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_tensors_ops.h>


// aten::align_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector align_tensors(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector align_tensors(@ByVal TensorVector tensors);




// Parsed from ATen/ops/align_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_to_ops.h>






// Parsed from ATen/ops/all.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/all_ops.h>


// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::all(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self);

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/allclose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/allclose_ops.h>


// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/alpha_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alpha_dropout_ops.h>


// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/amax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/amax_ops.h>


// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/amin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/amin_ops.h>


// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/aminmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/aminmax_ops.h>


// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
@Namespace("at") public static native @ByVal T_TensorTensor_T aminmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T aminmax(@Const @ByRef Tensor self);

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal T_TensorTensor_T aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self);
// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal T_TensorTensor_T aminmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor max);




// Parsed from ATen/ops/and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/and_ops.h>


// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/angle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/angle_ops.h>


// aten::angle(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor angle(@Const @ByRef Tensor self);

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/any.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/any_ops.h>


// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::any(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self);

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arange.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arange_ops.h>


// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end);
// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end);
// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar end);
// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar end, @ByRef Tensor out);

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);




// Parsed from ATen/ops/arccos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arccos_ops.h>


// aten::arccos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccos(@Const @ByRef Tensor self);

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_(@ByRef Tensor self);

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arccosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arccosh_ops.h>


// aten::arccosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccosh(@Const @ByRef Tensor self);

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_(@ByRef Tensor self);

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arcsin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arcsin_ops.h>


// aten::arcsin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsin(@Const @ByRef Tensor self);

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_(@ByRef Tensor self);

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arcsinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arcsinh_ops.h>


// aten::arcsinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsinh(@Const @ByRef Tensor self);

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_(@ByRef Tensor self);

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arctan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctan_ops.h>


// aten::arctan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctan(@Const @ByRef Tensor self);

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_(@ByRef Tensor self);

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arctan2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctan2_ops.h>


// aten::arctan2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/arctanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctanh_ops.h>


// aten::arctanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctanh(@Const @ByRef Tensor self);

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_(@ByRef Tensor self);

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/argmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argmax_ops.h>


// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self);

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/argmin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argmin_ops.h>


// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self);

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/argsort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argsort_ops.h>


// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self);

// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("bool") boolean stable);

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argsort_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByRef Tensor argsort_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean stable);
// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argsort_outf(@Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor out);




// Parsed from ATen/ops/argwhere.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argwhere_ops.h>


// aten::argwhere(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor argwhere(@Const @ByRef Tensor self);




// Parsed from ATen/ops/as_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_ops.h>


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor as_strided_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor as_strided__symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided__symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);





// Parsed from ATen/ops/as_strided_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_copy_ops.h>


// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal SymIntOptional storage_offset, @ByRef Tensor out);





// Parsed from ATen/ops/as_strided_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_scatter_ops.h>


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal SymIntOptional storage_offset, @ByRef Tensor out);





// Parsed from ATen/ops/asin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/asin_ops.h>


// aten::asin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asin(@Const @ByRef Tensor self);

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_(@ByRef Tensor self);

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/asinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/asinh_ops.h>


// aten::asinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asinh(@Const @ByRef Tensor self);

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_(@ByRef Tensor self);

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atan_ops.h>


// aten::atan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan(@Const @ByRef Tensor self);

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_(@ByRef Tensor self);

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atan2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atan2_ops.h>


// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::atan2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/atanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atanh_ops.h>


// aten::atanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atanh(@Const @ByRef Tensor self);

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_(@ByRef Tensor self);

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atleast_1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_1d_ops.h>


// aten::atleast_1d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_1d(@Const @ByRef Tensor self);

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_1d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_1d(@ByVal TensorVector tensors);




// Parsed from ATen/ops/atleast_2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_2d_ops.h>


// aten::atleast_2d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_2d(@Const @ByRef Tensor self);

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_2d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_2d(@ByVal TensorVector tensors);




// Parsed from ATen/ops/atleast_3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_3d_ops.h>


// aten::atleast_3d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_3d(@Const @ByRef Tensor self);

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_3d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector atleast_3d(@ByVal TensorVector tensors);




// Parsed from ATen/ops/avg_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool1d_ops.h>


// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool2d_ops.h>


// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/avg_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool2d_backward_ops.h>


// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);




// Parsed from ATen/ops/avg_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool3d_ops.h>


// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/avg_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool3d_backward_ops.h>


// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);




// Parsed from ATen/ops/baddbmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/baddbmm_ops.h>


// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/bartlett_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bartlett_window_ops.h>


// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length);
// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_ops.h>


// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);




// Parsed from ATen/ops/batch_norm_backward_elemt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_backward_elemt_ops.h>


// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_backward_elemt(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor sum_dy, @Const @ByRef Tensor sum_dy_xmu, @Const @ByRef Tensor count);

// aten::batch_norm_backward_elemt.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_backward_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor sum_dy, @Const @ByRef Tensor sum_dy_xmu, @Const @ByRef Tensor count);
// aten::batch_norm_backward_elemt.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_backward_elemt_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor sum_dy, @Const @ByRef Tensor sum_dy_xmu, @Const @ByRef Tensor count, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm_backward_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_backward_reduce_ops.h>


// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T batch_norm_backward_reduce(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);

// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T batch_norm_backward_reduce_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);
// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T batch_norm_backward_reduce_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/batch_norm_elemt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_elemt_ops.h>


// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_elemt(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);
// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm_gather_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_gather_stats_ops.h>


// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);

// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);
// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_gather_stats_with_counts.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_gather_stats_with_counts_ops.h>


// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats_with_counts(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);

// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats_with_counts_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);
// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_gather_stats_with_counts_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_stats_ops.h>


// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_stats(@Const @ByRef Tensor input, double eps);

// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, double eps);
// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_stats_outf(@Const @ByRef Tensor input, double eps, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_update_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_update_stats_ops.h>


// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_update_stats(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);

// aten::batch_norm_update_stats.out(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_update_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);
// aten::batch_norm_update_stats.out(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T batch_norm_update_stats_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/bernoulli.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bernoulli_ops.h>


// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self);

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p);

// aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::bernoulli.Tensor(Tensor self, Tensor p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @Const @ByRef Tensor p);

// aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p/*=0.5*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, double p, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/bilinear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bilinear_ops.h>


// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/binary_cross_entropy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_ops.h>


// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);




// Parsed from ATen/ops/binary_cross_entropy_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_backward_ops.h>


// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);




// Parsed from ATen/ops/binary_cross_entropy_with_logits.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_with_logits_ops.h>


// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional pos_weight, @Cast("int64_t") long reduction, @ByRef Tensor out);




// Parsed from ATen/ops/bincount.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bincount_ops.h>


// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self);

// aten::bincount.out(Tensor self, Tensor? weights=None, int minlength=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bincount_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByRef Tensor bincount_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::bincount.out(Tensor self, Tensor? weights=None, int minlength=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bincount_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional weights, @Cast("int64_t") long minlength, @ByRef Tensor out);




// Parsed from ATen/ops/binomial.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binomial_ops.h>


// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob);

// aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binomial_out(@ByRef Tensor out, @Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor binomial_out(@ByRef Tensor out, @Const @ByRef Tensor count, @Const @ByRef Tensor prob);
// aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binomial_outf(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_and_ops.h>


// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_left_shift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_left_shift_ops.h>


// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_not.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_not_ops.h>


// aten::bitwise_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_not(@Const @ByRef Tensor self);

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_or_ops.h>


// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_right_shift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_right_shift_ops.h>


// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_xor_ops.h>


// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/blackman_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/blackman_window_ops.h>


// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length);
// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::blackman_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::blackman_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::blackman_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::blackman_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/block_diag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/block_diag_ops.h>


// aten::block_diag(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor block_diag(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor block_diag(@ByVal TensorVector tensors);

// aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor block_diag_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor block_diag_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor block_diag_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor block_diag_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/bmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bmm_ops.h>


// aten::bmm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);




// Parsed from ATen/ops/broadcast_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/broadcast_tensors_ops.h>


// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector broadcast_tensors(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector broadcast_tensors(@ByVal TensorVector tensors);




// Parsed from ATen/ops/broadcast_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/broadcast_to_ops.h>


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor broadcast_to_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size);





// Parsed from ATen/ops/bucketize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bucketize_ops.h>


// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);
// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor boundaries);
// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);




// Parsed from ATen/ops/can_cast.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/can_cast_ops.h>


// aten::can_cast(ScalarType from, ScalarType to) -> bool
@Namespace("at") public static native @Cast("bool") boolean can_cast(ScalarType from, ScalarType to);




// Parsed from ATen/ops/cartesian_prod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cartesian_prod_ops.h>


// aten::cartesian_prod(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor cartesian_prod(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor cartesian_prod(@ByVal TensorVector tensors);




// Parsed from ATen/ops/cat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cat_ops.h>


// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorVector tensors);

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorVector tensors);
// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cat_outf(@Const @ByRef TensorVector tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorVector tensors, @ByVal Dimname dim);

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorVector tensors, @ByVal Dimname dim);
// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorVector tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/cauchy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cauchy_ops.h>


// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cauchy_out(@ByRef Tensor out, @Const @ByRef Tensor self, double median/*=0*/, double sigma/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor cauchy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cauchy_outf(@Const @ByRef Tensor self, double median, double sigma, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cauchy(@Const @ByRef Tensor self, double median/*=0*/, double sigma/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor cauchy(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ccol_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ccol_indices_ops.h>






// Parsed from ATen/ops/ccol_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ccol_indices_copy_ops.h>


// aten::ccol_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor ccol_indices_copy(@Const @ByRef Tensor self);

// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ccol_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ccol_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cdist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cdist_ops.h>


// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional compute_mode);
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/ceil.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ceil_ops.h>


// aten::ceil(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor ceil(@Const @ByRef Tensor self);

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_(@ByRef Tensor self);

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/celu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/celu_ops.h>


// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self);

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self);

// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/chain_matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chain_matmul_ops.h>


// aten::chain_matmul(Tensor[] matrices) -> Tensor
@Namespace("at") public static native @ByVal Tensor chain_matmul(@ByVal TensorArrayRef matrices);
@Namespace("at") public static native @ByVal Tensor chain_matmul(@ByVal TensorVector matrices);

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_out(@ByRef Tensor out, @ByVal TensorArrayRef matrices);
@Namespace("at") public static native @ByRef Tensor chain_matmul_out(@ByRef Tensor out, @ByVal TensorVector matrices);
// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_outf(@ByVal TensorArrayRef matrices, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor chain_matmul_outf(@ByVal TensorVector matrices, @ByRef Tensor out);




// Parsed from ATen/ops/chalf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chalf_ops.h>






// Parsed from ATen/ops/channel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/channel_shuffle_ops.h>


// aten::channel_shuffle(Tensor self, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);

// aten::channel_shuffle.out(Tensor self, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor channel_shuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long groups);
// aten::channel_shuffle.out(Tensor self, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor channel_shuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cholesky.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_ops.h>


// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cholesky_inverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_inverse_ops.h>


// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self);

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);




// Parsed from ATen/ops/cholesky_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_solve_ops.h>


// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);




// Parsed from ATen/ops/choose_qparams_optimized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/choose_qparams_optimized_ops.h>


// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T choose_qparams_optimized(@Const @ByRef Tensor input, @Cast("int64_t") long numel, @Cast("int64_t") long n_bins, double ratio, @Cast("int64_t") long bit_width);




// Parsed from ATen/ops/chunk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chunk_ops.h>


// aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);




// Parsed from ATen/ops/clamp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_ops.h>


// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self);

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self);

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);
// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);




// Parsed from ATen/ops/clamp_max.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_max_ops.h>


// aten::clamp_max(Tensor self, Scalar max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar max);
// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor max);
// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor max, @ByRef Tensor out);




// Parsed from ATen/ops/clamp_min.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_min_ops.h>


// aten::clamp_min(Tensor self, Scalar min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar min);
// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min, @ByRef Tensor out);

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor min);
// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor min, @ByRef Tensor out);




// Parsed from ATen/ops/clip.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clip_ops.h>


// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self);

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self);

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);
// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);




// Parsed from ATen/ops/clone.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clone_ops.h>


// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self);

// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clone_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor clone_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clone_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/coalesce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/coalesce_ops.h>






// Parsed from ATen/ops/col2im.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col2im_ops.h>


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);


// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);





// Parsed from ATen/ops/col_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col_indices_ops.h>






// Parsed from ATen/ops/col_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col_indices_copy_ops.h>


// aten::col_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor col_indices_copy(@Const @ByRef Tensor self);

// aten::col_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::col_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/column_stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/column_stack_ops.h>


// aten::column_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor column_stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor column_stack(@ByVal TensorVector tensors);

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor column_stack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor column_stack_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/combinations.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/combinations_ops.h>


// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self, @Cast("int64_t") long r/*=2*/, @Cast("bool") boolean with_replacement/*=false*/);
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self);




// Parsed from ATen/ops/complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/complex_ops.h>


// aten::complex(Tensor real, Tensor imag) -> Tensor


// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_out(@ByRef Tensor out, @Const @ByRef Tensor real, @Const @ByRef Tensor imag);
// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_outf(@Const @ByRef Tensor real, @Const @ByRef Tensor imag, @ByRef Tensor out);




// Parsed from ATen/ops/concat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/concat_ops.h>


// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorVector tensors);

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorVector tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorVector tensors, @ByVal Dimname dim);

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorVector tensors, @ByVal Dimname dim);
// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorVector tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/concatenate.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/concatenate_ops.h>


// aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorVector tensors);

// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorVector tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorVector tensors, @ByVal Dimname dim);

// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorVector tensors, @ByVal Dimname dim);
// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorVector tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conj_ops.h>


// aten::conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor __dispatch_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/conj_physical.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conj_physical_ops.h>


// aten::conj_physical(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor conj_physical(@Const @ByRef Tensor self);

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_(@ByRef Tensor self);




// Parsed from ATen/ops/constant_pad_nd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/constant_pad_nd_ops.h>


// aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal LongArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... pad);


// aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor constant_pad_nd_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef pad);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef pad, @Const @ByRef Scalar value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] pad, @Const @ByRef Scalar value, @ByRef Tensor out);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad, @Const @ByRef Scalar value, @ByRef Tensor out);





// Parsed from ATen/ops/contiguous.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/contiguous_ops.h>






// Parsed from ATen/ops/conv1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv1d_ops.h>


// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, SymInt[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, SymInt[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding);




// Parsed from ATen/ops/conv2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv2d_ops.h>


// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding);




// Parsed from ATen/ops/conv3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv3d_ops.h>


// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);


// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @StringView BytePointer padding);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @StringView String padding);




// Parsed from ATen/ops/conv_depthwise3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_depthwise3d_ops.h>


// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/conv_tbc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_tbc_ops.h>


// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);

// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_tbc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByRef Tensor conv_tbc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);
// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_tbc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad, @ByRef Tensor out);




// Parsed from ATen/ops/conv_tbc_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_tbc_backward_ops.h>


// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T conv_tbc_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad);




// Parsed from ATen/ops/conv_transpose1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose1d_ops.h>


// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, SymInt[1] padding=0, SymInt[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, SymInt[1] padding=0, SymInt[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);





// Parsed from ATen/ops/conv_transpose2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose2d_ops.h>


// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);





// Parsed from ATen/ops/conv_transpose3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose3d_ops.h>


// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);


// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);





// Parsed from ATen/ops/convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_ops.h>


// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups);


// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);





// Parsed from ATen/ops/convolution_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_backward_ops.h>


// aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/convolution_backward_overrideable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_backward_overrideable_ops.h>


// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T convolution_backward_overrideable_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/convolution_overrideable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_overrideable_ops.h>


// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups);

// aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups);
// aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal LongArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copy_ops.h>


// aten::copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor copy(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor copy(@Const @ByRef Tensor self, @Const @ByRef Tensor src);

// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking, @ByRef Tensor out);




// Parsed from ATen/ops/copy_sparse_to_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copy_sparse_to_sparse_ops.h>


// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src);

// aten::copy_sparse_to_sparse.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::copy_sparse_to_sparse.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking, @ByRef Tensor out);

// aten::copy_sparse_to_sparse(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor copy_sparse_to_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor copy_sparse_to_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor src);




// Parsed from ATen/ops/copysign.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copysign_ops.h>


// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/corrcoef.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/corrcoef_ops.h>


// aten::corrcoef(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor corrcoef(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cos_ops.h>


// aten::cos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cos(@Const @ByRef Tensor self);

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_(@ByRef Tensor self);

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosh_ops.h>


// aten::cosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosh(@Const @ByRef Tensor self);

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_(@ByRef Tensor self);

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cosine_embedding_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosine_embedding_loss_ops.h>


// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);




// Parsed from ATen/ops/cosine_similarity.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosine_similarity_ops.h>


// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, @Cast("int64_t") long dim/*=1*/, double eps/*=1e-08*/);
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/count_nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/count_nonzero_ops.h>


// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self);

// aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByRef Tensor out);

// aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @ByRef Tensor out);




// Parsed from ATen/ops/cov.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cov_ops.h>


// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self, @Cast("int64_t") long correction/*=1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional fweights, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional aweights);
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cross.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cross_ops.h>


// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/cross_entropy_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cross_entropy_loss_ops.h>


// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/, double label_smoothing/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index, double label_smoothing/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/crow_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/crow_indices_ops.h>






// Parsed from ATen/ops/crow_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/crow_indices_copy_ops.h>


// aten::crow_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor crow_indices_copy(@Const @ByRef Tensor self);

// aten::crow_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor crow_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::crow_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor crow_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/ctc_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ctc_loss_ops.h>


// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal LongArrayRef input_lengths, @ByVal LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal LongArrayRef input_lengths, @ByVal LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... target_lengths);

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);




// Parsed from ATen/ops/cudnn_affine_grid_generator.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_affine_grid_generator_ops.h>


// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_affine_grid_generator.out(Tensor theta, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
// aten::cudnn_affine_grid_generator.out(Tensor theta, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_outf(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_affine_grid_generator_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_affine_grid_generator_backward_ops.h>


// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_backward_outf(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_batch_norm_ops.h>


// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T cudnn_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T cudnn_batch_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
// aten::cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T cudnn_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/cudnn_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_batch_norm_backward_ops.h>


// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T cudnn_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);

// aten::cudnn_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T cudnn_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);
// aten::cudnn_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T cudnn_batch_norm_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/cudnn_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_ops.h>


// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution.out(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
// aten::cudnn_convolution.out(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_add_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_add_relu_ops.h>


// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);
// aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_relu_ops.h>


// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);
// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_transpose_ops.h>


// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
// aten::cudnn_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_grid_sampler.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_grid_sampler_ops.h>


// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
@Namespace("at") public static native @ByVal Tensor cudnn_grid_sampler(@Const @ByRef Tensor self, @Const @ByRef Tensor grid);

// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_grid_sampler_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor grid);
// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_grid_sampler_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_grid_sampler_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_grid_sampler_backward_ops.h>


// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
@Namespace("at") public static native @ByVal T_TensorTensor_T cudnn_grid_sampler_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);

// aten::cudnn_grid_sampler_backward.out(Tensor self, Tensor grid, Tensor grad_output, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T cudnn_grid_sampler_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);
// aten::cudnn_grid_sampler_backward.out(Tensor self, Tensor grid, Tensor grad_output, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T cudnn_grid_sampler_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/cudnn_is_acceptable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_is_acceptable_ops.h>


// aten::cudnn_is_acceptable(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean cudnn_is_acceptable(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cummax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummax_ops.h>


// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummax_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/cummaxmin_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummaxmin_backward_ops.h>


// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cummaxmin_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor indices, @Cast("int64_t") long dim);




// Parsed from ATen/ops/cummin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummin_ops.h>


// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T cummin_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/cumprod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumprod_ops.h>


// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/cumprod_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumprod_backward_ops.h>


// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Cast("int64_t") long dim, @Const @ByRef Tensor output);




// Parsed from ATen/ops/cumsum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumsum_ops.h>


// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/cumulative_trapezoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumulative_trapezoid_ops.h>


// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y);




// Parsed from ATen/ops/data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/data_ops.h>






// Parsed from ATen/ops/deg2rad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/deg2rad_ops.h>


// aten::deg2rad(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor deg2rad(@Const @ByRef Tensor self);

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_(@ByRef Tensor self);

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/dense_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dense_dim_ops.h>






// Parsed from ATen/ops/dequantize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dequantize_ops.h>


// aten::dequantize.self(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor dequantize(@Const @ByRef Tensor self);

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector dequantize(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector dequantize(@ByVal TensorVector tensors);

// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dequantize_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dequantize_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void dequantize_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native void dequantize_out(@ByVal TensorVector out, @ByVal TensorVector tensors);
// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void dequantize_outf(@ByVal TensorArrayRef tensors, @ByVal TensorArrayRef out);
@Namespace("at") public static native void dequantize_outf(@ByVal TensorVector tensors, @ByVal TensorVector out);




// Parsed from ATen/ops/det.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/det_ops.h>


// aten::det(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor det(@Const @ByRef Tensor self);




// Parsed from ATen/ops/detach.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/detach_ops.h>


// aten::detach(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor detach(@Const @ByRef Tensor self);

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_(@ByRef Tensor self);




// Parsed from ATen/ops/detach_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/detach_copy_ops.h>


// aten::detach_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor detach_copy(@Const @ByRef Tensor self);

// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/diag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diag_ops.h>


// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::diag(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self);




// Parsed from ATen/ops/diag_embed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diag_embed_ops.h>


// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self);

// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_embed_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByRef Tensor diag_embed_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_embed_outf(@Const @ByRef Tensor self, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diagflat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagflat_ops.h>


// aten::diagflat(Tensor self, int offset=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self);




// Parsed from ATen/ops/diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_ops.h>


// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self);

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2);




// Parsed from ATen/ops/diagonal_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_backward_ops.h>


// aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByRef Tensor diagonal_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor diagonal_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);





// Parsed from ATen/ops/diagonal_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_copy_ops.h>


// aten::diagonal_copy(Tensor self, int offset=0, int dim1=0, int dim2=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_copy(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal_copy(@Const @ByRef Tensor self);

// aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByRef Tensor diagonal_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diagonal_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_scatter_ops.h>


// aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src);

// aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diff.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diff_ops.h>


// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self);

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @Cast("int64_t") long dim, @Const @ByRef TensorOptional prepend, @Const @ByRef TensorOptional append, @ByRef Tensor out);




// Parsed from ATen/ops/digamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/digamma_ops.h>


// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor digamma(@Const @ByRef Tensor self);




// Parsed from ATen/ops/dist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dist_ops.h>


// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dist_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor dist_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dist_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar p, @ByRef Tensor out);




// Parsed from ATen/ops/div.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/div_ops.h>


// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode);

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode);
// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode, @ByRef Tensor out);

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal StringViewOptional rounding_mode);

// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal StringViewOptional rounding_mode);
// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal StringViewOptional rounding_mode, @ByRef Tensor out);




// Parsed from ATen/ops/divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/divide_ops.h>


// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode);

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode);
// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal StringViewOptional rounding_mode, @ByRef Tensor out);

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal StringViewOptional rounding_mode);




// Parsed from ATen/ops/dot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dot_ops.h>


// aten::dot(Tensor self, Tensor tensor) -> Tensor
@Namespace("at") public static native @ByVal Tensor dot(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor);

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor);
// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor, @ByRef Tensor out);




// Parsed from ATen/ops/dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dropout_ops.h>


// aten::dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/dsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dsplit_ops.h>


// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... indices);




// Parsed from ATen/ops/dstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dstack_ops.h>


// aten::dstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor dstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor dstack(@ByVal TensorVector tensors);

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor dstack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor dstack_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/einsum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/einsum_ops.h>


// aten::einsum(str equation, Tensor[] tensors, *, int[]? path=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor einsum(@StringView BytePointer equation, @ByVal TensorArrayRef tensors, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional path);
@Namespace("at") public static native @ByVal Tensor einsum(@StringView BytePointer equation, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor einsum(@StringView String equation, @ByVal TensorVector tensors, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... path);
@Namespace("at") public static native @ByVal Tensor einsum(@StringView String equation, @ByVal TensorVector tensors);




// Parsed from ATen/ops/elu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/elu_ops.h>


// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @ByRef Tensor out);

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self);

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self);




// Parsed from ATen/ops/elu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/elu_backward_ops.h>


// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);
// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result, @ByRef Tensor grad_input);

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);




// Parsed from ATen/ops/embedding.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_ops.h>


// aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_symint(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::SymInt(-1)") SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding_symint(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByRef Tensor embedding_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse, @ByRef Tensor out);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_symint_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::SymInt(-1)") SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByRef Tensor embedding_symint_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_symint_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse, @ByRef Tensor out);





// Parsed from ATen/ops/embedding_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_backward_ops.h>


// aten::embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);


// aten::embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);





// Parsed from ATen/ops/embedding_bag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_bag_ops.h>


// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset, @ByVal LongOptional padding_idx);




// Parsed from ATen/ops/embedding_dense_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_dense_backward_ops.h>


// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @ByRef Tensor out);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @ByRef Tensor out);





// Parsed from ATen/ops/embedding_renorm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_renorm_ops.h>


// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_(@ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);

// aten::embedding_renorm.out(Tensor self, Tensor indices, float max_norm, float norm_type, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);
// aten::embedding_renorm.out(Tensor self, Tensor indices, float max_norm, float norm_type, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type, @ByRef Tensor out);

// aten::embedding_renorm(Tensor self, Tensor indices, float max_norm, float norm_type) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_renorm(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);




// Parsed from ATen/ops/embedding_sparse_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_sparse_backward_ops.h>


// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);




// Parsed from ATen/ops/empty.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_ops.h>


// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntArrayRef size);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_symint_outf(@ByVal SymIntArrayRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::empty.names_out(int[] size, *, Dimname[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::empty.names_out(int[] size, *, Dimname[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_like_ops.h>


// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self);
// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_permuted.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_permuted_ops.h>


// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal LongArrayRef size, @ByVal LongArrayRef physical_layout, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal LongArrayRef size, @ByVal LongArrayRef physical_layout);
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... physical_layout);


// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal LongArrayRef size, @ByVal LongArrayRef physical_layout, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_permuted(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal LongArrayRef physical_layout, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal LongArrayRef physical_layout);
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... physical_layout);


// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal LongArrayRef physical_layout, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_permuted_symint(@ByVal SymIntArrayRef size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_permuted_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal LongArrayRef physical_layout);
@Namespace("at") public static native @ByRef Tensor empty_permuted_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... physical_layout);


// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_permuted_outf(@ByVal LongArrayRef size, @ByVal LongArrayRef physical_layout, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_permuted_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByRef Tensor out);


// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_permuted_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal LongArrayRef physical_layout);
@Namespace("at") public static native @ByRef Tensor empty_permuted_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... physical_layout);


// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_permuted_symint_outf(@ByVal SymIntArrayRef size, @ByVal LongArrayRef physical_layout, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_permuted_symint_outf(@ByVal SymIntArrayRef size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] physical_layout, @ByRef Tensor out);





// Parsed from ATen/ops/empty_quantized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_quantized_ops.h>


// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor);
// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_quantized.out(int[] size, Tensor qtensor, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor);
// aten::empty_quantized.out(int[] size, Tensor qtensor, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_quantized_outf(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_quantized_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_strided_ops.h>


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor empty_strided_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_outf(@ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_strided_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_symint_outf(@ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByRef Tensor out);





// Parsed from ATen/ops/eq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/eq_ops.h>


// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/equal_ops.h>


// aten::equal(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/erf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erf_ops.h>


// aten::erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erf(@Const @ByRef Tensor self);

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_(@ByRef Tensor self);

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/erfc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erfc_ops.h>


// aten::erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfc(@Const @ByRef Tensor self);

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_(@ByRef Tensor self);

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/erfinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erfinv_ops.h>


// aten::erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfinv(@Const @ByRef Tensor self);

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exp_ops.h>


// aten::exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp(@Const @ByRef Tensor self);

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_(@ByRef Tensor self);

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exp2_ops.h>


// aten::exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp2(@Const @ByRef Tensor self);

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_(@ByRef Tensor self);

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/expand.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_ops.h>






// Parsed from ATen/ops/expand_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_as_ops.h>






// Parsed from ATen/ops/expand_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_copy_ops.h>


// aten::expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor expand_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("bool") boolean implicit, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor expand_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("bool") boolean implicit, @ByRef Tensor out);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @Cast("bool") boolean implicit, @ByRef Tensor out);





// Parsed from ATen/ops/expm1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expm1_ops.h>


// aten::expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor expm1(@Const @ByRef Tensor self);

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_(@ByRef Tensor self);

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exponential.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exponential_ops.h>


// aten::exponential.out(Tensor self, float lambd=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exponential_out(@ByRef Tensor out, @Const @ByRef Tensor self, double lambd/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor exponential_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exponential.out(Tensor self, float lambd=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exponential_outf(@Const @ByRef Tensor self, double lambd, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::exponential(Tensor self, float lambd=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor exponential(@Const @ByRef Tensor self, double lambd/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor exponential(@Const @ByRef Tensor self);




// Parsed from ATen/ops/eye.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/eye_ops.h>


// aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n);


// aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n);


// aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m);


// aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n, @ByVal SymInt m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n, @ByVal SymInt m);


// aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye_symint(@ByVal SymInt n, @ByVal SymInt m, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n);


// aten::eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @ByRef Tensor out);


// aten::eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_symint_out(@ByRef Tensor out, @ByVal SymInt n);


// aten::eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_symint_outf(@ByVal SymInt n, @ByRef Tensor out);


// aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n, @Cast("int64_t") long m);


// aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByRef Tensor out);


// aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_symint_out(@ByRef Tensor out, @ByVal SymInt n, @ByVal SymInt m);


// aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_symint_outf(@ByVal SymInt n, @ByVal SymInt m, @ByRef Tensor out);





// Parsed from ATen/ops/fake_quantize_per_channel_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_ops.h>


// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/fake_quantize_per_channel_affine_cachemask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_ops.h>


// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_channel_affine_cachemask(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_channel_affine_cachemask.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_channel_affine_cachemask_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::fake_quantize_per_channel_affine_cachemask.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_channel_affine_cachemask_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/fake_quantize_per_channel_affine_cachemask_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward_ops.h>


// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_ops.h>


// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine_cachemask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_ops.h>


// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_tensor_affine_cachemask(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_tensor_affine_cachemask_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fake_quantize_per_tensor_affine_cachemask_outf(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward_ops.h>


// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/fbgemm_linear_fp16_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_fp16_weight_ops.h>


// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_fp16_weight_fp32_activation.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation_ops.h>


// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_int8_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_int8_weight_ops.h>


// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_int8_weight_fp32_activation.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation_ops.h>


// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_quantize_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_quantize_weight_ops.h>


// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
@Namespace("at") public static native @ByVal T_TensorTensorDoubleLong_T fbgemm_linear_quantize_weight(@Const @ByRef Tensor input);




// Parsed from ATen/ops/fbgemm_pack_gemm_matrix_fp16.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_pack_gemm_matrix_fp16_ops.h>


// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_gemm_matrix_fp16(@Const @ByRef Tensor input);




// Parsed from ATen/ops/fbgemm_pack_quantized_matrix.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_pack_quantized_matrix_ops.h>


// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input);

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input, @Cast("int64_t") long K, @Cast("int64_t") long N);




// Parsed from ATen/ops/feature_alpha_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/feature_alpha_dropout_ops.h>


// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/feature_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/feature_dropout_ops.h>


// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/fft_fft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fft_ops.h>


// aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self);


// aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft_symint(@Const @ByRef Tensor self);


// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_fft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fft2_ops.h>


// aten::fft_fft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_fftfreq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftfreq_ops.h>


// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n);
// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);




// Parsed from ATen/ops/fft_fftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftn_ops.h>


// aten::fft_fftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_fftshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftshift_ops.h>


// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/fft_hfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfft_ops.h>


// aten::fft_hfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self);


// aten::fft_hfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft_symint(@Const @ByRef Tensor self);


// aten::fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_hfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_hfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfft2_ops.h>


// aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);


// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);





// Parsed from ATen/ops/fft_hfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfftn_ops.h>


// aten::fft_hfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);


// aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);





// Parsed from ATen/ops/fft_ifft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifft_ops.h>


// aten::fft_ifft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self);


// aten::fft_ifft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft_symint(@Const @ByRef Tensor self);


// aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_ifft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifft2_ops.h>


// aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_ifftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifftn_ops.h>


// aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_ifftshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifftshift_ops.h>


// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/fft_ihfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfft_ops.h>


// aten::fft_ihfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self);


// aten::fft_ihfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft_symint(@Const @ByRef Tensor self);


// aten::fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_ihfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfft2_ops.h>


// aten::fft_ihfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);


// aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);





// Parsed from ATen/ops/fft_ihfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfftn_ops.h>


// aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);


// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @Const @ByRef Tensor out);





// Parsed from ATen/ops/fft_irfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfft_ops.h>


// aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self);


// aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft_symint(@Const @ByRef Tensor self);


// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_irfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfft2_ops.h>


// aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_irfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfftn_ops.h>


// aten::fft_irfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_rfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfft_ops.h>


// aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self);


// aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft_symint(@Const @ByRef Tensor self);


// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntOptional n, @Cast("int64_t") long dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_rfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfft2_ops.h>


// aten::fft_rfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft2_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfft2_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRef dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fft_rfftfreq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfftfreq_ops.h>


// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n);
// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);




// Parsed from ATen/ops/fft_rfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfftn_ops.h>


// aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfftn_symint(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfftn_symint(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);


// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);


// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringViewOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringViewOptional norm, @ByRef Tensor out);





// Parsed from ATen/ops/fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fill_ops.h>


// aten::fill.Scalar(Tensor self, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal @Name("fill") Tensor _fill(@Const @ByRef Tensor self, @Const @ByRef Scalar value);

// aten::fill.Tensor(Tensor self, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal @Name("fill") Tensor _fill(@Const @ByRef Tensor self, @Const @ByRef Tensor value);

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Scalar value);

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Tensor value);

// aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar value);
// aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor value);
// aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/fill_diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fill_diagonal_ops.h>






// Parsed from ATen/ops/fix.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fix_ops.h>


// aten::fix(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fix(@Const @ByRef Tensor self);

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_(@ByRef Tensor self);

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/flatten.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flatten_ops.h>


// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim/*=0*/, @Cast("int64_t") long end_dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self);

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim, @Cast("int64_t") long end_dim, @ByVal Dimname out_dim);

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal Dimname start_dim, @ByVal Dimname end_dim, @ByVal Dimname out_dim);

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dims, @ByVal Dimname out_dim);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal DimnameVector dims, @ByVal Dimname out_dim);




// Parsed from ATen/ops/flatten_dense_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flatten_dense_tensors_ops.h>


// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor flatten_dense_tensors(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor flatten_dense_tensors(@ByVal TensorVector tensors);




// Parsed from ATen/ops/flip.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flip_ops.h>


// aten::flip(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor flip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor flip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);
// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor flip_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor flip_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/fliplr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fliplr_ops.h>


// aten::fliplr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fliplr(@Const @ByRef Tensor self);




// Parsed from ATen/ops/flipud.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flipud_ops.h>


// aten::flipud(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor flipud(@Const @ByRef Tensor self);




// Parsed from ATen/ops/float_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/float_power_ops.h>


// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);
// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);
// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);




// Parsed from ATen/ops/floor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/floor_ops.h>


// aten::floor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor(@Const @ByRef Tensor self);

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_(@ByRef Tensor self);

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/floor_divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/floor_divide_ops.h>


// aten::floor_divide(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/fmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmax_ops.h>


// aten::fmax(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmax(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/fmin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmin_ops.h>


// aten::fmin(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmin(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/fmod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmod_ops.h>


// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/frac.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frac_ops.h>


// aten::frac(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor frac(@Const @ByRef Tensor self);

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_(@ByRef Tensor self);

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/fractional_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool2d_ops.h>


// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples);
// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples);




// Parsed from ATen/ops/fractional_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool2d_backward_ops.h>


// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices);
// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/fractional_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool3d_ops.h>


// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples);
// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal T_TensorTensor_T fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor random_samples);




// Parsed from ATen/ops/fractional_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool3d_backward_ops.h>


// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices);
// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/frexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frexp_ops.h>


// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
@Namespace("at") public static native @ByVal T_TensorTensor_T frexp(@Const @ByRef Tensor self);

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal T_TensorTensor_T frexp_out(@ByRef Tensor mantissa, @ByRef Tensor exponent, @Const @ByRef Tensor self);
// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal T_TensorTensor_T frexp_outf(@Const @ByRef Tensor self, @ByRef Tensor mantissa, @ByRef Tensor exponent);




// Parsed from ATen/ops/frobenius_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frobenius_norm_ops.h>


// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/from_blob.h

// #pragma once
// #include <ATen/core/Tensor.h>

@Namespace("at::detail") public static native void noopDelete(Pointer arg0);


// Targeting ../TensorMaker.java



@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    PointerConsumer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    PointerConsumer deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    PointerConsumer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    @Cast("int64_t") long storage_offset,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    @Cast("int64_t") long storage_offset,
    PointerConsumer deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    PointerConsumer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    PointerConsumer deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... strides);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

  // namespace at


// Parsed from ATen/ops/from_file.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/from_file_ops.h>


// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@StringView BytePointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@StringView BytePointer filename);
@Namespace("at") public static native @ByVal Tensor from_file(@StringView String filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@StringView String filename);
// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@StringView BytePointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor from_file(@StringView String filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::from_file.out(str filename, bool? shared=None, int? size=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @StringView BytePointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size);
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @StringView BytePointer filename);
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @StringView String filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size);
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @StringView String filename);
// aten::from_file.out(str filename, bool? shared=None, int? size=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor from_file_outf(@StringView BytePointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor from_file_outf(@StringView String filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByRef Tensor out);




// Parsed from ATen/ops/full.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/full_ops.h>


// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_symint_outf(@ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::full.names_out(int[] size, Scalar fill_value, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
// aten::full.names_out(int[] size, Scalar fill_value, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/full_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/full_like_ops.h>


// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor full_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
// aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_like_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/fused_moving_avg_obs_fake_quant.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fused_moving_avg_obs_fake_quant_ops.h>


// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);




// Parsed from ATen/ops/gather.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gather_ops.h>


// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);




// Parsed from ATen/ops/gather_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gather_backward_ops.h>


// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad);




// Parsed from ATen/ops/gcd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gcd_ops.h>


// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gcd(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gcd(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_(@ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/ge.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ge_ops.h>


// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/gelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gelu_ops.h>


// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer approximate/*="none"*/);
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String approximate/*="none"*/);
// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_outf(@Const @ByRef Tensor self, @StringView BytePointer approximate, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor gelu_outf(@Const @ByRef Tensor self, @StringView String approximate, @ByRef Tensor out);

// aten::gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_(@ByRef Tensor self, @StringView BytePointer approximate/*="none"*/);
@Namespace("at") public static native @ByRef Tensor gelu_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor gelu_(@ByRef Tensor self, @StringView String approximate/*="none"*/);

// aten::gelu(Tensor self, *, str approximate='none') -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self, @StringView BytePointer approximate/*="none"*/);
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self, @StringView String approximate/*="none"*/);




// Parsed from ATen/ops/gelu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gelu_backward_ops.h>


// aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView BytePointer approximate/*="none"*/);
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView String approximate/*="none"*/);
// aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView BytePointer approximate, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor gelu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView String approximate, @ByRef Tensor grad_input);

// aten::gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView BytePointer approximate/*="none"*/);
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @StringView String approximate/*="none"*/);




// Parsed from ATen/ops/geometric.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/geometric_ops.h>


// aten::geometric.out(Tensor self, float p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor geometric_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor geometric_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p);
// aten::geometric.out(Tensor self, float p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor geometric_outf(@Const @ByRef Tensor self, double p, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::geometric(Tensor self, float p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor geometric(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor geometric(@Const @ByRef Tensor self, double p);




// Parsed from ATen/ops/geqrf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/geqrf_ops.h>


// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal T_TensorTensor_T geqrf_out(@ByRef Tensor a, @ByRef Tensor tau, @Const @ByRef Tensor self);
// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal T_TensorTensor_T geqrf_outf(@Const @ByRef Tensor self, @ByRef Tensor a, @ByRef Tensor tau);

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
@Namespace("at") public static native @ByVal T_TensorTensor_T geqrf(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ger.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ger_ops.h>


// aten::ger(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor ger(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);




// Parsed from ATen/ops/glu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_ops.h>


// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::glu(Tensor self, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self);




// Parsed from ATen/ops/glu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_backward_ops.h>


// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor grad_input);

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/glu_backward_jvp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_backward_jvp_ops.h>


// aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_backward_jvp(@Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);

// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_jvp_out(@ByRef Tensor out, @Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);
// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_jvp_outf(@Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/glu_jvp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_jvp_ops.h>


// aten::glu_jvp(Tensor glu, Tensor x, Tensor dx, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_jvp(@Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);

// aten::glu_jvp.out(Tensor glu, Tensor x, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_jvp_out(@ByRef Tensor out, @Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);
// aten::glu_jvp.out(Tensor glu, Tensor x, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_jvp_outf(@Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/gradient.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gradient_ops.h>


// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self);

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing);

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorVector spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorVector spacing);

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorVector spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorVector spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);




// Parsed from ATen/ops/greater.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/greater_ops.h>


// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/greater_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/greater_equal_ops.h>


// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/grid_sampler.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_ops.h>


// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);




// Parsed from ATen/ops/grid_sampler_2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_2d_ops.h>


// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_2d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_2d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_2d_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
// aten::grid_sampler_2d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_2d_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/grid_sampler_2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_2d_backward_ops.h>


// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::grid_sampler_2d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_2d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::grid_sampler_2d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/grid_sampler_3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_3d_ops.h>


// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_3d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_3d_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_3d_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/grid_sampler_3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_3d_backward_ops.h>


// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_3d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T grid_sampler_3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/group_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/group_norm_ops.h>


// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enabled/*=true*/);
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups);




// Parsed from ATen/ops/gru.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gru_ops.h>


// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T gru(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal T_TensorTensor_T gru(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T gru(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal T_TensorTensor_T gru(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/gru_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gru_cell_ops.h>


// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/gt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gt_ops.h>


// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/hamming_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hamming_window_ops.h>


// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length);
// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);

// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByRef Tensor out);

// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByRef Tensor out);




// Parsed from ATen/ops/hann_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hann_window_ops.h>


// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length);
// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/hardshrink.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardshrink_ops.h>


// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self);




// Parsed from ATen/ops/hardshrink_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardshrink_backward_ops.h>


// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);
// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);




// Parsed from ATen/ops/hardsigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardsigmoid_ops.h>


// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardsigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid(@Const @ByRef Tensor self);

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_(@ByRef Tensor self);




// Parsed from ATen/ops/hardsigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardsigmoid_backward_ops.h>


// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/hardswish.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardswish_ops.h>


// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardswish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish(@Const @ByRef Tensor self);

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_(@ByRef Tensor self);




// Parsed from ATen/ops/hardswish_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardswish_backward_ops.h>


// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/hardtanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardtanh_ops.h>


// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor out);

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self);

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self);




// Parsed from ATen/ops/hardtanh_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardtanh_backward_ops.h>


// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);
// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor grad_input);

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);




// Parsed from ATen/ops/heaviside.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/heaviside_ops.h>


// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor values);
// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor values, @ByRef Tensor out);

// aten::heaviside(Tensor self, Tensor values) -> Tensor
@Namespace("at") public static native @ByVal Tensor heaviside(@Const @ByRef Tensor self, @Const @ByRef Tensor values);




// Parsed from ATen/ops/hinge_embedding_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hinge_embedding_loss_ops.h>


// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, double margin/*=1.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/histc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histc_ops.h>


// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @Const @ByRef Scalar min, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self);




// Parsed from ATen/ops/histogram.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histogram_ops.h>


// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins);
// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins);

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T histogram(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);




// Parsed from ATen/ops/histogramdd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histogramdd_ops.h>


// aten::histogramdd(Tensor self, int[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal LongArrayRef bins);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... bins);

// aten::histogramdd.int_bins(Tensor self, int bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @Cast("int64_t") long bins);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);

// aten::histogramdd.TensorList_bins(Tensor self, Tensor[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal TensorVector bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double[] range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorVector_T histogramdd(@Const @ByRef Tensor self, @ByVal TensorVector bins);




// Parsed from ATen/ops/hsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hsplit_ops.h>


// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... indices);




// Parsed from ATen/ops/hspmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hspmm_ops.h>


// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_out(@ByRef Tensor out, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_outf(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByRef Tensor out);

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor hspmm(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/hstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hstack_ops.h>


// aten::hstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor hstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor hstack(@ByVal TensorVector tensors);

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor hstack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor hstack_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/huber_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/huber_loss_ops.h>


// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor out);

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/huber_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/huber_loss_backward_ops.h>


// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);
// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor grad_input);

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);




// Parsed from ATen/ops/hypot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hypot_ops.h>


// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::hypot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor hypot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/i0_ops.h>


// aten::i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor i0(@Const @ByRef Tensor self);

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_(@ByRef Tensor self);

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/igamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/igamma_ops.h>


// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igamma(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igamma(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/igammac.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/igammac_ops.h>


// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igammac(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igammac(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/im2col.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/im2col_ops.h>


// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);
// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef dilation, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);




// Parsed from ATen/ops/imag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/imag_ops.h>


// aten::imag(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor imag(@Const @ByRef Tensor self);




// Parsed from ATen/ops/index.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_ops.h>


// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor index(@Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices);

// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices);
// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @ByRef Tensor out);




// Parsed from ATen/ops/index_add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_add_ops.h>


// aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor index_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_add_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);




// Parsed from ATen/ops/index_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_copy_ops.h>


// aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByRef Tensor out);

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);




// Parsed from ATen/ops/index_fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_fill_ops.h>


// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);
// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);
// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/index_put.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_put_ops.h>


// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_put_(@ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByRef Tensor index_put_(@ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values);

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_put(@Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByVal Tensor index_put(@Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values);

// aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByRef Tensor index_put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values);
// aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_put_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptionalList indices, @Const @ByRef Tensor values, @Cast("bool") boolean accumulate, @ByRef Tensor out);




// Parsed from ATen/ops/index_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_reduce_ops.h>


// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView BytePointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView BytePointer reduce);
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView String reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView String reduce);
// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView BytePointer reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor index_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView String reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);

// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView BytePointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView BytePointer reduce);
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView String reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @StringView String reduce);




// Parsed from ATen/ops/index_select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_select_ops.h>


// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);




// Parsed from ATen/ops/index_select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_select_backward_ops.h>


// aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal LongArrayRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);


// aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntArrayRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);





// Parsed from ATen/ops/indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/indices_ops.h>






// Parsed from ATen/ops/indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/indices_copy_ops.h>


// aten::indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor indices_copy(@Const @ByRef Tensor self);

// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/infinitely_differentiable_gelu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/infinitely_differentiable_gelu_backward_ops.h>


// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor infinitely_differentiable_gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);




// Parsed from ATen/ops/inner.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/inner_ops.h>


// aten::inner(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor inner(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/instance_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/instance_norm_ops.h>


// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean use_input_stats, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);




// Parsed from ATen/ops/int_repr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/int_repr_ops.h>


// aten::int_repr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor int_repr(@Const @ByRef Tensor self);

// aten::int_repr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor int_repr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::int_repr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor int_repr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/inverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/inverse_ops.h>


// aten::inverse(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor inverse(@Const @ByRef Tensor self);

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/is_coalesced.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_coalesced_ops.h>






// Parsed from ATen/ops/is_complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_complex_ops.h>


// aten::is_complex(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_complex(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_conj_ops.h>


// aten::is_conj(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_distributed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_distributed_ops.h>


// aten::is_distributed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_distributed(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_floating_point.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_floating_point_ops.h>


// aten::is_floating_point(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_floating_point(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_inference.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_inference_ops.h>


// aten::is_inference(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_inference(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_leaf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_leaf_ops.h>






// Parsed from ATen/ops/is_neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_neg_ops.h>


// aten::is_neg(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_neg(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_nonzero_ops.h>


// aten::is_nonzero(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_nonzero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_pinned.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_pinned_ops.h>






// Parsed from ATen/ops/is_same_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_same_size_ops.h>


// aten::is_same_size(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_same_size(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/is_set_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_set_to_ops.h>






// Parsed from ATen/ops/is_signed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_signed_ops.h>


// aten::is_signed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_signed(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_vulkan_available.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_vulkan_available_ops.h>


// aten::is_vulkan_available() -> bool
@Namespace("at") public static native @Cast("bool") boolean is_vulkan_available();




// Parsed from ATen/ops/isclose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isclose_ops.h>


// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/isfinite.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isfinite_ops.h>


// aten::isfinite(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isfinite(@Const @ByRef Tensor self);




// Parsed from ATen/ops/isin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isin_ops.h>


// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);
// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);
// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);
// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);




// Parsed from ATen/ops/isinf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isinf_ops.h>


// aten::isinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isinf(@Const @ByRef Tensor self);

// aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isnan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isnan_ops.h>


// aten::isnan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isnan(@Const @ByRef Tensor self);

// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isnan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isnan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isneginf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isneginf_ops.h>


// aten::isneginf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isneginf(@Const @ByRef Tensor self);

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isposinf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isposinf_ops.h>


// aten::isposinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isposinf(@Const @ByRef Tensor self);

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isreal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isreal_ops.h>


// aten::isreal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isreal(@Const @ByRef Tensor self);




// Parsed from ATen/ops/istft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/istft_ops.h>


// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional length, @Cast("bool") boolean return_complex/*=false*/);
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);




// Parsed from ATen/ops/item.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/item_ops.h>






// Parsed from ATen/ops/kaiser_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kaiser_window_ops.h>


// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length);
// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::kaiser_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::kaiser_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::kaiser_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);

// aten::kaiser_window.beta_out(int window_length, bool periodic, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
// aten::kaiser_window.beta_out(int window_length, bool periodic, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByRef Tensor out);




// Parsed from ATen/ops/kl_div.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kl_div_ops.h>


// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/kron.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kron_ops.h>


// aten::kron(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor kron(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/kthvalue.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kthvalue_ops.h>


// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);
// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);
// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/l1_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/l1_loss_ops.h>


// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/layer_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/layer_norm_ops.h>


// aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... normalized_shape);


// aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape);





// Parsed from ATen/ops/lcm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lcm_ops.h>


// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lcm(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lcm(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_(@ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/ldexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ldexp_ops.h>


// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ldexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/le.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/le_ops.h>


// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/leaky_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/leaky_relu_ops.h>


// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @ByRef Tensor out);

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self);

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self);




// Parsed from ATen/ops/leaky_relu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/leaky_relu_backward_ops.h>


// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);
// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result, @ByRef Tensor grad_input);

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);




// Parsed from ATen/ops/lerp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lerp_ops.h>


// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);
// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight, @ByRef Tensor out);

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);
// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight, @ByRef Tensor out);

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/less.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/less_ops.h>


// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/less_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/less_equal_ops.h>


// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/lgamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lgamma_ops.h>


// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::lgamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lgamma(@Const @ByRef Tensor self);




// Parsed from ATen/ops/lift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_ops.h>


// aten::lift(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lift(@Const @ByRef Tensor self);

// aten::lift.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lift.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/lift_fresh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_fresh_ops.h>


// aten::lift_fresh(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor lift_fresh(@Const @ByRef Tensor self);




// Parsed from ATen/ops/lift_fresh_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_fresh_copy_ops.h>


// aten::lift_fresh_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lift_fresh_copy(@Const @ByRef Tensor self);

// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_fresh_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_fresh_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cholesky.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cholesky_ops.h>


// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self);

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cholesky_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cholesky_ex_ops.h>


// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_cholesky_ex(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_cholesky_ex(@Const @ByRef Tensor self);

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self);
// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_cholesky_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @Cast("bool") boolean check_errors, @ByRef Tensor L, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_cond.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cond_ops.h>


// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self);

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByRef Tensor out);

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @StringView BytePointer p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @StringView String p);

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String p);
// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @StringView BytePointer p, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @StringView String p, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cross.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cross_ops.h>


// aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor linalg_cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_det.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_det_ops.h>


// aten::linalg_det(Tensor A) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_det(@Const @ByRef Tensor A);

// aten::linalg_det.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_det.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_outf(@Const @ByRef Tensor A, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_diagonal_ops.h>


// aten::linalg_diagonal(Tensor(a) A, *, int offset=0, int dim1=-2, int dim2=-1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor linalg_diagonal(@Const @ByRef Tensor A, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_diagonal(@Const @ByRef Tensor A);




// Parsed from ATen/ops/linalg_eig.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eig_ops.h>


// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eig(@Const @ByRef Tensor self);

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eig_out(@ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors, @Const @ByRef Tensor self);
// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eig_outf(@Const @ByRef Tensor self, @ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors);




// Parsed from ATen/ops/linalg_eigh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigh_ops.h>


// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh(@Const @ByRef Tensor self, @StringView BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh(@Const @ByRef Tensor self, @StringView String UPLO/*="L"*/);

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @StringView BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @StringView String UPLO/*="L"*/);
// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh_outf(@Const @ByRef Tensor self, @StringView BytePointer UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_eigh_outf(@Const @ByRef Tensor self, @StringView String UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);




// Parsed from ATen/ops/linalg_eigvals.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigvals_ops.h>


// aten::linalg_eigvals(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvals(@Const @ByRef Tensor self);

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_eigvalsh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigvalsh_ops.h>


// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @StringView BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @StringView String UPLO/*="L"*/);

// aten::linalg_eigvalsh.out(Tensor self, str UPLO="L", *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String UPLO/*="L"*/);
// aten::linalg_eigvalsh.out(Tensor self, str UPLO="L", *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @StringView BytePointer UPLO, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @StringView String UPLO, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_householder_product.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_householder_product_ops.h>


// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_householder_product(@Const @ByRef Tensor input, @Const @ByRef Tensor tau);

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tau);
// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tau, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_inv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_inv_ops.h>


// aten::linalg_inv(Tensor A) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_inv(@Const @ByRef Tensor A);

// aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_outf(@Const @ByRef Tensor A, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_inv_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_inv_ex_ops.h>


// aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_inv_ex(@Const @ByRef Tensor A, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_inv_ex(@Const @ByRef Tensor A);

// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor A, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor A);
// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_inv_ex_outf(@Const @ByRef Tensor A, @Cast("bool") boolean check_errors, @ByRef Tensor inverse, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_ldl_factor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_factor_ops.h>


// aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_ldl_factor(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_ldl_factor(@Const @ByRef Tensor self);

// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_ldl_factor_out(@ByRef Tensor LD, @ByRef Tensor pivots, @Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_ldl_factor_out(@ByRef Tensor LD, @ByRef Tensor pivots, @Const @ByRef Tensor self);
// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_ldl_factor_outf(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian, @ByRef Tensor LD, @ByRef Tensor pivots);




// Parsed from ATen/ops/linalg_ldl_factor_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_factor_ex_ops.h>


// aten::linalg_ldl_factor_ex(Tensor self, *, bool hermitian=False, bool check_errors=False) -> (Tensor LD, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_ldl_factor_ex(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_ldl_factor_ex(@Const @ByRef Tensor self);

// aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_ldl_factor_ex_out(@ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_ldl_factor_ex_out(@ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor self);
// aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_ldl_factor_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian, @Cast("bool") boolean check_errors, @ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_ldl_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_solve_ops.h>


// aten::linalg_ldl_solve(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_ldl_solve(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_ldl_solve(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);

// aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);
// aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_outf(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_lstsq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lstsq_ops.h>


// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b);

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b);
// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T linalg_lstsq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal DoubleOptional rcond, @ByVal StringViewOptional driver, @ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values);




// Parsed from ATen/ops/linalg_lu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_ops.h>


// aten::linalg_lu(Tensor A, *, bool pivot=True) -> (Tensor P, Tensor L, Tensor U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu(@Const @ByRef Tensor A);

// aten::linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor A);
// aten::linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U);




// Parsed from ATen/ops/linalg_lu_factor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_factor_ops.h>


// aten::linalg_lu_factor(Tensor A, *, bool pivot=True) -> (Tensor LU, Tensor pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_lu_factor(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_lu_factor(@Const @ByRef Tensor A);

// aten::linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -> (Tensor(a!) LU, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_lu_factor_out(@ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_lu_factor_out(@ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A);
// aten::linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -> (Tensor(a!) LU, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_lu_factor_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @ByRef Tensor LU, @ByRef Tensor pivots);




// Parsed from ATen/ops/linalg_lu_factor_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_factor_ex_ops.h>


// aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_factor_ex(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_factor_ex(@Const @ByRef Tensor A);

// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_factor_ex_out(@ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_factor_ex_out(@ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A);
// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_lu_factor_ex_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @Cast("bool") boolean check_errors, @ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_lu_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_solve_ops.h>


// aten::linalg_lu_solve(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_lu_solve(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean adjoint/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_lu_solve(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);

// aten::linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean adjoint/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);
// aten::linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_outf(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left, @Cast("bool") boolean adjoint, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matmul_ops.h>


// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_exp_ops.h>


// aten::linalg_matrix_exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_exp(@Const @ByRef Tensor self);

// aten::linalg_matrix_exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_matrix_exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_norm_ops.h>


// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @StringView BytePointer ord/*="fro"*/, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @StringView String ord/*="fro"*/, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer ord/*="fro"*/, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String ord/*="fro"*/, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @StringView BytePointer ord, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @StringView String ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_power_ops.h>


// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);
// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_rank.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_rank_ops.h>


// aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input);

// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input);
// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional atol, @Const @ByRef TensorOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);

// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);
// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, double tol);

// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, double tol);
// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol);

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol);
// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_multi_dot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_multi_dot_ops.h>


// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_multi_dot(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor linalg_multi_dot(@ByVal TensorVector tensors);

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_norm_ops.h>


// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StringView BytePointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StringView BytePointer ord);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StringView String ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StringView String ord);

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView BytePointer ord);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StringView String ord);
// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @StringView BytePointer ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @StringView String ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_pinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_pinv_ops.h>


// aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self);

// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional atol, @Const @ByRef TensorOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);

// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);
// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond);

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond);

// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond);
// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond);
// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_qr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_qr_ops.h>


// aten::linalg_qr(Tensor A, str mode='reduced') -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr(@Const @ByRef Tensor A, @StringView BytePointer mode/*="reduced"*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr(@Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr(@Const @ByRef Tensor A, @StringView String mode/*="reduced"*/);

// aten::linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor A, @StringView BytePointer mode/*="reduced"*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor A, @StringView String mode/*="reduced"*/);
// aten::linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr_outf(@Const @ByRef Tensor A, @StringView BytePointer mode, @ByRef Tensor Q, @ByRef Tensor R);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_qr_outf(@Const @ByRef Tensor A, @StringView String mode, @ByRef Tensor Q, @ByRef Tensor R);




// Parsed from ATen/ops/linalg_slogdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_slogdet_ops.h>


// aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_slogdet(@Const @ByRef Tensor A);

// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor A);
// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_slogdet_outf(@Const @ByRef Tensor A, @ByRef Tensor sign, @ByRef Tensor logabsdet);




// Parsed from ATen/ops/linalg_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_ops.h>


// aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/);
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor A, @Const @ByRef Tensor B);

// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/);
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor A, @Const @ByRef Tensor B);
// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_outf(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_solve_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_ex_ops.h>


// aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B);

// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B);
// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
@Namespace("at") public static native @ByVal T_TensorTensor_T linalg_solve_ex_outf(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left, @Cast("bool") boolean check_errors, @ByRef Tensor result, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_solve_triangular.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_triangular_ops.h>


// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper);
// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left, @Cast("bool") boolean unitriangular, @ByRef Tensor out);

// aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_solve_triangular(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_solve_triangular(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper);




// Parsed from ATen/ops/linalg_svd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_svd_ops.h>


// aten::linalg_svd(Tensor A, bool full_matrices=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_svd(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_svd(@Const @ByRef Tensor A);

// aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A);
// aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linalg_svd_outf(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices, @ByVal StringViewOptional driver, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh);




// Parsed from ATen/ops/linalg_svdvals.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_svdvals_ops.h>


// aten::linalg_svdvals(Tensor A, *, str? driver=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_svdvals(@Const @ByRef Tensor A, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByVal Tensor linalg_svdvals(@Const @ByRef Tensor A);

// aten::linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_out(@ByRef Tensor out, @Const @ByRef Tensor A, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional driver);
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_outf(@Const @ByRef Tensor A, @ByVal StringViewOptional driver, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_tensorinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_tensorinv_ops.h>


// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self);

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_outf(@Const @ByRef Tensor self, @Cast("int64_t") long ind, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_tensorsolve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_tensorsolve_ops.h>


// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRefOptional dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_vander.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vander_ops.h>


// aten::linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N);
@Namespace("at") public static native @ByVal Tensor linalg_vander(@Const @ByRef Tensor x);


// aten::linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vander_symint(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional N);
@Namespace("at") public static native @ByVal Tensor linalg_vander_symint(@Const @ByRef Tensor x);





// Parsed from ATen/ops/linalg_vecdot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vecdot_ops.h>


// aten::linalg_vecdot(Tensor x, Tensor y, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vecdot(@Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_vecdot(@Const @ByRef Tensor x, @Const @ByRef Tensor y);

// aten::linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor y);
// aten::linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_vector_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vector_norm_ops.h>


// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linear_ops.h>


// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight);
// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByRef Tensor out);




// Parsed from ATen/ops/linear_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linear_backward_ops.h>


// aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linear_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T linear_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/linspace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linspace_ops.h>


// aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByRef Tensor out);




// Parsed from ATen/ops/log.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_ops.h>


// aten::log(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log(@Const @ByRef Tensor self);

// aten::log_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_(@ByRef Tensor self);

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log10.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log10_ops.h>


// aten::log10(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log10(@Const @ByRef Tensor self);

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_(@ByRef Tensor self);

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log1p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log1p_ops.h>


// aten::log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log1p(@Const @ByRef Tensor self);

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_(@ByRef Tensor self);

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log2_ops.h>


// aten::log2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log2(@Const @ByRef Tensor self);

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_(@ByRef Tensor self);

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log_normal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_normal_ops.h>


// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_normal_out(@ByRef Tensor out, @Const @ByRef Tensor self, double mean/*=1*/, double std/*=2*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor log_normal_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_normal_outf(@Const @ByRef Tensor self, double mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_normal(@Const @ByRef Tensor self, double mean/*=1*/, double std/*=2*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor log_normal(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_sigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_ops.h>


// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log_sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_sigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_backward_ops.h>


// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);
// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer, @ByRef Tensor grad_input);

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);




// Parsed from ATen/ops/log_sigmoid_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_forward_ops.h>


// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T log_sigmoid_forward_out(@ByRef Tensor output, @ByRef Tensor buffer, @Const @ByRef Tensor self);
// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T log_sigmoid_forward_outf(@Const @ByRef Tensor self, @ByRef Tensor output, @ByRef Tensor buffer);

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
@Namespace("at") public static native @ByVal T_TensorTensor_T log_sigmoid_forward(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_softmax_ops.h>


// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/logaddexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logaddexp_ops.h>


// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/logaddexp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logaddexp2_ops.h>


// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/logcumsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logcumsumexp_ops.h>


// aten::logcumsumexp(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/logdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logdet_ops.h>


// aten::logdet(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logdet(@Const @ByRef Tensor self);




// Parsed from ATen/ops/logical_and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_and_ops.h>


// aten::logical_and(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logical_not.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_not_ops.h>


// aten::logical_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_not(@Const @ByRef Tensor self);

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/logical_or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_or_ops.h>


// aten::logical_or(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logical_xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_xor_ops.h>


// aten::logical_xor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logit_ops.h>


// aten::logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self);

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self);

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);




// Parsed from ATen/ops/logit_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logit_backward_ops.h>


// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor grad_input);

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/logspace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logspace_ops.h>


// aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/);
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base, @ByRef Tensor out);




// Parsed from ATen/ops/logsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logsumexp_ops.h>


// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim);
// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/lshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lshift_ops.h>


// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/lstm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_ops.h>


// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lstm(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lstm(@Const @ByRef Tensor input, @ByVal TensorVector hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lstm(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lstm(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @ByVal TensorVector hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/lstm_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_cell_ops.h>


// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal T_TensorTensor_T lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);
@Namespace("at") public static native @ByVal T_TensorTensor_T lstm_cell(@Const @ByRef Tensor input, @ByVal TensorVector hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal T_TensorTensor_T lstm_cell(@Const @ByRef Tensor input, @ByVal TensorVector hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/lstm_mps_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_mps_backward_ops.h>


// aten::lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])
@Namespace("at") public static native @ByVal T_TensorTensorVectorTensorVector_T lstm_mps_backward(@Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal T_TensorTensorVectorTensorVector_T lstm_mps_backward(@Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorVector hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
@Namespace("at") public static native void lstm_mps_backward_out(@ByRef Tensor out0, @ByVal TensorArrayRef out1, @ByVal TensorArrayRef out2, @Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native void lstm_mps_backward_out(@ByRef Tensor out0, @ByVal TensorVector out1, @ByVal TensorVector out2, @Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorVector hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
// aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
@Namespace("at") public static native void lstm_mps_backward_outf(@Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @ByRef Tensor out0, @ByVal TensorArrayRef out1, @ByVal TensorArrayRef out2);
@Namespace("at") public static native void lstm_mps_backward_outf(@Const @ByRef TensorOptional grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorVector hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @ByRef Tensor out0, @ByVal TensorVector out1, @ByVal TensorVector out2);




// Parsed from ATen/ops/lt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lt_ops.h>


// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/lu_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lu_solve_ops.h>


// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @ByRef Tensor out);

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
@Namespace("at") public static native @ByVal Tensor lu_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);




// Parsed from ATen/ops/lu_unpack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lu_unpack_ops.h>


// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T lu_unpack_outf(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data, @Cast("bool") boolean unpack_pivots, @ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U);




// Parsed from ATen/ops/mH.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mH_ops.h>






// Parsed from ATen/ops/mT.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mT_ops.h>






// Parsed from ATen/ops/margin_ranking_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/margin_ranking_loss_ops.h>


// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);




// Parsed from ATen/ops/masked_fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_fill_ops.h>


// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value);

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);

// aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value);
// aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);
// aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/masked_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_scatter_ops.h>


// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);

// aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);
// aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source, @ByRef Tensor out);




// Parsed from ATen/ops/masked_select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_select_ops.h>


// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select(@Const @ByRef Tensor self, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/masked_select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_select_backward_ops.h>


// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matmul_ops.h>


// aten::matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/matmul_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matmul_backward_ops.h>


// aten::matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T matmul_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask);

// aten::matmul_backward.out(Tensor grad, Tensor self, Tensor other, bool[2] mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T matmul_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask);
// aten::matmul_backward.out(Tensor grad, Tensor self, Tensor other, bool[2] mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T matmul_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/matrix_H.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_H_ops.h>






// Parsed from ATen/ops/matrix_exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_exp_ops.h>


// aten::matrix_exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp(@Const @ByRef Tensor self);




// Parsed from ATen/ops/matrix_exp_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_exp_backward_ops.h>


// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad);




// Parsed from ATen/ops/matrix_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_power_ops.h>


// aten::matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);
// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);




// Parsed from ATen/ops/max.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_ops.h>


// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::max(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self);

// aten::max.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool1d_ops.h>


// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool1d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool1d_with_indices_ops.h>


// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_ops.h>


// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_backward_ops.h>


// aten::max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::max_pool2d_backward.out(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::max_pool2d_backward.out(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/max_pool2d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_with_indices_ops.h>


// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool2d_with_indices_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_with_indices_backward_ops.h>


// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_ops.h>


// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool3d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_with_indices_ops.h>


// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/max_pool3d_with_indices_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_with_indices_backward_ops.h>


// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/max_unpool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_unpool2d_ops.h>


// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out);


// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size);


// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByRef Tensor out);


// aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/max_unpool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_unpool3d_ops.h>


// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);





// Parsed from ATen/ops/maximum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/maximum_ops.h>


// aten::maximum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor maximum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mean_ops.h>


// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self);

// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim);
// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/median.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/median_ops.h>


// aten::median(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor median(@Const @ByRef Tensor self);

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T median(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T median(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T median_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::median.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor median_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::median.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor median_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/meshgrid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/meshgrid_ops.h>


// aten::meshgrid(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorVector tensors);

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors, @StringView BytePointer indexing);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorVector tensors, @StringView String indexing);




// Parsed from ATen/ops/min.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/min_ops.h>


// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T min(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T min(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T min_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::min(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self);

// aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::min.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/minimum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/minimum_ops.h>


// aten::minimum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor minimum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/miopen_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_batch_norm_ops.h>


// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::miopen_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
// aten::miopen_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/miopen_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_batch_norm_backward_ops.h>


// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);

// aten::miopen_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);
// aten::miopen_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T miopen_batch_norm_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/miopen_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_ops.h>


// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_convolution_add_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_add_relu_ops.h>


// aten::miopen_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);




// Parsed from ATen/ops/miopen_convolution_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_relu_ops.h>


// aten::miopen_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);




// Parsed from ATen/ops/miopen_convolution_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_transpose_ops.h>


// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_depthwise_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_depthwise_convolution_ops.h>


// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_rnn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_rnn_ops.h>


// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);

// aten::miopen_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);
// aten::miopen_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensor_T miopen_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);




// Parsed from ATen/ops/miopen_rnn_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_rnn_backward_ops.h>


// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorVector_T miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorVector_T miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);

// aten::miopen_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void miopen_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native void miopen_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorVector out3, @Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
// aten::miopen_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void miopen_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);
@Namespace("at") public static native void miopen_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorVector weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorVector out3);




// Parsed from ATen/ops/mish.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mish_ops.h>


// aten::mish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish(@Const @ByRef Tensor self);

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_(@ByRef Tensor self);

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/mish_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mish_backward_ops.h>


// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/mkldnn_adaptive_avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h>


// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);

// aten::mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);
// aten::mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h>


// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::mkldnn_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::mkldnn_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_convolution_ops.h>


// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);





// Parsed from ATen/ops/mkldnn_linear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_ops.h>


// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);

// aten::mkldnn_linear.out(Tensor self, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight);
// aten::mkldnn_linear.out(Tensor self, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_linear_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_ops.h>


// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mkldnn_linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mkldnn_linear_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mkldnn_linear_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/mkldnn_linear_backward_input.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_input_ops.h>


// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);

// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_out(@ByRef Tensor out, @ByVal LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_outf(@ByVal LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_linear_backward_weights.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_weights_ops.h>


// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T mkldnn_linear_backward_weights(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);

// aten::mkldnn_linear_backward_weights.out(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T mkldnn_linear_backward_weights_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);
// aten::mkldnn_linear_backward_weights.out(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T mkldnn_linear_backward_weights_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/mkldnn_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool2d_ops.h>


// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool2d_backward_ops.h>


// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::mkldnn_max_pool2d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::mkldnn_max_pool2d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool3d_ops.h>


// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::mkldnn_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::mkldnn_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool3d_backward_ops.h>


// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::mkldnn_max_pool3d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::mkldnn_max_pool3d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_reorder_conv2d_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_reorder_conv2d_weight_ops.h>


// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional input_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional input_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);
// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal LongArrayRefOptional input_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_reorder_conv3d_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_reorder_conv3d_weight_ops.h>


// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::mkldnn_reorder_conv3d_weight.out(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups/*=1*/);
// aten::mkldnn_reorder_conv3d_weight.out(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_rnn_layer.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_rnn_layer_ops.h>


// aten::mkldnn_rnn_layer(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);

// aten::mkldnn_rnn_layer.out(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
// aten::mkldnn_rnn_layer.out(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensor_T mkldnn_rnn_layer_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/mkldnn_rnn_layer_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_rnn_layer_backward_ops.h>


// aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);

// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6, @Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6, @Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6);
@Namespace("at") public static native @ByVal T_TensorTensorTensorTensorTensorTensorTensor_T mkldnn_rnn_layer_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6);




// Parsed from ATen/ops/mm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mm_ops.h>


// aten::mm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor mm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);




// Parsed from ATen/ops/mode.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mode_ops.h>


// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T mode(@Const @ByRef Tensor self);

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T mode(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T mode_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/moveaxis.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/moveaxis_ops.h>


// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal LongArrayRef source, @ByVal LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... destination);

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);




// Parsed from ATen/ops/movedim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/movedim_ops.h>


// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal LongArrayRef source, @ByVal LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... destination);

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);




// Parsed from ATen/ops/mps_convolution_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mps_convolution_backward_ops.h>


// aten::mps_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::mps_convolution_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::mps_convolution_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T mps_convolution_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/mps_convolution_transpose_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mps_convolution_transpose_backward_ops.h>


// aten::mps_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::mps_convolution_transpose_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::mps_convolution_transpose_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef stride, @ByVal LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal T_TensorTensor_T mps_convolution_transpose_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/mse_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mse_loss_ops.h>


// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/mse_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mse_loss_backward_ops.h>


// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/msort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/msort_ops.h>


// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::msort(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor msort(@Const @ByRef Tensor self);




// Parsed from ATen/ops/mul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mul_ops.h>


// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/multi_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multi_margin_loss_ops.h>


// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/multi_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multi_margin_loss_backward_ops.h>


// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);
// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);




// Parsed from ATen/ops/multilabel_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_ops.h>


// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/multilabel_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_backward_ops.h>


// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);
// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target, @ByRef Tensor grad_input);

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);




// Parsed from ATen/ops/multilabel_margin_loss_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_forward_ops.h>


// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T multilabel_margin_loss_forward_out(@ByRef Tensor output, @ByRef Tensor is_target, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T multilabel_margin_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor output, @ByRef Tensor is_target);

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
@Namespace("at") public static native @ByVal T_TensorTensor_T multilabel_margin_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/multinomial.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multinomial_ops.h>


// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples);
// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_outf(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples);




// Parsed from ATen/ops/multiply.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multiply_ops.h>


// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/mv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mv_ops.h>


// aten::mv(Tensor self, Tensor vec) -> Tensor
@Namespace("at") public static native @ByVal Tensor mv(@Const @ByRef Tensor self, @Const @ByRef Tensor vec);

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec);
// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec, @ByRef Tensor out);




// Parsed from ATen/ops/mvlgamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mvlgamma_ops.h>


// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);
// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);

// aten::mvlgamma(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor mvlgamma(@Const @ByRef Tensor self, @Cast("int64_t") long p);




// Parsed from ATen/ops/nan_to_num.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nan_to_num_ops.h>


// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self);

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self);

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional nan, @ByVal DoubleOptional posinf, @ByVal DoubleOptional neginf, @ByRef Tensor out);




// Parsed from ATen/ops/nanmean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanmean_ops.h>


// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/nanmedian.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanmedian_ops.h>


// aten::nanmedian(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmedian(@Const @ByRef Tensor self);

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T nanmedian_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmedian_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nanmedian.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmedian_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/nanquantile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanquantile_ops.h>


// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);
// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView BytePointer interpolation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView String interpolation, @ByRef Tensor out);

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);
// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView BytePointer interpolation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView String interpolation, @ByRef Tensor out);




// Parsed from ATen/ops/nansum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nansum_ops.h>


// aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/narrow.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/narrow_ops.h>


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @Cast("int64_t") long length);


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @ByVal SymInt length);





// Parsed from ATen/ops/narrow_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/narrow_copy_ops.h>


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
@Namespace("at") public static native @ByVal Tensor narrow_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
@Namespace("at") public static native @ByVal Tensor narrow_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length, @ByRef Tensor out);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length, @ByRef Tensor out);





// Parsed from ATen/ops/native_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_batch_norm_ops.h>


// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);
// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);




// Parsed from ATen/ops/native_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_batch_norm_backward_ops.h>


// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_batch_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/native_channel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_channel_shuffle_ops.h>


// aten::native_channel_shuffle(Tensor self, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);




// Parsed from ATen/ops/native_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_dropout_ops.h>


// aten::native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T native_dropout(@Const @ByRef Tensor input, double p, @ByVal BoolOptional train);

// aten::native_dropout.out(Tensor input, float p, bool? train, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T native_dropout_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, double p, @ByVal BoolOptional train);
// aten::native_dropout.out(Tensor input, float p, bool? train, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T native_dropout_outf(@Const @ByRef Tensor input, double p, @ByVal BoolOptional train, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/native_dropout_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_dropout_backward_ops.h>


// aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_dropout_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale);

// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_dropout_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale);
// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_dropout_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale, @ByRef Tensor out);




// Parsed from ATen/ops/native_group_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_group_norm_ops.h>


// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_symint(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_symint_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_group_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_group_norm_backward_ops.h>


// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward_symint(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_group_norm_backward_symint_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_layer_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_layer_norm_ops.h>


// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm(@Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_outf(@Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_outf(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_symint_outf(@Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_layer_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_layer_norm_backward_ops.h>


// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_symint(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T native_layer_norm_backward_symint_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_norm_ops.h>


// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self);

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);

// aten::native_norm.out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::native_norm.out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @ByRef Tensor out);

// aten::native_norm.ScalarOpt_dim_dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
// aten::native_norm.ScalarOpt_dim_dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/ne.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ne_ops.h>


// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/neg_ops.h>


// aten::neg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor neg(@Const @ByRef Tensor self);

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_(@ByRef Tensor self);

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/negative.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/negative_ops.h>


// aten::negative(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor negative(@Const @ByRef Tensor self);

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_(@ByRef Tensor self);

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/nested_to_padded_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nested_to_padded_tensor_ops.h>


// aten::nested_to_padded_tensor(Tensor self, float padding, int[]? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional output_size);
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding);
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/new_empty.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_empty_ops.h>





// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_empty_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_empty_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByRef Tensor out);





// Parsed from ATen/ops/new_empty_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_empty_strided_ops.h>





// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor new_empty_strided_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_empty_strided_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByRef Tensor out);





// Parsed from ATen/ops/new_full.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_full_ops.h>





// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor new_full_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_full_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);





// Parsed from ATen/ops/new_ones.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_ones_ops.h>





// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_ones_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_ones_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByRef Tensor out);





// Parsed from ATen/ops/new_zeros.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_zeros_ops.h>





// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_zeros_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_zeros_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByRef Tensor out);





// Parsed from ATen/ops/nextafter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nextafter_ops.h>


// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::nextafter(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor nextafter(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/nll_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_ops.h>


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor out);


// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nll_loss2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_ops.h>


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor out);


// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nll_loss2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_backward_ops.h>


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);





// Parsed from ATen/ops/nll_loss2d_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_forward_ops.h>


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward_symint_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss2d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);





// Parsed from ATen/ops/nll_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_backward_ops.h>


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);





// Parsed from ATen/ops/nll_loss_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_forward_ops.h>


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward_symint_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal T_TensorTensor_T nll_loss_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);





// Parsed from ATen/ops/nll_loss_nd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_nd_ops.h>


// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_nd_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss_nd_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nonzero_ops.h>


// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::nonzero(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nonzero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/nonzero_numpy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nonzero_numpy_ops.h>


// aten::nonzero_numpy(Tensor self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector nonzero_numpy(@Const @ByRef Tensor self);




// Parsed from ATen/ops/nonzero_static.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nonzero_static_ops.h>


// aten::nonzero_static.out(Tensor self, *, int size, int fill_value=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_static_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("int64_t") long fill_value/*=-1*/);
@Namespace("at") public static native @ByRef Tensor nonzero_static_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size);
// aten::nonzero_static.out(Tensor self, *, int size, int fill_value=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_static_outf(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("int64_t") long fill_value, @ByRef Tensor out);

// aten::nonzero_static(Tensor self, *, int size, int fill_value=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor nonzero_static(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("int64_t") long fill_value/*=-1*/);
@Namespace("at") public static native @ByVal Tensor nonzero_static(@Const @ByRef Tensor self, @Cast("int64_t") long size);




// Parsed from ATen/ops/norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/norm_ops.h>


// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype);

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self);

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);
// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim);

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, ScalarType dtype);
// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim);
// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype);
// aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype, @ByRef Tensor out);

// aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @ByRef Tensor out);




// Parsed from ATen/ops/norm_except_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/norm_except_dim_ops.h>


// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v, @Cast("int64_t") long pow/*=2*/, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v);




// Parsed from ATen/ops/normal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/normal_ops.h>


// aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_functional(@Const @ByRef Tensor self, double mean/*=0*/, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal_functional(@Const @ByRef Tensor self);

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean);

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std);

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std);

// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntArrayRef size);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
  


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_symint_out(@ByRef Tensor out, double mean, double std, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_symint_out(@ByRef Tensor out, double mean, double std, @ByVal SymIntArrayRef size);
  


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_symint_outf(double mean, double std, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor self, double mean/*=0*/, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor self, double mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/not_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/not_equal_ops.h>


// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/nuclear_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nuclear_norm_ops.h>


// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self);

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/numpy_T.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/numpy_T_ops.h>






// Parsed from ATen/ops/one_hot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/one_hot_ops.h>


// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self, @Cast("int64_t") long num_classes/*=-1*/);
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ones.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ones_ops.h>


// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntArrayRef size);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_symint_outf(@ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/ones_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ones_like_ops.h>


// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self);
// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor ones_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/or_ops.h>


// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/orgqr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/orgqr_ops.h>


// aten::orgqr(Tensor self, Tensor input2) -> Tensor
@Namespace("at") public static native @ByVal Tensor orgqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @ByRef Tensor out);




// Parsed from ATen/ops/ormqr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ormqr_ops.h>


// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);
// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left, @Cast("bool") boolean transpose, @ByRef Tensor out);

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);




// Parsed from ATen/ops/outer.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/outer_ops.h>


// aten::outer(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor outer(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);




// Parsed from ATen/ops/output_nr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/output_nr_ops.h>






// Parsed from ATen/ops/pad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pad_ops.h>


// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal LongArrayRef pad, @StringView BytePointer mode/*="constant"*/, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] pad, @StringView String mode/*="constant"*/, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... pad);


// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad, @StringView BytePointer mode/*="constant"*/, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad);
@Namespace("at") public static native @ByVal Tensor pad_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef pad, @StringView String mode/*="constant"*/, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);





// Parsed from ATen/ops/pad_sequence.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pad_sequence_ops.h>


// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences, @Cast("bool") boolean batch_first/*=false*/, double padding_value/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences);
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorVector sequences, @Cast("bool") boolean batch_first/*=false*/, double padding_value/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorVector sequences);




// Parsed from ATen/ops/pairwise_distance.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pairwise_distance_ops.h>


// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/pdist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pdist_ops.h>


// aten::pdist(Tensor self, float p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self);




// Parsed from ATen/ops/permute.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/permute_ops.h>


// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);




// Parsed from ATen/ops/permute_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/permute_copy_ops.h>


// aten::permute_copy(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor permute_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor permute_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor permute_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor permute_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);
// aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor permute_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor permute_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/pin_memory.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pin_memory_ops.h>






// Parsed from ATen/ops/pinverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pinverse_ops.h>


// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self, double rcond/*=1e-15*/);
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self);




// Parsed from ATen/ops/pixel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pixel_shuffle_ops.h>


// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);

// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_shuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);
// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_shuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor, @ByRef Tensor out);




// Parsed from ATen/ops/pixel_unshuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pixel_unshuffle_ops.h>


// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_unshuffle(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);

// aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_unshuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);
// aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_unshuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor, @ByRef Tensor out);




// Parsed from ATen/ops/poisson.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/poisson_ops.h>


// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self);

// aten::poisson.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor poisson_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor poisson_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::poisson.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor poisson_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/poisson_nll_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/poisson_nll_loss_ops.h>


// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, @Cast("bool") boolean log_input, @Cast("bool") boolean full, double eps, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/polar.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/polar_ops.h>


// aten::polar(Tensor abs, Tensor angle) -> Tensor
@Namespace("at") public static native @ByVal Tensor polar(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle);

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_out(@ByRef Tensor out, @Const @ByRef Tensor abs, @Const @ByRef Tensor angle);
// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_outf(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle, @ByRef Tensor out);




// Parsed from ATen/ops/polygamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/polygamma_ops.h>


// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);
// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);

// aten::polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);




// Parsed from ATen/ops/positive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/positive_ops.h>


// aten::positive(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor positive(@Const @ByRef Tensor self);




// Parsed from ATen/ops/pow.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pow_ops.h>


// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);
// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);
// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);




// Parsed from ATen/ops/prelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/prelu_ops.h>


// aten::prelu(Tensor self, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor prelu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/prod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/prod_ops.h>


// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self);

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/promote_types.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/promote_types_ops.h>


// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
@Namespace("at") public static native ScalarType promote_types(ScalarType type1, ScalarType type2);




// Parsed from ATen/ops/put.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/put_ops.h>


// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::put.out(Tensor self, Tensor index, Tensor source, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByRef Tensor put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::put.out(Tensor self, Tensor index, Tensor source, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor put_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate, @ByRef Tensor out);




// Parsed from ATen/ops/q_per_channel_axis.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_axis_ops.h>


// aten::q_per_channel_axis(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_per_channel_axis(@Const @ByRef Tensor self);




// Parsed from ATen/ops/q_per_channel_scales.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_scales_ops.h>


// aten::q_per_channel_scales(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_scales(@Const @ByRef Tensor self);

// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_scales_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_scales_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/q_per_channel_zero_points.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_zero_points_ops.h>


// aten::q_per_channel_zero_points(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_zero_points(@Const @ByRef Tensor self);

// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_zero_points_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_zero_points_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/q_scale.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_scale_ops.h>


// aten::q_scale(Tensor self) -> float
@Namespace("at") public static native double q_scale(@Const @ByRef Tensor self);




// Parsed from ATen/ops/q_zero_point.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_zero_point_ops.h>


// aten::q_zero_point(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_zero_point(@Const @ByRef Tensor self);




// Parsed from ATen/ops/qr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/qr_ops.h>


// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal T_TensorTensor_T qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);
// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal T_TensorTensor_T qr_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @ByRef Tensor Q, @ByRef Tensor R);

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal T_TensorTensor_T qr(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T qr(@Const @ByRef Tensor self);




// Parsed from ATen/ops/qscheme.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/qscheme_ops.h>






// Parsed from ATen/ops/quantile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantile_ops.h>


// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);
// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView BytePointer interpolation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView String interpolation, @ByRef Tensor out);

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView BytePointer interpolation/*="linear"*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @StringView String interpolation/*="linear"*/);
// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView BytePointer interpolation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @StringView String interpolation, @ByRef Tensor out);




// Parsed from ATen/ops/quantize_per_channel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_channel_ops.h>


// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_channel(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);

// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_channel_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);
// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_channel_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype, @ByRef Tensor out);




// Parsed from ATen/ops/quantize_per_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_tensor_ops.h>


// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector quantize_per_tensor(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector quantize_per_tensor(@ByVal TensorVector tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);

// aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);
// aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_outf(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype, @ByRef Tensor out);

// aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype);
// aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype, @ByRef Tensor out);

// aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void quantize_per_tensor_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);
@Namespace("at") public static native void quantize_per_tensor_out(@ByVal TensorVector out, @ByVal TensorVector tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);
// aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void quantize_per_tensor_outf(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype, @ByVal TensorArrayRef out);
@Namespace("at") public static native void quantize_per_tensor_outf(@ByVal TensorVector tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype, @ByVal TensorVector out);




// Parsed from ATen/ops/quantize_per_tensor_dynamic.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_tensor_dynamic_ops.h>


// aten::quantize_per_tensor_dynamic(Tensor self, ScalarType dtype, bool reduce_range) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor_dynamic(@Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range);

// aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_dynamic_out(@ByRef Tensor out, @Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range);
// aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_dynamic_outf(@Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_batch_norm_ops.h>


// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);

// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_batch_norm_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);
// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_gru_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_gru_cell_ops.h>


// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_lstm_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_lstm_cell_ops.h>


// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T quantized_lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);
@Namespace("at") public static native @ByVal T_TensorTensor_T quantized_lstm_cell(@Const @ByRef Tensor input, @ByVal TensorVector hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_max_pool1d_ops.h>


// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::quantized_max_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::quantized_max_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_max_pool2d_ops.h>


// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_max_pool3d_ops.h>


// aten::quantized_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool3d(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);

// aten::quantized_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::quantized_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef kernel_size, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_rnn_relu_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_rnn_relu_cell_ops.h>


// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_rnn_tanh_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_rnn_tanh_cell_ops.h>


// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/rad2deg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rad2deg_ops.h>


// aten::rad2deg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rad2deg(@Const @ByRef Tensor self);

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_(@ByRef Tensor self);

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/rand.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rand_ops.h>


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal DimnameListOptional names);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);





// Parsed from ATen/ops/rand_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rand_like_ops.h>


// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self);
// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor rand_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/randint.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randint_ops.h>


// aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size);


// aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size);


// aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @ByVal SymInt high, @ByVal SymIntArrayRef size);


// aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size);


// aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@ByVal SymInt low, @ByVal SymInt high, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);





// Parsed from ATen/ops/randint_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randint_like_ops.h>


// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);


// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt high);


// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);


// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high);


// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like_symint(@Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long high);


// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_outf(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymInt high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymInt high);


// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);


// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_outf(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high);


// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt low, @ByVal SymInt high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);





// Parsed from ATen/ops/randn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randn_ops.h>


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal DimnameListOptional names);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);





// Parsed from ATen/ops/randn_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randn_like_ops.h>


// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self);
// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randn_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/random.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/random_ops.h>


// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to);
// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to);

// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long to);
// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long to);

// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random(Tensor self, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self);




// Parsed from ATen/ops/randperm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randperm_ops.h>


// aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n);


// aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n);


// aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);


// aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n, @ByVal GeneratorOptional generator);


// aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm_symint(@ByVal SymInt n, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n);


// aten::randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByRef Tensor out);


// aten::randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_symint_out(@ByRef Tensor out, @ByVal SymInt n);


// aten::randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_symint_outf(@ByVal SymInt n, @ByRef Tensor out);


// aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n, @ByVal GeneratorOptional generator);


// aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_symint_out(@ByRef Tensor out, @ByVal SymInt n, @ByVal GeneratorOptional generator);


// aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_symint_outf(@ByVal SymInt n, @ByVal GeneratorOptional generator, @ByRef Tensor out);





// Parsed from ATen/ops/range.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/range_ops.h>


// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);
// aten::range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByRef Tensor out);

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);




// Parsed from ATen/ops/ravel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ravel_ops.h>


// aten::ravel(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor ravel(@Const @ByRef Tensor self);




// Parsed from ATen/ops/real.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/real_ops.h>


// aten::real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor real(@Const @ByRef Tensor self);




// Parsed from ATen/ops/reciprocal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reciprocal_ops.h>


// aten::reciprocal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor reciprocal(@Const @ByRef Tensor self);

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_(@ByRef Tensor self);

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/record_stream.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/record_stream_ops.h>






// Parsed from ATen/ops/refine_names.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/refine_names_ops.h>






// Parsed from ATen/ops/reflection_pad1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad1d_ops.h>


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/reflection_pad1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad1d_backward_ops.h>


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/reflection_pad2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad2d_ops.h>


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/reflection_pad2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad2d_backward_ops.h>


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/reflection_pad3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad3d_ops.h>


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/reflection_pad3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad3d_backward_ops.h>


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/relu_ops.h>


// aten::relu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu(@Const @ByRef Tensor self);

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_(@ByRef Tensor self);

// aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/relu6.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/relu6_ops.h>


// aten::relu6(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu6(@Const @ByRef Tensor self);

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu6_(@ByRef Tensor self);




// Parsed from ATen/ops/remainder.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/remainder_ops.h>


// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/rename.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rename_ops.h>






// Parsed from ATen/ops/renorm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/renorm_ops.h>


// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);
// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm, @ByRef Tensor out);

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
@Namespace("at") public static native @ByVal Tensor renorm(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);




// Parsed from ATen/ops/repeat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/repeat_ops.h>



// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef repeats);
@Namespace("at") public static native @ByRef Tensor repeat_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... repeats);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef repeats, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor repeat_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] repeats, @ByRef Tensor out);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef repeats);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef repeats, @ByRef Tensor out);





// Parsed from ATen/ops/repeat_interleave.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/repeat_interleave_ops.h>


// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats);


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave_symint(@Const @ByRef Tensor self, @ByVal SymInt repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave_symint(@Const @ByRef Tensor self, @ByVal SymInt repeats);


// aten::repeat_interleave.Tensor_out(Tensor repeats, *, int? output_size=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_interleave_out(@ByRef Tensor out, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByRef Tensor repeat_interleave_out(@ByRef Tensor out, @Const @ByRef Tensor repeats);
// aten::repeat_interleave.Tensor_out(Tensor repeats, *, int? output_size=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_interleave_outf(@Const @ByRef Tensor repeats, @ByVal LongOptional output_size, @ByRef Tensor out);




// Parsed from ATen/ops/replication_pad1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad1d_ops.h>


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/replication_pad1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad1d_backward_ops.h>


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/replication_pad2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad2d_ops.h>


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/replication_pad2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad2d_backward_ops.h>


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/replication_pad3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad3d_ops.h>


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/replication_pad3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad3d_backward_ops.h>


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding, @ByRef Tensor grad_input);


// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/requires_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/requires_grad_ops.h>






// Parsed from ATen/ops/reshape.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reshape_ops.h>


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... shape);


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor reshape_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shape);





// Parsed from ATen/ops/reshape_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reshape_as_ops.h>






// Parsed from ATen/ops/resize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_ops.h>



// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor resize_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);


// aten::resize(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::resize(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size);





// Parsed from ATen/ops/resize_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_as_ops.h>


// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_as_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template);
// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);

// aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_as(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize_as(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);




// Parsed from ATen/ops/resize_as_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_as_sparse_ops.h>


// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template);
// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @Const @ByRef Tensor out);

// aten::resize_as_sparse(Tensor self, Tensor the_template) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_as_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);




// Parsed from ATen/ops/resolve_conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resolve_conj_ops.h>


// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/resolve_neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resolve_neg_ops.h>


// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_neg(@Const @ByRef Tensor self);




// Parsed from ATen/ops/result_type.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/result_type_ops.h>


// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Tensor other);

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Scalar other);

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar, @Const @ByRef Tensor tensor);

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar1, @Const @ByRef Scalar scalar2);




// Parsed from ATen/ops/retain_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/retain_grad_ops.h>






// Parsed from ATen/ops/retains_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/retains_grad_ops.h>






// Parsed from ATen/ops/rnn_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_relu_ops.h>


// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_relu(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_relu(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_relu(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_relu(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/rnn_relu_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_relu_cell_ops.h>


// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/rnn_tanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_tanh_ops.h>


// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_tanh(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_tanh(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_tanh(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal T_TensorTensor_T rnn_tanh(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorVector params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/rnn_tanh_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_tanh_cell_ops.h>


// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/roll.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/roll_ops.h>


// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal LongArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal LongArrayRef shifts);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... shifts);


// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor roll_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor roll_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts);
@Namespace("at") public static native @ByVal Tensor roll_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);


// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef shifts);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... shifts);


// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef shifts, @ByVal LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor roll_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shifts, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims, @ByRef Tensor out);


// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor roll_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts);
@Namespace("at") public static native @ByRef Tensor roll_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);


// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor roll_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef shifts, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims, @ByRef Tensor out);





// Parsed from ATen/ops/rot90.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rot90_ops.h>


// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);

// aten::rot90.out(Tensor self, int k=1, int[] dims=[0,1], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);
// aten::rot90.out(Tensor self, int k=1, int[] dims=[0,1], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rot90_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rot90_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/round.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/round_ops.h>


// aten::round(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self);

// aten::round_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self);

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::round.decimals(Tensor self, *, int decimals) -> Tensor
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self, @Cast("int64_t") long decimals);

// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self, @Cast("int64_t") long decimals);

// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long decimals);
// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @Cast("int64_t") long decimals, @ByRef Tensor out);




// Parsed from ATen/ops/row_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_indices_ops.h>






// Parsed from ATen/ops/row_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_indices_copy_ops.h>


// aten::row_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor row_indices_copy(@Const @ByRef Tensor self);

// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/row_stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_stack_ops.h>


// aten::row_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor row_stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor row_stack(@ByVal TensorVector tensors);

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor row_stack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor row_stack_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/rrelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_ops.h>


// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self);

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self);




// Parsed from ATen/ops/rrelu_with_noise.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_with_noise_ops.h>


// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise);
// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise);

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise);




// Parsed from ATen/ops/rrelu_with_noise_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_with_noise_backward_ops.h>


// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);

// aten::rrelu_with_noise_backward.out(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);
// aten::rrelu_with_noise_backward.out(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result, @ByRef Tensor out);




// Parsed from ATen/ops/rshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rshift_ops.h>


// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/rsqrt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rsqrt_ops.h>


// aten::rsqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsqrt(@Const @ByRef Tensor self);

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_(@ByRef Tensor self);

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/rsub.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rsub_ops.h>


// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/scalar_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scalar_tensor_ops.h>


// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s);
// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scalar_tensor_out(@ByRef Tensor out, @Const @ByRef Scalar s);
// aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scalar_tensor_outf(@Const @ByRef Scalar s, @ByRef Tensor out);




// Parsed from ATen/ops/scaled_dot_product_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scaled_dot_product_attention_ops.h>


// aten::scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional attn_mask, double dropout_p/*=0.0*/, @Cast("bool") boolean is_causal/*=false*/, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scale);
@Namespace("at") public static native @ByVal Tensor scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_ops.h>


// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);
// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce);

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce);
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce);
// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce, @ByRef Tensor out);

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView BytePointer reduce);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView String reduce);

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView BytePointer reduce);
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView String reduce);
// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView BytePointer reduce, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @StringView String reduce, @ByRef Tensor out);

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);




// Parsed from ATen/ops/scatter_add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_add_ops.h>


// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);




// Parsed from ATen/ops/scatter_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_reduce_ops.h>


// aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce);
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce);

// aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce);
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce);
// aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView BytePointer reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor scatter_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @StringView String reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);




// Parsed from ATen/ops/searchsorted.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/searchsorted_ops.h>


// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);
// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByVal StringViewOptional side, @Const @ByRef TensorOptional sorter, @ByRef Tensor out);

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self);

// aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self);
// aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByVal StringViewOptional side, @Const @ByRef TensorOptional sorter, @ByRef Tensor out);




// Parsed from ATen/ops/segment_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/segment_reduce_ops.h>


// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @StringView BytePointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @StringView BytePointer reduce);
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @StringView String reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @StringView String reduce);

// aten::segment_reduce.out(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @StringView BytePointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @StringView BytePointer reduce);
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @StringView String reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @StringView String reduce);
// aten::segment_reduce.out(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor segment_reduce_outf(@Const @ByRef Tensor data, @StringView BytePointer reduce, @Const @ByRef TensorOptional lengths, @Const @ByRef TensorOptional indices, @Const @ByRef TensorOptional offsets, @Cast("int64_t") long axis, @Cast("bool") boolean unsafe, @Const @ByRef ScalarOptional initial, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor segment_reduce_outf(@Const @ByRef Tensor data, @StringView String reduce, @Const @ByRef TensorOptional lengths, @Const @ByRef TensorOptional indices, @Const @ByRef TensorOptional offsets, @Cast("int64_t") long axis, @Cast("bool") boolean unsafe, @Const @ByRef ScalarOptional initial, @ByRef Tensor out);




// Parsed from ATen/ops/select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_ops.h>


// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("int64_t") long index);

// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);





// Parsed from ATen/ops/select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_backward_ops.h>


// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByRef Tensor select_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor select_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/select_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_copy_ops.h>


// aten::select_copy.int(Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_copy.int(Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/select_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_scatter_ops.h>


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/selu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/selu_ops.h>


// aten::selu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor selu(@Const @ByRef Tensor self);

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor selu_(@ByRef Tensor self);




// Parsed from ATen/ops/set.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/set_ops.h>





// aten::set.source_Storage_out(Tensor self, Storage source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source);
// aten::set.source_Storage_out(Tensor self, Storage source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByRef Tensor out);

// aten::set.source_Storage(Tensor self, Storage source) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source);

// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal LongArrayRef size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByRef Tensor out);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::SymIntArrayRef{}") SymIntArrayRef stride);
@Namespace("at") public static native @ByRef Tensor set_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntArrayRef size);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_symint_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntArrayRef size, @ByVal SymIntArrayRef stride, @ByRef Tensor out);


// aten::set.source_Storage_storage_offset(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal LongArrayRef size, @ByVal(nullValue = "at::IntArrayRef{}") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::set.source_Storage_storage_offset(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor set_symint(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntArrayRef size, @ByVal(nullValue = "c10::SymIntArrayRef{}") SymIntArrayRef stride);
@Namespace("at") public static native @ByVal Tensor set_symint(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntArrayRef size);


// aten::set.source_Tensor_out(Tensor self, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor source);
// aten::set.source_Tensor_out(Tensor self, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor source, @ByRef Tensor out);

// aten::set.source_Tensor(Tensor self, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Const @ByRef Tensor source);

// aten::set.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::set.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::set(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self);




// Parsed from ATen/ops/set_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/set_data_ops.h>






// Parsed from ATen/ops/sgn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sgn_ops.h>


// aten::sgn(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sgn(@Const @ByRef Tensor self);

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sigmoid_ops.h>


// aten::sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid(@Const @ByRef Tensor self);

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_(@ByRef Tensor self);

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sigmoid_backward_ops.h>


// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);




// Parsed from ATen/ops/sign.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sign_ops.h>


// aten::sign(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sign(@Const @ByRef Tensor self);

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/signbit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/signbit_ops.h>


// aten::signbit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor signbit(@Const @ByRef Tensor self);

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/silu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/silu_ops.h>


// aten::silu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu(@Const @ByRef Tensor self);

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_(@ByRef Tensor self);

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/silu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/silu_backward_ops.h>


// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/sin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sin_ops.h>


// aten::sin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sin(@Const @ByRef Tensor self);

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_(@ByRef Tensor self);

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sinc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sinc_ops.h>


// aten::sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinc(@Const @ByRef Tensor self);

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_(@ByRef Tensor self);

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sinh_ops.h>


// aten::sinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinh(@Const @ByRef Tensor self);

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_(@ByRef Tensor self);

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/size_ops.h>


// aten::size.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_size(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::size.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/slice.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_ops.h>


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self);


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor slice_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_symint(@Const @ByRef Tensor self);





// Parsed from ATen/ops/slice_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_backward_ops.h>


// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);


// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByRef Tensor slice_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slice_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slice_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_copy_ops.h>


// aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice_copy(@Const @ByRef Tensor self);


// aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_copy_symint(@Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByRef Tensor slice_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal LongOptional start, @ByVal LongOptional end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymIntOptional start, @ByVal SymIntOptional end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slice_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_scatter_ops.h>


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByRef Tensor slice_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal LongOptional start, @ByVal LongOptional end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymIntOptional start, @ByVal SymIntOptional end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slogdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slogdet_ops.h>


// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T slogdet(@Const @ByRef Tensor self);

// aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor self);
// aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal T_TensorTensor_T slogdet_outf(@Const @ByRef Tensor self, @ByRef Tensor sign, @ByRef Tensor logabsdet);




// Parsed from ATen/ops/slow_conv3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv3d_ops.h>


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByRef Tensor out);


// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);





// Parsed from ATen/ops/slow_conv3d_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv3d_forward_ops.h>


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor output);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByRef Tensor output);


// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);


// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding);





// Parsed from ATen/ops/slow_conv_dilated2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_dilated2d_ops.h>


// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/slow_conv_dilated3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_dilated3d_ops.h>


// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/slow_conv_transpose2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_transpose2d_ops.h>


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);





// Parsed from ATen/ops/slow_conv_transpose3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_transpose3d_ops.h>


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByVal LongArrayRef output_padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal SymIntArrayRef padding, @ByVal SymIntArrayRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);


// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);





// Parsed from ATen/ops/smm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smm_ops.h>


// aten::smm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor smm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/smooth_l1_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smooth_l1_loss_ops.h>


// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor out);

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/smooth_l1_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smooth_l1_loss_backward_ops.h>


// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);
// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor grad_input);

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);




// Parsed from ATen/ops/soft_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/soft_margin_loss_ops.h>


// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/soft_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/soft_margin_loss_backward_ops.h>


// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softmax_ops.h>


// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/softplus.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softplus_ops.h>


// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @ByRef Tensor out);

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self);




// Parsed from ATen/ops/softplus_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softplus_backward_ops.h>


// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold);
// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @ByRef Tensor grad_input);

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold);




// Parsed from ATen/ops/softshrink.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softshrink_ops.h>


// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self);




// Parsed from ATen/ops/softshrink_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softshrink_backward_ops.h>


// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);
// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);




// Parsed from ATen/ops/sort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sort_ops.h>


// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable);
// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self);

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable);

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);
// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);




// Parsed from ATen/ops/sparse_bsc_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_bsc_tensor_ops.h>


// aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_bsr_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_bsr_tensor_ops.h>


// aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_compressed_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_compressed_tensor_ops.h>


// aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_coo_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_coo_tensor_ops.h>


// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);
// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal BoolOptional is_coalesced);

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal BoolOptional is_coalesced);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal BoolOptional is_coalesced);

// aten::sparse_coo_tensor.size_out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
// aten::sparse_coo_tensor.size_out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_outf(@ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);




// Parsed from ATen/ops/sparse_csc_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_csc_tensor_ops.h>


// aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_csr_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_csr_tensor_ops.h>


// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_dim_ops.h>






// Parsed from ATen/ops/sparse_mask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_mask_ops.h>


// aten::sparse_mask.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_mask_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
// aten::sparse_mask.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_mask_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);




// Parsed from ATen/ops/sparse_resize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_resize_ops.h>


// aten::sparse_resize.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
// aten::sparse_resize.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);

// aten::sparse_resize(Tensor self, int[] size, int sparse_dim, int dense_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_resize(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @ByVal Tensor sparse_resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);




// Parsed from ATen/ops/sparse_resize_and_clear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_resize_and_clear_ops.h>


// aten::sparse_resize_and_clear.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
// aten::sparse_resize_and_clear.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);

// aten::sparse_resize_and_clear(Tensor self, int[] size, int sparse_dim, int dense_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_resize_and_clear(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @ByVal Tensor sparse_resize_and_clear(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);




// Parsed from ATen/ops/sparse_sampled_addmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_sampled_addmm_ops.h>


// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_sampled_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sparse_sampled_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/special_airy_ai.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_airy_ai_ops.h>


// aten::special_airy_ai(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_airy_ai(@Const @ByRef Tensor x);

// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_airy_ai_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_airy_ai_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_j0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_j0_ops.h>


// aten::special_bessel_j0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_j0(@Const @ByRef Tensor self);

// aten::special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_j1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_j1_ops.h>


// aten::special_bessel_j1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_j1(@Const @ByRef Tensor self);

// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_y0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_y0_ops.h>


// aten::special_bessel_y0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_y0(@Const @ByRef Tensor self);

// aten::special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_y1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_y1_ops.h>


// aten::special_bessel_y1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_y1(@Const @ByRef Tensor self);

// aten::special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_t_ops.h>


// aten::special_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_u.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_u_ops.h>


// aten::special_chebyshev_polynomial_u(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_v.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_v_ops.h>


// aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_w.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_w_ops.h>


// aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_digamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_digamma_ops.h>


// aten::special_digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_digamma(@Const @ByRef Tensor self);

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_entr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_entr_ops.h>


// aten::special_entr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_entr(@Const @ByRef Tensor self);

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erf_ops.h>


// aten::special_erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erf(@Const @ByRef Tensor self);

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfc_ops.h>


// aten::special_erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfc(@Const @ByRef Tensor self);

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfcx.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfcx_ops.h>


// aten::special_erfcx(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfcx(@Const @ByRef Tensor self);

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfinv_ops.h>


// aten::special_erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfinv(@Const @ByRef Tensor self);

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_exp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_exp2_ops.h>


// aten::special_exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_exp2(@Const @ByRef Tensor self);

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_expit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_expit_ops.h>


// aten::special_expit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expit(@Const @ByRef Tensor self);

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_expm1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_expm1_ops.h>


// aten::special_expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expm1(@Const @ByRef Tensor self);

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_gammainc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammainc_ops.h>


// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammainc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/special_gammaincc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammaincc_ops.h>


// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaincc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/special_gammaln.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammaln_ops.h>


// aten::special_gammaln(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaln(@Const @ByRef Tensor self);

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_hermite_polynomial_h.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_hermite_polynomial_h_ops.h>


// aten::special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_hermite_polynomial_he.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_hermite_polynomial_he_ops.h>


// aten::special_hermite_polynomial_he(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_he.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_he.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i0_ops.h>


// aten::special_i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0(@Const @ByRef Tensor self);

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i0e.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i0e_ops.h>


// aten::special_i0e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0e(@Const @ByRef Tensor self);

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i1_ops.h>


// aten::special_i1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1(@Const @ByRef Tensor self);

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i1e.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i1e_ops.h>


// aten::special_i1e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1e(@Const @ByRef Tensor self);

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_laguerre_polynomial_l.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_laguerre_polynomial_l_ops.h>


// aten::special_laguerre_polynomial_l(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_laguerre_polynomial_l.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_laguerre_polynomial_l.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_legendre_polynomial_p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_legendre_polynomial_p_ops.h>


// aten::special_legendre_polynomial_p(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_legendre_polynomial_p.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_legendre_polynomial_p.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_log1p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log1p_ops.h>


// aten::special_log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log1p(@Const @ByRef Tensor self);

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_log_ndtr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log_ndtr_ops.h>


// aten::special_log_ndtr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log_ndtr(@Const @ByRef Tensor self);

// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log_ndtr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log_ndtr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log_softmax_ops.h>


// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/special_logit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_logit_ops.h>


// aten::special_logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self);

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);




// Parsed from ATen/ops/special_logsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_logsumexp_ops.h>


// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_i0_ops.h>


// aten::special_modified_bessel_i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_i0(@Const @ByRef Tensor self);

// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_i1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_i1_ops.h>


// aten::special_modified_bessel_i1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_i1(@Const @ByRef Tensor self);

// aten::special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_k0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_k0_ops.h>


// aten::special_modified_bessel_k0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_k0(@Const @ByRef Tensor self);

// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_k1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_k1_ops.h>


// aten::special_modified_bessel_k1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_k1(@Const @ByRef Tensor self);

// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_multigammaln.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_multigammaln_ops.h>


// aten::special_multigammaln(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_multigammaln(@Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);
// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);




// Parsed from ATen/ops/special_ndtr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_ndtr_ops.h>


// aten::special_ndtr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtr(@Const @ByRef Tensor self);

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_ndtri.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_ndtri_ops.h>


// aten::special_ndtri(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtri(@Const @ByRef Tensor self);

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_polygamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_polygamma_ops.h>


// aten::special_polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);
// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_psi.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_psi_ops.h>


// aten::special_psi(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_psi(@Const @ByRef Tensor self);

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_round.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_round_ops.h>


// aten::special_round(Tensor self, *, int decimals=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_round(@Const @ByRef Tensor self, @Cast("int64_t") long decimals/*=0*/);
@Namespace("at") public static native @ByVal Tensor special_round(@Const @ByRef Tensor self);

// aten::special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long decimals/*=0*/);
@Namespace("at") public static native @ByRef Tensor special_round_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_outf(@Const @ByRef Tensor self, @Cast("int64_t") long decimals, @ByRef Tensor out);




// Parsed from ATen/ops/special_scaled_modified_bessel_k0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_scaled_modified_bessel_k0_ops.h>


// aten::special_scaled_modified_bessel_k0(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_scaled_modified_bessel_k0(@Const @ByRef Tensor x);

// aten::special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k0_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k0_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_scaled_modified_bessel_k1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_scaled_modified_bessel_k1_ops.h>


// aten::special_scaled_modified_bessel_k1(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_scaled_modified_bessel_k1(@Const @ByRef Tensor x);

// aten::special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k1_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k1_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>


// aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_u.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>


// aten::special_shifted_chebyshev_polynomial_u(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_v.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>


// aten::special_shifted_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_w.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>


// aten::special_shifted_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_sinc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_sinc_ops.h>


// aten::special_sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_sinc(@Const @ByRef Tensor self);

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_softmax_ops.h>


// aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor special_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/special_spherical_bessel_j0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_spherical_bessel_j0_ops.h>


// aten::special_spherical_bessel_j0(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_spherical_bessel_j0(@Const @ByRef Tensor x);

// aten::special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_spherical_bessel_j0_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_spherical_bessel_j0_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_xlog1py.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_xlog1py_ops.h>


// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/special_xlogy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_xlogy_ops.h>


// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/special_zeta.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_zeta_ops.h>


// aten::special_zeta(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_ops.h>


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal LongArrayRef split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal LongArrayRef split_size);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_size);


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_size);





// Parsed from ATen/ops/split_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_copy_ops.h>


// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_copy(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_copy(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_copy_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_copy_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);
@Namespace("at") public static native void split_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void split_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorVector out);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size);
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void split_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorVector out);





// Parsed from ATen/ops/split_with_sizes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_with_sizes_ops.h>


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_sizes);


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);





// Parsed from ATen/ops/split_with_sizes_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_with_sizes_copy_ops.h>


// aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_sizes);


// aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector split_with_sizes_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void split_with_sizes_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim, @ByVal TensorVector out);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void split_with_sizes_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorVector out);





// Parsed from ATen/ops/sqrt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sqrt_ops.h>


// aten::sqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sqrt(@Const @ByRef Tensor self);

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_(@ByRef Tensor self);

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/square.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/square_ops.h>


// aten::square(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor square(@Const @ByRef Tensor self);

// aten::square_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_(@ByRef Tensor self);

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/squeeze.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/squeeze_ops.h>


// aten::squeeze(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self);

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);




// Parsed from ATen/ops/squeeze_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/squeeze_copy_ops.h>


// aten::squeeze_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self);

// aten::squeeze_copy.dim(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::squeeze_copy.dims(Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);

// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dim);
// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim, @ByRef Tensor out);




// Parsed from ATen/ops/sspaddmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sspaddmm_ops.h>


// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stack_ops.h>


// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorVector tensors);

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorVector tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor stack_outf(@ByVal TensorVector tensors, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/std.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/std_ops.h>


// aten::std(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);
// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);
// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim);
// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/std_mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/std_mean_ops.h>


// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/stft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stft_ops.h>


// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal LongOptional hop_length, @ByVal LongOptional win_length, @Const @ByRef TensorOptional window, @Cast("bool") boolean normalized, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);

// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @StringView BytePointer pad_mode/*="reflect"*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @StringView String pad_mode/*="reflect"*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);




// Parsed from ATen/ops/stride.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stride_ops.h>


// aten::stride.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_stride(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/sub.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sub_ops.h>


// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/subtract.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/subtract_ops.h>


// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/sum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sum_ops.h>


// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self);

// aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim);
// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/sum_to_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sum_to_size_ops.h>






// Parsed from ATen/ops/svd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/svd_ops.h>


// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self);
// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V);

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T svd(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T svd(@Const @ByRef Tensor self);




// Parsed from ATen/ops/swapaxes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/swapaxes_ops.h>


// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapaxes(@Const @ByRef Tensor self, @Cast("int64_t") long axis0, @Cast("int64_t") long axis1);




// Parsed from ATen/ops/swapdims.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/swapdims_ops.h>


// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapdims(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);




// Parsed from ATen/ops/sym_constrain_range.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_constrain_range_ops.h>


// aten::sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -> ()
@Namespace("at") public static native void sym_constrain_range(@Const @ByRef Scalar size, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional min, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional max);
@Namespace("at") public static native void sym_constrain_range(@Const @ByRef Scalar size);




// Parsed from ATen/ops/sym_constrain_range_for_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_constrain_range_for_size_ops.h>


// aten::sym_constrain_range_for_size(Scalar size, *, int? min, int? max) -> ()
@Namespace("at") public static native void sym_constrain_range_for_size(@Const @ByRef Scalar size, @ByVal LongOptional min, @ByVal LongOptional max);




// Parsed from ATen/ops/sym_numel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_numel_ops.h>


// aten::sym_numel(Tensor self) -> SymInt
@Namespace("at") public static native @ByVal SymInt __dispatch_sym_numel(@Const @ByRef Tensor self);




// Parsed from ATen/ops/sym_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_size_ops.h>


// aten::sym_size.int(Tensor self, int dim) -> SymInt
@Namespace("at") public static native @ByVal SymInt __dispatch_sym_size(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/sym_storage_offset.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_storage_offset_ops.h>


// aten::sym_storage_offset(Tensor self) -> SymInt
@Namespace("at") public static native @ByVal SymInt __dispatch_sym_storage_offset(@Const @ByRef Tensor self);




// Parsed from ATen/ops/sym_stride.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sym_stride_ops.h>


// aten::sym_stride.int(Tensor self, int dim) -> SymInt
@Namespace("at") public static native @ByVal SymInt __dispatch_sym_stride(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/t_ops.h>


// aten::t(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor t(@Const @ByRef Tensor self);




// Parsed from ATen/ops/t_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/t_copy_ops.h>


// aten::t_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor t_copy(@Const @ByRef Tensor self);

// aten::t_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor t_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::t_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor t_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/take.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/take_ops.h>


// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index);
// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::take(Tensor self, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor take(@Const @ByRef Tensor self, @Const @ByRef Tensor index);




// Parsed from ATen/ops/take_along_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/take_along_dim_ops.h>


// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/tan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tan_ops.h>


// aten::tan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tan(@Const @ByRef Tensor self);

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_(@ByRef Tensor self);

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/tanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tanh_ops.h>


// aten::tanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh(@Const @ByRef Tensor self);

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_(@ByRef Tensor self);

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/tanh_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tanh_backward_ops.h>


// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);




// Parsed from ATen/ops/tensor_split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tensor_split_ops.h>


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections);


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymInt sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymInt sections);


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal LongArrayRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... indices);


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef indices);


// aten::tensor_split.tensor_indices_or_sections(Tensor(a -> *) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections);




// Parsed from ATen/ops/tensor.h

// #pragma once
// #include <ATen/core/Tensor.h>
// #include <c10/core/ScalarType.h>

// These functions are defined in ATen/Utils.cpp.
// #define TENSOR(T, S)
//   TORCH_API Tensor tensor(ArrayRef<T> values, const TensorOptions& options);
//   inline Tensor tensor(
//       std::initializer_list<T> values, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(values), options);
//   }
//   inline Tensor tensor(T value, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(value), options);
//   }
//   inline Tensor tensor(ArrayRef<T> values) {
//     return at::tensor(std::move(values), at::dtype(k##S));
//   }
//   inline Tensor tensor(std::initializer_list<T> values) {
//     return at::tensor(ArrayRef<T>(values));
//   }
//   inline Tensor tensor(T value) {
//     return at::tensor(ArrayRef<T>(value));
//   }
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal ByteArrayRef values, @Const @ByRef TensorOptions options);
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jbyte*", "c10::ArrayRef<jbyte>", "std::vector<jbyte>&"}) @StdVector("jbyte") byte[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal ByteArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jbyte*", "c10::ArrayRef<jbyte>", "std::vector<jbyte>&"}) @StdVector("jbyte") byte... values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal ShortArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jshort*", "c10::ArrayRef<jshort>", "std::vector<jshort>&"}) @StdVector("jshort") short[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(short value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal ShortArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jshort*", "c10::ArrayRef<jshort>", "std::vector<jshort>&"}) @StdVector("jshort") short... values);
  @Namespace("at") public static native @ByVal Tensor tensor(short value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal IntArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jint*", "c10::ArrayRef<jint>", "std::vector<jint>&"}) @StdVector("jint") int[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(int value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal IntArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"jint*", "c10::ArrayRef<jint>", "std::vector<jint>&"}) @StdVector("jint") int... values);
  @Namespace("at") public static native @ByVal Tensor tensor(int value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal LongArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal LongArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"float*", "c10::ArrayRef<float>", "std::vector<float>&"}) @StdVector("float") float[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(float value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"float*", "c10::ArrayRef<float>", "std::vector<float>&"}) @StdVector("float") float... values);
  @Namespace("at") public static native @ByVal Tensor tensor(float value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(double value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(double value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value);
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplex value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplex value);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplex value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplex value);
// #undef TENSOR

  // namespace at


// Parsed from ATen/ops/tensordot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tensordot_ops.h>


// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRef dims_self, @ByVal LongArrayRef dims_other);
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims_other);

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRef dims_self, @ByVal LongArrayRef dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims_other);
// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRef dims_self, @ByVal LongArrayRef dims_other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dims_other, @ByRef Tensor out);




// Parsed from ATen/ops/thnn_conv2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/thnn_conv2d_ops.h>


// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);
// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal LongArrayRef stride, @ByVal LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding, @ByRef Tensor out);

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... kernel_size);




// Parsed from ATen/ops/threshold.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/threshold_ops.h>


// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_(@ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);
// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value, @ByRef Tensor out);




// Parsed from ATen/ops/threshold_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/threshold_backward_ops.h>


// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);
// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @ByRef Tensor grad_input);

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);




// Parsed from ATen/ops/tile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tile_ops.h>


// aten::tile(Tensor self, SymInt[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dims);


// aten::tile(Tensor self, SymInt[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor tile_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef dims);





// Parsed from ATen/ops/to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_ops.h>






// Parsed from ATen/ops/to_dense.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_dense_ops.h>






// Parsed from ATen/ops/to_dense_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_dense_backward_ops.h>


// aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional masked_grad);
@Namespace("at") public static native @ByVal Tensor to_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);




// Parsed from ATen/ops/to_mkldnn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_mkldnn_ops.h>


// aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_mkldnn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor to_mkldnn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_mkldnn_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/to_mkldnn_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_mkldnn_backward_ops.h>


// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_mkldnn_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);




// Parsed from ATen/ops/to_padded_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_padded_tensor_ops.h>



// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional output_size);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_outf(@Const @ByRef Tensor self, double padding, @ByVal LongArrayRefOptional output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_outf(@Const @ByRef Tensor self, double padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional output_size);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_outf(@Const @ByRef Tensor self, double padding, @ByVal SymIntArrayRefOptional output_size, @ByRef Tensor out);





// Parsed from ATen/ops/to_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_ops.h>






// Parsed from ATen/ops/to_sparse_bsc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_bsc_ops.h>






// Parsed from ATen/ops/to_sparse_bsr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_bsr_ops.h>






// Parsed from ATen/ops/to_sparse_csc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_csc_ops.h>






// Parsed from ATen/ops/to_sparse_csr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_csr_ops.h>






// Parsed from ATen/ops/topk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/topk_ops.h>


// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);


// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean largest, @Cast("bool") boolean sorted, @ByRef Tensor values, @ByRef Tensor indices);


// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_symint_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal SymInt k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_symint_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal SymInt k);


// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt k, @Cast("int64_t") long dim, @Cast("bool") boolean largest, @Cast("bool") boolean sorted, @ByRef Tensor values, @ByRef Tensor indices);


// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T topk(@Const @ByRef Tensor self, @Cast("int64_t") long k);


// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_symint(@Const @ByRef Tensor self, @ByVal SymInt k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T topk_symint(@Const @ByRef Tensor self, @ByVal SymInt k);





// Parsed from ATen/ops/trace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trace_ops.h>


// aten::trace(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace(@Const @ByRef Tensor self);

// aten::trace.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trace_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::trace.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trace_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/trace_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trace_backward_ops.h>


// aten::trace_backward(Tensor grad, SymInt[] sizes) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);


// aten::trace_backward(Tensor grad, SymInt[] sizes) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntArrayRef sizes);





// Parsed from ATen/ops/transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/transpose_ops.h>


// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @ByVal Dimname dim0, @ByVal Dimname dim1);




// Parsed from ATen/ops/transpose_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/transpose_copy_ops.h>


// aten::transpose_copy.int(Tensor self, int dim0, int dim1) -> Tensor
@Namespace("at") public static native @ByVal Tensor transpose_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor transpose_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
// aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor transpose_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1, @ByRef Tensor out);




// Parsed from ATen/ops/trapezoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trapezoid_ops.h>


// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y);




// Parsed from ATen/ops/trapz.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trapz_ops.h>


// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, double dx/*=1*/, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y);




// Parsed from ATen/ops/triangular_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triangular_solve_ops.h>


// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal T_TensorTensor_T triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A);
// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal T_TensorTensor_T triangular_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @Cast("bool") boolean transpose, @Cast("bool") boolean unitriangular, @ByRef Tensor X, @ByRef Tensor M);

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
@Namespace("at") public static native @ByVal T_TensorTensor_T triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);




// Parsed from ATen/ops/tril.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tril_ops.h>


// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::tril(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self);




// Parsed from ATen/ops/tril_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tril_indices_ops.h>


// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_indices_outf(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByRef Tensor out);




// Parsed from ATen/ops/triplet_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triplet_margin_loss_ops.h>


// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative, double margin/*=1.0*/, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean swap/*=false*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative);




// Parsed from ATen/ops/triu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triu_ops.h>


// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::triu(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self);




// Parsed from ATen/ops/triu_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triu_indices_ops.h>


// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_indices_outf(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByRef Tensor out);




// Parsed from ATen/ops/true_divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/true_divide_ops.h>


// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/trunc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trunc_ops.h>


// aten::trunc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trunc(@Const @ByRef Tensor self);

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_(@ByRef Tensor self);

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/type_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/type_as_ops.h>






// Parsed from ATen/ops/unbind.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unbind_ops.h>


// aten::unbind.int(Tensor(a -> *) self, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self);

// aten::unbind.Dimname(Tensor(a -> *) self, Dimname dim) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/unbind_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unbind_copy_ops.h>


// aten::unbind_copy.int(Tensor self, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unbind_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unbind_copy(@Const @ByRef Tensor self);

// aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self);
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorVector out, @Const @ByRef Tensor self);
// aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unbind_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unbind_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal TensorVector out);




// Parsed from ATen/ops/unflatten.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unflatten_ops.h>


// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);


// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymIntArrayRef sizes);


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal LongArrayRef sizes, @ByVal DimnameArrayRef names);
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes, @ByVal DimnameVector names);


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten_symint(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal SymIntArrayRef sizes, @ByVal DimnameArrayRef names);
@Namespace("at") public static native @ByVal Tensor unflatten_symint(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal SymIntArrayRef sizes, @ByVal DimnameVector names);





// Parsed from ATen/ops/unflatten_dense_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unflatten_dense_tensors_ops.h>


// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unflatten_dense_tensors(@Const @ByRef Tensor flat, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unflatten_dense_tensors(@Const @ByRef Tensor flat, @ByVal TensorVector tensors);




// Parsed from ATen/ops/unfold.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_ops.h>






// Parsed from ATen/ops/unfold_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_backward_ops.h>


// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_backward_symint(@Const @ByRef Tensor grad_in, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByRef Tensor unfold_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_outf(@Const @ByRef Tensor grad_in, @ByVal LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor unfold_backward_outf(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_symint_outf(@Const @ByRef Tensor grad_in, @ByVal SymIntArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);





// Parsed from ATen/ops/unfold_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_copy_ops.h>


// aten::unfold_copy(Tensor self, int dimension, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step);

// aten::unfold_copy.out(Tensor self, int dimension, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step);
// aten::unfold_copy.out(Tensor self, int dimension, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);




// Parsed from ATen/ops/uniform.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/uniform_ops.h>


// aten::uniform.out(Tensor self, float from=0, float to=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor uniform_out(@ByRef Tensor out, @Const @ByRef Tensor self, double from/*=0*/, double to/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor uniform_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::uniform.out(Tensor self, float from=0, float to=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor uniform_outf(@Const @ByRef Tensor self, double from, double to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::uniform(Tensor self, float from=0, float to=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor uniform(@Const @ByRef Tensor self, double from/*=0*/, double to/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor uniform(@Const @ByRef Tensor self);




// Parsed from ATen/ops/unique_consecutive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_consecutive_ops.h>


// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_consecutive(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_consecutive(@Const @ByRef Tensor self);

// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self);
// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_consecutive_outf(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByVal LongOptional dim, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unique_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_dim_ops.h>


// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unique_dim_consecutive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_dim_consecutive_ops.h>


// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal T_TensorTensorTensor_T unique_dim_consecutive_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unsafe_chunk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_chunk_ops.h>


// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);




// Parsed from ATen/ops/unsafe_split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_split_ops.h>


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unsafe_split_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorVector out);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size);
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unsafe_split_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorVector out);





// Parsed from ATen/ops/unsafe_split_with_sizes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_split_with_sizes_ops.h>


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_sizes);


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unsafe_split_with_sizes_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] split_sizes, @Cast("int64_t") long dim, @ByVal TensorVector out);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorVector out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unsafe_split_with_sizes_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorVector out);





// Parsed from ATen/ops/unsqueeze.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsqueeze_ops.h>


// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unsqueeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/unsqueeze_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsqueeze_copy_ops.h>


// aten::unsqueeze_copy(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor unsqueeze_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unsqueeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unsqueeze_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/upsample_bicubic2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bicubic2d_ops.h>


// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bicubic2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bicubic2d_backward_ops.h>


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bilinear2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bilinear2d_ops.h>


// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bilinear2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bilinear2d_backward_ops.h>


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_linear1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_linear1d_ops.h>


// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_linear1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_linear1d_backward_ops.h>


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_nearest1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest1d_ops.h>


// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/upsample_nearest1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest1d_backward_ops.h>


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);





// Parsed from ATen/ops/upsample_nearest2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest2d_ops.h>


// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/upsample_nearest2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest2d_backward_ops.h>


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);





// Parsed from ATen/ops/upsample_nearest3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest3d_ops.h>


// aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... output_size);


// aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size);





// Parsed from ATen/ops/upsample_nearest3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest3d_backward_ops.h>


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... input_size);


// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size);





// Parsed from ATen/ops/upsample_trilinear3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_trilinear3d_ops.h>


// aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal @Cast({"double*", "c10::ArrayRef<double>", "std::vector<double>&"}) @StdVector double... scale_factors);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_trilinear3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_trilinear3d_backward_ops.h>


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRef output_size, @ByVal LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntArrayRef output_size, @ByVal SymIntArrayRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/value_selecting_reduction_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/value_selecting_reduction_backward_ops.h>


// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal LongArrayRef sizes, @Cast("bool") boolean keepdim);
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes, @Cast("bool") boolean keepdim);


// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward_symint(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal SymIntArrayRef sizes, @Cast("bool") boolean keepdim);





// Parsed from ATen/ops/values.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/values_ops.h>






// Parsed from ATen/ops/values_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/values_copy_ops.h>


// aten::values_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor values_copy(@Const @ByRef Tensor self);

// aten::values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor values_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor values_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/vander.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vander_ops.h>


// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N, @Cast("bool") boolean increasing/*=false*/);
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x);




// Parsed from ATen/ops/var.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/var_ops.h>


// aten::var(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);
// aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);
// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameVector dim);
// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/var_mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/var_mean_ops.h>


// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::var_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Cast("bool") boolean unbiased);

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, @ByVal DimnameVector dim);

// aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Const @ByRef ScalarOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/vdot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vdot_ops.h>


// aten::vdot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor vdot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/view.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_ops.h>






// Parsed from ATen/ops/view_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_ops.h>






// Parsed from ATen/ops/view_as_complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_complex_ops.h>


// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_complex(@Const @ByRef Tensor self);




// Parsed from ATen/ops/view_as_complex_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_complex_copy_ops.h>


// aten::view_as_complex_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_as_complex_copy(@Const @ByRef Tensor self);

// aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_complex_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_complex_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/view_as_real.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_real_ops.h>


// aten::view_as_real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_real(@Const @ByRef Tensor self);




// Parsed from ATen/ops/view_as_real_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_real_copy_ops.h>


// aten::view_as_real_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_as_real_copy(@Const @ByRef Tensor self);

// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_real_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_real_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/view_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_copy_ops.h>


// aten::view_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::view_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::view_copy.dtype(Tensor self, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, ScalarType dtype);

// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, @ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntArrayRef size);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, ScalarType dtype);
// aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, ScalarType dtype, @ByRef Tensor out);




// Parsed from ATen/ops/vsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vsplit_ops.h>


// aten::vsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::vsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... indices);




// Parsed from ATen/ops/vstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vstack_ops.h>


// aten::vstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor vstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor vstack(@ByVal TensorVector tensors);

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor vstack_out(@ByRef Tensor out, @ByVal TensorVector tensors);
// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor vstack_outf(@ByVal TensorVector tensors, @ByRef Tensor out);




// Parsed from ATen/ops/where.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/where_ops.h>


// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor where_out(@ByRef Tensor out, @Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor where_outf(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Scalar other);

// aten::where(Tensor condition) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector where(@Const @ByRef Tensor condition);




// Parsed from ATen/ops/xlogy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/xlogy_ops.h>


// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/xor_ops.h>


// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/zero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zero_ops.h>


// aten::zero_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_(@ByRef Tensor self);

// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::zero(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal @Name("zero") Tensor _zero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/zeros.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zeros_ops.h>


// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntArrayRef size);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByRef Tensor out);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_symint_out(@ByRef Tensor out, @ByVal SymIntArrayRef size);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_symint_outf(@ByVal SymIntArrayRef size, @ByRef Tensor out);


// aten::zeros.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
// aten::zeros.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/zeros_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zeros_like_ops.h>


// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self);
// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor zeros_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/Functions.h

// #pragma once

// @generated by torchgen/gen.py from Functions.h

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// #if defined(AT_PER_OPERATOR_HEADERS) && defined(TORCH_ASSERT_ONLY_METHOD_OPERATORS)
// #error This change adds a dependency on all pytorch operators, meaning the
//   file will need to be re-compiled every time an operator is changed or added.
//   Consider including a specific operator from <ATen/ops/{my_operator}.h> and
//   see NOTE [TORCH_ASSERT_ONLY_METHOD_OPERATORS].
// #endif

// NOTE: [TORCH_ASSERT_ONLY_METHOD_OPERATORS]
//
// In ATen, certain generated headers files include the definitions of
// every single operator in PyTorch. Unfortunately this means every
// time an operator signature is updated or changed in
// native_functions.yaml, you (and every other PyTorch developer) need
// to recompile every source file that includes any of these headers.
//
// To break up these header dependencies, and improve incremental
// build times for all PyTorch developers. These headers are split
// into per-operator headers in the `ATen/ops` folder. This limits
// incremental builds to only changes to methods of `Tensor`, or files
// that use the specific operator being changed. With `at::sum` as an
// example, you should include
//
//   <ATen/ops/sum.h>               // instead of ATen/Functions.h
//   <ATen/ops/sum_native.h>        // instead of ATen/NativeFunctions.h
//   <ATen/ops/sum_ops.h>           // instead of ATen/Operators.h
//   <ATen/ops/sum_cpu_dispatch.h>  // instead of ATen/CPUFunctions.h
//
// However, even if you're careful to use this in your own code.
// `Functions.h` might be included indirectly through another header
// without you realising. To avoid this, you can add
//
//   #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
//
// to the top of your source file. This way any time the non-specific
// headers are included, the compiler will error out.
//
// Also, be aware that `ops` are not available in all build
// configurations (namely fb-internal) so you must guard these
// includes with `#ifdef AT_PER_OPERATOR_HEADERS`. e.g.
//
//   #ifndef AT_PER_OPERATOR_HEADERS
//   #include <ATen/Functions.h>
//   #else
//   #include <ATen/ops/sum.h>
//   #endif

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <c10/core/SymInt.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>
// #include <c10/util/OptionalArrayRef.h>

// #include <ATen/ops/from_blob.h>
// #include <ATen/ops/tensor.h>

// #include <ATen/ops/_adaptive_avg_pool2d.h>
// #include <ATen/ops/_adaptive_avg_pool2d_backward.h>
// #include <ATen/ops/_adaptive_avg_pool3d.h>
// #include <ATen/ops/_adaptive_avg_pool3d_backward.h>
// #include <ATen/ops/_add_batch_dim.h>
// #include <ATen/ops/_add_relu.h>
// #include <ATen/ops/_addmm_activation.h>
// #include <ATen/ops/_aminmax.h>
// #include <ATen/ops/_amp_foreach_non_finite_check_and_unscale.h>
// #include <ATen/ops/_amp_update_scale.h>
// #include <ATen/ops/_assert_async.h>
// #include <ATen/ops/_assert_tensor_metadata.h>
// #include <ATen/ops/_autocast_to_full_precision.h>
// #include <ATen/ops/_autocast_to_reduced_precision.h>
// #include <ATen/ops/_backward.h>
// #include <ATen/ops/_batch_norm_impl_index.h>
// #include <ATen/ops/_batch_norm_impl_index_backward.h>
// #include <ATen/ops/_cast_Byte.h>
// #include <ATen/ops/_cast_Char.h>
// #include <ATen/ops/_cast_Double.h>
// #include <ATen/ops/_cast_Float.h>
// #include <ATen/ops/_cast_Half.h>
// #include <ATen/ops/_cast_Int.h>
// #include <ATen/ops/_cast_Long.h>
// #include <ATen/ops/_cast_Short.h>
// #include <ATen/ops/_cdist_backward.h>
// #include <ATen/ops/_cdist_forward.h>
// #include <ATen/ops/_cholesky_solve_helper.h>
// #include <ATen/ops/_choose_qparams_per_tensor.h>
// #include <ATen/ops/_coalesce.h>
// #include <ATen/ops/_coalesced.h>
// #include <ATen/ops/_compute_linear_combination.h>
// #include <ATen/ops/_conj.h>
// #include <ATen/ops/_conj_copy.h>
// #include <ATen/ops/_conj_physical.h>
// #include <ATen/ops/_conv_depthwise2d.h>
// #include <ATen/ops/_convert_indices_from_coo_to_csr.h>
// #include <ATen/ops/_convert_indices_from_csr_to_coo.h>
// #include <ATen/ops/_convolution.h>
// #include <ATen/ops/_convolution_double_backward.h>
// #include <ATen/ops/_convolution_mode.h>
// #include <ATen/ops/_copy_from.h>
// #include <ATen/ops/_copy_from_and_resize.h>
// #include <ATen/ops/_cslt_compress.h>
// #include <ATen/ops/_cslt_sparse_mm.h>
// #include <ATen/ops/_ctc_loss.h>
// #include <ATen/ops/_ctc_loss_backward.h>
// #include <ATen/ops/_cudnn_ctc_loss.h>
// #include <ATen/ops/_cudnn_init_dropout_state.h>
// #include <ATen/ops/_cudnn_rnn.h>
// #include <ATen/ops/_cudnn_rnn_backward.h>
// #include <ATen/ops/_cudnn_rnn_flatten_weight.h>
// #include <ATen/ops/_cufft_clear_plan_cache.h>
// #include <ATen/ops/_cufft_get_plan_cache_max_size.h>
// #include <ATen/ops/_cufft_get_plan_cache_size.h>
// #include <ATen/ops/_cufft_set_plan_cache_max_size.h>
// #include <ATen/ops/_cummax_helper.h>
// #include <ATen/ops/_cummin_helper.h>
// #include <ATen/ops/_debug_has_internal_overlap.h>
// #include <ATen/ops/_dimI.h>
// #include <ATen/ops/_dimV.h>
// #include <ATen/ops/_dim_arange.h>
// #include <ATen/ops/_dirichlet_grad.h>
// #include <ATen/ops/_efficient_attention_backward.h>
// #include <ATen/ops/_efficient_attention_forward.h>
// #include <ATen/ops/_efficientzerotensor.h>
// #include <ATen/ops/_embedding_bag.h>
// #include <ATen/ops/_embedding_bag_backward.h>
// #include <ATen/ops/_embedding_bag_dense_backward.h>
// #include <ATen/ops/_embedding_bag_forward_only.h>
// #include <ATen/ops/_embedding_bag_per_sample_weights_backward.h>
// #include <ATen/ops/_embedding_bag_sparse_backward.h>
// #include <ATen/ops/_empty_affine_quantized.h>
// #include <ATen/ops/_empty_per_channel_affine_quantized.h>
// #include <ATen/ops/_euclidean_dist.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward.h>
// #include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.h>
// #include <ATen/ops/_fft_c2c.h>
// #include <ATen/ops/_fft_c2r.h>
// #include <ATen/ops/_fft_r2c.h>
// #include <ATen/ops/_fill_mem_eff_dropout_mask.h>
// #include <ATen/ops/_flash_attention_backward.h>
// #include <ATen/ops/_flash_attention_forward.h>
// #include <ATen/ops/_foobar.h>
// #include <ATen/ops/_foreach_abs.h>
// #include <ATen/ops/_foreach_acos.h>
// #include <ATen/ops/_foreach_add.h>
// #include <ATen/ops/_foreach_addcdiv.h>
// #include <ATen/ops/_foreach_addcmul.h>
// #include <ATen/ops/_foreach_asin.h>
// #include <ATen/ops/_foreach_atan.h>
// #include <ATen/ops/_foreach_ceil.h>
// #include <ATen/ops/_foreach_clamp_max.h>
// #include <ATen/ops/_foreach_clamp_min.h>
// #include <ATen/ops/_foreach_copy.h>
// #include <ATen/ops/_foreach_cos.h>
// #include <ATen/ops/_foreach_cosh.h>
// #include <ATen/ops/_foreach_div.h>
// #include <ATen/ops/_foreach_erf.h>
// #include <ATen/ops/_foreach_erfc.h>
// #include <ATen/ops/_foreach_exp.h>
// #include <ATen/ops/_foreach_expm1.h>
// #include <ATen/ops/_foreach_floor.h>
// #include <ATen/ops/_foreach_frac.h>
// #include <ATen/ops/_foreach_lerp.h>
// #include <ATen/ops/_foreach_lgamma.h>
// #include <ATen/ops/_foreach_log.h>
// #include <ATen/ops/_foreach_log10.h>
// #include <ATen/ops/_foreach_log1p.h>
// #include <ATen/ops/_foreach_log2.h>
// #include <ATen/ops/_foreach_maximum.h>
// #include <ATen/ops/_foreach_minimum.h>
// #include <ATen/ops/_foreach_mul.h>
// #include <ATen/ops/_foreach_neg.h>
// #include <ATen/ops/_foreach_norm.h>
// #include <ATen/ops/_foreach_pow.h>
// #include <ATen/ops/_foreach_reciprocal.h>
// #include <ATen/ops/_foreach_round.h>
// #include <ATen/ops/_foreach_sigmoid.h>
// #include <ATen/ops/_foreach_sign.h>
// #include <ATen/ops/_foreach_sin.h>
// #include <ATen/ops/_foreach_sinh.h>
// #include <ATen/ops/_foreach_sqrt.h>
// #include <ATen/ops/_foreach_sub.h>
// #include <ATen/ops/_foreach_tan.h>
// #include <ATen/ops/_foreach_tanh.h>
// #include <ATen/ops/_foreach_trunc.h>
// #include <ATen/ops/_foreach_zero.h>
// #include <ATen/ops/_functional_assert_async.h>
// #include <ATen/ops/_functional_sym_constrain_range.h>
// #include <ATen/ops/_functional_sym_constrain_range_for_size.h>
// #include <ATen/ops/_fused_adam.h>
// #include <ATen/ops/_fused_adamw.h>
// #include <ATen/ops/_fused_dropout.h>
// #include <ATen/ops/_fused_moving_avg_obs_fq_helper.h>
// #include <ATen/ops/_fused_sdp_choice.h>
// #include <ATen/ops/_fw_primal.h>
// #include <ATen/ops/_fw_primal_copy.h>
// #include <ATen/ops/_gather_sparse_backward.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward.h>
// #include <ATen/ops/_has_compatible_shallow_copy_type.h>
// #include <ATen/ops/_has_same_storage_numel.h>
// #include <ATen/ops/_histogramdd_bin_edges.h>
// #include <ATen/ops/_histogramdd_from_bin_cts.h>
// #include <ATen/ops/_histogramdd_from_bin_tensors.h>
// #include <ATen/ops/_index_put_impl.h>
// #include <ATen/ops/_indices.h>
// #include <ATen/ops/_indices_copy.h>
// #include <ATen/ops/_int_mm.h>
// #include <ATen/ops/_is_all_true.h>
// #include <ATen/ops/_is_any_true.h>
// #include <ATen/ops/_is_zerotensor.h>
// #include <ATen/ops/_linalg_check_errors.h>
// #include <ATen/ops/_linalg_det.h>
// #include <ATen/ops/_linalg_eigh.h>
// #include <ATen/ops/_linalg_slogdet.h>
// #include <ATen/ops/_linalg_solve_ex.h>
// #include <ATen/ops/_linalg_svd.h>
// #include <ATen/ops/_local_scalar_dense.h>
// #include <ATen/ops/_log_softmax.h>
// #include <ATen/ops/_log_softmax_backward_data.h>
// #include <ATen/ops/_logcumsumexp.h>
// #include <ATen/ops/_lstm_mps.h>
// #include <ATen/ops/_lu_with_info.h>
// #include <ATen/ops/_make_dep_token.h>
// #include <ATen/ops/_make_dual.h>
// #include <ATen/ops/_make_dual_copy.h>
// #include <ATen/ops/_make_per_channel_quantized_tensor.h>
// #include <ATen/ops/_make_per_tensor_quantized_tensor.h>
// #include <ATen/ops/_masked_scale.h>
// #include <ATen/ops/_masked_softmax.h>
// #include <ATen/ops/_masked_softmax_backward.h>
// #include <ATen/ops/_mkldnn_reshape.h>
// #include <ATen/ops/_mkldnn_transpose.h>
// #include <ATen/ops/_mps_convolution.h>
// #include <ATen/ops/_mps_convolution_transpose.h>
// #include <ATen/ops/_native_batch_norm_legit.h>
// #include <ATen/ops/_native_batch_norm_legit_no_training.h>
// #include <ATen/ops/_native_multi_head_attention.h>
// #include <ATen/ops/_neg_view.h>
// #include <ATen/ops/_neg_view_copy.h>
// #include <ATen/ops/_nested_from_padded.h>
// #include <ATen/ops/_nested_from_padded_and_nested_example.h>
// #include <ATen/ops/_nested_select_backward.h>
// #include <ATen/ops/_nested_sum_backward.h>
// #include <ATen/ops/_nested_tensor_from_mask.h>
// #include <ATen/ops/_nested_tensor_from_mask_left_aligned.h>
// #include <ATen/ops/_nested_tensor_from_tensor_list.h>
// #include <ATen/ops/_nested_tensor_size.h>
// #include <ATen/ops/_nested_tensor_softmax_with_shape.h>
// #include <ATen/ops/_nested_tensor_storage_offsets.h>
// #include <ATen/ops/_nested_tensor_strides.h>
// #include <ATen/ops/_nested_view_from_buffer.h>
// #include <ATen/ops/_nested_view_from_buffer_copy.h>
// #include <ATen/ops/_new_zeros_with_same_feature_meta.h>
// #include <ATen/ops/_nnpack_available.h>
// #include <ATen/ops/_nnpack_spatial_convolution.h>
// #include <ATen/ops/_nnz.h>
// #include <ATen/ops/_pack_padded_sequence.h>
// #include <ATen/ops/_pack_padded_sequence_backward.h>
// #include <ATen/ops/_pad_circular.h>
// #include <ATen/ops/_pad_enum.h>
// #include <ATen/ops/_pad_packed_sequence.h>
// #include <ATen/ops/_pdist_backward.h>
// #include <ATen/ops/_pdist_forward.h>
// #include <ATen/ops/_pin_memory.h>
// #include <ATen/ops/_prelu_kernel.h>
// #include <ATen/ops/_prelu_kernel_backward.h>
// #include <ATen/ops/_propagate_xla_data.h>
// #include <ATen/ops/_remove_batch_dim.h>
// #include <ATen/ops/_reshape_alias.h>
// #include <ATen/ops/_reshape_alias_copy.h>
// #include <ATen/ops/_reshape_copy.h>
// #include <ATen/ops/_reshape_from_tensor.h>
// #include <ATen/ops/_resize_output.h>
// #include <ATen/ops/_rowwise_prune.h>
// #include <ATen/ops/_sample_dirichlet.h>
// #include <ATen/ops/_saturate_weight_to_fp16.h>
// #include <ATen/ops/_scaled_dot_product_attention_math.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention_backward.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention_backward.h>
// #include <ATen/ops/_scaled_mm.h>
// #include <ATen/ops/_segment_reduce_backward.h>
// #include <ATen/ops/_shape_as_tensor.h>
// #include <ATen/ops/_slow_conv2d_backward.h>
// #include <ATen/ops/_slow_conv2d_forward.h>
// #include <ATen/ops/_sobol_engine_draw.h>
// #include <ATen/ops/_sobol_engine_ff.h>
// #include <ATen/ops/_sobol_engine_initialize_state.h>
// #include <ATen/ops/_sobol_engine_scramble.h>
// #include <ATen/ops/_softmax.h>
// #include <ATen/ops/_softmax_backward_data.h>
// #include <ATen/ops/_sparse_addmm.h>
// #include <ATen/ops/_sparse_broadcast_to.h>
// #include <ATen/ops/_sparse_broadcast_to_copy.h>
// #include <ATen/ops/_sparse_bsc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_compressed_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors.h>
// #include <ATen/ops/_sparse_csc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csr_prod.h>
// #include <ATen/ops/_sparse_csr_sum.h>
// #include <ATen/ops/_sparse_csr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_log_softmax.h>
// #include <ATen/ops/_sparse_log_softmax_backward_data.h>
// #include <ATen/ops/_sparse_mask_projection.h>
// #include <ATen/ops/_sparse_mm.h>
// #include <ATen/ops/_sparse_mm_reduce_impl.h>
// #include <ATen/ops/_sparse_mm_reduce_impl_backward.h>
// #include <ATen/ops/_sparse_semi_structured_linear.h>
// #include <ATen/ops/_sparse_softmax.h>
// #include <ATen/ops/_sparse_softmax_backward_data.h>
// #include <ATen/ops/_sparse_sparse_matmul.h>
// #include <ATen/ops/_sparse_sum.h>
// #include <ATen/ops/_sparse_sum_backward.h>
// #include <ATen/ops/_spdiags.h>
// #include <ATen/ops/_stack.h>
// #include <ATen/ops/_standard_gamma.h>
// #include <ATen/ops/_standard_gamma_grad.h>
// #include <ATen/ops/_test_ambiguous_defaults.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view_copy.h>
// #include <ATen/ops/_test_check_tensor.h>
// #include <ATen/ops/_test_functorch_fallback.h>
// #include <ATen/ops/_test_optional_filled_intlist.h>
// #include <ATen/ops/_test_optional_floatlist.h>
// #include <ATen/ops/_test_optional_intlist.h>
// #include <ATen/ops/_test_serialization_subcmul.h>
// #include <ATen/ops/_test_string_default.h>
// #include <ATen/ops/_test_warn_in_autograd.h>
// #include <ATen/ops/_thnn_differentiable_gru_cell_backward.h>
// #include <ATen/ops/_thnn_differentiable_lstm_cell_backward.h>
// #include <ATen/ops/_thnn_fused_gru_cell.h>
// #include <ATen/ops/_thnn_fused_gru_cell_backward.h>
// #include <ATen/ops/_thnn_fused_lstm_cell.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward_impl.h>
// #include <ATen/ops/_to_copy.h>
// #include <ATen/ops/_to_cpu.h>
// #include <ATen/ops/_to_dense.h>
// #include <ATen/ops/_to_sparse.h>
// #include <ATen/ops/_to_sparse_bsc.h>
// #include <ATen/ops/_to_sparse_bsr.h>
// #include <ATen/ops/_to_sparse_csc.h>
// #include <ATen/ops/_to_sparse_csr.h>
// #include <ATen/ops/_to_sparse_semi_structured.h>
// #include <ATen/ops/_transform_bias_rescale_qkv.h>
// #include <ATen/ops/_transformer_encoder_layer_fwd.h>
// #include <ATen/ops/_trilinear.h>
// #include <ATen/ops/_triton_multi_head_attention.h>
// #include <ATen/ops/_triton_scaled_dot_attention.h>
// #include <ATen/ops/_unique.h>
// #include <ATen/ops/_unique2.h>
// #include <ATen/ops/_unpack_dual.h>
// #include <ATen/ops/_unsafe_index.h>
// #include <ATen/ops/_unsafe_index_put.h>
// #include <ATen/ops/_unsafe_view.h>
// #include <ATen/ops/_upsample_bicubic2d_aa.h>
// #include <ATen/ops/_upsample_bicubic2d_aa_backward.h>
// #include <ATen/ops/_upsample_bilinear2d_aa.h>
// #include <ATen/ops/_upsample_bilinear2d_aa_backward.h>
// #include <ATen/ops/_upsample_nearest_exact1d.h>
// #include <ATen/ops/_upsample_nearest_exact1d_backward.h>
// #include <ATen/ops/_upsample_nearest_exact2d.h>
// #include <ATen/ops/_upsample_nearest_exact2d_backward.h>
// #include <ATen/ops/_upsample_nearest_exact3d.h>
// #include <ATen/ops/_upsample_nearest_exact3d_backward.h>
// #include <ATen/ops/_use_cudnn_ctc_loss.h>
// #include <ATen/ops/_use_cudnn_rnn_flatten_weight.h>
// #include <ATen/ops/_validate_compressed_sparse_indices.h>
// #include <ATen/ops/_validate_sparse_bsc_tensor_args.h>
// #include <ATen/ops/_validate_sparse_bsr_tensor_args.h>
// #include <ATen/ops/_validate_sparse_compressed_tensor_args.h>
// #include <ATen/ops/_validate_sparse_coo_tensor_args.h>
// #include <ATen/ops/_validate_sparse_csc_tensor_args.h>
// #include <ATen/ops/_validate_sparse_csr_tensor_args.h>
// #include <ATen/ops/_values.h>
// #include <ATen/ops/_values_copy.h>
// #include <ATen/ops/_version.h>
// #include <ATen/ops/_weight_norm.h>
// #include <ATen/ops/_weight_norm_differentiable_backward.h>
// #include <ATen/ops/_weight_norm_interface.h>
// #include <ATen/ops/_weight_norm_interface_backward.h>
// #include <ATen/ops/abs.h>
// #include <ATen/ops/absolute.h>
// #include <ATen/ops/acos.h>
// #include <ATen/ops/acosh.h>
// #include <ATen/ops/adaptive_avg_pool1d.h>
// #include <ATen/ops/adaptive_avg_pool2d.h>
// #include <ATen/ops/adaptive_avg_pool3d.h>
// #include <ATen/ops/adaptive_avg_pool3d_backward.h>
// #include <ATen/ops/adaptive_max_pool1d.h>
// #include <ATen/ops/adaptive_max_pool2d.h>
// #include <ATen/ops/adaptive_max_pool2d_backward.h>
// #include <ATen/ops/adaptive_max_pool3d.h>
// #include <ATen/ops/adaptive_max_pool3d_backward.h>
// #include <ATen/ops/add.h>
// #include <ATen/ops/addbmm.h>
// #include <ATen/ops/addcdiv.h>
// #include <ATen/ops/addcmul.h>
// #include <ATen/ops/addmm.h>
// #include <ATen/ops/addmv.h>
// #include <ATen/ops/addr.h>
// #include <ATen/ops/adjoint.h>
// #include <ATen/ops/affine_grid_generator.h>
// #include <ATen/ops/affine_grid_generator_backward.h>
// #include <ATen/ops/alias.h>
// #include <ATen/ops/alias_copy.h>
// #include <ATen/ops/align_as.h>
// #include <ATen/ops/align_tensors.h>
// #include <ATen/ops/align_to.h>
// #include <ATen/ops/all.h>
// #include <ATen/ops/allclose.h>
// #include <ATen/ops/alpha_dropout.h>
// #include <ATen/ops/amax.h>
// #include <ATen/ops/amin.h>
// #include <ATen/ops/aminmax.h>
// #include <ATen/ops/and.h>
// #include <ATen/ops/angle.h>
// #include <ATen/ops/any.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arccos.h>
// #include <ATen/ops/arccosh.h>
// #include <ATen/ops/arcsin.h>
// #include <ATen/ops/arcsinh.h>
// #include <ATen/ops/arctan.h>
// #include <ATen/ops/arctan2.h>
// #include <ATen/ops/arctanh.h>
// #include <ATen/ops/argmax.h>
// #include <ATen/ops/argmin.h>
// #include <ATen/ops/argsort.h>
// #include <ATen/ops/argwhere.h>
// #include <ATen/ops/as_strided.h>
// #include <ATen/ops/as_strided_copy.h>
// #include <ATen/ops/as_strided_scatter.h>
// #include <ATen/ops/asin.h>
// #include <ATen/ops/asinh.h>
// #include <ATen/ops/atan.h>
// #include <ATen/ops/atan2.h>
// #include <ATen/ops/atanh.h>
// #include <ATen/ops/atleast_1d.h>
// #include <ATen/ops/atleast_2d.h>
// #include <ATen/ops/atleast_3d.h>
// #include <ATen/ops/avg_pool1d.h>
// #include <ATen/ops/avg_pool2d.h>
// #include <ATen/ops/avg_pool2d_backward.h>
// #include <ATen/ops/avg_pool3d.h>
// #include <ATen/ops/avg_pool3d_backward.h>
// #include <ATen/ops/baddbmm.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/batch_norm.h>
// #include <ATen/ops/batch_norm_backward_elemt.h>
// #include <ATen/ops/batch_norm_backward_reduce.h>
// #include <ATen/ops/batch_norm_elemt.h>
// #include <ATen/ops/batch_norm_gather_stats.h>
// #include <ATen/ops/batch_norm_gather_stats_with_counts.h>
// #include <ATen/ops/batch_norm_stats.h>
// #include <ATen/ops/batch_norm_update_stats.h>
// #include <ATen/ops/bernoulli.h>
// #include <ATen/ops/bilinear.h>
// #include <ATen/ops/binary_cross_entropy.h>
// #include <ATen/ops/binary_cross_entropy_backward.h>
// #include <ATen/ops/binary_cross_entropy_with_logits.h>
// #include <ATen/ops/bincount.h>
// #include <ATen/ops/binomial.h>
// #include <ATen/ops/bitwise_and.h>
// #include <ATen/ops/bitwise_left_shift.h>
// #include <ATen/ops/bitwise_not.h>
// #include <ATen/ops/bitwise_or.h>
// #include <ATen/ops/bitwise_right_shift.h>
// #include <ATen/ops/bitwise_xor.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/block_diag.h>
// #include <ATen/ops/bmm.h>
// #include <ATen/ops/broadcast_tensors.h>
// #include <ATen/ops/broadcast_to.h>
// #include <ATen/ops/bucketize.h>
// #include <ATen/ops/can_cast.h>
// #include <ATen/ops/cartesian_prod.h>
// #include <ATen/ops/cat.h>
// #include <ATen/ops/cauchy.h>
// #include <ATen/ops/ccol_indices.h>
// #include <ATen/ops/ccol_indices_copy.h>
// #include <ATen/ops/cdist.h>
// #include <ATen/ops/ceil.h>
// #include <ATen/ops/celu.h>
// #include <ATen/ops/chain_matmul.h>
// #include <ATen/ops/chalf.h>
// #include <ATen/ops/channel_shuffle.h>
// #include <ATen/ops/cholesky.h>
// #include <ATen/ops/cholesky_inverse.h>
// #include <ATen/ops/cholesky_solve.h>
// #include <ATen/ops/choose_qparams_optimized.h>
// #include <ATen/ops/chunk.h>
// #include <ATen/ops/clamp.h>
// #include <ATen/ops/clamp_max.h>
// #include <ATen/ops/clamp_min.h>
// #include <ATen/ops/clip.h>
// #include <ATen/ops/clone.h>
// #include <ATen/ops/coalesce.h>
// #include <ATen/ops/col2im.h>
// #include <ATen/ops/col_indices.h>
// #include <ATen/ops/col_indices_copy.h>
// #include <ATen/ops/column_stack.h>
// #include <ATen/ops/combinations.h>
// #include <ATen/ops/complex.h>
// #include <ATen/ops/concat.h>
// #include <ATen/ops/concatenate.h>
// #include <ATen/ops/conj.h>
// #include <ATen/ops/conj_physical.h>
// #include <ATen/ops/constant_pad_nd.h>
// #include <ATen/ops/contiguous.h>
// #include <ATen/ops/conv1d.h>
// #include <ATen/ops/conv2d.h>
// #include <ATen/ops/conv3d.h>
// #include <ATen/ops/conv_depthwise3d.h>
// #include <ATen/ops/conv_tbc.h>
// #include <ATen/ops/conv_tbc_backward.h>
// #include <ATen/ops/conv_transpose1d.h>
// #include <ATen/ops/conv_transpose2d.h>
// #include <ATen/ops/conv_transpose3d.h>
// #include <ATen/ops/convolution.h>
// #include <ATen/ops/convolution_backward.h>
// #include <ATen/ops/convolution_backward_overrideable.h>
// #include <ATen/ops/convolution_overrideable.h>
// #include <ATen/ops/copy.h>
// #include <ATen/ops/copy_sparse_to_sparse.h>
// #include <ATen/ops/copysign.h>
// #include <ATen/ops/corrcoef.h>
// #include <ATen/ops/cos.h>
// #include <ATen/ops/cosh.h>
// #include <ATen/ops/cosine_embedding_loss.h>
// #include <ATen/ops/cosine_similarity.h>
// #include <ATen/ops/count_nonzero.h>
// #include <ATen/ops/cov.h>
// #include <ATen/ops/cross.h>
// #include <ATen/ops/cross_entropy_loss.h>
// #include <ATen/ops/crow_indices.h>
// #include <ATen/ops/crow_indices_copy.h>
// #include <ATen/ops/ctc_loss.h>
// #include <ATen/ops/cudnn_affine_grid_generator.h>
// #include <ATen/ops/cudnn_affine_grid_generator_backward.h>
// #include <ATen/ops/cudnn_batch_norm.h>
// #include <ATen/ops/cudnn_batch_norm_backward.h>
// #include <ATen/ops/cudnn_convolution.h>
// #include <ATen/ops/cudnn_convolution_add_relu.h>
// #include <ATen/ops/cudnn_convolution_relu.h>
// #include <ATen/ops/cudnn_convolution_transpose.h>
// #include <ATen/ops/cudnn_grid_sampler.h>
// #include <ATen/ops/cudnn_grid_sampler_backward.h>
// #include <ATen/ops/cudnn_is_acceptable.h>
// #include <ATen/ops/cummax.h>
// #include <ATen/ops/cummaxmin_backward.h>
// #include <ATen/ops/cummin.h>
// #include <ATen/ops/cumprod.h>
// #include <ATen/ops/cumprod_backward.h>
// #include <ATen/ops/cumsum.h>
// #include <ATen/ops/cumulative_trapezoid.h>
// #include <ATen/ops/data.h>
// #include <ATen/ops/deg2rad.h>
// #include <ATen/ops/dense_dim.h>
// #include <ATen/ops/dequantize.h>
// #include <ATen/ops/det.h>
// #include <ATen/ops/detach.h>
// #include <ATen/ops/detach_copy.h>
// #include <ATen/ops/diag.h>
// #include <ATen/ops/diag_embed.h>
// #include <ATen/ops/diagflat.h>
// #include <ATen/ops/diagonal.h>
// #include <ATen/ops/diagonal_backward.h>
// #include <ATen/ops/diagonal_copy.h>
// #include <ATen/ops/diagonal_scatter.h>
// #include <ATen/ops/diff.h>
// #include <ATen/ops/digamma.h>
// #include <ATen/ops/dist.h>
// #include <ATen/ops/div.h>
// #include <ATen/ops/divide.h>
// #include <ATen/ops/dot.h>
// #include <ATen/ops/dropout.h>
// #include <ATen/ops/dsplit.h>
// #include <ATen/ops/dstack.h>
// #include <ATen/ops/einsum.h>
// #include <ATen/ops/elu.h>
// #include <ATen/ops/elu_backward.h>
// #include <ATen/ops/embedding.h>
// #include <ATen/ops/embedding_backward.h>
// #include <ATen/ops/embedding_bag.h>
// #include <ATen/ops/embedding_dense_backward.h>
// #include <ATen/ops/embedding_renorm.h>
// #include <ATen/ops/embedding_sparse_backward.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/empty_like.h>
// #include <ATen/ops/empty_permuted.h>
// #include <ATen/ops/empty_quantized.h>
// #include <ATen/ops/empty_strided.h>
// #include <ATen/ops/eq.h>
// #include <ATen/ops/equal.h>
// #include <ATen/ops/erf.h>
// #include <ATen/ops/erfc.h>
// #include <ATen/ops/erfinv.h>
// #include <ATen/ops/exp.h>
// #include <ATen/ops/exp2.h>
// #include <ATen/ops/expand.h>
// #include <ATen/ops/expand_as.h>
// #include <ATen/ops/expand_copy.h>
// #include <ATen/ops/expm1.h>
// #include <ATen/ops/exponential.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/fake_quantize_per_channel_affine.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation.h>
// #include <ATen/ops/fbgemm_linear_int8_weight.h>
// #include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation.h>
// #include <ATen/ops/fbgemm_linear_quantize_weight.h>
// #include <ATen/ops/fbgemm_pack_gemm_matrix_fp16.h>
// #include <ATen/ops/fbgemm_pack_quantized_matrix.h>
// #include <ATen/ops/feature_alpha_dropout.h>
// #include <ATen/ops/feature_dropout.h>
// #include <ATen/ops/fft_fft.h>
// #include <ATen/ops/fft_fft2.h>
// #include <ATen/ops/fft_fftfreq.h>
// #include <ATen/ops/fft_fftn.h>
// #include <ATen/ops/fft_fftshift.h>
// #include <ATen/ops/fft_hfft.h>
// #include <ATen/ops/fft_hfft2.h>
// #include <ATen/ops/fft_hfftn.h>
// #include <ATen/ops/fft_ifft.h>
// #include <ATen/ops/fft_ifft2.h>
// #include <ATen/ops/fft_ifftn.h>
// #include <ATen/ops/fft_ifftshift.h>
// #include <ATen/ops/fft_ihfft.h>
// #include <ATen/ops/fft_ihfft2.h>
// #include <ATen/ops/fft_ihfftn.h>
// #include <ATen/ops/fft_irfft.h>
// #include <ATen/ops/fft_irfft2.h>
// #include <ATen/ops/fft_irfftn.h>
// #include <ATen/ops/fft_rfft.h>
// #include <ATen/ops/fft_rfft2.h>
// #include <ATen/ops/fft_rfftfreq.h>
// #include <ATen/ops/fft_rfftn.h>
// #include <ATen/ops/fill.h>
// #include <ATen/ops/fill_diagonal.h>
// #include <ATen/ops/fix.h>
// #include <ATen/ops/flatten.h>
// #include <ATen/ops/flatten_dense_tensors.h>
// #include <ATen/ops/flip.h>
// #include <ATen/ops/fliplr.h>
// #include <ATen/ops/flipud.h>
// #include <ATen/ops/float_power.h>
// #include <ATen/ops/floor.h>
// #include <ATen/ops/floor_divide.h>
// #include <ATen/ops/fmax.h>
// #include <ATen/ops/fmin.h>
// #include <ATen/ops/fmod.h>
// #include <ATen/ops/frac.h>
// #include <ATen/ops/fractional_max_pool2d.h>
// #include <ATen/ops/fractional_max_pool2d_backward.h>
// #include <ATen/ops/fractional_max_pool3d.h>
// #include <ATen/ops/fractional_max_pool3d_backward.h>
// #include <ATen/ops/frexp.h>
// #include <ATen/ops/frobenius_norm.h>
// #include <ATen/ops/from_file.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full_like.h>
// #include <ATen/ops/fused_moving_avg_obs_fake_quant.h>
// #include <ATen/ops/gather.h>
// #include <ATen/ops/gather_backward.h>
// #include <ATen/ops/gcd.h>
// #include <ATen/ops/ge.h>
// #include <ATen/ops/gelu.h>
// #include <ATen/ops/gelu_backward.h>
// #include <ATen/ops/geometric.h>
// #include <ATen/ops/geqrf.h>
// #include <ATen/ops/ger.h>
// #include <ATen/ops/glu.h>
// #include <ATen/ops/glu_backward.h>
// #include <ATen/ops/glu_backward_jvp.h>
// #include <ATen/ops/glu_jvp.h>
// #include <ATen/ops/gradient.h>
// #include <ATen/ops/greater.h>
// #include <ATen/ops/greater_equal.h>
// #include <ATen/ops/grid_sampler.h>
// #include <ATen/ops/grid_sampler_2d.h>
// #include <ATen/ops/grid_sampler_2d_backward.h>
// #include <ATen/ops/grid_sampler_3d.h>
// #include <ATen/ops/grid_sampler_3d_backward.h>
// #include <ATen/ops/group_norm.h>
// #include <ATen/ops/gru.h>
// #include <ATen/ops/gru_cell.h>
// #include <ATen/ops/gt.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hardshrink.h>
// #include <ATen/ops/hardshrink_backward.h>
// #include <ATen/ops/hardsigmoid.h>
// #include <ATen/ops/hardsigmoid_backward.h>
// #include <ATen/ops/hardswish.h>
// #include <ATen/ops/hardswish_backward.h>
// #include <ATen/ops/hardtanh.h>
// #include <ATen/ops/hardtanh_backward.h>
// #include <ATen/ops/heaviside.h>
// #include <ATen/ops/hinge_embedding_loss.h>
// #include <ATen/ops/histc.h>
// #include <ATen/ops/histogram.h>
// #include <ATen/ops/histogramdd.h>
// #include <ATen/ops/hsplit.h>
// #include <ATen/ops/hspmm.h>
// #include <ATen/ops/hstack.h>
// #include <ATen/ops/huber_loss.h>
// #include <ATen/ops/huber_loss_backward.h>
// #include <ATen/ops/hypot.h>
// #include <ATen/ops/i0.h>
// #include <ATen/ops/igamma.h>
// #include <ATen/ops/igammac.h>
// #include <ATen/ops/im2col.h>
// #include <ATen/ops/imag.h>
// #include <ATen/ops/index.h>
// #include <ATen/ops/index_add.h>
// #include <ATen/ops/index_copy.h>
// #include <ATen/ops/index_fill.h>
// #include <ATen/ops/index_put.h>
// #include <ATen/ops/index_reduce.h>
// #include <ATen/ops/index_select.h>
// #include <ATen/ops/index_select_backward.h>
// #include <ATen/ops/indices.h>
// #include <ATen/ops/indices_copy.h>
// #include <ATen/ops/infinitely_differentiable_gelu_backward.h>
// #include <ATen/ops/inner.h>
// #include <ATen/ops/instance_norm.h>
// #include <ATen/ops/int_repr.h>
// #include <ATen/ops/inverse.h>
// #include <ATen/ops/is_coalesced.h>
// #include <ATen/ops/is_complex.h>
// #include <ATen/ops/is_conj.h>
// #include <ATen/ops/is_distributed.h>
// #include <ATen/ops/is_floating_point.h>
// #include <ATen/ops/is_inference.h>
// #include <ATen/ops/is_leaf.h>
// #include <ATen/ops/is_neg.h>
// #include <ATen/ops/is_nonzero.h>
// #include <ATen/ops/is_pinned.h>
// #include <ATen/ops/is_same_size.h>
// #include <ATen/ops/is_set_to.h>
// #include <ATen/ops/is_signed.h>
// #include <ATen/ops/is_vulkan_available.h>
// #include <ATen/ops/isclose.h>
// #include <ATen/ops/isfinite.h>
// #include <ATen/ops/isin.h>
// #include <ATen/ops/isinf.h>
// #include <ATen/ops/isnan.h>
// #include <ATen/ops/isneginf.h>
// #include <ATen/ops/isposinf.h>
// #include <ATen/ops/isreal.h>
// #include <ATen/ops/istft.h>
// #include <ATen/ops/item.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kl_div.h>
// #include <ATen/ops/kron.h>
// #include <ATen/ops/kthvalue.h>
// #include <ATen/ops/l1_loss.h>
// #include <ATen/ops/layer_norm.h>
// #include <ATen/ops/lcm.h>
// #include <ATen/ops/ldexp.h>
// #include <ATen/ops/le.h>
// #include <ATen/ops/leaky_relu.h>
// #include <ATen/ops/leaky_relu_backward.h>
// #include <ATen/ops/lerp.h>
// #include <ATen/ops/less.h>
// #include <ATen/ops/less_equal.h>
// #include <ATen/ops/lgamma.h>
// #include <ATen/ops/lift.h>
// #include <ATen/ops/lift_fresh.h>
// #include <ATen/ops/lift_fresh_copy.h>
// #include <ATen/ops/linalg_cholesky.h>
// #include <ATen/ops/linalg_cholesky_ex.h>
// #include <ATen/ops/linalg_cond.h>
// #include <ATen/ops/linalg_cross.h>
// #include <ATen/ops/linalg_det.h>
// #include <ATen/ops/linalg_diagonal.h>
// #include <ATen/ops/linalg_eig.h>
// #include <ATen/ops/linalg_eigh.h>
// #include <ATen/ops/linalg_eigvals.h>
// #include <ATen/ops/linalg_eigvalsh.h>
// #include <ATen/ops/linalg_householder_product.h>
// #include <ATen/ops/linalg_inv.h>
// #include <ATen/ops/linalg_inv_ex.h>
// #include <ATen/ops/linalg_ldl_factor.h>
// #include <ATen/ops/linalg_ldl_factor_ex.h>
// #include <ATen/ops/linalg_ldl_solve.h>
// #include <ATen/ops/linalg_lstsq.h>
// #include <ATen/ops/linalg_lu.h>
// #include <ATen/ops/linalg_lu_factor.h>
// #include <ATen/ops/linalg_lu_factor_ex.h>
// #include <ATen/ops/linalg_lu_solve.h>
// #include <ATen/ops/linalg_matmul.h>
// #include <ATen/ops/linalg_matrix_exp.h>
// #include <ATen/ops/linalg_matrix_norm.h>
// #include <ATen/ops/linalg_matrix_power.h>
// #include <ATen/ops/linalg_matrix_rank.h>
// #include <ATen/ops/linalg_multi_dot.h>
// #include <ATen/ops/linalg_norm.h>
// #include <ATen/ops/linalg_pinv.h>
// #include <ATen/ops/linalg_qr.h>
// #include <ATen/ops/linalg_slogdet.h>
// #include <ATen/ops/linalg_solve.h>
// #include <ATen/ops/linalg_solve_ex.h>
// #include <ATen/ops/linalg_solve_triangular.h>
// #include <ATen/ops/linalg_svd.h>
// #include <ATen/ops/linalg_svdvals.h>
// #include <ATen/ops/linalg_tensorinv.h>
// #include <ATen/ops/linalg_tensorsolve.h>
// #include <ATen/ops/linalg_vander.h>
// #include <ATen/ops/linalg_vecdot.h>
// #include <ATen/ops/linalg_vector_norm.h>
// #include <ATen/ops/linear.h>
// #include <ATen/ops/linear_backward.h>
// #include <ATen/ops/linspace.h>
// #include <ATen/ops/log.h>
// #include <ATen/ops/log10.h>
// #include <ATen/ops/log1p.h>
// #include <ATen/ops/log2.h>
// #include <ATen/ops/log_normal.h>
// #include <ATen/ops/log_sigmoid.h>
// #include <ATen/ops/log_sigmoid_backward.h>
// #include <ATen/ops/log_sigmoid_forward.h>
// #include <ATen/ops/log_softmax.h>
// #include <ATen/ops/logaddexp.h>
// #include <ATen/ops/logaddexp2.h>
// #include <ATen/ops/logcumsumexp.h>
// #include <ATen/ops/logdet.h>
// #include <ATen/ops/logical_and.h>
// #include <ATen/ops/logical_not.h>
// #include <ATen/ops/logical_or.h>
// #include <ATen/ops/logical_xor.h>
// #include <ATen/ops/logit.h>
// #include <ATen/ops/logit_backward.h>
// #include <ATen/ops/logspace.h>
// #include <ATen/ops/logsumexp.h>
// #include <ATen/ops/lshift.h>
// #include <ATen/ops/lstm.h>
// #include <ATen/ops/lstm_cell.h>
// #include <ATen/ops/lstm_mps_backward.h>
// #include <ATen/ops/lt.h>
// #include <ATen/ops/lu_solve.h>
// #include <ATen/ops/lu_unpack.h>
// #include <ATen/ops/mH.h>
// #include <ATen/ops/mT.h>
// #include <ATen/ops/margin_ranking_loss.h>
// #include <ATen/ops/masked_fill.h>
// #include <ATen/ops/masked_scatter.h>
// #include <ATen/ops/masked_select.h>
// #include <ATen/ops/masked_select_backward.h>
// #include <ATen/ops/matmul.h>
// #include <ATen/ops/matmul_backward.h>
// #include <ATen/ops/matrix_H.h>
// #include <ATen/ops/matrix_exp.h>
// #include <ATen/ops/matrix_exp_backward.h>
// #include <ATen/ops/matrix_power.h>
// #include <ATen/ops/max.h>
// #include <ATen/ops/max_pool1d.h>
// #include <ATen/ops/max_pool1d_with_indices.h>
// #include <ATen/ops/max_pool2d.h>
// #include <ATen/ops/max_pool2d_backward.h>
// #include <ATen/ops/max_pool2d_with_indices.h>
// #include <ATen/ops/max_pool2d_with_indices_backward.h>
// #include <ATen/ops/max_pool3d.h>
// #include <ATen/ops/max_pool3d_with_indices.h>
// #include <ATen/ops/max_pool3d_with_indices_backward.h>
// #include <ATen/ops/max_unpool2d.h>
// #include <ATen/ops/max_unpool3d.h>
// #include <ATen/ops/maximum.h>
// #include <ATen/ops/mean.h>
// #include <ATen/ops/median.h>
// #include <ATen/ops/meshgrid.h>
// #include <ATen/ops/min.h>
// #include <ATen/ops/minimum.h>
// #include <ATen/ops/miopen_batch_norm.h>
// #include <ATen/ops/miopen_batch_norm_backward.h>
// #include <ATen/ops/miopen_convolution.h>
// #include <ATen/ops/miopen_convolution_add_relu.h>
// #include <ATen/ops/miopen_convolution_relu.h>
// #include <ATen/ops/miopen_convolution_transpose.h>
// #include <ATen/ops/miopen_depthwise_convolution.h>
// #include <ATen/ops/miopen_rnn.h>
// #include <ATen/ops/miopen_rnn_backward.h>
// #include <ATen/ops/mish.h>
// #include <ATen/ops/mish_backward.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h>
// #include <ATen/ops/mkldnn_convolution.h>
// #include <ATen/ops/mkldnn_linear.h>
// #include <ATen/ops/mkldnn_linear_backward.h>
// #include <ATen/ops/mkldnn_linear_backward_input.h>
// #include <ATen/ops/mkldnn_linear_backward_weights.h>
// #include <ATen/ops/mkldnn_max_pool2d.h>
// #include <ATen/ops/mkldnn_max_pool2d_backward.h>
// #include <ATen/ops/mkldnn_max_pool3d.h>
// #include <ATen/ops/mkldnn_max_pool3d_backward.h>
// #include <ATen/ops/mkldnn_reorder_conv2d_weight.h>
// #include <ATen/ops/mkldnn_reorder_conv3d_weight.h>
// #include <ATen/ops/mkldnn_rnn_layer.h>
// #include <ATen/ops/mkldnn_rnn_layer_backward.h>
// #include <ATen/ops/mm.h>
// #include <ATen/ops/mode.h>
// #include <ATen/ops/moveaxis.h>
// #include <ATen/ops/movedim.h>
// #include <ATen/ops/mps_convolution_backward.h>
// #include <ATen/ops/mps_convolution_transpose_backward.h>
// #include <ATen/ops/mse_loss.h>
// #include <ATen/ops/mse_loss_backward.h>
// #include <ATen/ops/msort.h>
// #include <ATen/ops/mul.h>
// #include <ATen/ops/multi_margin_loss.h>
// #include <ATen/ops/multi_margin_loss_backward.h>
// #include <ATen/ops/multilabel_margin_loss.h>
// #include <ATen/ops/multilabel_margin_loss_backward.h>
// #include <ATen/ops/multilabel_margin_loss_forward.h>
// #include <ATen/ops/multinomial.h>
// #include <ATen/ops/multiply.h>
// #include <ATen/ops/mv.h>
// #include <ATen/ops/mvlgamma.h>
// #include <ATen/ops/nan_to_num.h>
// #include <ATen/ops/nanmean.h>
// #include <ATen/ops/nanmedian.h>
// #include <ATen/ops/nanquantile.h>
// #include <ATen/ops/nansum.h>
// #include <ATen/ops/narrow.h>
// #include <ATen/ops/narrow_copy.h>
// #include <ATen/ops/native_batch_norm.h>
// #include <ATen/ops/native_batch_norm_backward.h>
// #include <ATen/ops/native_channel_shuffle.h>
// #include <ATen/ops/native_dropout.h>
// #include <ATen/ops/native_dropout_backward.h>
// #include <ATen/ops/native_group_norm.h>
// #include <ATen/ops/native_group_norm_backward.h>
// #include <ATen/ops/native_layer_norm.h>
// #include <ATen/ops/native_layer_norm_backward.h>
// #include <ATen/ops/native_norm.h>
// #include <ATen/ops/ne.h>
// #include <ATen/ops/neg.h>
// #include <ATen/ops/negative.h>
// #include <ATen/ops/nested_to_padded_tensor.h>
// #include <ATen/ops/new_empty.h>
// #include <ATen/ops/new_empty_strided.h>
// #include <ATen/ops/new_full.h>
// #include <ATen/ops/new_ones.h>
// #include <ATen/ops/new_zeros.h>
// #include <ATen/ops/nextafter.h>
// #include <ATen/ops/nll_loss.h>
// #include <ATen/ops/nll_loss2d.h>
// #include <ATen/ops/nll_loss2d_backward.h>
// #include <ATen/ops/nll_loss2d_forward.h>
// #include <ATen/ops/nll_loss_backward.h>
// #include <ATen/ops/nll_loss_forward.h>
// #include <ATen/ops/nll_loss_nd.h>
// #include <ATen/ops/nonzero.h>
// #include <ATen/ops/nonzero_numpy.h>
// #include <ATen/ops/nonzero_static.h>
// #include <ATen/ops/norm.h>
// #include <ATen/ops/norm_except_dim.h>
// #include <ATen/ops/normal.h>
// #include <ATen/ops/not_equal.h>
// #include <ATen/ops/nuclear_norm.h>
// #include <ATen/ops/numpy_T.h>
// #include <ATen/ops/one_hot.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones_like.h>
// #include <ATen/ops/or.h>
// #include <ATen/ops/orgqr.h>
// #include <ATen/ops/ormqr.h>
// #include <ATen/ops/outer.h>
// #include <ATen/ops/output_nr.h>
// #include <ATen/ops/pad.h>
// #include <ATen/ops/pad_sequence.h>
// #include <ATen/ops/pairwise_distance.h>
// #include <ATen/ops/pdist.h>
// #include <ATen/ops/permute.h>
// #include <ATen/ops/permute_copy.h>
// #include <ATen/ops/pin_memory.h>
// #include <ATen/ops/pinverse.h>
// #include <ATen/ops/pixel_shuffle.h>
// #include <ATen/ops/pixel_unshuffle.h>
// #include <ATen/ops/poisson.h>
// #include <ATen/ops/poisson_nll_loss.h>
// #include <ATen/ops/polar.h>
// #include <ATen/ops/polygamma.h>
// #include <ATen/ops/positive.h>
// #include <ATen/ops/pow.h>
// #include <ATen/ops/prelu.h>
// #include <ATen/ops/prod.h>
// #include <ATen/ops/promote_types.h>
// #include <ATen/ops/put.h>
// #include <ATen/ops/q_per_channel_axis.h>
// #include <ATen/ops/q_per_channel_scales.h>
// #include <ATen/ops/q_per_channel_zero_points.h>
// #include <ATen/ops/q_scale.h>
// #include <ATen/ops/q_zero_point.h>
// #include <ATen/ops/qr.h>
// #include <ATen/ops/qscheme.h>
// #include <ATen/ops/quantile.h>
// #include <ATen/ops/quantize_per_channel.h>
// #include <ATen/ops/quantize_per_tensor.h>
// #include <ATen/ops/quantize_per_tensor_dynamic.h>
// #include <ATen/ops/quantized_batch_norm.h>
// #include <ATen/ops/quantized_gru_cell.h>
// #include <ATen/ops/quantized_lstm_cell.h>
// #include <ATen/ops/quantized_max_pool1d.h>
// #include <ATen/ops/quantized_max_pool2d.h>
// #include <ATen/ops/quantized_max_pool3d.h>
// #include <ATen/ops/quantized_rnn_relu_cell.h>
// #include <ATen/ops/quantized_rnn_tanh_cell.h>
// #include <ATen/ops/rad2deg.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand_like.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn_like.h>
// #include <ATen/ops/random.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/ravel.h>
// #include <ATen/ops/real.h>
// #include <ATen/ops/reciprocal.h>
// #include <ATen/ops/record_stream.h>
// #include <ATen/ops/refine_names.h>
// #include <ATen/ops/reflection_pad1d.h>
// #include <ATen/ops/reflection_pad1d_backward.h>
// #include <ATen/ops/reflection_pad2d.h>
// #include <ATen/ops/reflection_pad2d_backward.h>
// #include <ATen/ops/reflection_pad3d.h>
// #include <ATen/ops/reflection_pad3d_backward.h>
// #include <ATen/ops/relu.h>
// #include <ATen/ops/relu6.h>
// #include <ATen/ops/remainder.h>
// #include <ATen/ops/rename.h>
// #include <ATen/ops/renorm.h>
// #include <ATen/ops/repeat.h>
// #include <ATen/ops/repeat_interleave.h>
// #include <ATen/ops/replication_pad1d.h>
// #include <ATen/ops/replication_pad1d_backward.h>
// #include <ATen/ops/replication_pad2d.h>
// #include <ATen/ops/replication_pad2d_backward.h>
// #include <ATen/ops/replication_pad3d.h>
// #include <ATen/ops/replication_pad3d_backward.h>
// #include <ATen/ops/requires_grad.h>
// #include <ATen/ops/reshape.h>
// #include <ATen/ops/reshape_as.h>
// #include <ATen/ops/resize.h>
// #include <ATen/ops/resize_as.h>
// #include <ATen/ops/resize_as_sparse.h>
// #include <ATen/ops/resolve_conj.h>
// #include <ATen/ops/resolve_neg.h>
// #include <ATen/ops/result_type.h>
// #include <ATen/ops/retain_grad.h>
// #include <ATen/ops/retains_grad.h>
// #include <ATen/ops/rnn_relu.h>
// #include <ATen/ops/rnn_relu_cell.h>
// #include <ATen/ops/rnn_tanh.h>
// #include <ATen/ops/rnn_tanh_cell.h>
// #include <ATen/ops/roll.h>
// #include <ATen/ops/rot90.h>
// #include <ATen/ops/round.h>
// #include <ATen/ops/row_indices.h>
// #include <ATen/ops/row_indices_copy.h>
// #include <ATen/ops/row_stack.h>
// #include <ATen/ops/rrelu.h>
// #include <ATen/ops/rrelu_with_noise.h>
// #include <ATen/ops/rrelu_with_noise_backward.h>
// #include <ATen/ops/rshift.h>
// #include <ATen/ops/rsqrt.h>
// #include <ATen/ops/rsub.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/scaled_dot_product_attention.h>
// #include <ATen/ops/scatter.h>
// #include <ATen/ops/scatter_add.h>
// #include <ATen/ops/scatter_reduce.h>
// #include <ATen/ops/searchsorted.h>
// #include <ATen/ops/segment_reduce.h>
// #include <ATen/ops/select.h>
// #include <ATen/ops/select_backward.h>
// #include <ATen/ops/select_copy.h>
// #include <ATen/ops/select_scatter.h>
// #include <ATen/ops/selu.h>
// #include <ATen/ops/set.h>
// #include <ATen/ops/set_data.h>
// #include <ATen/ops/sgn.h>
// #include <ATen/ops/sigmoid.h>
// #include <ATen/ops/sigmoid_backward.h>
// #include <ATen/ops/sign.h>
// #include <ATen/ops/signbit.h>
// #include <ATen/ops/silu.h>
// #include <ATen/ops/silu_backward.h>
// #include <ATen/ops/sin.h>
// #include <ATen/ops/sinc.h>
// #include <ATen/ops/sinh.h>
// #include <ATen/ops/size.h>
// #include <ATen/ops/slice.h>
// #include <ATen/ops/slice_backward.h>
// #include <ATen/ops/slice_copy.h>
// #include <ATen/ops/slice_scatter.h>
// #include <ATen/ops/slogdet.h>
// #include <ATen/ops/slow_conv3d.h>
// #include <ATen/ops/slow_conv3d_forward.h>
// #include <ATen/ops/slow_conv_dilated2d.h>
// #include <ATen/ops/slow_conv_dilated3d.h>
// #include <ATen/ops/slow_conv_transpose2d.h>
// #include <ATen/ops/slow_conv_transpose3d.h>
// #include <ATen/ops/smm.h>
// #include <ATen/ops/smooth_l1_loss.h>
// #include <ATen/ops/smooth_l1_loss_backward.h>
// #include <ATen/ops/soft_margin_loss.h>
// #include <ATen/ops/soft_margin_loss_backward.h>
// #include <ATen/ops/softmax.h>
// #include <ATen/ops/softplus.h>
// #include <ATen/ops/softplus_backward.h>
// #include <ATen/ops/softshrink.h>
// #include <ATen/ops/softshrink_backward.h>
// #include <ATen/ops/sort.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_dim.h>
// #include <ATen/ops/sparse_mask.h>
// #include <ATen/ops/sparse_resize.h>
// #include <ATen/ops/sparse_resize_and_clear.h>
// #include <ATen/ops/sparse_sampled_addmm.h>
// #include <ATen/ops/special_airy_ai.h>
// #include <ATen/ops/special_bessel_j0.h>
// #include <ATen/ops/special_bessel_j1.h>
// #include <ATen/ops/special_bessel_y0.h>
// #include <ATen/ops/special_bessel_y1.h>
// #include <ATen/ops/special_chebyshev_polynomial_t.h>
// #include <ATen/ops/special_chebyshev_polynomial_u.h>
// #include <ATen/ops/special_chebyshev_polynomial_v.h>
// #include <ATen/ops/special_chebyshev_polynomial_w.h>
// #include <ATen/ops/special_digamma.h>
// #include <ATen/ops/special_entr.h>
// #include <ATen/ops/special_erf.h>
// #include <ATen/ops/special_erfc.h>
// #include <ATen/ops/special_erfcx.h>
// #include <ATen/ops/special_erfinv.h>
// #include <ATen/ops/special_exp2.h>
// #include <ATen/ops/special_expit.h>
// #include <ATen/ops/special_expm1.h>
// #include <ATen/ops/special_gammainc.h>
// #include <ATen/ops/special_gammaincc.h>
// #include <ATen/ops/special_gammaln.h>
// #include <ATen/ops/special_hermite_polynomial_h.h>
// #include <ATen/ops/special_hermite_polynomial_he.h>
// #include <ATen/ops/special_i0.h>
// #include <ATen/ops/special_i0e.h>
// #include <ATen/ops/special_i1.h>
// #include <ATen/ops/special_i1e.h>
// #include <ATen/ops/special_laguerre_polynomial_l.h>
// #include <ATen/ops/special_legendre_polynomial_p.h>
// #include <ATen/ops/special_log1p.h>
// #include <ATen/ops/special_log_ndtr.h>
// #include <ATen/ops/special_log_softmax.h>
// #include <ATen/ops/special_logit.h>
// #include <ATen/ops/special_logsumexp.h>
// #include <ATen/ops/special_modified_bessel_i0.h>
// #include <ATen/ops/special_modified_bessel_i1.h>
// #include <ATen/ops/special_modified_bessel_k0.h>
// #include <ATen/ops/special_modified_bessel_k1.h>
// #include <ATen/ops/special_multigammaln.h>
// #include <ATen/ops/special_ndtr.h>
// #include <ATen/ops/special_ndtri.h>
// #include <ATen/ops/special_polygamma.h>
// #include <ATen/ops/special_psi.h>
// #include <ATen/ops/special_round.h>
// #include <ATen/ops/special_scaled_modified_bessel_k0.h>
// #include <ATen/ops/special_scaled_modified_bessel_k1.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_u.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_v.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_w.h>
// #include <ATen/ops/special_sinc.h>
// #include <ATen/ops/special_softmax.h>
// #include <ATen/ops/special_spherical_bessel_j0.h>
// #include <ATen/ops/special_xlog1py.h>
// #include <ATen/ops/special_xlogy.h>
// #include <ATen/ops/special_zeta.h>
// #include <ATen/ops/split.h>
// #include <ATen/ops/split_copy.h>
// #include <ATen/ops/split_with_sizes.h>
// #include <ATen/ops/split_with_sizes_copy.h>
// #include <ATen/ops/sqrt.h>
// #include <ATen/ops/square.h>
// #include <ATen/ops/squeeze.h>
// #include <ATen/ops/squeeze_copy.h>
// #include <ATen/ops/sspaddmm.h>
// #include <ATen/ops/stack.h>
// #include <ATen/ops/std.h>
// #include <ATen/ops/std_mean.h>
// #include <ATen/ops/stft.h>
// #include <ATen/ops/stride.h>
// #include <ATen/ops/sub.h>
// #include <ATen/ops/subtract.h>
// #include <ATen/ops/sum.h>
// #include <ATen/ops/sum_to_size.h>
// #include <ATen/ops/svd.h>
// #include <ATen/ops/swapaxes.h>
// #include <ATen/ops/swapdims.h>
// #include <ATen/ops/sym_constrain_range.h>
// #include <ATen/ops/sym_constrain_range_for_size.h>
// #include <ATen/ops/sym_numel.h>
// #include <ATen/ops/sym_size.h>
// #include <ATen/ops/sym_storage_offset.h>
// #include <ATen/ops/sym_stride.h>
// #include <ATen/ops/t.h>
// #include <ATen/ops/t_copy.h>
// #include <ATen/ops/take.h>
// #include <ATen/ops/take_along_dim.h>
// #include <ATen/ops/tan.h>
// #include <ATen/ops/tanh.h>
// #include <ATen/ops/tanh_backward.h>
// #include <ATen/ops/tensor_split.h>
// #include <ATen/ops/tensordot.h>
// #include <ATen/ops/thnn_conv2d.h>
// #include <ATen/ops/threshold.h>
// #include <ATen/ops/threshold_backward.h>
// #include <ATen/ops/tile.h>
// #include <ATen/ops/to.h>
// #include <ATen/ops/to_dense.h>
// #include <ATen/ops/to_dense_backward.h>
// #include <ATen/ops/to_mkldnn.h>
// #include <ATen/ops/to_mkldnn_backward.h>
// #include <ATen/ops/to_padded_tensor.h>
// #include <ATen/ops/to_sparse.h>
// #include <ATen/ops/to_sparse_bsc.h>
// #include <ATen/ops/to_sparse_bsr.h>
// #include <ATen/ops/to_sparse_csc.h>
// #include <ATen/ops/to_sparse_csr.h>
// #include <ATen/ops/topk.h>
// #include <ATen/ops/trace.h>
// #include <ATen/ops/trace_backward.h>
// #include <ATen/ops/transpose.h>
// #include <ATen/ops/transpose_copy.h>
// #include <ATen/ops/trapezoid.h>
// #include <ATen/ops/trapz.h>
// #include <ATen/ops/triangular_solve.h>
// #include <ATen/ops/tril.h>
// #include <ATen/ops/tril_indices.h>
// #include <ATen/ops/triplet_margin_loss.h>
// #include <ATen/ops/triu.h>
// #include <ATen/ops/triu_indices.h>
// #include <ATen/ops/true_divide.h>
// #include <ATen/ops/trunc.h>
// #include <ATen/ops/type_as.h>
// #include <ATen/ops/unbind.h>
// #include <ATen/ops/unbind_copy.h>
// #include <ATen/ops/unflatten.h>
// #include <ATen/ops/unflatten_dense_tensors.h>
// #include <ATen/ops/unfold.h>
// #include <ATen/ops/unfold_backward.h>
// #include <ATen/ops/unfold_copy.h>
// #include <ATen/ops/uniform.h>
// #include <ATen/ops/unique_consecutive.h>
// #include <ATen/ops/unique_dim.h>
// #include <ATen/ops/unique_dim_consecutive.h>
// #include <ATen/ops/unsafe_chunk.h>
// #include <ATen/ops/unsafe_split.h>
// #include <ATen/ops/unsafe_split_with_sizes.h>
// #include <ATen/ops/unsqueeze.h>
// #include <ATen/ops/unsqueeze_copy.h>
// #include <ATen/ops/upsample_bicubic2d.h>
// #include <ATen/ops/upsample_bicubic2d_backward.h>
// #include <ATen/ops/upsample_bilinear2d.h>
// #include <ATen/ops/upsample_bilinear2d_backward.h>
// #include <ATen/ops/upsample_linear1d.h>
// #include <ATen/ops/upsample_linear1d_backward.h>
// #include <ATen/ops/upsample_nearest1d.h>
// #include <ATen/ops/upsample_nearest1d_backward.h>
// #include <ATen/ops/upsample_nearest2d.h>
// #include <ATen/ops/upsample_nearest2d_backward.h>
// #include <ATen/ops/upsample_nearest3d.h>
// #include <ATen/ops/upsample_nearest3d_backward.h>
// #include <ATen/ops/upsample_trilinear3d.h>
// #include <ATen/ops/upsample_trilinear3d_backward.h>
// #include <ATen/ops/value_selecting_reduction_backward.h>
// #include <ATen/ops/values.h>
// #include <ATen/ops/values_copy.h>
// #include <ATen/ops/vander.h>
// #include <ATen/ops/var.h>
// #include <ATen/ops/var_mean.h>
// #include <ATen/ops/vdot.h>
// #include <ATen/ops/view.h>
// #include <ATen/ops/view_as.h>
// #include <ATen/ops/view_as_complex.h>
// #include <ATen/ops/view_as_complex_copy.h>
// #include <ATen/ops/view_as_real.h>
// #include <ATen/ops/view_as_real_copy.h>
// #include <ATen/ops/view_copy.h>
// #include <ATen/ops/vsplit.h>
// #include <ATen/ops/vstack.h>
// #include <ATen/ops/where.h>
// #include <ATen/ops/xlogy.h>
// #include <ATen/ops/xor.h>
// #include <ATen/ops/zero.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/zeros_like.h>



// Special C++ only overloads for std()-like functions (See gh-40287)
// These are needed because int -> bool conversion takes precedence over int -> IntArrayRef
// So, for example std(0) would select the std(unbiased=False) overload
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal T_TensorTensor_T var_mean(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal T_TensorTensor_T std_mean(@Const @ByRef Tensor self, int dim);

@Namespace("at") public static native @Cast("int64_t") long numel(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);



@Namespace("at") public static native @Cast("bool") boolean is_floating_point(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_signed(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_inference(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean _is_zerotensor(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @ByVal Tensor conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_neg(@Const @ByRef Tensor tensor);




// Parsed from ATen/ExpandUtils.h

// #pragma once

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/view.h>
// #include <ATen/ops/view_copy.h>
// #endif

// #include <ATen/Tensor.h>
// #include <ATen/core/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/irange.h>

// #include <functional>
// #include <sstream>
// #include <tuple>
// #include <utility>

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal LongArrayRef a, @ByVal LongArrayRef b);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... b);
@Namespace("at") public static native @ByVal SymIntVector infer_size_symint(
    @ByVal SymIntArrayRef a,
    @ByVal SymIntArrayRef b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal LongArrayRef a, @ByVal LongArrayRef b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... b);
@Namespace("at") public static native @ByVal SymDimVector infer_size_symdimvector(@ByVal SymIntArrayRef a, @ByVal SymIntArrayRef b);
// Targeting ../DimVectorInferExpandGeometryResult.java



@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal LongArrayRef tensor_sizes,
    @ByVal LongArrayRef tensor_strides,
    @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal LongArrayRef tensor_sizes,
    @ByVal LongArrayRef tensor_strides,
    @ByVal LongArrayRef sizes);
@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal LongArrayRef tensor_sizes,
    @ByVal LongArrayRef tensor_strides);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... tensor_strides);

// True if input shapes are expandable
// NOTE: infer_size did a similar check, please keep them sync if change is
// needed
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal LongArrayRef shape1, @ByVal LongArrayRef shape2);
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shape1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... shape2);

// avoid copy-construction of Tensor by using a reference_wrapper.

// NOTE [ ExpandUtils Borrowing ]
//
// Functions in ExpandUtils return `c10::MaybeOwned<Tensor>` because
// expansion may not actually be needed, in which case we can improve
// efficiency by returning
// `c10::MaybeOwned<Tensor>::borrowed(to_expand)`. However, this means
// that you need to be careful: the returned `c10::MaybeOwned<Tensor>`
// must not outlive the original `Tensor` object that `to_expand`
// referred to! The deleted rvalue reference overloads of these
// functions help with this by preventing trivial use of a temporary
// resulting from a function call, but it is still possible to make a
// mistake.

@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand);



@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand,
    String api_name);



@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    String api_name);





// See NOTE [ ExpandUtils Borrowing ] above for `MaybeOwned` explanation.
@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwned_T expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    String api_name);





@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwned_T expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3);









@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwned_T expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal T_TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwned_T expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3,
    String api_name);









@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal LongArrayRef sizes);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);



@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal LongArrayRef sizes,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    String api_name);



@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector expand_outplace(@ByVal TensorArrayRef to_expand);
@Namespace("at") public static native @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector expand_outplace(@ByVal TensorVector to_expand);

@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal SymIntArrayRef shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal SymIntArrayRef shape);

// Sums `tensor` repeatedly to produce a tensor of shape `shape`.
// Precondition: is_expandable_to(shape, tensor.sizes()) must be true
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal LongArrayRef shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... shape);

@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(
    @ByVal SymIntArrayRef shape,
    @ByVal SymIntArrayRef desired);

@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal LongArrayRef shape, @ByVal LongArrayRef desired);
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] shape, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... desired);

 // namespace at


// Parsed from ATen/MemoryOverlap.h

// #pragma once

// #include <c10/macros/Export.h>


// MemOverlap: Whether or not there is memory overlap
//
// No: Absolutely no memory overlap
// Yes: Absolutely yes memory overlap
// TooHard: There might be memory overlap, but it was too expensive to compute.
//
// NB: Please update the python test for these if you renumber them.
@Namespace("at") public enum MemOverlap { No(0), Yes(1), TooHard(2);

    public final int value;
    private MemOverlap(int v) { this.value = v; }
    private MemOverlap(MemOverlap e) { this.value = e.value; }
    public MemOverlap intern() { for (MemOverlap e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public enum MemOverlapStatus { Full(0), Partial(1), No(2), TooHard(3);

    public final int value;
    private MemOverlapStatus(int v) { this.value = v; }
    private MemOverlapStatus(MemOverlapStatus e) { this.value = e.value; }
    public MemOverlapStatus intern() { for (MemOverlapStatus e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public static native MemOverlap has_internal_overlap(@Const @ByRef TensorBase t);
@Namespace("at") public static native MemOverlap has_internal_overlap(TensorImpl t);

@Namespace("at") public static native void assert_no_internal_overlap(@Const @ByRef TensorBase t);
@Namespace("at") public static native void assert_no_internal_overlap(TensorImpl t);

@Namespace("at") public static native MemOverlapStatus get_overlap_status(@Const @ByRef TensorBase a, @Const @ByRef TensorBase b);
@Namespace("at") public static native MemOverlapStatus get_overlap_status(@Const TensorImpl a, @Const TensorImpl b);

@Namespace("at") public static native void assert_no_partial_overlap(
    @Const @ByRef TensorBase a,
    @Const @ByRef TensorBase b);


@Namespace("at") public static native void assert_no_overlap(@Const @ByRef TensorBase a, @Const @ByRef TensorBase b);
@Namespace("at") public static native void assert_no_overlap(TensorImpl a, TensorImpl b);

 // namespace at


// Parsed from ATen/NestedTensorImpl.h

// #pragma once
// #include <ATen/MemoryOverlap.h>
// #include <ATen/Tensor.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/irange.h>
@Namespace("at::native") public static native @Cast("bool") boolean nested_tensor_impl_is_contiguous(@Const NestedTensorImpl nt);

// Targeting ../NestedTensorImpl.java



@Namespace("at::native") public static native NestedTensorImpl get_nested_tensor_impl_or_null(
    @Const @ByRef Tensor tensor);

@Namespace("at::native") public static native NestedTensorImpl get_nested_tensor_impl(@Const @ByRef Tensor tensor);

@Namespace("at::native") public static native @Const @ByRef Tensor get_nested_sizes(@Const @ByRef Tensor tensor);

 // namespace native
 // namespace at


// Parsed from torch/csrc/autograd/input_metadata.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <ATen/NestedTensorImpl.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Device.h>
// #include <c10/core/DeviceType.h>
// #include <c10/core/Stream.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>
// #include <c10/util/variant.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/zeros.h>
// #endif

// #include <cstdint>
// #include <utility>

/**
 * Records TensorOptions, shape of the tensor, whether or not the Python
 * dispatch key is set (tensor subclass), and, where applicable, the stream the
 * corresponding operation took place on.
 *
 * If is_valid() is false, then the corresponding input is not used and may be
 * an undefined tensor.
 */
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/saved_variable_hooks.h

// #pragma once

// #include <ATen/core/Tensor.h>
// Targeting ../SavedVariableHooks.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/saved_variable.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/forward_grad.h>
// #include <torch/csrc/autograd/saved_variable_hooks.h>

// #include <ATen/core/Tensor.h>

// #include <cstdint>
// #include <memory>

@Namespace("torch::autograd") public static native @Cast("const char*") BytePointer ERR_BACKWARD_TWICE(); public static native void ERR_BACKWARD_TWICE(BytePointer setter);

/** A snapshot of a variable at a certain version. A {@code SavedVariable} stores
 *  enough information to reconstruct a variable from a certain point in time. */
 // namespace autograd
 // namespace torch


// Parsed from ATen/core/Variadic.h

// #pragma once

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>

// #include <c10/util/ArrayRef.h>
// #include <ATen/core/List.h>

// This class allows you to write variadic functions which
// call a (possibly overloaded) function on each argument,
// in order.  This is most commonly used in autogenerated code,
// where it is convenient to have a function that can uniformly
// take arguments of different types.  If your arguments
// are homogenous consider using a std::initializer_list instead.
//
// For examples of this in use, see torch/csrc/utils/variadic.h

 // namespace torch


// Parsed from torch/csrc/utils/variadic.h

// #pragma once

// #include <ATen/core/Tensor.h>
// #include <ATen/core/Variadic.h>
// #include <torch/csrc/autograd/variable.h>

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>

//===----------------------------------------------------------------------===//
//                std::index_sequence shim for C++11
//===----------------------------------------------------------------------===//

// A container of type-template parameter indices.

// Decrements the index N, adds N-1 to the list of indices and forwards
// whatever we already have.

// Partial specialization that forms our base case. When N is zero, we stop
// and define a typedef that will be visible to earlier classes due to
// inheritance. The typedef we define is an index list containing the numbers
// 0 through N-1.

//===----------------------------------------------------------------------===//
//                                 Utilities
//===----------------------------------------------------------------------===//
 // namespace detail

 // namespace torch


// Parsed from ATen/SequenceNumber.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <cstdint>

// A simple thread local enumeration, used to link forward and backward pass
// ops and is used by autograd and observers framework

@Namespace("at::sequence_number") public static native @Cast("uint64_t") long peek();
@Namespace("at::sequence_number") public static native @Cast("uint64_t") long get_and_increment();

 // namespace sequence_number
 // namespace at


// Parsed from torch/csrc/autograd/function.h

// #pragma once

// #include <torch/csrc/autograd/anomaly_mode.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/graph_task.h>
// #include <torch/csrc/autograd/input_metadata.h>
// #include <torch/csrc/autograd/saved_variable.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/python_stub.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/SequenceNumber.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/record_function.h>
// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <algorithm>
// #include <cstdint>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// Custom deleter to prevent stack overflows.
@Namespace("torch::autograd") public static native void deleteNode(Node function);

// Guard that sets and restores the evaluating node

// Return the Node currently being evaluated (if any)
// This is only set during the backward pass while a Node is being
// executed.
@Namespace("torch::autograd") public static native @SharedPtr Node get_current_node();
// Targeting ../Node.java


// Targeting ../TraceableFunction.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                       Associated Free Nodes
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Implementation of `collect_next_edges` (see below).
 // namespace detail

/** Create an {@code Edge} between the given {@code variable} and the {@code function}, which is
 *  assumed to be the gradient function of this variable (i.e. the function
 *  through which this variable is backpropagated during the backward pass).
 *  This sets the {@code grad_fn} property of the {@code variable}. This function assumes
 *  that the {@code Variable} is a new input to the gradient function and its
 *  {@code input_nr} thus equal to {@code function->num_inputs()}. Additionally, it
 *  increments the {@code Node}'s number of inputs by one. Approximately
 *  equivalent to {@code variable.set_gradient_edge(function,
 *  function->add_input_metadata(variable.dispatch_type(), variable.sizes()))}.
 *  If you don't want the {@code Node}'s {@code num_inputs} to be incremented, use
 *  {@code set_gradient_edge} directly. */
@Namespace("torch::autograd") public static native void create_gradient_edge(
    @Cast("torch::autograd::Variable*") @ByRef Tensor variable,
    @SharedPtr Node function);

/** Return true if any of the variables in the list require a gradient. */
@Namespace("torch::autograd") public static native @Cast("bool") boolean any_variable_requires_grad(@Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector variables);

/** Return the next edges of all the given variables, or tuples of variables. */

 // namespace autograd
 // namespace torch



// Parsed from torch/csrc/autograd/custom_function.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/core/SymInt.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/irange.h>
// #include <torch/csrc/autograd/function.h>
// #include <torch/csrc/autograd/variable.h>
// #include <vector>



@Namespace("torch::autograd") public static native void check_variable_result(
    @Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result,
    @StdString BytePointer hook_name);
@Namespace("torch::autograd") public static native void check_variable_result(
    @Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result,
    @StdString String hook_name);

// Get the return type of the forward function of the custom Function class X

///
///
///
///
///
// Targeting ../FunctionCrossMapLRN2d.java


// Targeting ../AutogradContext.java


// Targeting ../VariableInfo.java



// CppNode<T> is the Node in the autograd graph that represents the user defined
// backward function for Function<T>. Calls to CppNode::apply are forward to
// T::backward().

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@Cast("torch::autograd::Variable*") @ByRef Tensor output);

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@ByRef TensorVector output);



// The logic here is the same as PyNode::apply, so changes to it should be done
// in both the places








 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/api/include/torch/autograd.h

// #pragma once

// #include <torch/csrc/autograd/autograd.h>
// #include <torch/csrc/autograd/autograd_not_implemented_fallback.h>
// #include <torch/csrc/autograd/custom_function.h>


// Parsed from torch/csrc/api/include/torch/cuda.h

// #pragma once

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <cstdint>

/** Returns the number of CUDA devices available. */
@Namespace("torch::cuda") public static native @Cast("size_t") @Name("device_count") long cuda_device_count();

/** Returns true if at least one CUDA device is available. */
@Namespace("torch::cuda") public static native @Cast("bool") @Name("is_available") boolean cuda_is_available();

/** Returns true if CUDA is available, and CuDNN is available. */
@Namespace("torch::cuda") public static native @Cast("bool") boolean cudnn_is_available();

/** Sets the seed for the current GPU. */
@Namespace("torch::cuda") public static native @Name("manual_seed") void cuda_manual_seed(@Cast("uint64_t") long seed);

/** Sets the seed for all available GPUs. */
@Namespace("torch::cuda") public static native @Name("manual_seed_all") void cuda_manual_seed_all(@Cast("uint64_t") long seed);

/** Waits for all kernels in all streams on a CUDA device to complete. */
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize(@Cast("int64_t") long device_index/*=-1*/);
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize();

 // namespace cuda
 // namespace torch


// Parsed from torch/csrc/api/include/torch/arg.h

// #pragma once

// #include <utility>

// #define TORCH_ARG(T, name)
//  public:
//   inline auto name(const T& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = new_##name;
//     return *this;
//   }
//   inline auto name(T&& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = std::move(new_##name);
//     return *this;
//   }
//   inline const T& name() const noexcept { /* NOLINT */
//     return this->name##_;
//   }
//   inline T& name() noexcept { /* NOLINT */
//     return this->name##_;
//   }
// 
//  private:
//   T name##_ /* NOLINT */


// Parsed from ATen/Device.h

// #pragma once
// #include <c10/core/Device.h>


// Parsed from ATen/Dispatch.h

// #pragma once

// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/complex.h>
// #include <c10/util/string_view.h>

// #ifdef __CUDACC__
// #include <cuda.h> // For CUDA_VERSION
// #endif

// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #else
/**
 * The method should_include_kernel_dtype() returns true/false
 * based on whether the switching code for a specific dtype should be
 * included based on build time constants generated from tracing model
 * execution. This method will be implmeneted via code-generation and
 * included in this file when code-gen is ready.
 */
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
    @Cast("const char*") BytePointer arg0,
    ScalarType arg1
);
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
    String arg0,
    ScalarType arg1
);
 // namespace at
// #endif

/**
 * In the Facebook internal build (using BUCK), this macro is enabled by
 * passing in -c pt.enable_record_kernel_dtype=1 when building the tracer
 * binary.
 */
// #if defined ENABLE_RECORD_KERNEL_FUNCTION_DTYPE
// #else
// #define RECORD_KERNEL_FUNCTION_DTYPE(NAME, enum_type)
// #endif

// #define AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type)
//   do {
//     if constexpr (!at::should_include_kernel_dtype(
//                       at_dispatch_name, enum_type)) {
//       AT_ERROR(
//           "dtype '",
//           toString(enum_type),
//           "' not selected for kernel tag ",
//           at_dispatch_name);
//     }
//   } while (0)

// #define AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, HINT, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using HINT C10_UNUSED = c10::impl::ScalarTypeToCPPTypeT<enum_type>;
//     return __VA_ARGS__();
//   }

// #define AT_DISPATCH_CASE(enum_type, ...)
//   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)

// #define AT_DISPATCH_CASE_QINT(enum_type, scalar_type, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using scalar_t = scalar_type;
//     using underlying_t C10_UNUSED = typename scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED = toUnderlying(enum_type);
//     return __VA_ARGS__();
//   }

// #define AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//     enum_type, scalar_type, bitwidth, qmin, qmax, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using scalar_t = scalar_type;
//     using underlying_t C10_UNUSED = typename scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED = toUnderlying(enum_type);
//     C10_UNUSED int bit_width = bitwidth;
//     C10_UNUSED int64_t quant_min = qmin;
//     C10_UNUSED int64_t quant_max = qmax;
//     return __VA_ARGS__();
//   }

@Namespace("detail") public static native ScalarType scalar_type(ScalarType s);







 // namespace detail

// The AT_DISPATCH_* family of macros provides the ability to
// conveniently generate specializations of a kernel over all of the
// dtypes we care about in PyTorch.  We call it "dispatch" because
// we are "dispatching" to the correct, dtype-specific kernel.
//
// A standard usage looks like:
//
//      AT_DISPATCH_ALL_TYPES(self.scalar_type(), "op_name", [&] {
//          // Your code here, with 'scalar_t' now defined to
//          // be the dtype in question
//      });
//
// There are many variations of this macro, so it's important to
// understand exactly /which/ dtypes you want to get instantiated, as
// well as what the "default" set is.
//
// The default set of dtypes that are instantiated (e.g., by
// AT_DISPATCH_ALL_TYPES) are floating point types (float, double),
// and integral types (int32_t, int64_t, int16_t, int8_t, uint8_t),
// but NOT booleans (bool), half-precision floats (Half) or
// complex number (c10::complex<float>, c10::complex<double>).
// This "cut" is somewhat historical (the default types are the
// ones that TH historically supported), but it also reflects the
// fact that the non-default types are "poorly" behaved (booleans
// are NOT integers mod 2, half precision operations ~essentially
// don't exist on CPU, complex numbers are an experimental application).
//
// Here are the questions you should generally ask to decide which
// dispatch you want:
//
// 1. Is this an integral or floating point specific operation?
//    (If so, you'll want one of the FLOATING or INTEGRAL macros.)
//
// 2. Should half be supported?  (If you're on CPU, the answer is almost
//    definitely no.  If you do want support, use one of the AND_HALF
//    macros)
//
// Much rarer situations:
//
// 3. Should bool be supported?  (You often have to write your kernel
//    differently if arithmetic operations are involved.)  If so,
//    Use AT_DISPATCH_ALL_TYPES_AND along with ScalarType::Bool
//
// 4. Should complex be supported?  The answer is almost always no,
//    unless you are working on "generic" code that should work on
//    all dtypes.
//
// Parameters:
// -----------
//
// 1. The NAME argument is a "tag" that is used to trace and then
//    conditionally compile fragments of the case statements such
//    that the kernel functions are specialized only for the dtypes
//    that are needed. The NAME parameter *must* be a build time
//    const char* (can't be std::string, etc...)
//
// Please ensure that the NAME is unique for every implementation
// or you run the risk of over-including code for the kernel
// functions. There is no risk of missing out on any code, so
// it's mostly a risk of a Type-2 error, and not a Type-1 error.
//
// Switch-like syntax:
// -------------------
// There is also a switch-case like syntax which is useful if a kernel
// needs to be specialized for particular scalar types
//
//      AT_DISPATCH_SWITCH(self.scalar_type(), "op_name",
//          AT_DISPATCH_CASE_INTEGRAL_TYPES([&] {
//            op_integral<scalar_t>(iter);
//          })
//          AT_DISPATCH_CASE_FLOATING_TYPES([&] {
//            op_floating<scalar_t>(iter);
//          })
//          AT_DISPATCH_CASE(kBool, [&] {
//            op_bool(iter);
//          })
//      );
//
// For each AT_DISPATCH_FOO macro, there is a corresponding
// AT_DISPATCH_CASE_FOO macro which can be used inside of an
// AT_DISPATCH_SWITCH block.

// NB: the the_type variable is not used, but we have kept it for
// backwards compatibility.  It's probably not used by anyone though;
// but we're just being safe (and it doesn't hurt.)  Note we must
// use it to shut up warnings about unused store.

// #define AT_DISPATCH_SWITCH(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     constexpr const char* at_dispatch_name = NAME;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(at_dispatch_name, _st);
//     switch (_st) {
//       __VA_ARGS__
//       default:
//         AT_ERROR(
//             '"',
//             at_dispatch_name,
//             "\" not implemented for '",
//             toString(_st),
//             "'");
//     }
//   }()

// #define AT_DISPATCH_CASE_FLOATING_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(...)
//   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND_HALF(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))

// #define AT_DISPATCH_CASE_REDUCED_FLOATING_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)

// #define AT_DISPATCH_REDUCED_FLOATING_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_REDUCED_FLOATING_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND4(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, __VA_ARGS__))

// #define AT_DISPATCH_CASE_COMPLEX_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::ComplexDouble, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::ComplexFloat, __VA_ARGS__)

// #define AT_DISPATCH_COMPLEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_COMPLEX_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_COMPLEX_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_COMPLEX_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND1(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
//     SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND1(
//           SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND4(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, __VA_ARGS__))

// #define AT_DISPATCH_CASE_INTEGRAL_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::Byte, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Char, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Int, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Long, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Short, __VA_ARGS__)

// #define AT_DISPATCH_INTEGRAL_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_INTEGRAL_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_INTEGRAL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_INTEGRAL_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES(...)
//   AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_TYPES(...)
//   AT_DISPATCH_CASE_QINT(at::kQInt8, at::qint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQUInt8, at::quint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQInt32, at::qint32, __VA_ARGS__)

// #define AT_DISPATCH_QINT_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_QINT_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_QINT_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_QINT_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_QINT_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_BYTE_TYPES(...)
//   AT_DISPATCH_CASE_QINT(at::kQInt8, at::qint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQUInt8, at::quint8, __VA_ARGS__)

// #define AT_DISPATCH_QINT_BYTE_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_QINT_BYTE_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_AND_SUB_BYTE_TYPES(...)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQInt8, at::qint8, CHAR_BIT, SCHAR_MIN, SCHAR_MAX, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt8, at::quint8, CHAR_BIT, 0, UCHAR_MAX, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQInt32,
//       at::qint32,
//       CHAR_BIT * sizeof(int),
//       INT_MIN,
//       INT_MAX,
//       __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt4x2, at::quint4x2, 4, 0, 15, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt2x4, at::quint2x4, 2, 0, 3, __VA_ARGS__)

// #define AT_DISPATCH_QINT_AND_SUB_BYTE_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_QINT_AND_SUB_BYTE_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND4(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND5(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, SCALARTYPE5, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE5, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND5(
//     SCALARTYPE1,
//     SCALARTYPE2,
//     SCALARTYPE3,
//     SCALARTYPE4,
//     SCALARTYPE5,
//     TYPE,
//     NAME,
//     ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND5(
//           SCALARTYPE1,
//           SCALARTYPE2,
//           SCALARTYPE3,
//           SCALARTYPE4,
//           SCALARTYPE5,
//           __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND6(
//     SCALARTYPE1,
//     SCALARTYPE2,
//     SCALARTYPE3,
//     SCALARTYPE4,
//     SCALARTYPE5,
//     SCALARTYPE6,
//     ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE5, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE6, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND6(
//     SCALARTYPE1,
//     SCALARTYPE2,
//     SCALARTYPE3,
//     SCALARTYPE4,
//     SCALARTYPE5,
//     SCALARTYPE6,
//     TYPE,
//     NAME,
//     ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND6(
//           SCALARTYPE1,
//           SCALARTYPE2,
//           SCALARTYPE3,
//           SCALARTYPE4,
//           SCALARTYPE5,
//           SCALARTYPE6,
//           __VA_ARGS__))

// #define AT_DISPATCH_INDEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_PRIVATE_CASE_TYPE_USING_HINT(
//           at::ScalarType::Int, index_t, __VA_ARGS__)
//           AT_PRIVATE_CASE_TYPE_USING_HINT(
//               at::ScalarType::Long, index_t, __VA_ARGS__))

// ----------------------------------------------------------------------------
// DEPRECATED MACROS, DON'T USE THESE
// ----------------------------------------------------------------------------

// #define AT_DISPATCH_ALL_TYPES_AND_HALF(TYPE, NAME, ...)
//   detail::deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND(at::ScalarType::Half, __VA_ARGS__))


// Parsed from ATen/ScalarOps.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/core/Scalar.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/scalar_tensor.h>
// #endif
// When filling a number to 1-element CPU tensor, we want to skip
// everything but manipulate data ptr directly.
// Ideally this fast pass should be implemented in TensorIterator,
// but we also want to skip compute_types which in not avoidable
// in TensorIterator for now.

@Namespace("at::detail") public static native @ByVal Tensor scalar_tensor_static(
    @Const @ByRef Scalar s,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal DeviceOptional device_opt);
 // namespace detail
 // namespace at

// This is in the c10 namespace because we use ADL to find the functions in it.

// FIXME: this should be (and was) Scalar::toTensor, but there is currently no
// way to implement this without going through Derived Types (which are not part
// of core).
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(
    @Const @ByRef Scalar s,
    @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(
    @Const @ByRef Scalar s);

 // namespace c10

@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(
    @Const @ByRef Scalar scalar,
    @Const @ByVal(nullValue = "at::Device(at::kCPU)") Device device);
@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(
    @Const @ByRef Scalar scalar);

 // namespace native
 // namespace at


// Parsed from c10/util/strides.h

// #pragma once
// #include <c10/util/ArrayRef.h>
// #include <c10/util/DimVector.h>

// Computes the contiguous strides of a tensor, given its sizes.
@Namespace("c10") public static native @ByVal @Cast("c10::DimVector*") SymDimVector contiguous_strides(@Const @ByVal LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("c10::DimVector*") SymDimVector contiguous_strides(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... sizes);

 // namespace c10


// Parsed from ATen/TensorMeta.h

// #pragma once

// #include <ATen/DimVector.h>
// #include <ATen/core/Dimname.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/strides.h>

// #if C10_CLANG_HAS_WARNING("-Wdeprecated-copy-dtor")
// #endif

// Use this to define the prototype for a meta function.  There are two
// versions; one that takes one argument (just the operator name), or FUNC2
// variant that takes two arguments (operator name and overload name).
//
// Example usage:
//
//    TORCH_META_FUNC2(add, Tensor) (
//      const Tensor& self, const Tensor& other
//    ) {
//      ... compute sizes and options ...
//      set_output(sizes, options);
//    }
//
// #define TORCH_META_FUNC(name) void structured_##name::meta
// #define TORCH_META_FUNC2(name, overload)
//   void structured_##name##_##overload::meta

// These are versions of TORCH_META_FUNC(2) that include a precompute_out struct
// as a return value. They should be used when the kernel in question has
// precomputed values declared in native_functions.yaml and the corresponding
// implementation should return an instance of the aforementioned struct.
// #define TORCH_PRECOMPUTE_META_FUNC(name)
//   structured_##name::meta_return_ty structured_##name::meta
// #define TORCH_PRECOMPUTE_META_FUNC2(name, overload)
//   structured_##name##_##overload::meta_return_ty
//       structured_##name##_##overload::meta

// Use this to create a precompute struct in a meta function.
// #define TORCH_PRECOMPUTE_STRUCT(name) structured_##name::precompute_out<>
// #define TORCH_PRECOMPUTE_STRUCT2(name, overload)
//   structured_##name##_##overload::precompute_out<>

// Use this to define the prototype for an implementation.  This takes only
// one argument, which is the name of the dispatch key entry you're
// implementing.
//
// Example usage:
//
//    TORCH_IMPL_FUNC(add_cpu) (
//      Tensor& result, const Tensor& self, const Tensor& other
//    ) {
//      ... do the actual implementation ...
//    }
//
// #define TORCH_IMPL_FUNC(name) void structured_##name::impl
// Targeting ../MetaBase.java



 // namespace impl

 // namespace at



// Parsed from ATen/core/Range.h

// #pragma once

// #include <cstdint>
// #include <iosfwd>



  // namespace at


// Parsed from c10/util/Load.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <cstring>

 // namespace detail

 // namespace c10


// Parsed from c10/core/DynamicCast.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Load.h>
// #include <c10/util/TypeCast.h>

// Dynamic type casting utils:
// - fetch_and_cast
// - cast_and_store
//
// fetch_and_cast fetch a value with dynamic type specified by a ScalarType
// from a void pointer and cast it to a static type.
//
// cast_and_store casts a static typed value into dynamic type specified
// by a ScalarType, and store it into a void pointer.
//
// NOTE:
//
// Dynamic casting allows us to support type promotion without blowing up
// the combination space: For example, without dynamic cast, in order to
// implement `add_` with type promotion, we would need something like
//
// AT_DISPATCH_ALL_TYPES(output.dtype(),
//    AT_DISPATCH_ALL_TYPES(input1.dtype(),
//       AT_DISPATCH_ALL_TYPES(input2.dtype(),
//           [](arg0_t a, arg1_t b) -> out_t { return a + b; }
//       )
//    )
// )
//
// If we support N dtypes, the above code would generate the a+b kernel for
// all the N * N * N different supported types, the compilation time and
// binary size would become horrible.
//
// Dynamic casting might sounds like a bad idea in terms of performance.
// Especially if you ever do it in a loop, you are going to do a billion tests.
// But in practice it is not as bad as it might look:
//
// - on CPU, this is a branch that always has the same outcome, therefore
//   hopefully the branch predictor could do the job pretty well
// - on GPU, these branches will not diverge, so we could still have the same
//   warp executing the same line of code
// - Most kernels, like `add`, are bandwidth bound, adding a few clock cycles to
//   check an integer does not hurt the performance much because the ALUs would
//   wait for load instructions anyway.
//
// For the discussion and benchmark, refer to:
// - https://github.com/pytorch/pytorch/pull/28343
// - https://github.com/pytorch/pytorch/pull/28344
// - https://github.com/pytorch/pytorch/pull/28345
//

// #ifdef C10_HOST_DEVICE
// #else
// #define ERROR_UNSUPPORTED_CAST TORCH_CHECK(false, "Unexpected scalar type");
// #endif

// Fetch a value with dynamic type src_type from ptr, and cast it to static type
// dest_t.
// #define FETCH_AND_CAST_CASE(type, scalartype)
//   case ScalarType::scalartype:
//     return c10::convert<dest_t>(c10::load<type>(ptr));

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint8>") qint8 fetch_and_cast_to_qint8(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint8>") quint8 fetch_and_cast_to_quint8(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint32>") qint32 fetch_and_cast_to_quint32(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint4x2>") quint4x2 fetch_and_cast_to_quint4x2(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint2x4>") quint2x4 fetch_and_cast_to_quint2x4(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Name("fetch_and_cast<int8_t>") byte fetch_and_cast_to_byte(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Name("fetch_and_cast<int16_t>") short fetch_and_cast_to_short(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Name("fetch_and_cast<int>") int fetch_and_cast_to_int(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Cast("int64_t") @Name("fetch_and_cast<int64_t>") long fetch_and_cast_to_long(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<at::Half>") Half fetch_and_cast_to_Half(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Name("fetch_and_cast<float>") float fetch_and_cast_to_float(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Name("fetch_and_cast<double>") double fetch_and_cast_to_double(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::complex<float> >") FloatComplex fetch_and_cast_to_ComplexFloat(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::complex<double> >") DoubleComplex fetch_and_cast_to_ComplexDouble(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @Cast("bool") @Name("fetch_and_cast<bool>") boolean fetch_and_cast_to_boolean(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<at::BFloat16>") BFloat16 fetch_and_cast_to_BFloat16(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<at::Float8_e4m3fn>") Float8_e4m3fn fetch_and_cast_to_Float8_e4m3fn(
    ScalarType src_type,
    @Const Pointer ptr);

@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<at::Float8_e5m2>") Float8_e5m2 fetch_and_cast_to_Float8_e5m2(
    ScalarType src_type,
    @Const Pointer ptr);

// Cast a value with static type src_t into dynamic dest_type, and store it to
// ptr.
// #define CAST_AND_STORE_CASE(type, scalartype)
//   case ScalarType::scalartype:
//     *(type*)ptr = c10::convert<type>(value);
//     return;
@Namespace("c10") public static native @Name("cast_and_store<c10::qint8>") void cast_and_store_from_qint8(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal qint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint8>") void cast_and_store_from_quint8(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal quint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::qint32>") void cast_and_store_from_quint32(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal qint32 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint4x2>") void cast_and_store_from_quint4x2(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal quint4x2 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint2x4>") void cast_and_store_from_quint2x4(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal quint2x4 value);
@Namespace("c10") public static native @Name("cast_and_store<int8_t>") void cast_and_store_from_byte(
    ScalarType dest_type,
    Pointer ptr,
    byte value);
@Namespace("c10") public static native @Name("cast_and_store<int16_t>") void cast_and_store_from_short(
    ScalarType dest_type,
    Pointer ptr,
    short value);
@Namespace("c10") public static native @Name("cast_and_store<int>") void cast_and_store_from_int(
    ScalarType dest_type,
    Pointer ptr,
    int value);
@Namespace("c10") public static native @Name("cast_and_store<int64_t>") void cast_and_store_from_long(
    ScalarType dest_type,
    Pointer ptr,
    @Cast("int64_t") long value);
@Namespace("c10") public static native @Name("cast_and_store<at::Half>") void cast_and_store_from_Half(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal Half value);
@Namespace("c10") public static native @Name("cast_and_store<float>") void cast_and_store_from_float(
    ScalarType dest_type,
    Pointer ptr,
    float value);
@Namespace("c10") public static native @Name("cast_and_store<double>") void cast_and_store_from_double(
    ScalarType dest_type,
    Pointer ptr,
    double value);
@Namespace("c10") public static native @Name("cast_and_store<c10::complex<float> >") void cast_and_store_from_ComplexFloat(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal FloatComplex value);
@Namespace("c10") public static native @Name("cast_and_store<c10::complex<double> >") void cast_and_store_from_ComplexDouble(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal DoubleComplex value);
@Namespace("c10") public static native @Name("cast_and_store<bool>") void cast_and_store_from_boolean(
    ScalarType dest_type,
    Pointer ptr,
    @Cast("bool") boolean value);
@Namespace("c10") public static native @Name("cast_and_store<at::BFloat16>") void cast_and_store_from_BFloat16(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal BFloat16 value);
@Namespace("c10") public static native @Name("cast_and_store<at::Float8_e4m3fn>") void cast_and_store_from_Float8_e4m3fn(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal Float8_e4m3fn value);
@Namespace("c10") public static native @Name("cast_and_store<at::Float8_e5m2>") void cast_and_store_from_Float8_e5m2(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal Float8_e5m2 value);

// #define DEFINE_UNCASTABLE(T, scalartype_)
//   template <>
//   C10_HOST_DEVICE inline T fetch_and_cast<T>(
//       const ScalarType src_type, const void* ptr) {
//     CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == src_type);
//     return c10::load<T>(ptr);
//   }
//   template <>
//   C10_HOST_DEVICE inline void cast_and_store<T>(
//       const ScalarType dest_type, void* ptr, T value) {
//     CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == dest_type);
//     *(T*)ptr = value;
//   }

// #undef FETCH_AND_CAST_CASE
// #undef CAST_AND_STORE_CASE
// #undef DEFINE_UNCASTABLE
// #undef ERROR_UNSUPPORTED_CAST

 // namespace c10


// Parsed from ATen/TensorIterator.h

// #pragma once

// #include <ATen/TensorMeta.h>
// #include <ATen/core/Dimname.h>
// #include <ATen/core/Range.h>
// #include <ATen/core/TensorBase.h>
// #include <c10/core/DynamicCast.h>
// #include <c10/util/FunctionRef.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/SmallVector.h>
// #include <c10/util/TypeCast.h>
// #include <c10/util/irange.h>

// #include <array>
// #include <bitset>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wdeprecated-copy-dtor")
// #endif
 // namespace at

// TensorIterator is a helper class for element-wise operations, such as
// arithmetic, comparisons, and trigonometric functions. It handles
// broadcasting and type conversions of operands.
//
// This is inspired by NumPy's Array Iterator API (NpyIter).
//
// The files Loops.h and Loops.cuh provide functions to build kernels that
// use TensorIterator.
//
// Example:
//
//   auto iter = TensorIteratorConfig()
//     .add_output(output)
//     .add_input(input)
//     .build()
//
// [MyKernel.cpp / MyKernel.cu]
//   cpu_kernel(iter, [](float a, float b) {
//     return a + b;
//   });
//
//   gpu_kernel(iter, []GPU_LAMBDA(float a, float b) -> float {
//     return a + b;
//   });
//
// Note [Order of Construction]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// When setting up the tensor iterator configuration, the output Tensors
// have to be added first via
// TensorIteratorConfig::add_owned_output(at::Tensor). After adding all outputs,
// the inputs can be added via
// TensorIteratorConfig::add_owned_input(at::Tensor).
// Adding another output after inputs have been added will rise an exception.
//
// Note [Common Dtype Computation]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Some operations have a natural notion of a "common dtype" or
//   "computation dtype" where all inputs are cast to one dtype, the
//   operation is performed, and then the results are cast to all outputs.
//
// TensorIterator infers a common dtype if all inputs have the same dtype,
//   and it computes one using type promotion rules on its inputs if
//   promote_inputs_to_common_dtype_ is true. Attempting to query
//   a common dtype otherwise will throw an exception.
//
// Note that the outputs are not considered when computing a common dtype.
// This parameter is heuristically chosen to determine the minimum number of
// work that warrants parallelism. For example, when summing an array, it is
// deemed inefficient to parallelise over arrays shorter than 32768. Further,
// no parallel algorithm (such as parallel_reduce) should split work into
// smaller than GRAIN_SIZE chunks.
@Namespace("at::internal") @MemberGetter public static native @Cast("const int64_t") long GRAIN_SIZE();

// Storage for a non-owning Tensor, without needing to include Tensor.h

// Targeting ../OperandInfo.java



@Namespace("at") public enum FastSetupType {
  NONE((byte)(0)),
  CONTIGUOUS((byte)(1)),
  CHANNELS_LAST((byte)(2)),
  NON_OVERLAPPING_DENSE((byte)(3));

    public final byte value;
    private FastSetupType(byte v) { this.value = v; }
    private FastSetupType(FastSetupType e) { this.value = e.value; }
    public FastSetupType intern() { for (FastSetupType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../TensorIteratorBase.java


// Targeting ../TensorIterator.java


// Targeting ../TensorIteratorConfig.java


// Targeting ../SplitUntil32Bit.java



 // namespace at



// Parsed from ATen/NativeFunctions.h

// #pragma once

// @generated by torchgen/gen.py from NativeFunctions.h

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// #if defined(AT_PER_OPERATOR_HEADERS) && defined(TORCH_ASSERT_ONLY_METHOD_OPERATORS)
// #error This change adds a dependency on all pytorch operators, meaning the
//   file will need to be re-compiled every time an operator is changed or added.
//   Consider including a specific operator from <ATen/ops/{my_operator}_native.h>
//   and see NOTE [TORCH_ASSERT_ONLY_METHOD_OPERATORS].
// #endif

// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>
// #include <c10/core/QScheme.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <tuple>
// #include <vector>

// #include <ATen/ops/_adaptive_avg_pool2d_native.h>
// #include <ATen/ops/_adaptive_avg_pool2d_backward_native.h>
// #include <ATen/ops/_adaptive_avg_pool3d_native.h>
// #include <ATen/ops/_adaptive_avg_pool3d_backward_native.h>
// #include <ATen/ops/_add_batch_dim_native.h>
// #include <ATen/ops/_add_relu_native.h>
// #include <ATen/ops/_addmm_activation_native.h>
// #include <ATen/ops/_aminmax_native.h>
// #include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_native.h>
// #include <ATen/ops/_amp_update_scale_native.h>
// #include <ATen/ops/_assert_async_native.h>
// #include <ATen/ops/_assert_tensor_metadata_native.h>
// #include <ATen/ops/_autocast_to_full_precision_native.h>
// #include <ATen/ops/_autocast_to_reduced_precision_native.h>
// #include <ATen/ops/_backward_native.h>
// #include <ATen/ops/_batch_norm_impl_index_native.h>
// #include <ATen/ops/_batch_norm_impl_index_backward_native.h>
// #include <ATen/ops/_cast_Byte_native.h>
// #include <ATen/ops/_cast_Char_native.h>
// #include <ATen/ops/_cast_Double_native.h>
// #include <ATen/ops/_cast_Float_native.h>
// #include <ATen/ops/_cast_Half_native.h>
// #include <ATen/ops/_cast_Int_native.h>
// #include <ATen/ops/_cast_Long_native.h>
// #include <ATen/ops/_cast_Short_native.h>
// #include <ATen/ops/_cdist_backward_native.h>
// #include <ATen/ops/_cdist_forward_native.h>
// #include <ATen/ops/_cholesky_solve_helper_native.h>
// #include <ATen/ops/_choose_qparams_per_tensor_native.h>
// #include <ATen/ops/_coalesce_native.h>
// #include <ATen/ops/_coalesced_native.h>
// #include <ATen/ops/_compute_linear_combination_native.h>
// #include <ATen/ops/_conj_native.h>
// #include <ATen/ops/_conj_copy_native.h>
// #include <ATen/ops/_conj_physical_native.h>
// #include <ATen/ops/_conv_depthwise2d_native.h>
// #include <ATen/ops/_convert_indices_from_coo_to_csr_native.h>
// #include <ATen/ops/_convert_indices_from_csr_to_coo_native.h>
// #include <ATen/ops/_convolution_native.h>
// #include <ATen/ops/_convolution_double_backward_native.h>
// #include <ATen/ops/_convolution_mode_native.h>
// #include <ATen/ops/_copy_from_native.h>
// #include <ATen/ops/_copy_from_and_resize_native.h>
// #include <ATen/ops/_cslt_compress_native.h>
// #include <ATen/ops/_cslt_sparse_mm_native.h>
// #include <ATen/ops/_ctc_loss_native.h>
// #include <ATen/ops/_ctc_loss_backward_native.h>
// #include <ATen/ops/_cudnn_ctc_loss_native.h>
// #include <ATen/ops/_cudnn_init_dropout_state_native.h>
// #include <ATen/ops/_cudnn_rnn_native.h>
// #include <ATen/ops/_cudnn_rnn_backward_native.h>
// #include <ATen/ops/_cudnn_rnn_flatten_weight_native.h>
// #include <ATen/ops/_cufft_clear_plan_cache_native.h>
// #include <ATen/ops/_cufft_get_plan_cache_max_size_native.h>
// #include <ATen/ops/_cufft_get_plan_cache_size_native.h>
// #include <ATen/ops/_cufft_set_plan_cache_max_size_native.h>
// #include <ATen/ops/_cummax_helper_native.h>
// #include <ATen/ops/_cummin_helper_native.h>
// #include <ATen/ops/_debug_has_internal_overlap_native.h>
// #include <ATen/ops/_dimI_native.h>
// #include <ATen/ops/_dimV_native.h>
// #include <ATen/ops/_dim_arange_native.h>
// #include <ATen/ops/_dirichlet_grad_native.h>
// #include <ATen/ops/_efficient_attention_backward_native.h>
// #include <ATen/ops/_efficient_attention_forward_native.h>
// #include <ATen/ops/_efficientzerotensor_native.h>
// #include <ATen/ops/_embedding_bag_native.h>
// #include <ATen/ops/_embedding_bag_backward_native.h>
// #include <ATen/ops/_embedding_bag_dense_backward_native.h>
// #include <ATen/ops/_embedding_bag_forward_only_native.h>
// #include <ATen/ops/_embedding_bag_per_sample_weights_backward_native.h>
// #include <ATen/ops/_embedding_bag_sparse_backward_native.h>
// #include <ATen/ops/_empty_affine_quantized_native.h>
// #include <ATen/ops/_empty_per_channel_affine_quantized_native.h>
// #include <ATen/ops/_euclidean_dist_native.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_native.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward_native.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_native.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward_native.h>
// #include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_native.h>
// #include <ATen/ops/_fft_c2c_native.h>
// #include <ATen/ops/_fft_c2r_native.h>
// #include <ATen/ops/_fft_r2c_native.h>
// #include <ATen/ops/_fill_mem_eff_dropout_mask_native.h>
// #include <ATen/ops/_flash_attention_backward_native.h>
// #include <ATen/ops/_flash_attention_forward_native.h>
// #include <ATen/ops/_foobar_native.h>
// #include <ATen/ops/_foreach_abs_native.h>
// #include <ATen/ops/_foreach_acos_native.h>
// #include <ATen/ops/_foreach_add_native.h>
// #include <ATen/ops/_foreach_addcdiv_native.h>
// #include <ATen/ops/_foreach_addcmul_native.h>
// #include <ATen/ops/_foreach_asin_native.h>
// #include <ATen/ops/_foreach_atan_native.h>
// #include <ATen/ops/_foreach_ceil_native.h>
// #include <ATen/ops/_foreach_clamp_max_native.h>
// #include <ATen/ops/_foreach_clamp_min_native.h>
// #include <ATen/ops/_foreach_copy_native.h>
// #include <ATen/ops/_foreach_cos_native.h>
// #include <ATen/ops/_foreach_cosh_native.h>
// #include <ATen/ops/_foreach_div_native.h>
// #include <ATen/ops/_foreach_erf_native.h>
// #include <ATen/ops/_foreach_erfc_native.h>
// #include <ATen/ops/_foreach_exp_native.h>
// #include <ATen/ops/_foreach_expm1_native.h>
// #include <ATen/ops/_foreach_floor_native.h>
// #include <ATen/ops/_foreach_frac_native.h>
// #include <ATen/ops/_foreach_lerp_native.h>
// #include <ATen/ops/_foreach_lgamma_native.h>
// #include <ATen/ops/_foreach_log_native.h>
// #include <ATen/ops/_foreach_log10_native.h>
// #include <ATen/ops/_foreach_log1p_native.h>
// #include <ATen/ops/_foreach_log2_native.h>
// #include <ATen/ops/_foreach_maximum_native.h>
// #include <ATen/ops/_foreach_minimum_native.h>
// #include <ATen/ops/_foreach_mul_native.h>
// #include <ATen/ops/_foreach_neg_native.h>
// #include <ATen/ops/_foreach_norm_native.h>
// #include <ATen/ops/_foreach_pow_native.h>
// #include <ATen/ops/_foreach_reciprocal_native.h>
// #include <ATen/ops/_foreach_round_native.h>
// #include <ATen/ops/_foreach_sigmoid_native.h>
// #include <ATen/ops/_foreach_sign_native.h>
// #include <ATen/ops/_foreach_sin_native.h>
// #include <ATen/ops/_foreach_sinh_native.h>
// #include <ATen/ops/_foreach_sqrt_native.h>
// #include <ATen/ops/_foreach_sub_native.h>
// #include <ATen/ops/_foreach_tan_native.h>
// #include <ATen/ops/_foreach_tanh_native.h>
// #include <ATen/ops/_foreach_trunc_native.h>
// #include <ATen/ops/_foreach_zero_native.h>
// #include <ATen/ops/_functional_assert_async_native.h>
// #include <ATen/ops/_functional_sym_constrain_range_native.h>
// #include <ATen/ops/_functional_sym_constrain_range_for_size_native.h>
// #include <ATen/ops/_fused_adam_native.h>
// #include <ATen/ops/_fused_adamw_native.h>
// #include <ATen/ops/_fused_dropout_native.h>
// #include <ATen/ops/_fused_moving_avg_obs_fq_helper_native.h>
// #include <ATen/ops/_fused_sdp_choice_native.h>
// #include <ATen/ops/_fw_primal_native.h>
// #include <ATen/ops/_fw_primal_copy_native.h>
// #include <ATen/ops/_gather_sparse_backward_native.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_native.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward_native.h>
// #include <ATen/ops/_has_compatible_shallow_copy_type_native.h>
// #include <ATen/ops/_has_same_storage_numel_native.h>
// #include <ATen/ops/_histogramdd_bin_edges_native.h>
// #include <ATen/ops/_histogramdd_from_bin_cts_native.h>
// #include <ATen/ops/_histogramdd_from_bin_tensors_native.h>
// #include <ATen/ops/_index_put_impl_native.h>
// #include <ATen/ops/_indices_native.h>
// #include <ATen/ops/_indices_copy_native.h>
// #include <ATen/ops/_int_mm_native.h>
// #include <ATen/ops/_is_all_true_native.h>
// #include <ATen/ops/_is_any_true_native.h>
// #include <ATen/ops/_is_zerotensor_native.h>
// #include <ATen/ops/_linalg_check_errors_native.h>
// #include <ATen/ops/_linalg_det_native.h>
// #include <ATen/ops/_linalg_eigh_native.h>
// #include <ATen/ops/_linalg_slogdet_native.h>
// #include <ATen/ops/_linalg_solve_ex_native.h>
// #include <ATen/ops/_linalg_svd_native.h>
// #include <ATen/ops/_local_scalar_dense_native.h>
// #include <ATen/ops/_log_softmax_native.h>
// #include <ATen/ops/_log_softmax_backward_data_native.h>
// #include <ATen/ops/_logcumsumexp_native.h>
// #include <ATen/ops/_lstm_mps_native.h>
// #include <ATen/ops/_lu_with_info_native.h>
// #include <ATen/ops/_make_dep_token_native.h>
// #include <ATen/ops/_make_dual_native.h>
// #include <ATen/ops/_make_dual_copy_native.h>
// #include <ATen/ops/_make_per_channel_quantized_tensor_native.h>
// #include <ATen/ops/_make_per_tensor_quantized_tensor_native.h>
// #include <ATen/ops/_masked_scale_native.h>
// #include <ATen/ops/_masked_softmax_native.h>
// #include <ATen/ops/_masked_softmax_backward_native.h>
// #include <ATen/ops/_mkldnn_reshape_native.h>
// #include <ATen/ops/_mkldnn_transpose_native.h>
// #include <ATen/ops/_mps_convolution_native.h>
// #include <ATen/ops/_mps_convolution_transpose_native.h>
// #include <ATen/ops/_native_batch_norm_legit_native.h>
// #include <ATen/ops/_native_batch_norm_legit_no_training_native.h>
// #include <ATen/ops/_native_multi_head_attention_native.h>
// #include <ATen/ops/_neg_view_native.h>
// #include <ATen/ops/_neg_view_copy_native.h>
// #include <ATen/ops/_nested_from_padded_native.h>
// #include <ATen/ops/_nested_from_padded_and_nested_example_native.h>
// #include <ATen/ops/_nested_select_backward_native.h>
// #include <ATen/ops/_nested_sum_backward_native.h>
// #include <ATen/ops/_nested_tensor_from_mask_native.h>
// #include <ATen/ops/_nested_tensor_from_mask_left_aligned_native.h>
// #include <ATen/ops/_nested_tensor_from_tensor_list_native.h>
// #include <ATen/ops/_nested_tensor_size_native.h>
// #include <ATen/ops/_nested_tensor_softmax_with_shape_native.h>
// #include <ATen/ops/_nested_tensor_storage_offsets_native.h>
// #include <ATen/ops/_nested_tensor_strides_native.h>
// #include <ATen/ops/_nested_view_from_buffer_native.h>
// #include <ATen/ops/_nested_view_from_buffer_copy_native.h>
// #include <ATen/ops/_new_zeros_with_same_feature_meta_native.h>
// #include <ATen/ops/_nnpack_available_native.h>
// #include <ATen/ops/_nnpack_spatial_convolution_native.h>
// #include <ATen/ops/_nnz_native.h>
// #include <ATen/ops/_pack_padded_sequence_native.h>
// #include <ATen/ops/_pack_padded_sequence_backward_native.h>
// #include <ATen/ops/_pad_circular_native.h>
// #include <ATen/ops/_pad_enum_native.h>
// #include <ATen/ops/_pad_packed_sequence_native.h>
// #include <ATen/ops/_pdist_backward_native.h>
// #include <ATen/ops/_pdist_forward_native.h>
// #include <ATen/ops/_pin_memory_native.h>
// #include <ATen/ops/_prelu_kernel_native.h>
// #include <ATen/ops/_prelu_kernel_backward_native.h>
// #include <ATen/ops/_propagate_xla_data_native.h>
// #include <ATen/ops/_remove_batch_dim_native.h>
// #include <ATen/ops/_reshape_alias_native.h>
// #include <ATen/ops/_reshape_alias_copy_native.h>
// #include <ATen/ops/_reshape_copy_native.h>
// #include <ATen/ops/_reshape_from_tensor_native.h>
// #include <ATen/ops/_resize_output_native.h>
// #include <ATen/ops/_rowwise_prune_native.h>
// #include <ATen/ops/_sample_dirichlet_native.h>
// #include <ATen/ops/_saturate_weight_to_fp16_native.h>
// #include <ATen/ops/_scaled_dot_product_attention_math_native.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention_native.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention_backward_native.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention_native.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention_backward_native.h>
// #include <ATen/ops/_scaled_mm_native.h>
// #include <ATen/ops/_segment_reduce_backward_native.h>
// #include <ATen/ops/_shape_as_tensor_native.h>
// #include <ATen/ops/_slow_conv2d_backward_native.h>
// #include <ATen/ops/_slow_conv2d_forward_native.h>
// #include <ATen/ops/_sobol_engine_draw_native.h>
// #include <ATen/ops/_sobol_engine_ff_native.h>
// #include <ATen/ops/_sobol_engine_initialize_state_native.h>
// #include <ATen/ops/_sobol_engine_scramble_native.h>
// #include <ATen/ops/_softmax_native.h>
// #include <ATen/ops/_softmax_backward_data_native.h>
// #include <ATen/ops/_sparse_addmm_native.h>
// #include <ATen/ops/_sparse_broadcast_to_native.h>
// #include <ATen/ops/_sparse_broadcast_to_copy_native.h>
// #include <ATen/ops/_sparse_bsc_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_bsr_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_compressed_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_coo_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_native.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_native.h>
// #include <ATen/ops/_sparse_csc_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_csr_prod_native.h>
// #include <ATen/ops/_sparse_csr_sum_native.h>
// #include <ATen/ops/_sparse_csr_tensor_unsafe_native.h>
// #include <ATen/ops/_sparse_log_softmax_native.h>
// #include <ATen/ops/_sparse_log_softmax_backward_data_native.h>
// #include <ATen/ops/_sparse_mask_projection_native.h>
// #include <ATen/ops/_sparse_mm_native.h>
// #include <ATen/ops/_sparse_mm_reduce_impl_native.h>
// #include <ATen/ops/_sparse_mm_reduce_impl_backward_native.h>
// #include <ATen/ops/_sparse_semi_structured_linear_native.h>
// #include <ATen/ops/_sparse_softmax_native.h>
// #include <ATen/ops/_sparse_softmax_backward_data_native.h>
// #include <ATen/ops/_sparse_sparse_matmul_native.h>
// #include <ATen/ops/_sparse_sum_native.h>
// #include <ATen/ops/_sparse_sum_backward_native.h>
// #include <ATen/ops/_spdiags_native.h>
// #include <ATen/ops/_stack_native.h>
// #include <ATen/ops/_standard_gamma_native.h>
// #include <ATen/ops/_standard_gamma_grad_native.h>
// #include <ATen/ops/_test_ambiguous_defaults_native.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_native.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view_native.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_native.h>
// #include <ATen/ops/_test_check_tensor_native.h>
// #include <ATen/ops/_test_functorch_fallback_native.h>
// #include <ATen/ops/_test_optional_filled_intlist_native.h>
// #include <ATen/ops/_test_optional_floatlist_native.h>
// #include <ATen/ops/_test_optional_intlist_native.h>
// #include <ATen/ops/_test_serialization_subcmul_native.h>
// #include <ATen/ops/_test_string_default_native.h>
// #include <ATen/ops/_test_warn_in_autograd_native.h>
// #include <ATen/ops/_thnn_differentiable_gru_cell_backward_native.h>
// #include <ATen/ops/_thnn_differentiable_lstm_cell_backward_native.h>
// #include <ATen/ops/_thnn_fused_gru_cell_native.h>
// #include <ATen/ops/_thnn_fused_gru_cell_backward_native.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_native.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward_native.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_native.h>
// #include <ATen/ops/_to_copy_native.h>
// #include <ATen/ops/_to_cpu_native.h>
// #include <ATen/ops/_to_dense_native.h>
// #include <ATen/ops/_to_sparse_native.h>
// #include <ATen/ops/_to_sparse_bsc_native.h>
// #include <ATen/ops/_to_sparse_bsr_native.h>
// #include <ATen/ops/_to_sparse_csc_native.h>
// #include <ATen/ops/_to_sparse_csr_native.h>
// #include <ATen/ops/_to_sparse_semi_structured_native.h>
// #include <ATen/ops/_transform_bias_rescale_qkv_native.h>
// #include <ATen/ops/_transformer_encoder_layer_fwd_native.h>
// #include <ATen/ops/_trilinear_native.h>
// #include <ATen/ops/_triton_multi_head_attention_native.h>
// #include <ATen/ops/_triton_scaled_dot_attention_native.h>
// #include <ATen/ops/_unique_native.h>
// #include <ATen/ops/_unique2_native.h>
// #include <ATen/ops/_unpack_dual_native.h>
// #include <ATen/ops/_unsafe_index_native.h>
// #include <ATen/ops/_unsafe_index_put_native.h>
// #include <ATen/ops/_unsafe_view_native.h>
// #include <ATen/ops/_upsample_bicubic2d_aa_native.h>
// #include <ATen/ops/_upsample_bicubic2d_aa_backward_native.h>
// #include <ATen/ops/_upsample_bilinear2d_aa_native.h>
// #include <ATen/ops/_upsample_bilinear2d_aa_backward_native.h>
// #include <ATen/ops/_upsample_nearest_exact1d_native.h>
// #include <ATen/ops/_upsample_nearest_exact1d_backward_native.h>
// #include <ATen/ops/_upsample_nearest_exact2d_native.h>
// #include <ATen/ops/_upsample_nearest_exact2d_backward_native.h>
// #include <ATen/ops/_upsample_nearest_exact3d_native.h>
// #include <ATen/ops/_upsample_nearest_exact3d_backward_native.h>
// #include <ATen/ops/_use_cudnn_ctc_loss_native.h>
// #include <ATen/ops/_use_cudnn_rnn_flatten_weight_native.h>
// #include <ATen/ops/_validate_compressed_sparse_indices_native.h>
// #include <ATen/ops/_validate_sparse_bsc_tensor_args_native.h>
// #include <ATen/ops/_validate_sparse_bsr_tensor_args_native.h>
// #include <ATen/ops/_validate_sparse_compressed_tensor_args_native.h>
// #include <ATen/ops/_validate_sparse_coo_tensor_args_native.h>
// #include <ATen/ops/_validate_sparse_csc_tensor_args_native.h>
// #include <ATen/ops/_validate_sparse_csr_tensor_args_native.h>
// #include <ATen/ops/_values_native.h>
// #include <ATen/ops/_values_copy_native.h>
// #include <ATen/ops/_version_native.h>
// #include <ATen/ops/_weight_norm_native.h>
// #include <ATen/ops/_weight_norm_differentiable_backward_native.h>
// #include <ATen/ops/_weight_norm_interface_native.h>
// #include <ATen/ops/_weight_norm_interface_backward_native.h>
// #include <ATen/ops/abs_native.h>
// #include <ATen/ops/absolute_native.h>
// #include <ATen/ops/acos_native.h>
// #include <ATen/ops/acosh_native.h>
// #include <ATen/ops/adaptive_avg_pool1d_native.h>
// #include <ATen/ops/adaptive_avg_pool2d_native.h>
// #include <ATen/ops/adaptive_avg_pool3d_native.h>
// #include <ATen/ops/adaptive_avg_pool3d_backward_native.h>
// #include <ATen/ops/adaptive_max_pool1d_native.h>
// #include <ATen/ops/adaptive_max_pool2d_native.h>
// #include <ATen/ops/adaptive_max_pool2d_backward_native.h>
// #include <ATen/ops/adaptive_max_pool3d_native.h>
// #include <ATen/ops/adaptive_max_pool3d_backward_native.h>
// #include <ATen/ops/add_native.h>
// #include <ATen/ops/addbmm_native.h>
// #include <ATen/ops/addcdiv_native.h>
// #include <ATen/ops/addcmul_native.h>
// #include <ATen/ops/addmm_native.h>
// #include <ATen/ops/addmv_native.h>
// #include <ATen/ops/addr_native.h>
// #include <ATen/ops/adjoint_native.h>
// #include <ATen/ops/affine_grid_generator_native.h>
// #include <ATen/ops/affine_grid_generator_backward_native.h>
// #include <ATen/ops/alias_native.h>
// #include <ATen/ops/alias_copy_native.h>
// #include <ATen/ops/align_as_native.h>
// #include <ATen/ops/align_tensors_native.h>
// #include <ATen/ops/align_to_native.h>
// #include <ATen/ops/all_native.h>
// #include <ATen/ops/allclose_native.h>
// #include <ATen/ops/alpha_dropout_native.h>
// #include <ATen/ops/amax_native.h>
// #include <ATen/ops/amin_native.h>
// #include <ATen/ops/aminmax_native.h>
// #include <ATen/ops/and_native.h>
// #include <ATen/ops/angle_native.h>
// #include <ATen/ops/any_native.h>
// #include <ATen/ops/arange_native.h>
// #include <ATen/ops/arccos_native.h>
// #include <ATen/ops/arccosh_native.h>
// #include <ATen/ops/arcsin_native.h>
// #include <ATen/ops/arcsinh_native.h>
// #include <ATen/ops/arctan_native.h>
// #include <ATen/ops/arctan2_native.h>
// #include <ATen/ops/arctanh_native.h>
// #include <ATen/ops/argmax_native.h>
// #include <ATen/ops/argmin_native.h>
// #include <ATen/ops/argsort_native.h>
// #include <ATen/ops/argwhere_native.h>
// #include <ATen/ops/as_strided_native.h>
// #include <ATen/ops/as_strided_copy_native.h>
// #include <ATen/ops/as_strided_scatter_native.h>
// #include <ATen/ops/asin_native.h>
// #include <ATen/ops/asinh_native.h>
// #include <ATen/ops/atan_native.h>
// #include <ATen/ops/atan2_native.h>
// #include <ATen/ops/atanh_native.h>
// #include <ATen/ops/atleast_1d_native.h>
// #include <ATen/ops/atleast_2d_native.h>
// #include <ATen/ops/atleast_3d_native.h>
// #include <ATen/ops/avg_pool1d_native.h>
// #include <ATen/ops/avg_pool2d_native.h>
// #include <ATen/ops/avg_pool2d_backward_native.h>
// #include <ATen/ops/avg_pool3d_native.h>
// #include <ATen/ops/avg_pool3d_backward_native.h>
// #include <ATen/ops/baddbmm_native.h>
// #include <ATen/ops/bartlett_window_native.h>
// #include <ATen/ops/batch_norm_native.h>
// #include <ATen/ops/batch_norm_backward_elemt_native.h>
// #include <ATen/ops/batch_norm_backward_reduce_native.h>
// #include <ATen/ops/batch_norm_elemt_native.h>
// #include <ATen/ops/batch_norm_gather_stats_native.h>
// #include <ATen/ops/batch_norm_gather_stats_with_counts_native.h>
// #include <ATen/ops/batch_norm_stats_native.h>
// #include <ATen/ops/batch_norm_update_stats_native.h>
// #include <ATen/ops/bernoulli_native.h>
// #include <ATen/ops/bilinear_native.h>
// #include <ATen/ops/binary_cross_entropy_native.h>
// #include <ATen/ops/binary_cross_entropy_backward_native.h>
// #include <ATen/ops/binary_cross_entropy_with_logits_native.h>
// #include <ATen/ops/bincount_native.h>
// #include <ATen/ops/binomial_native.h>
// #include <ATen/ops/bitwise_and_native.h>
// #include <ATen/ops/bitwise_left_shift_native.h>
// #include <ATen/ops/bitwise_not_native.h>
// #include <ATen/ops/bitwise_or_native.h>
// #include <ATen/ops/bitwise_right_shift_native.h>
// #include <ATen/ops/bitwise_xor_native.h>
// #include <ATen/ops/blackman_window_native.h>
// #include <ATen/ops/block_diag_native.h>
// #include <ATen/ops/bmm_native.h>
// #include <ATen/ops/broadcast_tensors_native.h>
// #include <ATen/ops/broadcast_to_native.h>
// #include <ATen/ops/bucketize_native.h>
// #include <ATen/ops/can_cast_native.h>
// #include <ATen/ops/cartesian_prod_native.h>
// #include <ATen/ops/cat_native.h>
// #include <ATen/ops/cauchy_native.h>
// #include <ATen/ops/ccol_indices_native.h>
// #include <ATen/ops/ccol_indices_copy_native.h>
// #include <ATen/ops/cdist_native.h>
// #include <ATen/ops/ceil_native.h>
// #include <ATen/ops/celu_native.h>
// #include <ATen/ops/chain_matmul_native.h>
// #include <ATen/ops/chalf_native.h>
// #include <ATen/ops/channel_shuffle_native.h>
// #include <ATen/ops/cholesky_native.h>
// #include <ATen/ops/cholesky_inverse_native.h>
// #include <ATen/ops/cholesky_solve_native.h>
// #include <ATen/ops/choose_qparams_optimized_native.h>
// #include <ATen/ops/chunk_native.h>
// #include <ATen/ops/clamp_native.h>
// #include <ATen/ops/clamp_max_native.h>
// #include <ATen/ops/clamp_min_native.h>
// #include <ATen/ops/clip_native.h>
// #include <ATen/ops/clone_native.h>
// #include <ATen/ops/coalesce_native.h>
// #include <ATen/ops/col2im_native.h>
// #include <ATen/ops/col_indices_native.h>
// #include <ATen/ops/col_indices_copy_native.h>
// #include <ATen/ops/column_stack_native.h>
// #include <ATen/ops/combinations_native.h>
// #include <ATen/ops/complex_native.h>
// #include <ATen/ops/concat_native.h>
// #include <ATen/ops/concatenate_native.h>
// #include <ATen/ops/conj_native.h>
// #include <ATen/ops/conj_physical_native.h>
// #include <ATen/ops/constant_pad_nd_native.h>
// #include <ATen/ops/contiguous_native.h>
// #include <ATen/ops/conv1d_native.h>
// #include <ATen/ops/conv2d_native.h>
// #include <ATen/ops/conv3d_native.h>
// #include <ATen/ops/conv_depthwise3d_native.h>
// #include <ATen/ops/conv_tbc_native.h>
// #include <ATen/ops/conv_tbc_backward_native.h>
// #include <ATen/ops/conv_transpose1d_native.h>
// #include <ATen/ops/conv_transpose2d_native.h>
// #include <ATen/ops/conv_transpose3d_native.h>
// #include <ATen/ops/convolution_native.h>
// #include <ATen/ops/convolution_backward_native.h>
// #include <ATen/ops/convolution_backward_overrideable_native.h>
// #include <ATen/ops/convolution_overrideable_native.h>
// #include <ATen/ops/copy_native.h>
// #include <ATen/ops/copy_sparse_to_sparse_native.h>
// #include <ATen/ops/copysign_native.h>
// #include <ATen/ops/corrcoef_native.h>
// #include <ATen/ops/cos_native.h>
// #include <ATen/ops/cosh_native.h>
// #include <ATen/ops/cosine_embedding_loss_native.h>
// #include <ATen/ops/cosine_similarity_native.h>
// #include <ATen/ops/count_nonzero_native.h>
// #include <ATen/ops/cov_native.h>
// #include <ATen/ops/cross_native.h>
// #include <ATen/ops/cross_entropy_loss_native.h>
// #include <ATen/ops/crow_indices_native.h>
// #include <ATen/ops/crow_indices_copy_native.h>
// #include <ATen/ops/ctc_loss_native.h>
// #include <ATen/ops/cudnn_affine_grid_generator_native.h>
// #include <ATen/ops/cudnn_affine_grid_generator_backward_native.h>
// #include <ATen/ops/cudnn_batch_norm_native.h>
// #include <ATen/ops/cudnn_batch_norm_backward_native.h>
// #include <ATen/ops/cudnn_convolution_native.h>
// #include <ATen/ops/cudnn_convolution_add_relu_native.h>
// #include <ATen/ops/cudnn_convolution_relu_native.h>
// #include <ATen/ops/cudnn_convolution_transpose_native.h>
// #include <ATen/ops/cudnn_grid_sampler_native.h>
// #include <ATen/ops/cudnn_grid_sampler_backward_native.h>
// #include <ATen/ops/cudnn_is_acceptable_native.h>
// #include <ATen/ops/cummax_native.h>
// #include <ATen/ops/cummaxmin_backward_native.h>
// #include <ATen/ops/cummin_native.h>
// #include <ATen/ops/cumprod_native.h>
// #include <ATen/ops/cumprod_backward_native.h>
// #include <ATen/ops/cumsum_native.h>
// #include <ATen/ops/cumulative_trapezoid_native.h>
// #include <ATen/ops/data_native.h>
// #include <ATen/ops/deg2rad_native.h>
// #include <ATen/ops/dense_dim_native.h>
// #include <ATen/ops/dequantize_native.h>
// #include <ATen/ops/det_native.h>
// #include <ATen/ops/detach_native.h>
// #include <ATen/ops/detach_copy_native.h>
// #include <ATen/ops/diag_native.h>
// #include <ATen/ops/diag_embed_native.h>
// #include <ATen/ops/diagflat_native.h>
// #include <ATen/ops/diagonal_native.h>
// #include <ATen/ops/diagonal_backward_native.h>
// #include <ATen/ops/diagonal_copy_native.h>
// #include <ATen/ops/diagonal_scatter_native.h>
// #include <ATen/ops/diff_native.h>
// #include <ATen/ops/digamma_native.h>
// #include <ATen/ops/dist_native.h>
// #include <ATen/ops/div_native.h>
// #include <ATen/ops/divide_native.h>
// #include <ATen/ops/dot_native.h>
// #include <ATen/ops/dropout_native.h>
// #include <ATen/ops/dsplit_native.h>
// #include <ATen/ops/dstack_native.h>
// #include <ATen/ops/einsum_native.h>
// #include <ATen/ops/elu_native.h>
// #include <ATen/ops/elu_backward_native.h>
// #include <ATen/ops/embedding_native.h>
// #include <ATen/ops/embedding_backward_native.h>
// #include <ATen/ops/embedding_bag_native.h>
// #include <ATen/ops/embedding_dense_backward_native.h>
// #include <ATen/ops/embedding_renorm_native.h>
// #include <ATen/ops/embedding_sparse_backward_native.h>
// #include <ATen/ops/empty_native.h>
// #include <ATen/ops/empty_like_native.h>
// #include <ATen/ops/empty_permuted_native.h>
// #include <ATen/ops/empty_quantized_native.h>
// #include <ATen/ops/empty_strided_native.h>
// #include <ATen/ops/eq_native.h>
// #include <ATen/ops/equal_native.h>
// #include <ATen/ops/erf_native.h>
// #include <ATen/ops/erfc_native.h>
// #include <ATen/ops/erfinv_native.h>
// #include <ATen/ops/exp_native.h>
// #include <ATen/ops/exp2_native.h>
// #include <ATen/ops/expand_native.h>
// #include <ATen/ops/expand_as_native.h>
// #include <ATen/ops/expand_copy_native.h>
// #include <ATen/ops/expm1_native.h>
// #include <ATen/ops/exponential_native.h>
// #include <ATen/ops/eye_native.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_native.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_native.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward_native.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_native.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_native.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward_native.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight_native.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation_native.h>
// #include <ATen/ops/fbgemm_linear_int8_weight_native.h>
// #include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation_native.h>
// #include <ATen/ops/fbgemm_linear_quantize_weight_native.h>
// #include <ATen/ops/fbgemm_pack_gemm_matrix_fp16_native.h>
// #include <ATen/ops/fbgemm_pack_quantized_matrix_native.h>
// #include <ATen/ops/feature_alpha_dropout_native.h>
// #include <ATen/ops/feature_dropout_native.h>
// #include <ATen/ops/fft_fft_native.h>
// #include <ATen/ops/fft_fft2_native.h>
// #include <ATen/ops/fft_fftfreq_native.h>
// #include <ATen/ops/fft_fftn_native.h>
// #include <ATen/ops/fft_fftshift_native.h>
// #include <ATen/ops/fft_hfft_native.h>
// #include <ATen/ops/fft_hfft2_native.h>
// #include <ATen/ops/fft_hfftn_native.h>
// #include <ATen/ops/fft_ifft_native.h>
// #include <ATen/ops/fft_ifft2_native.h>
// #include <ATen/ops/fft_ifftn_native.h>
// #include <ATen/ops/fft_ifftshift_native.h>
// #include <ATen/ops/fft_ihfft_native.h>
// #include <ATen/ops/fft_ihfft2_native.h>
// #include <ATen/ops/fft_ihfftn_native.h>
// #include <ATen/ops/fft_irfft_native.h>
// #include <ATen/ops/fft_irfft2_native.h>
// #include <ATen/ops/fft_irfftn_native.h>
// #include <ATen/ops/fft_rfft_native.h>
// #include <ATen/ops/fft_rfft2_native.h>
// #include <ATen/ops/fft_rfftfreq_native.h>
// #include <ATen/ops/fft_rfftn_native.h>
// #include <ATen/ops/fill_native.h>
// #include <ATen/ops/fill_diagonal_native.h>
// #include <ATen/ops/fix_native.h>
// #include <ATen/ops/flatten_native.h>
// #include <ATen/ops/flatten_dense_tensors_native.h>
// #include <ATen/ops/flip_native.h>
// #include <ATen/ops/fliplr_native.h>
// #include <ATen/ops/flipud_native.h>
// #include <ATen/ops/float_power_native.h>
// #include <ATen/ops/floor_native.h>
// #include <ATen/ops/floor_divide_native.h>
// #include <ATen/ops/fmax_native.h>
// #include <ATen/ops/fmin_native.h>
// #include <ATen/ops/fmod_native.h>
// #include <ATen/ops/frac_native.h>
// #include <ATen/ops/fractional_max_pool2d_native.h>
// #include <ATen/ops/fractional_max_pool2d_backward_native.h>
// #include <ATen/ops/fractional_max_pool3d_native.h>
// #include <ATen/ops/fractional_max_pool3d_backward_native.h>
// #include <ATen/ops/frexp_native.h>
// #include <ATen/ops/frobenius_norm_native.h>
// #include <ATen/ops/from_file_native.h>
// #include <ATen/ops/full_native.h>
// #include <ATen/ops/full_like_native.h>
// #include <ATen/ops/fused_moving_avg_obs_fake_quant_native.h>
// #include <ATen/ops/gather_native.h>
// #include <ATen/ops/gather_backward_native.h>
// #include <ATen/ops/gcd_native.h>
// #include <ATen/ops/ge_native.h>
// #include <ATen/ops/gelu_native.h>
// #include <ATen/ops/gelu_backward_native.h>
// #include <ATen/ops/geometric_native.h>
// #include <ATen/ops/geqrf_native.h>
// #include <ATen/ops/ger_native.h>
// #include <ATen/ops/glu_native.h>
// #include <ATen/ops/glu_backward_native.h>
// #include <ATen/ops/glu_backward_jvp_native.h>
// #include <ATen/ops/glu_jvp_native.h>
// #include <ATen/ops/gradient_native.h>
// #include <ATen/ops/greater_native.h>
// #include <ATen/ops/greater_equal_native.h>
// #include <ATen/ops/grid_sampler_native.h>
// #include <ATen/ops/grid_sampler_2d_native.h>
// #include <ATen/ops/grid_sampler_2d_backward_native.h>
// #include <ATen/ops/grid_sampler_3d_native.h>
// #include <ATen/ops/grid_sampler_3d_backward_native.h>
// #include <ATen/ops/group_norm_native.h>
// #include <ATen/ops/gru_native.h>
// #include <ATen/ops/gru_cell_native.h>
// #include <ATen/ops/gt_native.h>
// #include <ATen/ops/hamming_window_native.h>
// #include <ATen/ops/hann_window_native.h>
// #include <ATen/ops/hardshrink_native.h>
// #include <ATen/ops/hardshrink_backward_native.h>
// #include <ATen/ops/hardsigmoid_native.h>
// #include <ATen/ops/hardsigmoid_backward_native.h>
// #include <ATen/ops/hardswish_native.h>
// #include <ATen/ops/hardswish_backward_native.h>
// #include <ATen/ops/hardtanh_native.h>
// #include <ATen/ops/hardtanh_backward_native.h>
// #include <ATen/ops/heaviside_native.h>
// #include <ATen/ops/hinge_embedding_loss_native.h>
// #include <ATen/ops/histc_native.h>
// #include <ATen/ops/histogram_native.h>
// #include <ATen/ops/histogramdd_native.h>
// #include <ATen/ops/hsplit_native.h>
// #include <ATen/ops/hspmm_native.h>
// #include <ATen/ops/hstack_native.h>
// #include <ATen/ops/huber_loss_native.h>
// #include <ATen/ops/huber_loss_backward_native.h>
// #include <ATen/ops/hypot_native.h>
// #include <ATen/ops/i0_native.h>
// #include <ATen/ops/igamma_native.h>
// #include <ATen/ops/igammac_native.h>
// #include <ATen/ops/im2col_native.h>
// #include <ATen/ops/imag_native.h>
// #include <ATen/ops/index_native.h>
// #include <ATen/ops/index_add_native.h>
// #include <ATen/ops/index_copy_native.h>
// #include <ATen/ops/index_fill_native.h>
// #include <ATen/ops/index_put_native.h>
// #include <ATen/ops/index_reduce_native.h>
// #include <ATen/ops/index_select_native.h>
// #include <ATen/ops/index_select_backward_native.h>
// #include <ATen/ops/indices_native.h>
// #include <ATen/ops/indices_copy_native.h>
// #include <ATen/ops/infinitely_differentiable_gelu_backward_native.h>
// #include <ATen/ops/inner_native.h>
// #include <ATen/ops/instance_norm_native.h>
// #include <ATen/ops/int_repr_native.h>
// #include <ATen/ops/inverse_native.h>
// #include <ATen/ops/is_coalesced_native.h>
// #include <ATen/ops/is_complex_native.h>
// #include <ATen/ops/is_conj_native.h>
// #include <ATen/ops/is_distributed_native.h>
// #include <ATen/ops/is_floating_point_native.h>
// #include <ATen/ops/is_inference_native.h>
// #include <ATen/ops/is_leaf_native.h>
// #include <ATen/ops/is_neg_native.h>
// #include <ATen/ops/is_nonzero_native.h>
// #include <ATen/ops/is_pinned_native.h>
// #include <ATen/ops/is_same_size_native.h>
// #include <ATen/ops/is_set_to_native.h>
// #include <ATen/ops/is_signed_native.h>
// #include <ATen/ops/is_vulkan_available_native.h>
// #include <ATen/ops/isclose_native.h>
// #include <ATen/ops/isfinite_native.h>
// #include <ATen/ops/isin_native.h>
// #include <ATen/ops/isinf_native.h>
// #include <ATen/ops/isnan_native.h>
// #include <ATen/ops/isneginf_native.h>
// #include <ATen/ops/isposinf_native.h>
// #include <ATen/ops/isreal_native.h>
// #include <ATen/ops/istft_native.h>
// #include <ATen/ops/item_native.h>
// #include <ATen/ops/kaiser_window_native.h>
// #include <ATen/ops/kl_div_native.h>
// #include <ATen/ops/kron_native.h>
// #include <ATen/ops/kthvalue_native.h>
// #include <ATen/ops/l1_loss_native.h>
// #include <ATen/ops/layer_norm_native.h>
// #include <ATen/ops/lcm_native.h>
// #include <ATen/ops/ldexp_native.h>
// #include <ATen/ops/le_native.h>
// #include <ATen/ops/leaky_relu_native.h>
// #include <ATen/ops/leaky_relu_backward_native.h>
// #include <ATen/ops/lerp_native.h>
// #include <ATen/ops/less_native.h>
// #include <ATen/ops/less_equal_native.h>
// #include <ATen/ops/lgamma_native.h>
// #include <ATen/ops/lift_native.h>
// #include <ATen/ops/lift_fresh_native.h>
// #include <ATen/ops/lift_fresh_copy_native.h>
// #include <ATen/ops/linalg_cholesky_native.h>
// #include <ATen/ops/linalg_cholesky_ex_native.h>
// #include <ATen/ops/linalg_cond_native.h>
// #include <ATen/ops/linalg_cross_native.h>
// #include <ATen/ops/linalg_det_native.h>
// #include <ATen/ops/linalg_diagonal_native.h>
// #include <ATen/ops/linalg_eig_native.h>
// #include <ATen/ops/linalg_eigh_native.h>
// #include <ATen/ops/linalg_eigvals_native.h>
// #include <ATen/ops/linalg_eigvalsh_native.h>
// #include <ATen/ops/linalg_householder_product_native.h>
// #include <ATen/ops/linalg_inv_native.h>
// #include <ATen/ops/linalg_inv_ex_native.h>
// #include <ATen/ops/linalg_ldl_factor_native.h>
// #include <ATen/ops/linalg_ldl_factor_ex_native.h>
// #include <ATen/ops/linalg_ldl_solve_native.h>
// #include <ATen/ops/linalg_lstsq_native.h>
// #include <ATen/ops/linalg_lu_native.h>
// #include <ATen/ops/linalg_lu_factor_native.h>
// #include <ATen/ops/linalg_lu_factor_ex_native.h>
// #include <ATen/ops/linalg_lu_solve_native.h>
// #include <ATen/ops/linalg_matmul_native.h>
// #include <ATen/ops/linalg_matrix_exp_native.h>
// #include <ATen/ops/linalg_matrix_norm_native.h>
// #include <ATen/ops/linalg_matrix_power_native.h>
// #include <ATen/ops/linalg_matrix_rank_native.h>
// #include <ATen/ops/linalg_multi_dot_native.h>
// #include <ATen/ops/linalg_norm_native.h>
// #include <ATen/ops/linalg_pinv_native.h>
// #include <ATen/ops/linalg_qr_native.h>
// #include <ATen/ops/linalg_slogdet_native.h>
// #include <ATen/ops/linalg_solve_native.h>
// #include <ATen/ops/linalg_solve_ex_native.h>
// #include <ATen/ops/linalg_solve_triangular_native.h>
// #include <ATen/ops/linalg_svd_native.h>
// #include <ATen/ops/linalg_svdvals_native.h>
// #include <ATen/ops/linalg_tensorinv_native.h>
// #include <ATen/ops/linalg_tensorsolve_native.h>
// #include <ATen/ops/linalg_vander_native.h>
// #include <ATen/ops/linalg_vecdot_native.h>
// #include <ATen/ops/linalg_vector_norm_native.h>
// #include <ATen/ops/linear_native.h>
// #include <ATen/ops/linear_backward_native.h>
// #include <ATen/ops/linspace_native.h>
// #include <ATen/ops/log_native.h>
// #include <ATen/ops/log10_native.h>
// #include <ATen/ops/log1p_native.h>
// #include <ATen/ops/log2_native.h>
// #include <ATen/ops/log_normal_native.h>
// #include <ATen/ops/log_sigmoid_native.h>
// #include <ATen/ops/log_sigmoid_backward_native.h>
// #include <ATen/ops/log_sigmoid_forward_native.h>
// #include <ATen/ops/log_softmax_native.h>
// #include <ATen/ops/logaddexp_native.h>
// #include <ATen/ops/logaddexp2_native.h>
// #include <ATen/ops/logcumsumexp_native.h>
// #include <ATen/ops/logdet_native.h>
// #include <ATen/ops/logical_and_native.h>
// #include <ATen/ops/logical_not_native.h>
// #include <ATen/ops/logical_or_native.h>
// #include <ATen/ops/logical_xor_native.h>
// #include <ATen/ops/logit_native.h>
// #include <ATen/ops/logit_backward_native.h>
// #include <ATen/ops/logspace_native.h>
// #include <ATen/ops/logsumexp_native.h>
// #include <ATen/ops/lshift_native.h>
// #include <ATen/ops/lstm_native.h>
// #include <ATen/ops/lstm_cell_native.h>
// #include <ATen/ops/lstm_mps_backward_native.h>
// #include <ATen/ops/lt_native.h>
// #include <ATen/ops/lu_solve_native.h>
// #include <ATen/ops/lu_unpack_native.h>
// #include <ATen/ops/mH_native.h>
// #include <ATen/ops/mT_native.h>
// #include <ATen/ops/margin_ranking_loss_native.h>
// #include <ATen/ops/masked_fill_native.h>
// #include <ATen/ops/masked_scatter_native.h>
// #include <ATen/ops/masked_select_native.h>
// #include <ATen/ops/masked_select_backward_native.h>
// #include <ATen/ops/matmul_native.h>
// #include <ATen/ops/matmul_backward_native.h>
// #include <ATen/ops/matrix_H_native.h>
// #include <ATen/ops/matrix_exp_native.h>
// #include <ATen/ops/matrix_exp_backward_native.h>
// #include <ATen/ops/matrix_power_native.h>
// #include <ATen/ops/max_native.h>
// #include <ATen/ops/max_pool1d_native.h>
// #include <ATen/ops/max_pool1d_with_indices_native.h>
// #include <ATen/ops/max_pool2d_native.h>
// #include <ATen/ops/max_pool2d_backward_native.h>
// #include <ATen/ops/max_pool2d_with_indices_native.h>
// #include <ATen/ops/max_pool2d_with_indices_backward_native.h>
// #include <ATen/ops/max_pool3d_native.h>
// #include <ATen/ops/max_pool3d_with_indices_native.h>
// #include <ATen/ops/max_pool3d_with_indices_backward_native.h>
// #include <ATen/ops/max_unpool2d_native.h>
// #include <ATen/ops/max_unpool3d_native.h>
// #include <ATen/ops/maximum_native.h>
// #include <ATen/ops/mean_native.h>
// #include <ATen/ops/median_native.h>
// #include <ATen/ops/meshgrid_native.h>
// #include <ATen/ops/min_native.h>
// #include <ATen/ops/minimum_native.h>
// #include <ATen/ops/miopen_batch_norm_native.h>
// #include <ATen/ops/miopen_batch_norm_backward_native.h>
// #include <ATen/ops/miopen_convolution_native.h>
// #include <ATen/ops/miopen_convolution_add_relu_native.h>
// #include <ATen/ops/miopen_convolution_relu_native.h>
// #include <ATen/ops/miopen_convolution_transpose_native.h>
// #include <ATen/ops/miopen_depthwise_convolution_native.h>
// #include <ATen/ops/miopen_rnn_native.h>
// #include <ATen/ops/miopen_rnn_backward_native.h>
// #include <ATen/ops/mish_native.h>
// #include <ATen/ops/mish_backward_native.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_native.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h>
// #include <ATen/ops/mkldnn_convolution_native.h>
// #include <ATen/ops/mkldnn_linear_native.h>
// #include <ATen/ops/mkldnn_linear_backward_native.h>
// #include <ATen/ops/mkldnn_linear_backward_input_native.h>
// #include <ATen/ops/mkldnn_linear_backward_weights_native.h>
// #include <ATen/ops/mkldnn_max_pool2d_native.h>
// #include <ATen/ops/mkldnn_max_pool2d_backward_native.h>
// #include <ATen/ops/mkldnn_max_pool3d_native.h>
// #include <ATen/ops/mkldnn_max_pool3d_backward_native.h>
// #include <ATen/ops/mkldnn_reorder_conv2d_weight_native.h>
// #include <ATen/ops/mkldnn_reorder_conv3d_weight_native.h>
// #include <ATen/ops/mkldnn_rnn_layer_native.h>
// #include <ATen/ops/mkldnn_rnn_layer_backward_native.h>
// #include <ATen/ops/mm_native.h>
// #include <ATen/ops/mode_native.h>
// #include <ATen/ops/moveaxis_native.h>
// #include <ATen/ops/movedim_native.h>
// #include <ATen/ops/mps_convolution_backward_native.h>
// #include <ATen/ops/mps_convolution_transpose_backward_native.h>
// #include <ATen/ops/mse_loss_native.h>
// #include <ATen/ops/mse_loss_backward_native.h>
// #include <ATen/ops/msort_native.h>
// #include <ATen/ops/mul_native.h>
// #include <ATen/ops/multi_margin_loss_native.h>
// #include <ATen/ops/multi_margin_loss_backward_native.h>
// #include <ATen/ops/multilabel_margin_loss_native.h>
// #include <ATen/ops/multilabel_margin_loss_backward_native.h>
// #include <ATen/ops/multilabel_margin_loss_forward_native.h>
// #include <ATen/ops/multinomial_native.h>
// #include <ATen/ops/multiply_native.h>
// #include <ATen/ops/mv_native.h>
// #include <ATen/ops/mvlgamma_native.h>
// #include <ATen/ops/nan_to_num_native.h>
// #include <ATen/ops/nanmean_native.h>
// #include <ATen/ops/nanmedian_native.h>
// #include <ATen/ops/nanquantile_native.h>
// #include <ATen/ops/nansum_native.h>
// #include <ATen/ops/narrow_native.h>
// #include <ATen/ops/narrow_copy_native.h>
// #include <ATen/ops/native_batch_norm_native.h>
// #include <ATen/ops/native_batch_norm_backward_native.h>
// #include <ATen/ops/native_channel_shuffle_native.h>
// #include <ATen/ops/native_dropout_native.h>
// #include <ATen/ops/native_dropout_backward_native.h>
// #include <ATen/ops/native_group_norm_native.h>
// #include <ATen/ops/native_group_norm_backward_native.h>
// #include <ATen/ops/native_layer_norm_native.h>
// #include <ATen/ops/native_layer_norm_backward_native.h>
// #include <ATen/ops/native_norm_native.h>
// #include <ATen/ops/ne_native.h>
// #include <ATen/ops/neg_native.h>
// #include <ATen/ops/negative_native.h>
// #include <ATen/ops/nested_to_padded_tensor_native.h>
// #include <ATen/ops/new_empty_native.h>
// #include <ATen/ops/new_empty_strided_native.h>
// #include <ATen/ops/new_full_native.h>
// #include <ATen/ops/new_ones_native.h>
// #include <ATen/ops/new_zeros_native.h>
// #include <ATen/ops/nextafter_native.h>
// #include <ATen/ops/nll_loss_native.h>
// #include <ATen/ops/nll_loss2d_native.h>
// #include <ATen/ops/nll_loss2d_backward_native.h>
// #include <ATen/ops/nll_loss2d_forward_native.h>
// #include <ATen/ops/nll_loss_backward_native.h>
// #include <ATen/ops/nll_loss_forward_native.h>
// #include <ATen/ops/nll_loss_nd_native.h>
// #include <ATen/ops/nonzero_native.h>
// #include <ATen/ops/nonzero_numpy_native.h>
// #include <ATen/ops/nonzero_static_native.h>
// #include <ATen/ops/norm_native.h>
// #include <ATen/ops/norm_except_dim_native.h>
// #include <ATen/ops/normal_native.h>
// #include <ATen/ops/not_equal_native.h>
// #include <ATen/ops/nuclear_norm_native.h>
// #include <ATen/ops/numpy_T_native.h>
// #include <ATen/ops/one_hot_native.h>
// #include <ATen/ops/ones_native.h>
// #include <ATen/ops/ones_like_native.h>
// #include <ATen/ops/or_native.h>
// #include <ATen/ops/orgqr_native.h>
// #include <ATen/ops/ormqr_native.h>
// #include <ATen/ops/outer_native.h>
// #include <ATen/ops/output_nr_native.h>
// #include <ATen/ops/pad_native.h>
// #include <ATen/ops/pad_sequence_native.h>
// #include <ATen/ops/pairwise_distance_native.h>
// #include <ATen/ops/pdist_native.h>
// #include <ATen/ops/permute_native.h>
// #include <ATen/ops/permute_copy_native.h>
// #include <ATen/ops/pin_memory_native.h>
// #include <ATen/ops/pinverse_native.h>
// #include <ATen/ops/pixel_shuffle_native.h>
// #include <ATen/ops/pixel_unshuffle_native.h>
// #include <ATen/ops/poisson_native.h>
// #include <ATen/ops/poisson_nll_loss_native.h>
// #include <ATen/ops/polar_native.h>
// #include <ATen/ops/polygamma_native.h>
// #include <ATen/ops/positive_native.h>
// #include <ATen/ops/pow_native.h>
// #include <ATen/ops/prelu_native.h>
// #include <ATen/ops/prod_native.h>
// #include <ATen/ops/promote_types_native.h>
// #include <ATen/ops/put_native.h>
// #include <ATen/ops/q_per_channel_axis_native.h>
// #include <ATen/ops/q_per_channel_scales_native.h>
// #include <ATen/ops/q_per_channel_zero_points_native.h>
// #include <ATen/ops/q_scale_native.h>
// #include <ATen/ops/q_zero_point_native.h>
// #include <ATen/ops/qr_native.h>
// #include <ATen/ops/qscheme_native.h>
// #include <ATen/ops/quantile_native.h>
// #include <ATen/ops/quantize_per_channel_native.h>
// #include <ATen/ops/quantize_per_tensor_native.h>
// #include <ATen/ops/quantize_per_tensor_dynamic_native.h>
// #include <ATen/ops/quantized_batch_norm_native.h>
// #include <ATen/ops/quantized_gru_cell_native.h>
// #include <ATen/ops/quantized_lstm_cell_native.h>
// #include <ATen/ops/quantized_max_pool1d_native.h>
// #include <ATen/ops/quantized_max_pool2d_native.h>
// #include <ATen/ops/quantized_max_pool3d_native.h>
// #include <ATen/ops/quantized_rnn_relu_cell_native.h>
// #include <ATen/ops/quantized_rnn_tanh_cell_native.h>
// #include <ATen/ops/rad2deg_native.h>
// #include <ATen/ops/rand_native.h>
// #include <ATen/ops/rand_like_native.h>
// #include <ATen/ops/randint_native.h>
// #include <ATen/ops/randint_like_native.h>
// #include <ATen/ops/randn_native.h>
// #include <ATen/ops/randn_like_native.h>
// #include <ATen/ops/random_native.h>
// #include <ATen/ops/randperm_native.h>
// #include <ATen/ops/range_native.h>
// #include <ATen/ops/ravel_native.h>
// #include <ATen/ops/real_native.h>
// #include <ATen/ops/reciprocal_native.h>
// #include <ATen/ops/record_stream_native.h>
// #include <ATen/ops/refine_names_native.h>
// #include <ATen/ops/reflection_pad1d_native.h>
// #include <ATen/ops/reflection_pad1d_backward_native.h>
// #include <ATen/ops/reflection_pad2d_native.h>
// #include <ATen/ops/reflection_pad2d_backward_native.h>
// #include <ATen/ops/reflection_pad3d_native.h>
// #include <ATen/ops/reflection_pad3d_backward_native.h>
// #include <ATen/ops/relu_native.h>
// #include <ATen/ops/relu6_native.h>
// #include <ATen/ops/remainder_native.h>
// #include <ATen/ops/rename_native.h>
// #include <ATen/ops/renorm_native.h>
// #include <ATen/ops/repeat_native.h>
// #include <ATen/ops/repeat_interleave_native.h>
// #include <ATen/ops/replication_pad1d_native.h>
// #include <ATen/ops/replication_pad1d_backward_native.h>
// #include <ATen/ops/replication_pad2d_native.h>
// #include <ATen/ops/replication_pad2d_backward_native.h>
// #include <ATen/ops/replication_pad3d_native.h>
// #include <ATen/ops/replication_pad3d_backward_native.h>
// #include <ATen/ops/requires_grad_native.h>
// #include <ATen/ops/reshape_native.h>
// #include <ATen/ops/reshape_as_native.h>
// #include <ATen/ops/resize_native.h>
// #include <ATen/ops/resize_as_native.h>
// #include <ATen/ops/resize_as_sparse_native.h>
// #include <ATen/ops/resolve_conj_native.h>
// #include <ATen/ops/resolve_neg_native.h>
// #include <ATen/ops/result_type_native.h>
// #include <ATen/ops/retain_grad_native.h>
// #include <ATen/ops/retains_grad_native.h>
// #include <ATen/ops/rnn_relu_native.h>
// #include <ATen/ops/rnn_relu_cell_native.h>
// #include <ATen/ops/rnn_tanh_native.h>
// #include <ATen/ops/rnn_tanh_cell_native.h>
// #include <ATen/ops/roll_native.h>
// #include <ATen/ops/rot90_native.h>
// #include <ATen/ops/round_native.h>
// #include <ATen/ops/row_indices_native.h>
// #include <ATen/ops/row_indices_copy_native.h>
// #include <ATen/ops/row_stack_native.h>
// #include <ATen/ops/rrelu_native.h>
// #include <ATen/ops/rrelu_with_noise_native.h>
// #include <ATen/ops/rrelu_with_noise_backward_native.h>
// #include <ATen/ops/rshift_native.h>
// #include <ATen/ops/rsqrt_native.h>
// #include <ATen/ops/rsub_native.h>
// #include <ATen/ops/scalar_tensor_native.h>
// #include <ATen/ops/scaled_dot_product_attention_native.h>
// #include <ATen/ops/scatter_native.h>
// #include <ATen/ops/scatter_add_native.h>
// #include <ATen/ops/scatter_reduce_native.h>
// #include <ATen/ops/searchsorted_native.h>
// #include <ATen/ops/segment_reduce_native.h>
// #include <ATen/ops/select_native.h>
// #include <ATen/ops/select_backward_native.h>
// #include <ATen/ops/select_copy_native.h>
// #include <ATen/ops/select_scatter_native.h>
// #include <ATen/ops/selu_native.h>
// #include <ATen/ops/set_native.h>
// #include <ATen/ops/set_data_native.h>
// #include <ATen/ops/sgn_native.h>
// #include <ATen/ops/sigmoid_native.h>
// #include <ATen/ops/sigmoid_backward_native.h>
// #include <ATen/ops/sign_native.h>
// #include <ATen/ops/signbit_native.h>
// #include <ATen/ops/silu_native.h>
// #include <ATen/ops/silu_backward_native.h>
// #include <ATen/ops/sin_native.h>
// #include <ATen/ops/sinc_native.h>
// #include <ATen/ops/sinh_native.h>
// #include <ATen/ops/size_native.h>
// #include <ATen/ops/slice_native.h>
// #include <ATen/ops/slice_backward_native.h>
// #include <ATen/ops/slice_copy_native.h>
// #include <ATen/ops/slice_scatter_native.h>
// #include <ATen/ops/slogdet_native.h>
// #include <ATen/ops/slow_conv3d_native.h>
// #include <ATen/ops/slow_conv3d_forward_native.h>
// #include <ATen/ops/slow_conv_dilated2d_native.h>
// #include <ATen/ops/slow_conv_dilated3d_native.h>
// #include <ATen/ops/slow_conv_transpose2d_native.h>
// #include <ATen/ops/slow_conv_transpose3d_native.h>
// #include <ATen/ops/smm_native.h>
// #include <ATen/ops/smooth_l1_loss_native.h>
// #include <ATen/ops/smooth_l1_loss_backward_native.h>
// #include <ATen/ops/soft_margin_loss_native.h>
// #include <ATen/ops/soft_margin_loss_backward_native.h>
// #include <ATen/ops/softmax_native.h>
// #include <ATen/ops/softplus_native.h>
// #include <ATen/ops/softplus_backward_native.h>
// #include <ATen/ops/softshrink_native.h>
// #include <ATen/ops/softshrink_backward_native.h>
// #include <ATen/ops/sort_native.h>
// #include <ATen/ops/sparse_bsc_tensor_native.h>
// #include <ATen/ops/sparse_bsr_tensor_native.h>
// #include <ATen/ops/sparse_compressed_tensor_native.h>
// #include <ATen/ops/sparse_coo_tensor_native.h>
// #include <ATen/ops/sparse_csc_tensor_native.h>
// #include <ATen/ops/sparse_csr_tensor_native.h>
// #include <ATen/ops/sparse_dim_native.h>
// #include <ATen/ops/sparse_mask_native.h>
// #include <ATen/ops/sparse_resize_native.h>
// #include <ATen/ops/sparse_resize_and_clear_native.h>
// #include <ATen/ops/sparse_sampled_addmm_native.h>
// #include <ATen/ops/special_airy_ai_native.h>
// #include <ATen/ops/special_bessel_j0_native.h>
// #include <ATen/ops/special_bessel_j1_native.h>
// #include <ATen/ops/special_bessel_y0_native.h>
// #include <ATen/ops/special_bessel_y1_native.h>
// #include <ATen/ops/special_chebyshev_polynomial_t_native.h>
// #include <ATen/ops/special_chebyshev_polynomial_u_native.h>
// #include <ATen/ops/special_chebyshev_polynomial_v_native.h>
// #include <ATen/ops/special_chebyshev_polynomial_w_native.h>
// #include <ATen/ops/special_digamma_native.h>
// #include <ATen/ops/special_entr_native.h>
// #include <ATen/ops/special_erf_native.h>
// #include <ATen/ops/special_erfc_native.h>
// #include <ATen/ops/special_erfcx_native.h>
// #include <ATen/ops/special_erfinv_native.h>
// #include <ATen/ops/special_exp2_native.h>
// #include <ATen/ops/special_expit_native.h>
// #include <ATen/ops/special_expm1_native.h>
// #include <ATen/ops/special_gammainc_native.h>
// #include <ATen/ops/special_gammaincc_native.h>
// #include <ATen/ops/special_gammaln_native.h>
// #include <ATen/ops/special_hermite_polynomial_h_native.h>
// #include <ATen/ops/special_hermite_polynomial_he_native.h>
// #include <ATen/ops/special_i0_native.h>
// #include <ATen/ops/special_i0e_native.h>
// #include <ATen/ops/special_i1_native.h>
// #include <ATen/ops/special_i1e_native.h>
// #include <ATen/ops/special_laguerre_polynomial_l_native.h>
// #include <ATen/ops/special_legendre_polynomial_p_native.h>
// #include <ATen/ops/special_log1p_native.h>
// #include <ATen/ops/special_log_ndtr_native.h>
// #include <ATen/ops/special_log_softmax_native.h>
// #include <ATen/ops/special_logit_native.h>
// #include <ATen/ops/special_logsumexp_native.h>
// #include <ATen/ops/special_modified_bessel_i0_native.h>
// #include <ATen/ops/special_modified_bessel_i1_native.h>
// #include <ATen/ops/special_modified_bessel_k0_native.h>
// #include <ATen/ops/special_modified_bessel_k1_native.h>
// #include <ATen/ops/special_multigammaln_native.h>
// #include <ATen/ops/special_ndtr_native.h>
// #include <ATen/ops/special_ndtri_native.h>
// #include <ATen/ops/special_polygamma_native.h>
// #include <ATen/ops/special_psi_native.h>
// #include <ATen/ops/special_round_native.h>
// #include <ATen/ops/special_scaled_modified_bessel_k0_native.h>
// #include <ATen/ops/special_scaled_modified_bessel_k1_native.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
// #include <ATen/ops/special_sinc_native.h>
// #include <ATen/ops/special_softmax_native.h>
// #include <ATen/ops/special_spherical_bessel_j0_native.h>
// #include <ATen/ops/special_xlog1py_native.h>
// #include <ATen/ops/special_xlogy_native.h>
// #include <ATen/ops/special_zeta_native.h>
// #include <ATen/ops/split_native.h>
// #include <ATen/ops/split_copy_native.h>
// #include <ATen/ops/split_with_sizes_native.h>
// #include <ATen/ops/split_with_sizes_copy_native.h>
// #include <ATen/ops/sqrt_native.h>
// #include <ATen/ops/square_native.h>
// #include <ATen/ops/squeeze_native.h>
// #include <ATen/ops/squeeze_copy_native.h>
// #include <ATen/ops/sspaddmm_native.h>
// #include <ATen/ops/stack_native.h>
// #include <ATen/ops/std_native.h>
// #include <ATen/ops/std_mean_native.h>
// #include <ATen/ops/stft_native.h>
// #include <ATen/ops/stride_native.h>
// #include <ATen/ops/sub_native.h>
// #include <ATen/ops/subtract_native.h>
// #include <ATen/ops/sum_native.h>
// #include <ATen/ops/sum_to_size_native.h>
// #include <ATen/ops/svd_native.h>
// #include <ATen/ops/swapaxes_native.h>
// #include <ATen/ops/swapdims_native.h>
// #include <ATen/ops/sym_constrain_range_native.h>
// #include <ATen/ops/sym_constrain_range_for_size_native.h>
// #include <ATen/ops/sym_numel_native.h>
// #include <ATen/ops/sym_size_native.h>
// #include <ATen/ops/sym_storage_offset_native.h>
// #include <ATen/ops/sym_stride_native.h>
// #include <ATen/ops/t_native.h>
// #include <ATen/ops/t_copy_native.h>
// #include <ATen/ops/take_native.h>
// #include <ATen/ops/take_along_dim_native.h>
// #include <ATen/ops/tan_native.h>
// #include <ATen/ops/tanh_native.h>
// #include <ATen/ops/tanh_backward_native.h>
// #include <ATen/ops/tensor_split_native.h>
// #include <ATen/ops/tensordot_native.h>
// #include <ATen/ops/thnn_conv2d_native.h>
// #include <ATen/ops/threshold_native.h>
// #include <ATen/ops/threshold_backward_native.h>
// #include <ATen/ops/tile_native.h>
// #include <ATen/ops/to_native.h>
// #include <ATen/ops/to_dense_native.h>
// #include <ATen/ops/to_dense_backward_native.h>
// #include <ATen/ops/to_mkldnn_native.h>
// #include <ATen/ops/to_mkldnn_backward_native.h>
// #include <ATen/ops/to_padded_tensor_native.h>
// #include <ATen/ops/to_sparse_native.h>
// #include <ATen/ops/to_sparse_bsc_native.h>
// #include <ATen/ops/to_sparse_bsr_native.h>
// #include <ATen/ops/to_sparse_csc_native.h>
// #include <ATen/ops/to_sparse_csr_native.h>
// #include <ATen/ops/topk_native.h>
// #include <ATen/ops/trace_native.h>
// #include <ATen/ops/trace_backward_native.h>
// #include <ATen/ops/transpose_native.h>
// #include <ATen/ops/transpose_copy_native.h>
// #include <ATen/ops/trapezoid_native.h>
// #include <ATen/ops/trapz_native.h>
// #include <ATen/ops/triangular_solve_native.h>
// #include <ATen/ops/tril_native.h>
// #include <ATen/ops/tril_indices_native.h>
// #include <ATen/ops/triplet_margin_loss_native.h>
// #include <ATen/ops/triu_native.h>
// #include <ATen/ops/triu_indices_native.h>
// #include <ATen/ops/true_divide_native.h>
// #include <ATen/ops/trunc_native.h>
// #include <ATen/ops/type_as_native.h>
// #include <ATen/ops/unbind_native.h>
// #include <ATen/ops/unbind_copy_native.h>
// #include <ATen/ops/unflatten_native.h>
// #include <ATen/ops/unflatten_dense_tensors_native.h>
// #include <ATen/ops/unfold_native.h>
// #include <ATen/ops/unfold_backward_native.h>
// #include <ATen/ops/unfold_copy_native.h>
// #include <ATen/ops/uniform_native.h>
// #include <ATen/ops/unique_consecutive_native.h>
// #include <ATen/ops/unique_dim_native.h>
// #include <ATen/ops/unique_dim_consecutive_native.h>
// #include <ATen/ops/unsafe_chunk_native.h>
// #include <ATen/ops/unsafe_split_native.h>
// #include <ATen/ops/unsafe_split_with_sizes_native.h>
// #include <ATen/ops/unsqueeze_native.h>
// #include <ATen/ops/unsqueeze_copy_native.h>
// #include <ATen/ops/upsample_bicubic2d_native.h>
// #include <ATen/ops/upsample_bicubic2d_backward_native.h>
// #include <ATen/ops/upsample_bilinear2d_native.h>
// #include <ATen/ops/upsample_bilinear2d_backward_native.h>
// #include <ATen/ops/upsample_linear1d_native.h>
// #include <ATen/ops/upsample_linear1d_backward_native.h>
// #include <ATen/ops/upsample_nearest1d_native.h>
// #include <ATen/ops/upsample_nearest1d_backward_native.h>
// #include <ATen/ops/upsample_nearest2d_native.h>
// #include <ATen/ops/upsample_nearest2d_backward_native.h>
// #include <ATen/ops/upsample_nearest3d_native.h>
// #include <ATen/ops/upsample_nearest3d_backward_native.h>
// #include <ATen/ops/upsample_trilinear3d_native.h>
// #include <ATen/ops/upsample_trilinear3d_backward_native.h>
// #include <ATen/ops/value_selecting_reduction_backward_native.h>
// #include <ATen/ops/values_native.h>
// #include <ATen/ops/values_copy_native.h>
// #include <ATen/ops/vander_native.h>
// #include <ATen/ops/var_native.h>
// #include <ATen/ops/var_mean_native.h>
// #include <ATen/ops/vdot_native.h>
// #include <ATen/ops/view_native.h>
// #include <ATen/ops/view_as_native.h>
// #include <ATen/ops/view_as_complex_native.h>
// #include <ATen/ops/view_as_complex_copy_native.h>
// #include <ATen/ops/view_as_real_native.h>
// #include <ATen/ops/view_as_real_copy_native.h>
// #include <ATen/ops/view_copy_native.h>
// #include <ATen/ops/vsplit_native.h>
// #include <ATen/ops/vstack_native.h>
// #include <ATen/ops/where_native.h>
// #include <ATen/ops/xlogy_native.h>
// #include <ATen/ops/xor_native.h>
// #include <ATen/ops/zero_native.h>
// #include <ATen/ops/zeros_native.h>
// #include <ATen/ops/zeros_like_native.h>




// Parsed from ATen/TensorIndexing.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/TensorBody.h>
// #include <c10/core/SymInt.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #include <ATen/NativeFunctions.h>
// #else
// #include <ATen/ops/alias.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/zeros.h>
// #endif

// #include <ATen/core/List.h>

// #include <utility>

@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MIN();
@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MAX();

@Namespace("at::indexing") public enum TensorIndexType { None(0), Ellipsis(1), SymInt(2), Boolean(3), Slice(4), Tensor(5);

    public final int value;
    private TensorIndexType(int v) { this.value = v; }
    private TensorIndexType(TensorIndexType e) { this.value = e.value; }
    public TensorIndexType intern() { for (TensorIndexType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at::indexing") @MemberGetter public static native @ByRef @Cast("const c10::nullopt_t*") Pointer None();
// Targeting ../EllipsisIndexType.java


@Namespace("at::indexing") @MemberGetter public static native @Const @ByRef EllipsisIndexType Ellipsis();
// Targeting ../Slice.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Slice slice);
// Targeting ../TensorIndex.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef TensorIndex tensor_index);
@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef TensorIndexVector tensor_indices);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @ByVal SymInt start,
    @ByVal SymInt stop,
    @ByVal SymInt step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @Const @ByRef SymIntArrayRefOptional self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @ByVal SymInt index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device arg4,
    @Const @ByRef SymIntArrayRefOptional self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorCPUOrCUDA(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorNonNativeDeviceType(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensor(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value,
    @Const @ByRef Device self_device);

@Namespace("at::indexing::impl") public static native @ByVal Tensor scalarToTensorNonNativeDeviceType(
    @Const @ByRef Scalar v,
    @Const @ByRef TensorOptions options);

@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") LongPointer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") LongBuffer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") long[] dim_ptr);

@Namespace("at::indexing::impl") public static native @ByVal TensorOptionalList typeConvertIndices(
    @Const @ByRef Tensor arg0,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector indices);

// NOTE: Why do we mirror instead of replace the `count_specified_dimensions`
// function in torch/csrc/autograd/python_variable_indexing.cpp? It's because
// `count_specified_dimensions` is on the hot path of Python tensor multi-dim
// indexing (i.e. it's called by `applySlicing` which is called by
// `THPVariable_getitem` / `THPVariable_setitem` when handling indexing of more
// than one dimension). If we were to merge the Python/C++
// `count_specified_dimensions` function, on the Python side we would have to
// construct a `std::vector` container to be consumed by the C++
// `count_specified_dimensions` function, which adds 100s of nanoseconds
// overhead and is undesirable.
@Namespace("at::indexing::impl") public static native @Cast("int64_t") long count_specified_dimensions(
    @Const @ByRef TensorIndexArrayRef indices);
@Namespace("at::indexing::impl") public static native @Cast("int64_t") long count_specified_dimensions(
    @Const @ByRef TensorIndexVector indices);
 // namespace impl

// NOTE: Many functions below are only for consumption from Python indexing
// implementation, they include:
//
// - `Tensor scalarToTensor(...)`
// - `IntArrayRef slicePrefix1sSize(...)`
// - `void copy_to(...)`
// - `Tensor handleDimInMultiDimIndexing(...)`
// - `Tensor dispatch_index(...)`
// - `Tensor dispatch_index_put_(...)`
// - `Tensor get_item(...)`
// - `void set_item(...)`
//
// The rest of the functions are in `at::indexing::impl` namespace, signifying
// that they shouldn't be used from Python indexing implementation.
@Namespace("at::indexing") public static native @ByVal Tensor scalarToTensor(
    @Const @ByRef Scalar v,
    @Const @ByRef TensorOptions options,
    @Const @ByRef Device self_device);

// To match numpy semantics:
// As a special case for backwards compatibility,
// strip away unit dimensions from the left of 'src'
@Namespace("at::indexing") public static native @ByVal SymIntArrayRef slicePrefix1sSize(@Const @ByRef SymIntArrayRef sizes);

@Namespace("at::indexing") public static native void copy_to(@Const @ByRef Tensor dst, @Const @ByRef Tensor src);

// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor
// indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
// This mirrors `applySlicing` in
// torch/csrc/autograd/python_variable_indexing.cpp
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @Const @ByRef SymIntArrayRefOptional self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexVector indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @Const @ByRef SymIntArrayRefOptional self_sizes);
 // namespace impl

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index(
    @Const @ByRef Tensor self,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector indices);

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index_put_(
    @ByRef Tensor self,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector indices,
    @Const @ByRef Tensor value);

// NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing
// functions from Python ]
//
// Question: When should we set `disable_slice_optimization` to `true` when
// calling C++ tensor indexing functions from Python indexing code?
//
// Answer: What "slice optimization" means: when we have a slicing expression
// like `x[0:5, 0]`, where the sliced tensor was of size 5 in dimension 0, we
// would skip dispatching the actual slice call as an optimization. However,
// here are the cases where we DON'T want this optimization:
//
// 1. When we are doing 1-D slicing (e.g. `tensor[:]`).
//    Reason: we always return a shallow copy for expressions such as
//    `tensor[:]` / `tensor[...]` / `tensor[:, :]`. (Note that for `tensor[:,
//    :]`, we return an alias of `tensor` by doing the following:
//    ```
//    Tensor sliced = impl::applySlicing(self, indices, tensorIndices,
//    disable_slice_optimization, self_device, self_sizes); if
//    (tensorIndices.empty()) {
//      if (sliced.is_same(self)) {
//        // ensure we return a shallow copy for things like x[...]
//        sliced = at::alias(sliced);
//      }
//      return sliced;
//    }
//    ```)
// 2. When we are doing JIT tracing.
//    Reason: JIT tracing needs the `self.slice(...)` call to properly trace the
//    slice operation.

// This mirrors `THPVariable_getitem` in
// torch/csrc/autograd/python_variable_indexing.cpp See NOTE [ Setting
// `disable_slice_optimization` when calling C++ tensor indexing functions from
// Python ]
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexVector indices,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexVector indices);

// This mirrors `THPVariable_setitem` in
// torch/csrc/autograd/python_variable_indexing.cpp for "the assigned value is a
// Tensor" case See NOTE [ Setting `disable_slice_optimization` when calling C++
// tensor indexing functions from Python ]
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Const @ByRef Tensor value,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Const @ByRef Tensor value);
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexVector indices,
    @Const @ByRef Tensor value,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexVector indices,
    @Const @ByRef Tensor value);

 // namespace indexing
 // namespace at


// Parsed from ATen/TensorOperators.h

// #pragma once

// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/empty_like.h>
// #endif

// #include <stdexcept>
// #include <string>

// #define AT_FORALL_BINARY_OPS(_)
//   _(+, x.add(y), y.add(x))
//   _(*, x.mul(y), y.mul(x))
//   _(-,
//     x.sub(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).sub_(y))
//   _(/,
//     x.div(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).div_(y))
//   _(%,
//     x.remainder(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).remainder_(y))
//   _(&, x.bitwise_and(y), y.bitwise_and(x))
//   _(|, x.bitwise_or(y), y.bitwise_or(x))
//   _(^, x.bitwise_xor(y), y.bitwise_xor(x))
//   _(<, x.t(y), y.gt(x))
//   _(<=, x.e(y), y.ge(x))
//   _(>, x.gt(y), y.t(x))
//   _(>=, x.ge(y), y.e(x))
//   _(==, x.eq(y), y.eq(x))
//   _(!=, x.ne(y), y.ne(x))

// #define DEFINE_OPERATOR(op, body, reverse_scalar_body)
//   static inline Tensor operator op(const Tensor& x, const Tensor& y) {
//     return body;
//   }
//   static inline Tensor operator op(const Tensor& x, const Scalar& y) {
//     return body;
//   }
//   static inline Tensor operator op(const Scalar& x, const Tensor& y) {
//     return reverse_scalar_body;
//   }
  @Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
// #undef DEFINE_OPERATOR
// #undef AT_FORALL_BINARY_OPS

 // namespace at


// Parsed from ATen/Version.h

// #include <ATen/Context.h>

/** Returns a detailed string describing the configuration PyTorch. */
@Namespace("at") public static native @StdString BytePointer show_config();

@Namespace("at") public static native @StdString BytePointer get_mkl_version();

@Namespace("at") public static native @StdString BytePointer get_mkldnn_version();

@Namespace("at") public static native @StdString BytePointer get_openmp_version();

@Namespace("at") public static native @StdString BytePointer get_cxx_flags();

@Namespace("at") public static native @StdString BytePointer get_cpu_capability();

 // namespace at


// Parsed from ATen/core/Scalar.h

// #include <c10/core/Scalar.h>


// Parsed from ATen/core/UnsafeFromTH.h

// #pragma once
// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @ByVal Tensor unsafeTensorFromTH(Pointer th_pointer, @Cast("bool") boolean retain);

@Namespace("at") public static native @Cast({"", "c10::Storage&&"}) @StdMove Storage unsafeStorageFromTH(Pointer th_pointer, @Cast("bool") boolean retain);




// Parsed from ATen/ATen.h

// #pragma once

// #if !defined(_MSC_VER) && __cplusplus < 201703L
// #error C++17 or later compatible compiler is required to use ATen.
// #endif

// #include <ATen/Context.h>
// #include <ATen/Device.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/DimVector.h>
// #include <ATen/Dispatch.h>
// #include <ATen/Formatting.h>
// #include <ATen/Functions.h>
// #include <ATen/NamedTensor.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/TensorIndexing.h>
// #include <ATen/TensorOperators.h>
// #include <ATen/Version.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Scalar.h>
// #include <ATen/core/UnsafeFromTH.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/core/Allocator.h>
// #include <c10/core/InferenceMode.h>
// #include <c10/core/Layout.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// TODO: try to remove this
// There is some back story, see https://github.com/pytorch/pytorch/issues/48684
// #include <ATen/NativeFunctions.h>


// Parsed from torch/csrc/api/include/torch/detail/TensorDataContainer.h

// #pragma once

// #include <ATen/Dispatch.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/grad_mode.h>

// #include <c10/util/irange.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/empty.h>
// #include <ATen/ops/tensor.h>
// #endif

// #include <initializer_list>

@Namespace("torch::detail") public enum TensorDataContainerType { Scalar(0), InitList(1), Tensor(2);

    public final int value;
    private TensorDataContainerType(int v) { this.value = v; }
    private TensorDataContainerType(TensorDataContainerType e) { this.value = e.value; }
    public TensorDataContainerType intern() { for (TensorDataContainerType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::detail") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Cast("const torch::detail::TensorDataContainer*") @ByRef Pointer tensor_data_container);

// FIXME: There is no `operator<<` overload for `at::kBFloat16` type,
// and we need to convert it to `float` type using `operator float()` function
// defined in `c10/util/BFloat16.h`.
// Tracking issue: https://github.com/pytorch/pytorch/issues/28845
@Namespace("torch::detail") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @ByVal BFloat16 value);

@Namespace("torch::detail") public static native ScalarType compute_desired_dtype(ScalarType scalar_type);

// We use `TensorDataContainer` to support converting the following data
// container types into the equivalent Tensor:
//
// 1. Arbitrarily nested braced-init-list (e.g. `{{1, 2}, {3, 4}}`).
// 2. `at::ArrayRef` of supported tensor data types.
// 3. `std::vector` of supported tensor data types.
//
// At any time, a `TensorDataContainer` object represents one of the following:
//
// 1. A scalar with value `scalar()` and type `scalar_type()`.
// 2. A Tensor represented in `std::initializer_list<TensorDataContainer>` form,
//    with value `init_list()`, Tensor scalar type `scalar_type()`, and Tensor
//    sizes `sizes()`.
// 3. A Tensor represented in `at::Tensor` form, with value `tensor()`, scalar
// type `scalar_type()`,
//    and Tensor sizes `sizes()`.
//
// All the infrastructure here is mostly to support converting an arbitrarily
// nested braced-init-list to the equivalent Tensor successfully. Consider the
// following example:
//
// `torch::tensor({{1}, {2}})`
//
// this will call into the `torch::tensor` function:
//
// `at::Tensor tensor(detail::TensorDataContainer tensor_data_container, const
// at::TensorOptions& options = {})`
//
// the compiler will first try to convert `{{1}, {2}}` to `TensorDataContainer`
// type:
//
// `TensorDataContainer({{1}, {2}})`
//
// which matches to the
// `TensorDataContainer(std::initializer_list<TensorDataContainer>)`
// constructor, and in an attempt to convert `{1}` and `{2}` to
// `TensorDataContainer`, it calls the following:
//
// `TensorDataContainer({1})`  (same call path happens for `{2}`, and we'll just
// focus on `{1}` here)
//
// At this point, theoretically there are two plausible ways for `{1}` to be
// matched to one of the constructors of `TensorDataContainer`:
//
// 1. It can be a list-initialization of a scalar value, thus matching
// `TensorDataContainer(int value)`.
// 2. It can be converted to `std::initializer_list<TensorDataContainer>`, thus
// matching
//    `TensorDataContainer(std::initializer_list<TensorDataContainer>)`.
//
// How does the compiler decide which one to choose? According to
// `https://en.cppreference.com/w/cpp/language/list_initialization`,
// braced-init-list always prefers the constructor that takes
// `std::initializer_list`. Hence we happily move forward with constructor #2,
// and it calls the following:
//
// `TensorDataContainer(1)`
//
// Now it matches `TensorDataContainer(int value)`, which stores `1` as a scalar
// value. All is good.

 // namespace detail

 // namespace torch


// Parsed from torch/csrc/autograd/generated/variable_factories.h

// #pragma once

// @generated from ../tools/autograd/templates/variable_factories.h

// #include <ATen/core/Tensor.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/grad_mode.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/core/MemoryFormat.h>
// #include <torch/csrc/api/include/torch/detail/TensorDataContainer.h>
// #include <torch/csrc/autograd/variable.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/from_blob.h>
// #include <ATen/ops/_make_dep_token.h>
// #include <ATen/ops/_cudnn_init_dropout_state.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/empty_permuted.h>
// #include <ATen/ops/_empty_affine_quantized.h>
// #include <ATen/ops/_empty_per_channel_affine_quantized.h>
// #include <ATen/ops/empty_quantized.h>
// #include <ATen/ops/empty_like.h>
// #include <ATen/ops/empty_strided.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full_like.h>
// #include <ATen/ops/from_file.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/linspace.h>
// #include <ATen/ops/logspace.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones_like.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand_like.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn_like.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/_efficientzerotensor.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/zeros_like.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/_sparse_compressed_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsc_tensor_unsafe.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/_sparse_coo_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors.h>
// #include <ATen/ops/_to_copy.h>
// #include <ATen/ops/tril_indices.h>
// #include <ATen/ops/triu_indices.h>
// #include <ATen/ops/normal.h>
// #include <ATen/ops/fft_fftfreq.h>
// #include <ATen/ops/fft_rfftfreq.h>
// #endif

// #include <functional>
// #include <initializer_list>
// #include <utility>

/** NOTE: Currently {@code torch::tensor(...)} doesn't support mixed data types
 *  (i.e. {@code torch::tensor({{bool, 2.0}})} doesn't work). We might be able to
 *  support it in the future by iterating over all sub-lists to find
 *  the largest data type that can represent all of the elements, or by using
 *  variadic templates.
 * 
 *  NOTE: C++ {@code torch::tensor} with a floating-point type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of floating-point types always produces a tensor of dtype
 *  {@code torch::get_default_dtype()}, matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: C++ {@code torch::tensor} with an integer type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of integer types always produces a tensor of dtype {@code at::kLong}
 *  (aka. int64_t), matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: The following dtypes are not supported by {@code torch::tensor} currently:
 *  - {@code unsigned int}
 *  - {@code unsigned long int}
 *  - {@code unsigned long long int}
 *  - {@code long long int} */
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container, @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container);

/** A generic deleter function. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code deleter} function (a
 *  {@code std::function<void(void*)>}) will be called on the {@code data} when the Tensor
 *  data would normally be deallocated. The {@code TensorOptions} specify additional
 *  configuration options for the returned tensor, such as what type to
 *  interpret the {@code data} as. */
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    @ByVal LongArrayRef strides,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] strides,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code TensorOptions}
 *  specify additional configuration options for the returned tensor, such as
 *  what type to interpret the {@code data} as. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The {@code deleter}
 *  (a {@code std::function<void(void*)>}) function will be called on the {@code data} when
 *  the Tensor data would normally be deallocated. The {@code TensorOptions} specify
 *  additional configuration options for the returned tensor, such as what type
 *  to interpret the {@code data} as. */
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal LongArrayRef sizes,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] sizes,
    PointerConsumer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The
 *  {@code TensorOptions} specify additional configuration options for the returned
 *  tensor, such as what type to interpret the {@code data} as. */

@Namespace("torch") public static native @ByVal Tensor _make_dep_token(@ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal Tensor _make_dep_token();
@Namespace("torch") public static native @ByVal @Name("_cudnn_init_dropout_state") Tensor torch__cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar end);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal Tensor _empty_affine_quantized_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal Tensor _empty_affine_quantized_symint(@ByVal SymIntArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("torch") public static native @ByVal Tensor _empty_per_channel_affine_quantized_symint(@ByVal SymIntArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal Tensor _empty_per_channel_affine_quantized_symint(@ByVal SymIntArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor qtensor);
@Namespace("torch") public static native @ByVal @Name("empty_like") Tensor torch_empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_like") Tensor torch_empty_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal LongArrayRef size, @ByVal LongArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal LongArrayRef size, @ByVal LongArrayRef stride);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... stride);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @Cast("int64_t") long m);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("full_like") Tensor torch_full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("full_like") Tensor torch_full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@StringView BytePointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@StringView BytePointer filename);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@StringView String filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@StringView String filename);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
@Namespace("torch") public static native @ByVal @Name("linspace") Tensor torch_linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("linspace") Tensor torch_linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
@Namespace("torch") public static native @ByVal @Name("logspace") Tensor torch_logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("logspace") Tensor torch_logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("ones_like") Tensor torch_ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("ones_like") Tensor torch_ones_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("scalar_tensor") Tensor torch_scalar_tensor(@Const @ByRef Scalar s, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("scalar_tensor") Tensor torch_scalar_tensor(@Const @ByRef Scalar s);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("rand_like") Tensor torch_rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("rand_like") Tensor torch_rand_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn_like") Tensor torch_randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randn_like") Tensor torch_randn_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("range") Tensor torch_range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("range") Tensor torch_range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal Tensor _efficientzerotensor_symint(@ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor _efficientzerotensor_symint(@ByVal SymIntArrayRef size);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("zeros_like") Tensor torch_zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("zeros_like") Tensor torch_zeros_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal Tensor _sparse_coo_tensor_unsafe_symint(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal SymIntArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal Tensor _sparse_coo_tensor_unsafe_symint(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal SymIntArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims") Tensor torch__sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims") Tensor torch__sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional is_coalesced);
@Namespace("torch") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_to_copy") Tensor torch__to_copy(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @Cast("bool") boolean non_blocking/*=false*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_to_copy") Tensor torch__to_copy(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("tril_indices") Tensor torch_tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("tril_indices") Tensor torch_tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("torch") public static native @ByVal @Name("triu_indices") Tensor torch_triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("triu_indices") Tensor torch_triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);
@Namespace("torch") public static native @ByVal @Name("fft_fftfreq") Tensor torch_fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("fft_fftfreq") Tensor torch_fft_fftfreq(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("fft_rfftfreq") Tensor torch_fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("fft_rfftfreq") Tensor torch_fft_rfftfreq(@Cast("int64_t") long n);

 // namespace torch


// Parsed from c10/core/PyHandleCache.h

// #pragma once

// #include <c10/core/impl/PyInterpreter.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/python_stub.h>

// #include <atomic>

// A PyHandleCache represents a cached pointer from a C++ object to
// a Python object that represents that object analogously in Python.
// Upon a cache hit, the relevant object can be retrieved after a test
// and then a memory load.  Two conditions must hold to be able to use this
// class:
//
//  - This must truly be a cache; e.g., the caller must be able to produce
//    the object some other way if the cache hit misses.
//
//  - This must truly be a handle; e.g., the Python object referenced by
//    this class must have static lifetime.  This means we don't have to
//    maintain strong ownership or deallocate the object when the C++ object
//    dies.  Static lifetime is a good idea in conjunction with the cache,
//    since if you are producing a fresh object on miss you won't be
//    maintaining object identity.  If you need bidirectional ownership,
//    you will want to factor out the pattern in TensorImpl with
//    resurrection.
//
// This cache is expected to not improve perf under torchdeploy, as one
// interpreter will fill up the cache, and all the interpreters will be
// unable to use the slot.  A potential improvement is to have multiple
// slots (one per interpreter), which will work in deployment scenarios
// where there a stable, fixed number of interpreters.  You can also store
// the relevant state in the Python library, rather than in the non-Python
// library (although in many cases, this is not convenient, as there may
// not be a way to conveniently index based on the object.)

 // namespace c10


// Parsed from c10/util/Bitset.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Optional.h>
// #if defined(_MSC_VER)
// #endif
// Targeting ../bitset.java



@Namespace("c10::utils") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@ByVal bitset lhs, @ByVal bitset rhs);

 // namespace utils
 // namespace c10


// Parsed from ATen/core/dispatch/DispatchKeyExtractor.h

// #pragma once

// #include <cstdint>
// #include <ATen/core/function_schema.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/Bitset.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/irange.h>
// #include <ATen/core/Variadic.h>
// #include <ATen/core/stack.h>

// Take a DispatchKeySet for a Tensor and determine what the actual dispatch
// DispatchKey should be, taking into account TLS, and skipping backends which
// fall through.
//
// Unlike Tensor::key_set(), the value of this on a tensor can change depending
// on TLS.
//
// NB: If there is no valid dispatch key, this will return Undefined
@Namespace("c10::impl") public static native @ByVal DispatchKeySet computeDispatchKeySet(
    @ByVal DispatchKeySet ks,
    @ByVal DispatchKeySet key_mask
);


  // A small gadget to extract the DispatchKeySet from types which are known
  // to have it.  Used to extract dispatch keys from unboxed calls.

  // NB: take by const reference (Don't do universal forwarding here! You
  // don't want to move into this function!)

// Targeting ../DispatchKeyExtractor.java






// Parsed from ATen/core/dispatch/OperatorEntry.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/either.h>
// #include <c10/util/Optional.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/PyHandleCache.h>
// #include <c10/core/SafePyObject.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/dispatch/DispatchKeyExtractor.h>

// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/core/enum_tag.h>

// #include <list>
// #include <array>

// #ifdef C10_MOBILE
// #endif

// This data structure represents a kernel that was registered to us from a
// user.  Unlike KernelFunction, AnnotatedKernel contains some extra metadata
// about the kernel that isn't necessary for actual dispatching (this is why
// we don't put AnnotatedKernel in the actual DispatchTable), but is useful for
// giving good error messages.

// This data structure represents operator schema, with metadata specifying
// where the registration of this schema occurred

// Internal data structure that records information about a specific operator.
// It's not part of the public API; typically, users will interact with
// OperatorHandle instead.
//
// Concurrent writes to OperatorEntry are protected by the GLOBAL Dispatcher
// lock (this is important because some methods in OperatorEntry access
// dispatcher state)

 // namespace impl
 // namespace c10


// Parsed from c10/util/Synchronized.h

// #pragma once

// #include <mutex>

// #include <c10/util/C++17.h>

/**
 * A very simple Synchronization class for error-free use of data
 * in a multi-threaded context. See folly/docs/Synchronized.md for
 * the inspiration of this class.
 *
 * Full URL:
 * https://github.com/facebook/folly/blob/main/folly/docs/Synchronized.md
 *
 * This class implements a small subset of the generic functionality
 * implemented by folly:Synchronized<T>. Specifically, only withLock<T>
 * is implemented here since it's the smallest possible API that is
 * able to cover a large surface area of functionality offered by
 * folly::Synchronized<T>.
 */
 // end namespace c10


// Parsed from ATen/core/dispatch/Dispatcher.h

// #pragma once

// #include <ATen/SequenceNumber.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/boxing/impl/boxing.h>
// #include <ATen/core/dispatch/OperatorEntry.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/record_function.h>
// #include <c10/util/Exception.h>
// #include <c10/util/LeftRight.h>
// #include <list>
// #include <mutex>
// #include <condition_variable>
// #include <type_traits>
// #include <c10/core/SafePyObject.h>

// #include <ATen/core/grad_mode.h>
// #include <ATen/core/enum_tag.h>

// #ifndef NDEBUG
// #include <iostream>
// #endif

@Namespace("c10") public static native @Cast("bool") boolean show_dispatch_trace();
@Namespace("c10") public static native void dispatch_trace_nesting_incr();
@Namespace("c10") public static native void dispatch_trace_nesting_decr();
@Namespace("c10") public static native @Cast("int64_t") long dispatch_trace_nesting_value();
// Targeting ../OpRegistrationListener.java



// Targeting ../Dispatcher.java


// Targeting ../OperatorHandle.java



/**
 * This is a handle to an operator schema registered with the dispatcher.
 * It holds the same information as an OperatorHandle, but it is templated
 * on the operator arguments and allows calling the operator in an
 * unboxed way.
 */

// CaptureKernelCall is intended to capture return values from Dispatcher
// unboxed kernel calls. A record function may request to get outputs from the
// kernel calls. For boxed kernels, it's straightforward, the returned values
// are in the stack object. The stack can be passed to record functions. For
// unboxed kernels, we need to handle different kinds of return values, cache
// them temporarily, then release the values for the actual function call
// return.

// Handle the lvalue reference differently since it should not be moved.


// Handle case where the kernel returns void.

 // namespace detail

// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&




// NB: this doesn't count as a "true" dispatcher jump, so no instrumentation




 // namespace c10

 // namespace std


// Parsed from torch/csrc/api/include/torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// TODO: These don't really belong here but torchvision builds in CI need them
// Remove once the torchvision version being compiled in CI is updated
// #include <ATen/core/dispatch/Dispatcher.h>
// #include <torch/library.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all
// declarations in `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section
// "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all
// declarations that are located in N and all declarations that are located in
// the inline namespace members of N (and, transitively, in their inline
// namespace members). If there are no declarations in that set then it
// considers declarations in all namespaces named by using-directives found in N
// and in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with
// the same signature (e.g. both `at::func()` and `torch::func()` exist), after
// `namespace torch { using namespace at; }`, when we call `torch::func()`, the
// `func()` function defined in `torch::` namespace will always be called, and
// the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/dataloader_options.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/types.h>

// #include <chrono>
// #include <cstddef>
// Targeting ../DataLoaderOptions.java


// Targeting ../FullDataLoaderOptions.java


 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/detail/queue.h

// #pragma once

// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <chrono>
// #include <condition_variable>
// #include <cstddef>
// #include <mutex>
// #include <queue>

/** A basic locked, blocking MPMC queue.
 * 
 *  Every {@code push} and {@code pop} is guarded by a mutex. A condition variable is used
 *  to communicate insertion of new elements, such that waiting threads will be
 *  woken up if they are currently waiting inside a call to {@code pop()}.
 * 
 *  Note that this data structure is written specifically for use with the
 *  {@code DataLoader}. Its behavior is tailored to this use case and may not be
 *  applicable to more general uses. */
 // namespace detail
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/detail/data_shuttle.h

// #pragma once

// #include <torch/data/detail/queue.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <chrono>
// #include <utility>

/** Encapsulates the full life cycle of DataLoader jobs.
 * 
 *  When a new job is enqueued to the {@code DataShuttle}, a counter for in-flight
 *  jobs is bumped. This job is said to be "in-flight" until its result is
 *  popped. Worker threads dequeue jobs as soon as they are available. When a
 *  worker finishes a job, it enqueues the result. Only when the main thread
 *  dequeues a result is the count of in-flight jobs decremented. When the main
 *  thread attempts to dequeue a job but no jobs are in-flight, that means the
 *  epoch is complete and {@code pop_result} returns an empty optional. */

 // namespace detail
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/detail/sequencers.h

// #pragma once

// #include <torch/types.h>

// #include <algorithm>
// #include <cstddef>
// #include <vector>
 // namespace detail

/** A {@code Sequencer} accepts a function that yields the next result of a
 *  {@code DataLoader} and then has the opportunity to influence the order in which
 *  these results are returned. The {@code NoSequencer} does not enforce any
 *  sequencing and returns any result directly. The {@code OrderedSequencer} instead
 *  buffers results internally to return them in order of their sequence number. */

/** A {@code Sequencer} that does not enforce any ordering. It is effectively the
 *  identity function. */

/** A {@code Sequencer} that buffers results and returns them in order of their
 *  sequence number. The {@code OrderedSequencer} maintains an internal, monotonically
 *  incrementing counter for the next sequence number it expects. If it receives
 *  a result with a higher sequence number, it will buffer it for later (when
 *  the sequence number reaches that of this result). Otherwise, if the sequence
 *  numbers match, the result is returned.
 * 
 *  Implementation note: The {@code OrderedSequencer} is implemented with a fixed-size
 *  buffer. Let {@code m} be the maximum number of jobs in the data loader's queue and
 *  {@code s} be the current sequence number. Assume {@code m} jobs are scheduled in the
 *  {@code DataLoader}. Any new result is stored at index {@code job.sqn mod m} in the
 *  {@code OrderedSequencer}. Why are we sure sequence numbers of new jobs will not
 *  collide with sequence numbers of buffered jobs? The {@code OrderedSequencer} will
 *  not return from {@code next()} until it receives the result with sqn {@code s}. This
 *  means no new jobs can be scheduled in the {@code DataLoader} in the meantime,
 *  which enforces that as long as sqn {@code s} has not been received, {@code s + m} (which
 *  would cause a collision in the fixed-size buffer) will not yet be scheduled. */
 // namespace sequencers
 // namespace detail
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/iterator.h

// #pragma once

// #include <torch/csrc/utils/variadic.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <functional>
// #include <iterator>
// #include <memory>
// #include <type_traits>
// #include <utility>
// For increased safety and more separated logic, this implementation of
// `Iterator` consists of a `ValidIterator` and a `SentinelIterator`. A
// `ValidIterator` yields new batches until the `DataLoader` is exhausted. While
// the `DataLoader` is not exhausted, `ValidIterator`s compare equal if they are
// the same object. When the `ValidIterator` becomes exhausted, it compares
// equal to the `SentinelIterator`, but not before. Half the code here is to
// implement double dispatch for the comparison. Got damnit, C++.

/** Base class for the {@code ValidIterator} and {@code SentinelIterator} */

// Targeting ../ExampleIterator.java


// Targeting ../ExampleVectorIterator.java


// Targeting ../TensorExampleIterator.java


// Targeting ../TensorExampleVectorIterator.java


 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/base.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <mutex>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../Sampler.java


// Targeting ../BatchSizeSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/random.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../RandomSampler.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/worker_exception.h

// #pragma once

// #include <exception>
// #include <string>
// #include <utility>

/** An exception thrown when a DataLoader's worker thread throws an exception,
 *  which is caught. A {@code WorkerException} stores an {@code exception_ptr} to the
 *  original exception thrown in the worker thread. */

 // namespace data
 // namespace torch


// Parsed from torch/csrc/utils/memory.h

// #pragma once

// #include <memory>

// Reference:
// https://github.com/llvm-mirror/libcxx/blob/master/include/memory#L3091


 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/dataloader/base.h

// #pragma once

// #include <torch/data/dataloader_options.h>
// #include <torch/data/detail/data_shuttle.h>
// #include <torch/data/detail/sequencers.h>
// #include <torch/data/iterator.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/worker_exception.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <cstddef>
// #include <exception>
// #include <memory>
// #include <thread>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../MNISTRandomDataLoaderBase.java


// Targeting ../ChunkRandomDataLoaderBase.java


// Targeting ../JavaRandomDataLoaderBase.java


// Targeting ../JavaDistributedRandomDataLoaderBase.java


// Targeting ../JavaDistributedSequentialDataLoaderBase.java


// Targeting ../JavaSequentialDataLoaderBase.java


// Targeting ../JavaStreamDataLoaderBase.java


// Targeting ../JavaStatefulDataLoaderBase.java


// Targeting ../ChunkRandomTensorDataLoaderBase.java


// Targeting ../JavaRandomTensorDataLoaderBase.java


// Targeting ../JavaDistributedRandomTensorDataLoaderBase.java


// Targeting ../JavaDistributedSequentialTensorDataLoaderBase.java


// Targeting ../JavaSequentialTensorDataLoaderBase.java


// Targeting ../JavaStreamTensorDataLoaderBase.java


// Targeting ../JavaStatefulTensorDataLoaderBase.java


 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/dataloader/stateful.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/data/dataloader/base.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../ChunkRandomDataLoader.java


// Targeting ../JavaStatefulDataLoader.java


// Targeting ../ChunkRandomTensorDataLoader.java


// Targeting ../JavaStatefulTensorDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/dataloader/stateless.h

// #pragma once

// #include <torch/data/dataloader/base.h>
// #include <torch/data/worker_exception.h>

// #include <torch/csrc/utils/memory.h>

// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../MNISTRandomDataLoader.java


// Targeting ../JavaRandomDataLoader.java


// Targeting ../JavaDistributedRandomDataLoader.java


// Targeting ../JavaDistributedSequentialDataLoader.java


// Targeting ../JavaSequentialDataLoader.java


// Targeting ../JavaStreamDataLoader.java


// Targeting ../JavaRandomTensorDataLoader.java


// Targeting ../JavaDistributedRandomTensorDataLoader.java


// Targeting ../JavaDistributedSequentialTensorDataLoader.java


// Targeting ../JavaSequentialTensorDataLoader.java


// Targeting ../JavaStreamTensorDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/dataloader.h

// #pragma once

// #include <torch/data/dataloader/stateful.h>
// #include <torch/data/dataloader/stateless.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <memory>
// #include <type_traits>
// #include <utility>

/** Creates a {@code DataLoader} instance for a stateless {@code dataset}, a {@code sampler} and
 *  some {@code options}. */

/** Creates a {@code DataLoader} instance for a stateless {@code dataset} and some
 *  {@code options}. A sampler (by default a {@code RandomSampler}) will be constructed from
 *  the size of the dataset. */

/** Creates a {@code DataLoader} for a stateful {@code dataset} and some {@code options}. */
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/example.h

// #pragma once

// #include <torch/types.h>
// Targeting ../Example.java


// Targeting ../TensorExample.java


// Targeting ../NoTarget.java


 // namespace example

/** A specialization for {@code Example} that does not have a target.
 * 
 *  This class exists so that code can be written for a templated {@code Example}
 *  type, and work both for labeled and unlabeled datasets. */
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/base.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <cstdint>
// #include <type_traits>
// #include <utility>
// #include <vector> // NOLINT
 // namespace datasets
 // namespace data
 // namespace torch

// Targeting ../MNISTBatchDataset.java


// Targeting ../MNISTMapBatchDataset.java


// Targeting ../TensorBatchDataset.java


// Targeting ../ChunkBatchDataset.java


// Targeting ../ChunkBatchSharedBatchDataset.java


// Targeting ../ChunkMapBatchDataset.java


// Targeting ../JavaBatchDataset.java


// Targeting ../JavaStreamBatchDataset.java


// Targeting ../JavaStatefulBatchDataset.java


// Targeting ../ChunkTensorBatchDataset.java


// Targeting ../ChunkBatchSharedTensorBatchDataset.java


// Targeting ../ChunkMapTensorBatchDataset.java


// Targeting ../JavaTensorBatchDataset.java


// Targeting ../JavaStreamTensorBatchDataset.java


// Targeting ../JavaStatefulTensorBatchDataset.java


// Targeting ../MNISTDataset.java


// Targeting ../TensorDatasetBase.java


// Targeting ../JavaDatasetBase.java


// Targeting ../JavaTensorDatasetBase.java



/** A {@code StreamDataset} represents a dataset that is a potentially infinite
 *  stream. It takes as batch index only a number, which is the batch size, and
 *  yields that many elements from the stream. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/stateful.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../ChunkStatefulDataset.java


// Targeting ../JavaStatefulDatasetBase.java


// Targeting ../ChunkStatefulTensorDataset.java


// Targeting ../JavaStatefulTensorDatasetBase.java



/** Serializes a statefulDataset to {@code OutputArchive}. */

/** Deserializes a statefulDataset from an {@code InputArchive}. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/custom_batch_request.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <cstddef>
// Targeting ../CustomBatchRequest.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/distributed.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../DistributedSampler.java


// Targeting ../DistributedRandomSampler.java


// Targeting ../DistributedSequentialSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/sequential.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../SequentialSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/imethod.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <vector>
// Targeting ../IMethod.java



 // namespace torch


// Parsed from torch/csrc/jit/ir/attributes.h

// #pragma once
// #include <ATen/core/Tensor.h>
// #include <string>
// #include <vector>

// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/symbol.h>

// #include <torch/csrc/Export.h>

@Namespace("torch::jit") @MemberGetter public static native int max_tensor_display_size();

@Name("torch::jit::AttributeKind") public enum JitAttributeKind {
  f(0),
  fs(1),
  c(2),
  cs(3),
  i(4),
  is(5),
  s(6),
  ss(7),
  t(8),
  ts(9),
  g(10),
  gs(11),
  ty(12),
  tys(13),
  ival(14);

    public final int value;
    private JitAttributeKind(int v) { this.value = v; }
    private JitAttributeKind(JitAttributeKind e) { this.value = e.value; }
    public JitAttributeKind intern() { for (JitAttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer toString(JitAttributeKind kind);
// Targeting ../AttributeValue.java


// Targeting ../GraphAttr.java


// Targeting ../GraphsAttr.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/graph_node_list.h

// #pragma once

// #include <c10/util/Exception.h>

// Intrusive doubly linked lists with sane reverse iterators.
// The header file is named generic_graph_node_list.h because it is ONLY
// used for Graph's Node lists, and if you want to use it for other
// things, you will have to do some refactoring.
//
// At the moment, the templated type T must support a few operations:
//
//  - It must have a field: T* next_in_graph[2] = { nullptr, nullptr };
//    which are used for the intrusive linked list pointers.
//
//  - It must have a method 'destroy()', which removes T from the
//    list and frees a T.
//
// In practice, we are only using it with Node and const Node.  'destroy()'
// needs to be renegotiated if you want to use this somewhere else.
//
// Regardless of the iteration direction, iterators always physically point
// to the element they logically point to, rather than
// the off-by-one behavior for all standard library reverse iterators like
// std::list.

// The list is includes two sentinel nodes, one at the beginning and one at the
// end with a circular link between them. It is an error to insert nodes after
// the end sentinel node but before the beginning node:

// Visualization showing only the next() links:
//  HEAD -> first -> second  -> ... -> last -> TAIL
//   ^------------------------------------------

// Visualization showing only the prev() links:
//  HEAD <- first <- second  <- ... <- last <- TAIL
//   ------------------------------------------^

@Namespace("torch::jit") @MemberGetter public static native int kNextDirection();
public static final int kNextDirection = kNextDirection();
@Namespace("torch::jit") @MemberGetter public static native int kPrevDirection();
public static final int kPrevDirection = kPrevDirection();
// Targeting ../graph_node_list_iterator.java


// Targeting ../graph_node_list.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/frontend/source_range.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <algorithm>
// #include <iterator>
// #include <memory>
// #include <numeric>
// #include <ostream>
// #include <sstream>
// #include <unordered_map>
// Targeting ../SourceRangeUnpickler.java


// Targeting ../StringCordView.java


// Targeting ../Source.java


// Targeting ../SourceRange.java



// OwnedSourceRange is just like a SourceRange except that it owns a `Source`
// instead of `Source`. Thus OwnedSourceRange owns a copy of source text.
// Targeting ../SourceRangeHasher.java


// Targeting ../StackEntry.java



@Namespace("torch::jit") public static native void format_stack_trace(
    @Cast("std::ostream*") @ByRef Pointer out,
    @StdVector StackEntry entries);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef SourceRange range);

// A pair of (byte offset, SourceRange) describing a specific segment
// of the output stream

 // namespace jit
 // namespace torch
 // namespace std


// Parsed from torch/csrc/jit/ir/scope.h

// #pragma once
// #include <ATen/core/jit_type.h>
// #include <ATen/core/symbol.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <unordered_map>
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kModuleInstanceInfo();

 // namespace utils

// Scope is a node of a trie that represents the tree of nested scopes.
// Individual scopes are pushed and popped from Graph, which holds a
// pointer to the current scope. Each Node in Graph holds a pointer
// to the scope that was current when the node was created.
// The trie never needs to shrink, it only grows until it is disposed
// of when Graph is deallocated. Hence, pointers to scopes held by nodes
// will always be valid as long as Graph is alive.
// Targeting ../Scope.java


// Targeting ../ModuleInstanceInfo.java



/**
 * InlinedCallStack is an element in a list representing callstack of functions
 * that have been inlined.
 *
 * Each such element holds info about the current callsite (Function and
 * SourceRange) and a pointer to the next element in the list. The last element
 * in the list represents the innermost function that was inlined.
 *
 * For instance, if a node has a callstack
 *    [foo, source_range1] -> [bar, source_range2]
 * it means that this node was originally from function 'bar' that was called
 * at 'source_range2' in function 'foo' that was called in the current function
 * at 'source_range1'.
 *
 * If a node did not come from any inlined function, its callstack will be
 * empty.
 *
 * The callstack lists only grow, we never remove elements from them, which
 * allows us to reuse same elements in different lists. For instance, if we
 * inline function 'bar' to 'foo' and then inline 'foo' to two functions 'ham'
 * and 'baz', the callstacks would look like:
 *
 *  [baz, source_range3]  --
 *                           \
 *                             --> [foo, source_range1] -> [bar, source_range2]
 *                           /
 *  [ham, source_range4]  --
 */
// Targeting ../InlinedCallStack.java



// {source range, node name, InlinedCallStack}
// We store node name because same debug infor will be used for
// profiling as well, so we need to know op names as well.
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleSourceRangeIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleNodeNameIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleInlinedCSIndex();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/constants.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/scope.h>

// helpers for handling constants in the IR
// - create constant nodes from ints, floats, complex, intlist, Tensors, and
// other types
// - implement primitive constant ops.

// thrown when insertConstant cannot encode the IValue into a graph

@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

// note: prefer g.insertConsant(val, loc) which does exactly the same thing
// this function is only declared/defined here because its implementation is
// closely related to the implementation of prim::Constant that is also in
// constants.cpp.
//
// returns a c10::nullopt if the IValue kind cannot be inserted as a constant
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

////////////////////////////////////////////////////////////////////////////////
// Helper for retrieving constants
////////////////////////////////////////////////////////////////////////////////

// attempt to convert a (possibly constant) Value* into an interpreter value
// (IValue). returns c10::nullopt if the Value* was not constant
@Namespace("torch::jit") public static native @ByVal IValueOptional toIValue(@Const Value v);

// if a value is a constant then try to turn into type T using the
// same rules as the interpreter
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/named_value.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/constants.h>
// #include <torch/csrc/utils/variadic.h>
// Targeting ../NamedValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator_options.h

// #pragma once

// #include <ATen/core/dispatch/OperatorOptions.h>

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator.h

// in memory description of all ATen Ops similar to Caffe2 schema
// once C10 exists this can be removed, or stubbed out, but we need
// it now to implement correct semantic checking for script
// #pragma once

// #include <ATen/core/dispatch/Dispatcher.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/op_registration/op_allowlist.h>
// #include <ATen/core/stack.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #include <torch/csrc/jit/runtime/operator_options.h>
// #include <torch/library.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/symbol.h>

// #include <functional>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>
// Targeting ../Operator.java



@Namespace("torch::jit") public static native @StdString BytePointer canonicalSchemaString(@Const @ByRef FunctionSchema schema);

@Namespace("torch::jit") public static native @Const @ByVal OperatorVector getAllOperators();
@Namespace("torch::jit") public static native @Const @ByRef OperatorVector getAllOperatorsFor(
    @ByVal Symbol name);

// given a operator with an overload name, find the specific operator related to
// it, may return nullptr if no operator exists.
@Namespace("torch::jit") public static native @SharedPtr("torch::jit::Operator") @ByVal Operator findOperatorFor(
    @Const @ByRef OperatorName full_name);

@Namespace("torch::jit") public static native @ByVal SymbolVector findSimilarOperators(@ByVal Symbol input_op);

@Namespace("torch::jit") public static native void registerOperator(@ByRef(true) Operator op);
@Namespace("torch::jit") public static native void deregisterOperator(@Const @ByRef FunctionSchema schema);

// XXX: this function is meant to be used with string literals only!
@Namespace("torch::jit") public static native @SharedPtr("torch::jit::Operator") @ByVal Operator getOperatorForLiteral(
    @Cast("const char*") BytePointer signature);
@Namespace("torch::jit") public static native @SharedPtr("torch::jit::Operator") @ByVal Operator getOperatorForLiteral(
    String signature);

// Ensure the thing that registers c10 ops is defined.
// Otherwise, our registry will not have c10 ops. You can run into this
// scenario if you're querying registered ops during static init.
//
// This fn is defined in register_c10_ops.cpp
@Namespace("torch::jit") public static native void ensure_c10_registerer_defined();

// Used to assert that unschematized operators have an analysis method written
@Namespace("torch::jit") public static native @Cast("bool") boolean aliasAnalysisHasSpecialCaseFor(@ByVal Symbol sym);

// A factory function to generate an optional operator. It has two
// instantiations depending on the template bool arg value. The arg can be a
// compile-time function for the selective op registration based on schema
// string.

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/utils/schema_info.h

// #pragma once

// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #include <unordered_set>
// Targeting ../SchemaInfo.java


 // namespace utils
 // namespace torch


// Parsed from ATen/core/enum_type.h

// #pragma once

// #include <ATen/core/ivalue.h>

// #include <utility>
// Targeting ../EnumType.java



 // namespace c10


// Parsed from torch/csrc/jit/ir/ir.h

// #pragma once

// #include <torch/csrc/jit/ir/attributes.h>
// #include <torch/csrc/jit/ir/graph_node_list.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/ir/scope.h>
// #include <torch/csrc/jit/runtime/operator.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/utils/python_stub.h>
// #include <torch/csrc/utils/schema_info.h>

// #include <ATen/Utils.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/dynamic_type.h>
// #include <ATen/core/enum_type.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <functional>
// #include <iosfwd>
// #include <unordered_set>
// #include <vector>

// Forward declare, the real meat is in python_ir.cpp
@Namespace("torch::jit::utils") public static native @StdString BytePointer getNodesModuleHierarchy(@Const @ByRef JitNode n);

// Targeting ../AliasDb.java



// #define C10_USING(T) using ::c10::T;
// #undef C10_USING

// #define C10_USING(T) using ::c10::T##Ptr;
// #undef C10_USING



// #if !defined(USE_ROCM)
// #endif
 // namespace cuda

// A Graph represents one "function" of computation.
// It uses a simple ownership model where the graph owns all the nodes inside
// it. All references inside the graph are raw pointers. Destroying the Graph
// will invalidate any pointers to nodes in the graph.

// Node is the base class of the IR graph. It represents one computation
// and dependencies on a list of Values. The "prim-ops", so to speak.

// A Value represents an input or output to node that is either a
// Tensor or an opaque Handle object, as determined by type().

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Graph g);
@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef JitNode n);

// A list of nodes, with inputs and outputs
// Targeting ../Use.java



// Note [User node does not uniquely identify use]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// A while back, we wrote some code manipulating uses that looked like this:
//
//    for (auto& use : used_val->uses_) {
//      if (use.user == this_node) {
//        use.offset += 1;
//        break;
//      }
//    }
//
// This code is trying to find a particular use (our node's use) to update it.
// However, it's wrong: there may be *multiple* uses of a value %x in a node,
// as might be the case in this IR:
//
//    %y = Add %x %x
//
// In this case, there are two uses of %x whose user is the node 'Add %x %x'.
// So, "use induced by this node" is not a well-formed concept.
//
// If you are looking for "use induced by an input", it's best to use
// findUseForInput() to get it.

// the list types are intentionally simple, but we type-def
// them here so if we need to change them, refactoring will be easier
// Targeting ../BlockWrap.java


// Targeting ../JitNodeWrap.java


// Targeting ../ValueWrap.java


// Targeting ../Value.java


// Targeting ../JitNode.java


// Targeting ../Block.java


// Targeting ../Graph.java



/** \brief An utility class for setting temporary insertion points.
 *
 * When an object of this class is created, it stores the current insertion
 * point, sets the new one, and restores the original insertion point when the
 * object is destroyed.
 */

/** \brief An utility class for setting temporary scopes.
 *
 * When an object of this class is created, it stores the current scope, sets
 * the new one, and restores the original scope when the object is destroyed.
 */

// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)








/************* All nodes not required to be defined before Graph **************/
// Targeting ../ProfileIValueOp.java


// Targeting ../PythonOp.java



@Namespace("torch::jit") public static native void LintGraph(@Const @SharedPtr("torch::jit::Graph") @ByRef Graph graph);

@Namespace("torch::jit") public static native @ByVal ValueArrayRef createTupleUnpack(Value v);

/** Insert graph \p CALLEE into graph \p G using \p INPUTS as input values.
 * The insertion happens at the current insertion point.
 * Optionally, one can also pass \p VALUE_MAP to get a map between \p CALLEE
 * values and their cloned copies in \p G.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs);
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueVector inputs);
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs,
    @ByRef ValueValueMap value_map);
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueVector inputs,
    @ByRef ValueValueMap value_map);

/** Insert function \p CALLEE after node \p TO_REPLACE, remove the node and
 * replace all its uses with corresponding outputs of the inserted function.
 * This asserts that the number of outputs of the original node and the
 * graph are the same.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee,
    @Cast("bool") boolean use_graph/*=true*/);
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee);

@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee,
    Graph callee_graph);

/** If there is only one value in \p OUTPUTS and its kind is Tuple, insert a
 * tuple unpack node and return the resulting values.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector unpackOutputs(@Const @ByRef ValueVector outputs);

@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(@ByRef Graph g, @ByVal Symbol kind, @Cast("bool") boolean recurse);
@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(@ByRef Block b, @ByVal Symbol kind, @Cast("bool") boolean recurse);
@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(
    @ByVal BlockArrayRef a,
    @ByVal Symbol kind,
    @Cast("bool") boolean recurse);
// Targeting ../OperatorSet.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/python/update_graph_executor_opt.h

// #pragma once
// #include <torch/csrc/Export.h>
@Namespace("torch::jit") public static native void setGraphExecutorOptimize(@Cast("bool") boolean o);
@Namespace("torch::jit") public static native @Cast("bool") boolean getGraphExecutorOptimize();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/argument_spec.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/stack.h>
// #include <c10/util/hash.h>
// #include <c10/util/irange.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <ostream>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif
// Targeting ../ArgumentInfo.java


// Targeting ../ArgumentSpec.java


@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long ARG_SPEC_DEPTH_LIMIT();
public static final long ARG_SPEC_DEPTH_LIMIT = ARG_SPEC_DEPTH_LIMIT();

// Targeting ../ArgumentSpecCreator.java



// CompleteArgumentSpec represents one particular specialization.
// It is designed so that it can be created, hashed, and compared quickly
// since it is used along the hot-path of the JIT to check if the code
// we have created is valid for the given inputs.

// COmpleteArgumentInfoPOD is only used internally in CompleteArgumentSpec
// API users should use ArgumentInfo

// public view of compressed CompleteArgumentInfo

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentInfo info);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentSpec spec);



@Namespace("torch::jit") public static native @ByVal ByteOptional convertOptional(
    @Const @ByRef ScalarTypeOptional from);

 // namespace jit
 // namespace torch
 // namespace std



// Parsed from torch/csrc/jit/runtime/interpreter.h

// #pragma once
// #include <c10/util/Optional.h>
// #include <memory>
// #include <vector>

// #include <ATen/ThreadLocalState.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>

// #if C10_CLANG_HAS_WARNING("-Wdeprecated-copy-dtor")
// #endif



 // namespace at
 // namespace c10
// Targeting ../CodeImpl.java



// Targeting ../Instruction.java


// Targeting ../Code.java


// Targeting ../MobileCode.java



// Created by wait()

// InterpreterContinuation propagates dist_autograd_context_id
// through (and only through) the forward pass manually, other
// thread local settings are propagated with ThreadLocalState

// what is the tensors type, including state from the current execution context
// that modifies how the tensor behaves. For instance if no_grad is enabled
// this will cause the TensorType to have requires_grad=False.
@Namespace("torch::jit") public static native @SharedPtr("c10::TensorType") @ByVal TensorType tensorTypeInCurrentExecutionContext(
    @Const @ByRef Tensor t);

// current (TLS) TorchScript interpreter callstack
@Namespace("torch::jit") public static native @StdVector StackEntry currentCallstack();
@Namespace("torch::jit") public static native @ByVal StringVector currentModuleHierarchy();

 // namespace jit
 // namespace torch



// Parsed from torch/csrc/jit/runtime/variable_tensor_list.h

// #pragma once
// #include <ATen/core/Tensor.h>

// a wrapper to mark places where we expect all the at::Tensors to be
// variables

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/graph_executor.h

// #pragma once

// #include <atomic>
// #include <memory>

// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/python/update_graph_executor_opt.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/interpreter.h>
// #include <torch/csrc/jit/runtime/variable_tensor_list.h>



@Namespace("torch::jit") public enum ExecutorExecutionMode {
  SIMPLE(0),
  PROFILING(1);

    public final int value;
    private ExecutorExecutionMode(int v) { this.value = v; }
    private ExecutorExecutionMode(ExecutorExecutionMode e) { this.value = e.value; }
    public ExecutorExecutionMode intern() { for (ExecutorExecutionMode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ExecutionPlan.java


// Targeting ../GraphExecutorState.java


// Targeting ../EnableProfilingGuard.java


// Targeting ../GraphExecutorImplBase.java


// Targeting ../GraphExecutor.java



@Namespace("torch::jit") public static native JitNode replaceBlockWithFallbackGraph(
    Block b,
    @ByVal ValueArrayRef inputs);
@Namespace("torch::jit") public static native JitNode replaceBlockWithFallbackGraph(
    Block b,
    @ByVal ValueVector inputs);

// These passes need to run before it is valid to pass to the interpreter
// regardless of whether sizes have been specialized or not.
@Namespace("torch::jit") public static native void runRequiredPasses(@Const @SharedPtr("torch::jit::Graph") @ByRef Graph g);

@Namespace("torch::jit") public static native void debugSetFusionGroupInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @Cast("bool") boolean getFusionGroupInlining();

@Namespace("torch::jit") public static native void debugSetAutodiffSubgraphInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @SharedPtr("torch::jit::Graph") @ByVal Graph lastExecutedOptimizedGraph();

@Namespace("torch::jit") public static native @Cast("std::atomic<bool>*") @ByRef BoolPointer getProfilingMode();
@Namespace("torch::jit") public static native @Cast("std::atomic<bool>*") @ByRef BoolPointer getExecutorMode();
@Namespace("torch::jit") public static native @Cast("std::atomic<size_t>*") @ByRef LongPointer getNumProfiledRuns();
@Namespace("torch::jit") public static native @Cast("size_t") long getBailoutDepth();
@Namespace("torch::jit") public static native @Cast("bool") boolean IsNewExecutorEnabled();
// Targeting ../GraphOptimizerEnabledGuard.java







// for debugging information we expose a way to get the last actually
// run graph. Previous approaches allowed querying the GraphExecutor
// for what graph it would run in certain circumstances (graphFor), but
// this is fragile because we sometimes change how these decisions are made.
// This interface still allows our tests to look at optimized graphs, but
// with less plumbing.
 // namespace detail

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/function_impl.h

// #pragma once

// #include <ATen/core/function.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>
// #include <torch/csrc/utils/memory.h>
// Targeting ../GraphFunction.java



// Short hands for dynamic_cast<GraphFunction*>.
@Namespace("torch::jit") public static native @NoException(true) GraphFunction tryToGraphFunction(@ByRef Function arg0);
@Namespace("torch::jit") public static native @ByRef GraphFunction toGraphFunction(@ByRef Function arg0);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/method.h

// #pragma once

// #include <ATen/core/function.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/stack.h>
// #include <torch/csrc/api/include/torch/imethod.h>
// #include <torch/csrc/jit/api/function_impl.h>
// Targeting ../Method.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/object.h

// #pragma once

// #include <ATen/core/functional.h>
// #include <ATen/core/ivalue.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/api/method.h>

// #include <utility>

// Throw this in C++ land if `attr` fails. This will be converted to a Python
// AttributeError by the Python binding code
// Targeting ../JitObject.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/api/include/torch/ordered_dict.h

// #pragma once

// #include <cstdint>
// #include <initializer_list>
// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>
// Targeting ../StringTensorDict.java


// Targeting ../StringAnyModuleDict.java


// Targeting ../StringSharedModuleDict.java


// Targeting ../StringTensorDictItem.java


// Targeting ../StringAnyModuleDictItem.java


// Targeting ../StringSharedModuleDictItem.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ OrderedDict ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



































































 // namespace torch


// Parsed from torch/csrc/jit/frontend/name_mangler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/Export.h>
// Targeting ../NameMangler.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/compilation_unit.h

// #pragma once
// #include <ATen/core/function.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/api/function_impl.h>
// #include <torch/csrc/jit/frontend/name_mangler.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <vector>
// Targeting ../Self.java


// Targeting ../CompilationUnit.java



// An owning pointer to a Function. Just a pair of a raw Function ptr and it's
// owning CU. We need this because pybind requires a ref-counted way to refer to
// Functions.
// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/module.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/api/object.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/api/include/torch/ordered_dict.h>
// #include <torch/csrc/jit/api/compilation_unit.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <utility>
// #include <vector>

// This file contains classes which assist in desugaring Python style
// modules and their methods into flattened graphs which don't have any
// function calls.
// Map which stores filename to content.
// Targeting ../NamedJitModule.java


// Targeting ../NamedTensor.java


// Targeting ../NamedIValue.java


 // namespace detail
// Targeting ../JitModule.java



// C++ equivalent api of `torch.jit.freeze`. See documentation there for
// details.
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module,
    @Const @ByRef(nullValue = "c10::optional<std::vector<std::string> >(c10::nullopt)") StringVectorOptional preserved_attrs,
    @Cast("bool") boolean optimize_numerics/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module);

// C++ equivalent api of `torch.jit.optimize_for_inference`. See documentation
// there for details.
@Namespace("torch::jit") public static native @ByVal JitModule optimize_for_inference(
    @ByRef JitModule module,
    @Const @ByRef(nullValue = "std::vector<std::string>{}") StringVector other_methods);
@Namespace("torch::jit") public static native @ByVal JitModule optimize_for_inference(
    @ByRef JitModule module);

@Namespace("torch::jit") public enum FusionBehavior { STATIC(0), DYNAMIC(1);

    public final int value;
    private FusionBehavior(int v) { this.value = v; }
    private FusionBehavior(FusionBehavior e) { this.value = e.value; }
    public FusionBehavior intern() { for (FusionBehavior e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// clang-format off
/*
Sets the type and number of specializations that can occur during fusion.

Usage: provide a list of pairs (type, depth) where type is one of STATIC or DYNAMIC
and depth is an integer.

Behavior - static vs dynamic:
    In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined
    based on some initial profiling runs.
    In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple
    shapes are possible.

In both cases, we also recompile on new striding behavior, device, or dtype.

Behavior - fallback functions & depth:
    When an input doesn't match the format required by the specialized compiled op, it will run
    a fallback function. Fallback functions are recursively be compiled and specialized based
    on the observed tensor shapes. Since compilation can be slow, the "depth" parameter is provided to
    limit the number of specializations that can be compiled, before giving up on recompiling and
    falling back to a completely un-fused, un-specialized implementation.

The list of (type, depth) pairs controls the type of specializations and the number of
specializations. For example: [(STATIC, 2), (DYNAMIC, 2)] indicates that the first
two specializations will use static fusions, the following two specializations will use
dynamic fusion, and any inputs that satisfy none of the 4 options will run an
unfused implementation.

NB: in the future, if more as more fusion backends are added there may be more granular
apis for specific fusers.
*/
// clang-format on
@Namespace("torch::jit") public static native @ByVal FusionStrategy getFusionStrategy();
// returns previous strategy
@Namespace("torch::jit") public static native @ByVal FusionStrategy setFusionStrategy(@ByRef FusionStrategy fusion_strategy);
// Targeting ../SlotCursor.java




// Targeting ../module_iterator.java


// Targeting ../named_module_iterator.java


// Targeting ../parameter_iterator.java


// Targeting ../named_parameter_iterator.java


// Targeting ../attribute_iterator.java


// Targeting ../named_attribute_iterator.java


// Targeting ../buffer_iterator.java


// Targeting ../named_buffer_iterator.java


// Targeting ../module_list.java


// Targeting ../named_module_list.java


// Targeting ../parameter_list.java


// Targeting ../named_parameter_list.java


// Targeting ../attribute_list.java


// Targeting ../named_attribute_list.java


// Targeting ../buffer_list.java


// Targeting ../named_buffer_list.java


// Targeting ../ModulePolicy.java


// Targeting ../ParameterPolicy.java


// Targeting ../BufferPolicy.java


// Targeting ../AttributePolicy.java



// take a Policy object, and make a version of it that returns the slot.
// along with the fully qualified name of that slot. This is used for the named_
// variants like named_parameters().

 // namespace detail

@Namespace("torch::jit") public static native @Cast("bool*") @ByRef BoolPointer getInlineEverythingMode();
// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/api/include/torch/serialize/input-archive.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/types.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit
 // namespace torch
// Targeting ../InputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/csrc/api/include/torch/serialize/output-archive.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit

// Targeting ../OutputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/csrc/api/include/torch/serialize/archive.h

// #pragma once

// #include <torch/serialize/input-archive.h>
// #include <torch/serialize/output-archive.h>


// Parsed from torch/csrc/api/include/torch/data/samplers/serialize.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/serialize/archive.h>
/** Serializes a {@code Sampler} into an {@code OutputArchive}. */

/** Deserializes a {@code Sampler} from an {@code InputArchive}. */
 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers/stream.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/types.h>

// #include <cstddef>
 // namespace serialize
 // namespace torch
// Targeting ../BatchSize.java


// Targeting ../StreamSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/samplers.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/data/samplers/distributed.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/samplers/sequential.h>
// #include <torch/data/samplers/serialize.h>
// #include <torch/data/samplers/stream.h>


// Parsed from torch/csrc/api/include/torch/serialize/tensor.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/types.h>
@Namespace("torch") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Tensor tensor);

@Namespace("torch") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Tensor tensor);
 // namespace torch


// Parsed from torch/csrc/api/include/torch/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/csrc/Export.h>
// #include <torch/serialize/archive.h>
// #include <torch/serialize/tensor.h>

// #include <utility>

/** Serializes the given {@code value}.
 *  There must be an overload of {@code operator<<} between {@code serialize::OutputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code save_to} method.
 *  For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::save(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::save(tensor, "my_tensor.pt");
 *  \endrst */

/** Serializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code tensor_vec} are forwarded to its {@code save_to}
 *  method. For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({1, 2}),
 *    torch::randn({3, 4}) }; torch::save(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({5, 6}),
 *    torch::randn({7, 8}) }; std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(tensor_vec, stream);
 *  \endrst */

@Namespace("torch") public static native @Cast("char*") @StdVector BytePointer pickle_save(@Const @ByRef IValue ivalue);

///
///
///
///
///
///
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector BytePointer data);
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector ByteBuffer data);
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector byte[] data);

/** Deserializes the given {@code value}.
 *  There must be an overload of {@code operator>>} between {@code serialize::InputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::load(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::istringstream stream("...");
 *    torch::load(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::load(tensor, "my_tensor.pt");
 *  \endrst */

/** Deserializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    torch::load(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    std::istringstream stream("...");
 *    torch::load(tensor_vec, stream);
 *  \endrst */
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/chunk.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/arg.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/samplers.h>
// #include <queue>
// #include <thread>

// #include <torch/serialize.h>
// Targeting ../ChunkDataReader.java


// Targeting ../ChunkTensorDataReader.java


/** BatchDataBuffer manages a queue of UnwrappedBatchData. After a new chunk is
 *  loaded, BatchDataBuffer splits it into small batches and push them into the
 *  queue. When get_batch is called from data loader, it pops cached batches and
 *  return. If the cache is empty, it either waits to load more chunks or return
 *  null if all chunks are loaded. */

// Targeting ../ChunkDatasetOptions.java


// Targeting ../ChunkDataset.java


// Targeting ../ChunkTensorDataset.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/map.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <type_traits>
// #include <utility>

// Targeting ../MNISTMapDataset.java


// Targeting ../ChunkMapDataset.java


// Targeting ../ChunkMapTensorDataset.java



/** Creates a {@code MapDataset} with the given dataset and transform. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/mnist.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <string>
// Targeting ../MNIST.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/shared.h

// #pragma once

// #include <torch/data/datasets/base.h>

// #include <memory>
// #include <utility>
// Targeting ../ChunkSharedBatchDataset.java


// Targeting ../ChunkSharedTensorBatchDataset.java



/** Constructs a new {@code SharedBatchDataset} by creating a
 *  {@code shared_ptr<UnderlyingDatase>}. All arguments are forwarded to
 *  {@code make_shared<UnderlyingDataset>}. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets/tensor.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../TensorDataset.java



 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/datasets.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/datasets/chunk.h>
// #include <torch/data/datasets/map.h>
// #include <torch/data/datasets/mnist.h>
// #include <torch/data/datasets/shared.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/datasets/tensor.h>


// Parsed from torch/csrc/api/include/torch/data/transforms/base.h

// #pragma once

// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleCollation.java


// Targeting ../TensorExampleCollation.java



/** A transformation of individual input examples to individual output examples.
 * 
 *  Just like a {@code Dataset} is a {@code BatchDataset}, a {@code Transform} is a
 *  {@code BatchTransform} that can operate on the level of individual examples rather
 *  than entire batches. The batch-level transform is implemented (by default)
 *  in terms of the example-level transform, though this can be customized. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/transforms/lambda.h

// #pragma once

// #include <torch/data/transforms/base.h>

// #include <functional>
// #include <utility>
// #include <vector>

/** A {@code BatchTransform} that applies a user-provided functor to a batch. */

// A `Transform` that applies a user-provided functor to individual examples.

 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/transforms/collate.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/lambda.h>

// #include <vector>

/** A {@code Collation} is a transform that reduces a batch into a single value.
 *  The result is a {@code BatchDataset} that has the type of the single value as its
 *  {@code BatchType}. */

///
///

/** A {@code Collate} allows passing a custom function to reduce/collate a batch
 *  into a single value. It's effectively the lambda version of {@code Collation},
 *  which you could subclass and override {@code operator()} to achieve the same.
 * 
 *  \rst
 *  .. code-block:: cpp
 *    using namespace torch::data;
 * 
 *    auto dataset = datasets::MNIST("path/to/mnist")
 *      .map(transforms::Collate<Example<>>([](std::vector<Example<>> e) {
 *        return std::move(e.front());
 *      }));
 *  \endrst */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/transforms/stack.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleStack.java


// Targeting ../TensorExampleStack.java



/** A {@code Collation} for {@code Example<Tensor, Tensor>} types that stacks all data
 *  tensors into one tensor, and all target (label) tensors into one tensor. */

/** A {@code Collation} for {@code Example<Tensor, NoTarget>} types that stacks all data
 *  tensors into one tensor. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/transforms/tensor.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/base.h>
// #include <torch/types.h>

// #include <functional>
// #include <utility>

/** A {@code Transform} that is specialized for the typical {@code Example<Tensor, Tensor>}
 *  combination. It exposes a single {@code operator()} interface hook (for
 *  subclasses), and calls this function on input {@code Example} objects. */

/** A {@code Lambda} specialized for the typical {@code Example<Tensor, Tensor>} input type. */

/** Normalizes input tensors by subtracting the supplied mean and dividing by
 *  the given standard deviation. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/data/transforms.h

// #pragma once

// #include <torch/data/transforms/base.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/data/transforms/lambda.h>
// #include <torch/data/transforms/stack.h>
// #include <torch/data/transforms/tensor.h>


// Parsed from torch/csrc/api/include/torch/data.h

// #pragma once

// #include <torch/data/dataloader.h>
// #include <torch/data/datasets.h>
// #include <torch/data/samplers.h>
// #include <torch/data/transforms.h>

// Some "exports".
 // namespace data
 // namespace torch


// Parsed from torch/csrc/api/include/torch/enum.h

// #pragma once

// #include <string>

// #include <ATen/core/Reduction.h>
// #include <c10/util/Exception.h>
// #include <c10/util/variant.h>
// #include <torch/csrc/Export.h>

// #define TORCH_ENUM_DECLARE(name)
//   namespace torch {
//   namespace enumtype {
//   /*                                                                  \
//    NOTE: We need to provide the default constructor for each struct, \
//    otherwise Clang 3.8 would complain:                               \
//    ```                                                               \
//    error: default initialization of an object of const type 'const   \
//    enumtype::Enum1' without a user-provided default constructor      \
//    ```                                                               \
//  */
//   struct k##name {
//     k##name() {}
//   };
//   }
//   TORCH_API extern const enumtype::k##name k##name;
//   }

// #define TORCH_ENUM_DEFINE(name)
//   namespace torch {
//   const enumtype::k##name k##name;
//   }

// #define TORCH_ENUM_PRETTY_PRINT(name)
//   std::string operator()(const enumtype::k##name& v) const {
//     std::string k("k");
//     return k + #name;
//   }

// NOTE: Backstory on why we need the following two macros:
//
// Consider the following options class:
//
// ```
// struct TORCH_API SomeOptions {
//   typedef c10::variant<enumtype::kNone, enumtype::kMean, enumtype::kSum>
//   reduction_t; SomeOptions(reduction_t reduction = torch::kMean) :
//   reduction_(reduction) {}
//
//   TORCH_ARG(reduction_t, reduction);
// };
// ```
//
// and the functional that uses it:
//
// ```
// Tensor some_functional(
//     const Tensor& input,
//     SomeOptions options = {}) {
//   ...
// }
// ```
//
// Normally, we would expect this to work:
//
// `F::some_functional(input, torch::kNone)`
//
// However, it throws the following error instead:
//
// ```
// error: could not convert `torch::kNone` from `const torch::enumtype::kNone`
// to `torch::nn::SomeOptions`
// ```
//
// To get around this problem, we explicitly provide the following constructors
// for `SomeOptions`:
//
// ```
// SomeOptions(torch::enumtype::kNone reduction) : reduction_(torch::kNone) {}
// SomeOptions(torch::enumtype::kMean reduction) : reduction_(torch::kMean) {}
// SomeOptions(torch::enumtype::kSum reduction) : reduction_(torch::kSum) {}
// ```
//
// so that the conversion from `torch::kNone` to `SomeOptions` would work.
//
// Note that we also provide the default constructor `SomeOptions() {}`, so that
// `SomeOptions options = {}` can work.
// #define TORCH_OPTIONS_CTOR_VARIANT_ARG3(
//     OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3)
//   OPTIONS_NAME() = default;
//   OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
//   OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
//   OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}

// #define TORCH_OPTIONS_CTOR_VARIANT_ARG4(
//     OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3, TYPE4)
//   OPTIONS_NAME() = default;
//   OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
//   OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
//   OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}
//   OPTIONS_NAME(torch::enumtype::TYPE4 ARG_NAME) : ARG_NAME##_(torch::TYPE4) {}
// Targeting ../kLinear.java

  
// Targeting ../kConv1D.java

  
// Targeting ../kConv2D.java

  
// Targeting ../kConv3D.java

  
// Targeting ../kConvTranspose1D.java

  
// Targeting ../kConvTranspose2D.java

  
// Targeting ../kConvTranspose3D.java

  
// Targeting ../kSigmoid.java

  
// Targeting ../kTanh.java

  
// Targeting ../kReLU.java

  
// Targeting ../kGELU.java

  
// Targeting ../kSiLU.java

  
// Targeting ../kMish.java

  
// Targeting ../kLeakyReLU.java

  
// Targeting ../kFanIn.java

  
// Targeting ../kFanOut.java

  
// Targeting ../kConstant.java

  
// Targeting ../kReflect.java

  
// Targeting ../kReplicate.java

  
// Targeting ../kCircular.java

  
// Targeting ../kNearest.java

  
// Targeting ../kBilinear.java

  
// Targeting ../kBicubic.java

  
// Targeting ../kTrilinear.java

  
// Targeting ../kArea.java

  
// Targeting ../kNearestExact.java

  
// Targeting ../kSum.java

  
// Targeting ../kMean.java

  
// Targeting ../kMax.java

  
// Targeting ../kNone.java

  
// Targeting ../kBatchMean.java

  
// Targeting ../kZeros.java

  
// Targeting ../kBorder.java

  
// Targeting ../kReflection.java

  
// Targeting ../kRNN_TANH.java

  
// Targeting ../kRNN_RELU.java

  
// Targeting ../kLSTM.java

  
// Targeting ../kGRU.java

  
// Targeting ../kValid.java

  
// Targeting ../kSame.java

  

 // namespace enumtype
 // namespace torch


// Parsed from torch/csrc/api/include/torch/fft.h

// #pragma once

// #include <ATen/ATen.h>

/** Computes the 1 dimensional fast Fourier transform over a given dimension.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.fft.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kComplexDouble);
 *  torch::fft::fft(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor fft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor fft(
    @Const @ByRef Tensor self);

/** Computes the 1 dimensional inverse Fourier transform over a given dimension.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.ifft.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kComplexDouble);
 *  torch::fft::ifft(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor ifft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ifft(
    @Const @ByRef Tensor self);

/** Computes the 2-dimensional fast Fourier transform over the given dimensions.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.fft2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::fft2(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor fft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor fft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor fft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the inverse of torch.fft.fft2
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.ifft2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::ifft2(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor ifft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ifft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor ifft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the N dimensional fast Fourier transform over given dimensions.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.fftn.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::fftn(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor fftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor fftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor fftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the N dimensional fast Fourier transform over given dimensions.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.ifftn.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::ifftn(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor ifftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ifftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor ifftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the 1 dimensional FFT of real input with onesided Hermitian output.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.rfft.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128);
 *  auto T = torch::fft::rfft(t);
 *  assert(T.is_complex() && T.numel() == 128 / 2 + 1);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor rfft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor rfft(
    @Const @ByRef Tensor self);

/** Computes the inverse of torch.fft.rfft
 * 
 *  The input is a onesided Hermitian Fourier domain signal, with real-valued
 *  output. See https://pytorch.org/docs/master/fft.html#torch.fft.irfft
 * 
 *  Example:
 *  <pre>{@code
 *  auto T = torch::randn(128 / 2 + 1, torch::kComplexDouble);
 *  auto t = torch::fft::irfft(t, /*n=* /128);
 *  assert(t.is_floating_point() && T.numel() == 128);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor irfft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor irfft(
    @Const @ByRef Tensor self);

/** Computes the 2-dimensional FFT of real input. Returns a onesided Hermitian
 *  output. See https://pytorch.org/docs/master/fft.html#torch.fft.rfft2
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kDouble);
 *  torch::fft::rfft2(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor rfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor rfft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor rfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the inverse of torch.fft.rfft2.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.irfft2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::irfft2(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor irfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor irfft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor irfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the N dimensional FFT of real input with onesided Hermitian output.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.rfftn
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kDouble);
 *  torch::fft::rfftn(t);
 *  }</pre> */

///
@Namespace("torch::fft") public static native @ByVal Tensor rfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor rfftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor rfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the inverse of torch.fft.rfftn.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.irfftn.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 128}, dtype=kComplexDouble);
 *  torch::fft::irfftn(t);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor irfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor irfftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor irfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the 1 dimensional FFT of a onesided Hermitian signal
 * 
 *  The input represents a Hermitian symmetric time domain signal. The returned
 *  Fourier domain representation of such a signal is a real-valued. See
 *  https://pytorch.org/docs/master/fft.html#torch.fft.hfft
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128 / 2 + 1, torch::kComplexDouble);
 *  auto T = torch::fft::hfft(t, /*n=* /128);
 *  assert(T.is_floating_point() && T.numel() == 128);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor hfft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor hfft(
    @Const @ByRef Tensor self);

/** Computes the inverse FFT of a real-valued Fourier domain signal.
 * 
 *  The output is a onesided representation of the Hermitian symmetric time
 *  domain signal. See https://pytorch.org/docs/master/fft.html#torch.fft.ihfft.
 * 
 *  Example:
 *  <pre>{@code
 *  auto T = torch::randn(128, torch::kDouble);
 *  auto t = torch::fft::ihfft(T);
 *  assert(t.is_complex() && T.numel() == 128 / 2 + 1);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor ihfft(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional n,
    @Cast("int64_t") long dim/*=-1*/,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ihfft(
    @Const @ByRef Tensor self);

/** Computes the 2-dimensional FFT of a Hermitian symmetric input signal.
 * 
 *  The input is a onesided representation of the Hermitian symmetric time
 *  domain signal. See https://pytorch.org/docs/master/fft.html#torch.fft.hfft2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 65}, torch::kComplexDouble);
 *  auto T = torch::fft::hfft2(t, /*s=* /{128, 128});
 *  assert(T.is_floating_point() && T.numel() == 128 * 128);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor hfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor hfft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor hfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the 2-dimensional IFFT of a real input signal.
 * 
 *  The output is a onesided representation of the Hermitian symmetric time
 *  domain signal. See
 *  https://pytorch.org/docs/master/fft.html#torch.fft.ihfft2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto T = torch::randn({128, 128}, torch::kDouble);
 *  auto t = torch::fft::hfft2(T);
 *  assert(t.is_complex() && t.size(1) == 65);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor ihfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ihfft2(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor ihfft2(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the N-dimensional FFT of a Hermitian symmetric input signal.
 * 
 *  The input is a onesided representation of the Hermitian symmetric time
 *  domain signal. See https://pytorch.org/docs/master/fft.html#torch.fft.hfftn.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn({128, 65}, torch::kComplexDouble);
 *  auto T = torch::fft::hfftn(t, /*s=* /{128, 128});
 *  assert(T.is_floating_point() && T.numel() == 128 * 128);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor hfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor hfftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor hfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the N-dimensional IFFT of a real input signal.
 * 
 *  The output is a onesided representation of the Hermitian symmetric time
 *  domain signal. See
 *  https://pytorch.org/docs/master/fft.html#torch.fft.ihfftn.
 * 
 *  Example:
 *  <pre>{@code
 *  auto T = torch::randn({128, 128}, torch::kDouble);
 *  auto t = torch::fft::hfft2(T);
 *  assert(t.is_complex() && t.size(1) == 65);
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor ihfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") LongArrayRef dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);
@Namespace("torch::fft") public static native @ByVal Tensor ihfftn(
    @Const @ByRef Tensor self);
@Namespace("torch::fft") public static native @ByVal Tensor ihfftn(
    @Const @ByRef Tensor self,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s,
    @ByVal(nullValue = "torch::IntArrayRef({-2, -1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") StringViewOptional norm);

/** Computes the discrete Fourier Transform sample frequencies for a signal of
 *  size n.
 * 
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.fftfreq
 * 
 *  Example:
 *  <pre>{@code
 *  auto frequencies = torch::fft::fftfreq(128, torch::kDouble);
 *  }</pre> */
@Namespace("torch::fft") public static native @ByVal Tensor fftfreq(@Cast("int64_t") long n, double d, @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("torch::fft") public static native @ByVal Tensor fftfreq(@Cast("int64_t") long n, double d);


///
///
@Namespace("torch::fft") public static native @ByVal Tensor fftfreq(@Cast("int64_t") long n, @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("torch::fft") public static native @ByVal Tensor fftfreq(@Cast("int64_t") long n);

/** Computes the sample frequencies for torch.fft.rfft with a signal of size n.
 * 
 *  Like torch.fft.rfft, only the positive frequencies are included.
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.rfftfreq
 * 
 *  Example:
 *  <pre>{@code
 *  auto frequencies = torch::fft::rfftfreq(128, torch::kDouble);
 *  }</pre> */
@Namespace("torch::fft") public static native @ByVal Tensor rfftfreq(@Cast("int64_t") long n, double d, @Const @ByRef TensorOptions options);


///
///
@Namespace("torch::fft") public static native @ByVal Tensor rfftfreq(@Cast("int64_t") long n, @Const @ByRef TensorOptions options);

/** Reorders n-dimensional FFT output to have negative frequency terms first, by
 *  a torch.roll operation.
 * 
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.fftshift
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::randn({127, 4});
 *  auto centred_fft = torch::fft::fftshift(torch::fft::fftn(x));
 *  }</pre> */

///
///
@Namespace("torch::fft") public static native @ByVal Tensor fftshift(
    @Const @ByRef Tensor x,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("torch::fft") public static native @ByVal Tensor fftshift(
    @Const @ByRef Tensor x);
@Namespace("torch::fft") public static native @ByVal Tensor fftshift(
    @Const @ByRef Tensor x,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

/** Inverse of torch.fft.fftshift
 * 
 *  See https://pytorch.org/docs/master/fft.html#torch.fft.ifftshift
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::randn({127, 4});
 *  auto shift = torch::fft::fftshift(x)
 *  auto unshift = torch::fft::ifftshift(shift);
 *  assert(torch::allclose(x, unshift));
 *  }</pre> */
@Namespace("torch::fft") public static native @ByVal Tensor ifftshift(
    @Const @ByRef Tensor x,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("torch::fft") public static native @ByVal Tensor ifftshift(
    @Const @ByRef Tensor x);
@Namespace("torch::fft") public static native @ByVal Tensor ifftshift(
    @Const @ByRef Tensor x,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

 // namespace fft
 // namespace torch


// Parsed from torch/csrc/api/include/torch/jit.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>

// #include <memory>
// #include <string>

/** Compiles script code into an executable graph.
 * 
 *  Takes a string containing functions in script syntax and compiles them into
 *  a module (graph). The returned module provides a {@code run_method} function
 *  that may be used to invoke the compiled functions.
 * 
 *  For example:
 *  \rst
 *  .. code-block:: cpp
 * 
 *    auto module = torch::jit::compile(R"JIT(
 *      def relu_script(a, b):
 *        return torch.relu(a + b)
 *      def test_while(a, i):
 *        while i < 10:
 *          a += a
 *          i += 1
 *        return a
 *    )JIT");
 *    IValue output = module->run_method("relu_script", a, b);
 *  \endrst */
@Namespace("torch::jit") public static native @SharedPtr CompilationUnit compile(@StdString BytePointer source);
@Namespace("torch::jit") public static native @SharedPtr CompilationUnit compile(@StdString String source);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/api/include/torch/linalg.h

// #pragma once

// #include <ATen/ATen.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T eig(@Const @ByRef Tensor self);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> eig_out(
    @ByRef Tensor eigvals,
    @ByRef Tensor eigvecs,
    @Const @ByRef Tensor self);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor eigvals(@Const @ByRef Tensor self);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor eigvals_out(@ByRef Tensor result, @Const @ByRef Tensor self);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T eigh(
    @Const @ByRef Tensor self,
    @StringView BytePointer uplo);
@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T eigh(
    @Const @ByRef Tensor self,
    @StringView String uplo);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> eigh_out(
    @ByRef Tensor eigvals,
    @ByRef Tensor eigvecs,
    @Const @ByRef Tensor self,
    @StringView BytePointer uplo);
@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> eigh_out(
    @ByRef Tensor eigvals,
    @ByRef Tensor eigvecs,
    @Const @ByRef Tensor self,
    @StringView String uplo);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor eigvalsh(@Const @ByRef Tensor self, @StringView BytePointer uplo);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor eigvalsh(@Const @ByRef Tensor self, @StringView String uplo);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor eigvalsh_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @StringView BytePointer uplo);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor eigvalsh_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @StringView String uplo);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor householder_product(@Const @ByRef Tensor input, @Const @ByRef Tensor tau);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor householder_product_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor tau);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T lu_factor(
    @Const @ByRef Tensor self,
    @Cast("const bool") boolean pivot);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> lu_factor_out(
    @ByRef Tensor LU,
    @ByRef Tensor pivots,
    @Const @ByRef Tensor self,
    @Cast("const bool") boolean pivot);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensorTensor_T lu(
    @Const @ByRef Tensor self,
    @Cast("const bool") boolean pivot);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> lu_out(
    @ByRef Tensor P,
    @ByRef Tensor L,
    @ByRef Tensor U,
    @Const @ByRef Tensor self,
    @Cast("const bool") boolean pivot);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensorTensorTensor_T lstsq(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor b,
    @ByVal DoubleOptional cond,
    @ByVal StringViewOptional driver);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor norm(
    @Const @ByRef Tensor self,
    @Const @ByRef ScalarOptional opt_ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor norm(
    @Const @ByRef Tensor self,
    @Const @ByRef ScalarOptional opt_ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor norm(
    @Const @ByRef Tensor self,
    @StringView BytePointer ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor norm(
    @Const @ByRef Tensor self,
    @StringView String ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef ScalarOptional opt_ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef ScalarOptional opt_ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @StringView BytePointer ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @StringView String ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor vector_norm(
    @Const @ByRef Tensor self,
    @ByVal Scalar ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor vector_norm(
    @Const @ByRef Tensor self,
    @ByVal Scalar ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor vector_norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @ByVal Scalar ord,
    @ByVal LongArrayRefOptional opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor vector_norm_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @ByVal Scalar ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] opt_dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional opt_dtype);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_norm(
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar ord,
    @ByVal LongArrayRef dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_norm(
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_norm_out(
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar ord,
    @ByVal LongArrayRef dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype,
    @ByRef Tensor result);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_norm_out(
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype,
    @ByRef Tensor result);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_norm(
    @Const @ByRef Tensor self,
    @StdString BytePointer ord,
    @ByVal LongArrayRef dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_norm(
    @Const @ByRef Tensor self,
    @StdString String ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_norm_out(
    @Const @ByRef Tensor self,
    @StdString BytePointer ord,
    @ByVal LongArrayRef dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype,
    @ByRef Tensor result);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_norm_out(
    @Const @ByRef Tensor self,
    @StdString String ord,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] dim,
    @Cast("bool") boolean keepdim,
    @ByVal ScalarTypeOptional dtype,
    @ByRef Tensor result);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_power_out(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor result);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor input, double tol, @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_rank(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor tol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_rank(
    @Const @ByRef Tensor input,
    @ByVal DoubleOptional atol,
    @ByVal DoubleOptional rtol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor matrix_rank(
    @Const @ByRef Tensor input,
    @Const @ByRef TensorOptional atol,
    @Const @ByRef TensorOptional rtol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_rank_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    double tol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_rank_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor tol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_rank_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @ByVal DoubleOptional atol,
    @ByVal DoubleOptional rtol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor matrix_rank_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @Const @ByRef TensorOptional atol,
    @Const @ByRef TensorOptional rtol,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor multi_dot(@ByVal TensorArrayRef tensors);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor multi_dot(@ByVal TensorVector tensors);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor multi_dot_out(@ByVal TensorArrayRef tensors, @ByRef Tensor result);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor multi_dot_out(@ByVal TensorVector tensors, @ByRef Tensor result);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor pinv(@Const @ByRef Tensor input, double rcond, @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor pinv_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    double rcond,
    @Cast("bool") boolean hermitian);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T qr(
    @Const @ByRef Tensor input,
    @StringView BytePointer mode);
@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T qr(
    @Const @ByRef Tensor input,
    @StringView String mode);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> qr_out(
    @ByRef Tensor Q,
    @ByRef Tensor R,
    @Const @ByRef Tensor input,
    @StringView BytePointer mode);
@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> qr_out(
    @ByRef Tensor Q,
    @ByRef Tensor R,
    @Const @ByRef Tensor input,
    @StringView String mode);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensor_T solve_ex(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor other,
    @Cast("bool") boolean left,
    @Cast("bool") boolean check_errors);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> solve_ex_out(
    @ByRef Tensor result,
    @ByRef Tensor info,
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor other,
    @Cast("bool") boolean left,
    @Cast("bool") boolean check_errors);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor solve(@Const @ByRef Tensor input, @Const @ByRef Tensor other, @Cast("bool") boolean left);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor solve_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor other,
    @Cast("bool") boolean left);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor solve_triangular(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor other,
    @Cast("bool") boolean upper,
    @Cast("bool") boolean left,
    @Cast("bool") boolean unitriangular);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor solve_triangular_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor other,
    @Cast("bool") boolean upper,
    @Cast("bool") boolean left,
    @Cast("bool") boolean unitriangular);

@Namespace("torch::linalg::detail") public static native @ByVal T_TensorTensorTensor_T svd(
    @Const @ByRef Tensor input,
    @Cast("bool") boolean full_matrices,
    @ByVal StringViewOptional driver);

@Namespace("torch::linalg::detail") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> svd_out(
    @ByRef Tensor U,
    @ByRef Tensor S,
    @ByRef Tensor Vh,
    @Const @ByRef Tensor input,
    @Cast("bool") boolean full_matrices,
    @ByVal StringViewOptional driver);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor svdvals(
    @Const @ByRef Tensor input,
    @ByVal StringViewOptional driver);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor svdvals_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input,
    @ByVal StringViewOptional driver);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor tensorinv(@Const @ByRef Tensor self, @Cast("int64_t") long ind);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor tensorinv_out(@ByRef Tensor result, @Const @ByRef Tensor self, @Cast("int64_t") long ind);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor tensorsolve(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @ByVal LongArrayRefOptional dims);
@Namespace("torch::linalg::detail") public static native @ByVal Tensor tensorsolve(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor tensorsolve_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @ByVal LongArrayRefOptional dims);
@Namespace("torch::linalg::detail") public static native @ByRef Tensor tensorsolve_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

@Namespace("torch::linalg::detail") public static native @ByVal Tensor inv(@Const @ByRef Tensor input);

@Namespace("torch::linalg::detail") public static native @ByRef Tensor inv_out(@ByRef Tensor result, @Const @ByRef Tensor input);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** Cholesky decomposition
/**
/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.cholesky
/**
/** Example:
/** <pre>{@code
/** auto A = torch::randn({4, 4});
/** auto A = torch::matmul(A, A.t());
/** auto L = torch::linalg::cholesky(A);
/** assert(torch::allclose(torch::matmul(L, L.t()), A));
/** }</pre> */

// C10_DEPRECATED_MESSAGE("linalg_det is deprecated, use det instead.")

/** See the documentation of torch.linalg.det */

/** Computes the sign and (natural) logarithm of the determinant
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.slogdet */

/** Computes eigenvalues and eigenvectors of non-symmetric/non-hermitian
 *  matrices
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.eig */

/** Computes eigenvalues of non-symmetric/non-hermitian matrices
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.eigvals */

/** Computes eigenvalues and eigenvectors
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.eigh */

/** Computes eigenvalues
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.eigvalsh */

/** Computes the product of Householder matrices
 * 
 *  See
 *  https://pytorch.org/docs/master/linalg.html#torch.linalg.householder_product */

/** Computes the matrix exponential
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.matrix_exp */

// C10_DEPRECATED_MESSAGE("linalg_norm is deprecated, use norm instead.")

// C10_DEPRECATED_MESSAGE("linalg_norm is deprecated, use norm instead.")

// C10_DEPRECATED_MESSAGE("linalg_norm_out is deprecated, use norm_out
// instead.")

// C10_DEPRECATED_MESSAGE("linalg_norm_out is deprecated, use norm_out
// instead.")

/** Computes the LU factorization with partial pivoting
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.lu_factor */
@Namespace("torch::linalg") public static native @ByVal T_TensorTensor_T lu_factor(
    @Const @ByRef Tensor input);


///
@Namespace("torch::linalg") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> lu_factor_out(
    @ByRef Tensor LU,
    @ByRef Tensor pivots,
    @Const @ByRef Tensor self);

/** Computes the LU factorization with partial pivoting
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.lu */
@Namespace("torch::linalg") public static native @ByVal T_TensorTensorTensor_T lu(
    @Const @ByRef Tensor input);

@Namespace("torch::linalg") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> lu_out(
    @ByRef Tensor P,
    @ByRef Tensor L,
    @ByRef Tensor U,
    @Const @ByRef Tensor self);

/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.vector_norm */

/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.matrix_norm */

/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.matrix_power */

/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.matrix_rank */

/** See https://pytorch.org/docs/master/linalg.html#torch.linalg.multi_dot */

/** Computes the pseudo-inverse
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.pinv */
@Namespace("torch::linalg") public static native @ByVal Tensor pinv(
    @Const @ByRef Tensor input);


///
@Namespace("torch::linalg") public static native @ByRef Tensor pinv_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor input);

/** Computes the QR decomposition
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.qr */

/** Computes the LDL decomposition
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.ldl_factor_ex */
@Namespace("torch::linalg") public static native @ByVal T_TensorTensorTensor_T ldl_factor_ex(
    @Const @ByRef Tensor input,
    @Cast("bool") boolean hermitian,
    @Cast("bool") boolean check_errors);


///
@Namespace("torch::linalg") public static native @ByVal @Cast("std::tuple<torch::Tensor&,torch::Tensor&,torch::Tensor&>*") PointerPointer<Tensor> ldl_factor_ex_out(
    @ByRef Tensor LD,
    @ByRef Tensor pivots,
    @ByRef Tensor info,
    @Const @ByRef Tensor input,
    @Cast("bool") boolean hermitian,
    @Cast("bool") boolean check_errors);

/** Solve a system of linear equations using the LDL decomposition
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.ldl_solve */
@Namespace("torch::linalg") public static native @ByVal Tensor ldl_solve(
    @Const @ByRef Tensor LD,
    @Const @ByRef Tensor pivots,
    @Const @ByRef Tensor B,
    @Cast("bool") boolean hermitian);


///
@Namespace("torch::linalg") public static native @ByRef Tensor ldl_solve_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor LD,
    @Const @ByRef Tensor pivots,
    @Const @ByRef Tensor B,
    @Cast("bool") boolean hermitian);

/** Solves a system linear system AX = B
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.solve_ex */

/** Computes a tensor {@code x} such that {@code matmul(input, x) = other}.
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.solve */

/** Computes a solution of a linear system AX = B for input = A and other = B
 *  whenever A is square upper or lower triangular and does not have zeros in
 *  the diagonal
 * 
 *  See
 *  https://pytorch.org/docs/master/linalg.html#torch.linalg.solve_triangular */

/** Computes the singular values and singular vectors
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.svd */

/** Computes the singular values
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.svdvals */

/** Computes the inverse of a tensor
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.tensorinv
 * 
 *  Example:
 *  <pre>{@code
 *  auto a = torch::eye(4*6).reshape({4, 6, 8, 3});
 *  int64_t ind = 2;
 *  auto ainv = torch::linalg::tensorinv(a, ind);
 *  }</pre> */

/** Computes a tensor {@code x} such that {@code tensordot(input, x, dims=x.dim()) = other}.
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.tensorsolve
 * 
 *  Example:
 *  <pre>{@code
 *  auto a = torch::eye(2*3*4).reshape({2*3, 4, 2, 3, 4});
 *  auto b = torch::randn(2*3, 4);
 *  auto x = torch::linalg::tensorsolve(a, b);
 *  }</pre> */

/** Computes a tensor {@code inverse_input} such that {@code dot(input, inverse_input) =
 *  eye(input.size(0))}.
 * 
 *  See https://pytorch.org/docs/master/linalg.html#torch.linalg.inv */

 // namespace linalg
 // namespace torch


// Parsed from torch/csrc/api/include/torch/mps.h

// #pragma once

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <cstdint>

// #ifdef __OBJC__
// #else
// #endif

/** Returns true if MPS device is available. */
@Namespace("torch::mps") public static native @Cast("bool") boolean is_available();

/** Sets the RNG seed for the MPS device. */

/** Waits for all streams on the MPS device to complete.
 *  This blocks the calling CPU thread by using the 'waitUntilCompleted()'
 *  method to wait for Metal command buffers finish executing all the
 *  encoded GPU operations before returning. */
@Namespace("torch::mps") public static native void synchronize();

/** Submits the currently active command buffer to run on the MPS device. */
@Namespace("torch::mps") public static native void commit();

/** Get the current command buffer to encode the Metal commands. */
@Namespace("torch::mps") public static native Pointer get_command_buffer();

/** Get the dispatch_queue_t to synchronize encoding the custom kernels
 *  with the PyTorch MPS backend. */
@Namespace("torch::mps") public static native Pointer get_dispatch_queue();

 // namespace mps
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nested.h

// #pragma once

// #include <ATen/ATen.h>
// #include <ATen/core/ATen_fwd.h>
// #include <torch/csrc/api/include/torch/detail/TensorDataContainer.h>
// #include <algorithm>

/** Nested tensor
 * 
 *  See
 *  https://pytorch.org/docs/master/nested.html#torch.nested.nested_tensor
 * 
 *  <pre>{@code */
// implemented on python object to allow torch.nested.nested_tensor to be
// constructed with arbitrarily nested python objects - for now, only arbitrary
// python lists and lists of Tensors
// See torch/csrc/autograd/python_nested_functions_manual.cpp for Python
// implementation
// See here for C++ implementation
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal TensorArrayRef nested_tensor_data,
    @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal TensorArrayRef nested_tensor_data);
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal TensorVector nested_tensor_data,
    @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal TensorVector nested_tensor_data);


///
///
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal @Cast("at::ArrayRef<torch::detail::TensorDataContainer>*") Pointer nested_tensor_data,
    @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch::nested") public static native @ByVal Tensor nested_tensor(
    @ByVal @Cast("at::ArrayRef<torch::detail::TensorDataContainer>*") Pointer nested_tensor_data);

/** As Nested Tensor
 * 
 *  See
 *  https://pytorch.org/docs/master/nested.html#torch.nested.as_nested_tensor
 * 
 *  <pre>{@code */

///
///
@Namespace("torch::nested") public static native @ByVal Tensor as_nested_tensor(
    @ByVal TensorArrayRef list,
    @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::nested") public static native @ByVal Tensor as_nested_tensor(
    @ByVal TensorArrayRef list);
@Namespace("torch::nested") public static native @ByVal Tensor as_nested_tensor(
    @ByVal TensorVector list,
    @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::nested") public static native @ByVal Tensor as_nested_tensor(
    @ByVal TensorVector list);

/** Nested to padded tensor
 * 
 *  See
 *  https://pytorch.org/docs/master/nested.html#torch.nested.to_padded_tensor
 * 
 *  <pre>{@code */
@Namespace("torch::nested") public static native @ByVal Tensor to_padded_tensor(
    @Const @ByRef Tensor self,
    double padding,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional output_size);
@Namespace("torch::nested") public static native @ByVal Tensor to_padded_tensor(
    @Const @ByRef Tensor self,
    double padding);
@Namespace("torch::nested") public static native @ByVal Tensor to_padded_tensor(
    @Const @ByRef Tensor self,
    double padding,
    @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

 // namespace nested
 // namespace torch


// Parsed from torch/csrc/api/include/torch/detail/static.h

// #pragma once

// #include <torch/csrc/utils/variadic.h>
// #include <torch/types.h>

// #include <cstdint>
// #include <type_traits>
 // namespace nn
 // namespace torch
/** Detects if a type T has a forward() method. */





/** A type trait whose {@code value} member is true if {@code M} derives from {@code Module}. */
 // namespace detail
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/pimpl-inl.h

// This class exists  only to do SFINAE on abstract types `T` that are really
// `ModuleHolder<ModuleType>`, because there's no good way to say that `T` is a
// `ModuleHolder` over some unknown type `ModuleType`. With this, you can do
// `enable_if_t<is_base_of_v<ModuleHolderIndicator, T>>`.

// A type trait that is true for types that are `ModuleHolder`s.

// A collection of templates that answer the question whether a type `T` is a
// `ModuleHolder`, and if so whether its contained type is of type `C`. This is
// tricky because it is hard to short circuit in template metaprogramming. A
// naive and incorrect solution to this problem would be something like
// `disable_if<is_module_holder<T>::value && typename T::ContainedType == C>`.
// This would disable all types that are not `ModuleHolder`s, because even
// though the `is_module_holder<T>::value` may be `false` for such types the
// `T::ContainedType` access would be ill-formed and thus fail the whole
// expression by the rules of SFINAE. Instead we have to use template
// specialization to statically branch on the first condition
// (`is_module_holder<T>`) and are only then allowed to query
// `T::ContainedType` in the branch for which the condition was true.

// Base template.

// False branch. `T` is not a `ModuleHolder` and thus not a `ModuleHolder` with
// contained type `C`.

// True branch. `T` is a `ModuleHolder` and thus we can legit access its
// `ContainedType` and compare it against `C`.

// Helper template.

// A collection of templates that allow deducing the return type of the
// `forward()` method, but only if a module actually has a `forward()` method,
// and otherwise deduces to the type `void`.


// Parsed from torch/csrc/api/include/torch/nn/pimpl.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/detail/static.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/variadic.h>

// #include <memory>
// #include <type_traits>
// #include <utility>
// Dump all the template metaprogramming in this file.
// #include <torch/csrc/api/include/torch/nn/pimpl-inl.h>
 // namespace detail

/** A {@code ModuleHolder} is essentially a wrapper around {@code std::shared_ptr<M>} where
 *  {@code M} is an {@code nn::Module} subclass, with convenient constructors defined for
 *  the kind of constructions we want to allow for our modules. */

/** Pretty prints the given {@code Module} into the {@code ostream}. */

/** Serializes a {@code ModuleHolder} into an {@code OutputArchive}. */

/** Deserializes a {@code ModuleHolder} from an {@code InputArchive}. */

 // namespace nn
 // namespace torch

// Workaround for CUDA 10.2 and below not allowing attribute unused on
// using declarations.
// #ifdef __CUDACC__
// #define TORCH_UNUSED_EXCEPT_CUDA
// #else
// #define TORCH_UNUSED_EXCEPT_CUDA C10_UNUSED
// #endif

/** Defines a class {@code Name} which inherits from {@code nn::ModuleHolder} to provide a
 *  wrapper over a {@code std::shared_ptr<ImplType>}.
 *  {@code Impl} is a type alias for {@code ImplType} which provides a way to call static
 *  method of {@code ImplType}. */
// #define TORCH_MODULE_IMPL(Name, ImplType)
//   class Name : public torch::nn::ModuleHolder<ImplType> { /* NOLINT */
//    public:
//     using torch::nn::ModuleHolder<ImplType>::ModuleHolder;
//     using Impl TORCH_UNUSED_EXCEPT_CUDA = ImplType;
//   }

/** Like {@code TORCH_MODULE_IMPL}, but defaults the {@code ImplType} name to {@code <Name>Impl}. */
// #define TORCH_MODULE(Name) TORCH_MODULE_IMPL(Name, Name##Impl)


// Parsed from torch/csrc/api/include/torch/nn/modules/container/any_value.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// Targeting ../AnyValue.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h

// #pragma once

// #include <torch/nn/modules/container/any_value.h>

// ~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModulePlaceholder ~~~~~~~~~~~~~~~~~~~~~~~~~~

/** The static type of the object we store in the {@code AnyModule}, which erases
 *  the actual type, but allows us to call {@code forward()} on the underlying
 *  module. */

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModuleHolder ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** The dynamic type of the object stored in the {@code AnyModule}. It contains the
 *  concrete instance to which all calls are forwarded. It is parameterized
 *  over the concrete type of the module, and the types of the arguments the
 *  module takes in its {@code forward()} method. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/module.h

// #pragma once

// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>

// #include <functional>
// #include <iosfwd>
// #include <map>
// #include <memory>
// #include <string>
// #include <type_traits>
// Targeting ../Module.java


@Namespace("torch::nn") public static Pointer shiftLeft(Pointer stream, Module module) { return _shiftLeft(stream, module.asModule()); }
private static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer _shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Module module);


/** Serialize a {@code Module} pointer into an {@code OutputArchive}. */
@Namespace("torch::nn") public static OutputArchive shiftLeft(OutputArchive archive, Module module) { return _shiftLeft(archive, module.asModule()); }
private static native @ByRef @Name("operator <<") OutputArchive _shiftLeft(
    @ByRef OutputArchive archive,
    @Const @SharedPtr("torch::nn::Module") @ByRef Module module);

/** Deserializes a {@code Module} from an {@code InputArchive}. */
@Namespace("torch::nn") public static InputArchive shiftRight(InputArchive archive, Module module) { return _shiftRight(archive, module.asModule()); }
private static native @ByRef @Name("operator >>") InputArchive _shiftRight(
    @ByRef InputArchive archive,
    @Const @SharedPtr("torch::nn::Module") @ByRef Module module);

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nn::Module ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



















 // namespace nn
 // namespace torch


// Parsed from ATen/Config.h

// #pragma once

// Test these using #if AT_MKL_ENABLED(), not #ifdef, so that it's
// obvious if you forgot to include Config.h
//    c.f. https://stackoverflow.com/questions/33759787/generating-an-error-if-checked-boolean-macro-is-not-defined
//
// DO NOT put the macros for CUDA libraries in this file; they belong in cuda/CUDAConfig.h

// #define AT_MKLDNN_ENABLED() 1
// #define AT_MKLDNN_ACL_ENABLED() 0
// #define AT_MKL_ENABLED() 0
// #define AT_MKL_SEQUENTIAL() 0
// #define AT_FFTW_ENABLED() 1
// #define AT_POCKETFFT_ENABLED() 1
// #define AT_NNPACK_ENABLED() 1
// #define CAFFE2_STATIC_LINK_CUDA() 0
// #define AT_BUILD_WITH_BLAS() 1
// #define AT_BUILD_WITH_LAPACK() 1
public static final int AT_PARALLEL_OPENMP = 1;
public static final int AT_PARALLEL_NATIVE = 0;
public static final int AT_PARALLEL_NATIVE_TBB = 0;
// #define AT_BLAS_F2C() 0
// #define AT_BLAS_USE_CBLAS_DOT() 1


// Parsed from ATen/Parallel-inl.h

// #pragma once

// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>

 // namespace at


// Parsed from ATen/Parallel.h

// #pragma once
// #include <ATen/Config.h>
// #include <c10/macros/Macros.h>
// #include <functional>
// #include <string>

@Namespace("at") public static native @Cast("int64_t") long divup(@Cast("int64_t") long x, @Cast("int64_t") long y);

// Called during new thread initialization
@Namespace("at") public static native void init_num_threads();

// Sets the number of threads to be used in parallel region
@Namespace("at") public static native void set_num_threads(int arg0);

// Returns the maximum number of threads that may be used in a parallel region
@Namespace("at") public static native int get_num_threads();

// Returns the current thread number (starting from 0)
// in the current parallel region, or 0 in the sequential region
@Namespace("at") public static native int get_thread_num();

// Checks whether the code runs in parallel region
@Namespace("at") public static native @Cast("bool") boolean in_parallel_region();

// Initialise num_threads lazily at first parallel call
@Namespace("at::internal") public static native void lazy_init_num_threads();

@Namespace("at::internal") public static native void set_thread_num(int arg0);
// Targeting ../ThreadIdGuard.java



 // namespace internal

/*
parallel_for

begin: index at which to start applying user function

end: index at which to stop applying user function

grain_size: number of elements per chunk. impacts the degree of parallelization

f: user function applied in parallel to the chunks, signature:
  void f(int64_t begin, int64_t end)

Warning: parallel_for does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.
*/

/*
parallel_reduce

begin: index at which to start applying reduction

end: index at which to stop applying reduction

grain_size: number of elements per chunk. impacts number of elements in
intermediate results tensor and degree of parallelization.

ident: identity for binary combination function sf. sf(ident, x) needs to return
x.

f: function for reduction over a chunk. f needs to be of signature scalar_t
f(int64_t partial_begin, int64_t partial_end, scalar_t identifiy)

sf: function to combine two partial results. sf needs to be of signature
scalar_t sf(scalar_t x, scalar_t y)

For example, you might have a tensor of 10000 entires and want to sum together
all the elements. Parallel_reduce with a grain_size of 2500 will then allocate
an intermediate result tensor with 4 elements. Then it will execute the function
"f" you provide and pass the beginning and end index of these chunks, so
0-2499, 2500-4999, etc. and the combination identity. It will then write out
the result from each of these chunks into the intermediate result tensor. After
that it'll reduce the partial results from each chunk into a single number using
the combination function sf and the identity ident. For a total summation this
would be "+" and 0 respectively. This is similar to tbb's approach [1], where
you need to provide a function to accumulate a subrange, a function to combine
two partial results and an identity.

Warning: parallel_reduce does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.

[1] https://software.intel.com/en-us/node/506154
*/

// Returns a detailed string describing parallelization settings
@Namespace("at") public static native @StdString BytePointer get_parallel_info();

// Sets number of threads used for inter-op parallelism
@Namespace("at") public static native void set_num_interop_threads(int arg0);

// Returns the number of threads used for inter-op parallelism
@Namespace("at") public static native int get_num_interop_threads();

// Launches inter-op parallel task

 // namespace internal

// Launches intra-op parallel task
@Namespace("at") public static native void intraop_launch(@ByVal Func func);

// Returns number of intra-op threads used by default
@Namespace("at") public static native int intraop_default_num_threads();

 // namespace at

// #if AT_PARALLEL_OPENMP
// #include <ATen/ParallelOpenMP.h> // IWYU pragma: keep
// #elif AT_PARALLEL_NATIVE
// #include <ATen/ParallelNative.h> // IWYU pragma: keep
// #elif AT_PARALLEL_NATIVE_TBB
// #include <ATen/ParallelNativeTBB.h> // IWYU pragma: keep
// #endif

// #include <ATen/Parallel-inl.h> // IWYU pragma: keep


// Parsed from torch/csrc/profiler/orchestration/observer.h

// #pragma once

// #include <ATen/record_function.h>
// #include <torch/csrc/Export.h>

// #include <utility>

// ----------------------------------------------------------------------------
// -- Profiler Config ---------------------------------------------------------
// ----------------------------------------------------------------------------
@Namespace("torch::profiler::impl") public enum ActivityType {
  CPU(0),
  XPU(1), // XPU kernels, runtime
  CUDA(2), // CUDA kernels, runtime
  MTIA(3), // MTIA kernels, runtime
  NUM_KINETO_ACTIVITIES(4);// must be the last one

    public final int value;
    private ActivityType(int v) { this.value = v; }
    private ActivityType(ActivityType e) { this.value = e.value; }
    public ActivityType intern() { for (ActivityType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::profiler::impl") public enum ProfilerState {
  Disabled(0),
  CPU(1), // CPU-only profiling
  CUDA(2), // CPU + CUDA events
  NVTX(3), // only emit NVTX markers
  ITT(4), // only emit ITT markers
  KINETO(5), // use libkineto
  KINETO_GPU_FALLBACK(6), // use CUDA events when CUPTI is not available
  KINETO_PRIVATEUSE1_FALLBACK(7), // use PrivateUse1 events
  KINETO_ONDEMAND(8), // run the profiler in on-demand mode
  NUM_PROFILER_STATES(9);// must be the last one

    public final int value;
    private ProfilerState(int v) { this.value = v; }
    private ProfilerState(ProfilerState e) { this.value = e.value; }
    public ProfilerState intern() { for (ProfilerState e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::profiler::impl") public enum ActiveProfilerType {
  NONE(0),
  LEGACY(1),
  KINETO(2),
  NVTX(3),
  ITT(4);

    public final int value;
    private ActiveProfilerType(int v) { this.value = v; }
    private ActiveProfilerType(ActiveProfilerType e) { this.value = e.value; }
    public ActiveProfilerType intern() { for (ActiveProfilerType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ExperimentalConfig.java


// Targeting ../ProfilerConfig.java



// ----------------------------------------------------------------------------
// -- Profiler base class -----------------------------------------------------
// ----------------------------------------------------------------------------

// Note: The following are only for the active *thread local* profiler.
@Namespace("torch::profiler::impl") public static native @Cast("bool") boolean profilerEnabled();
@Namespace("torch::profiler::impl") public static native ActiveProfilerType profilerType();
@Namespace("torch::profiler::impl") public static native @ByVal ProfilerConfig getProfilerConfig();

 // namespace impl
 // namespace profiler
 // namespace torch


// Parsed from torch/csrc/profiler/api.h

// #pragma once

// #include <torch/csrc/profiler/orchestration/observer.h>

// There are some components which use these symbols. Until we migrate them
// we have to mirror them in the old autograd namespace.
 // namespace profiler
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/profiler/events.h

// #pragma once

// #include <array>
// #include <cstring>
// #include <vector>

/* A vector type to hold a list of performance counters */

/* Standard list of performance events independent of hardware or backend */
@Namespace("torch::profiler") @MemberGetter public static native @Const @ByRef PointerPointer<BytePointer> ProfilerPerfEvents();
 // namespace profiler
 // namespace torch


// Parsed from torch/csrc/profiler/stubs/base.h

// #pragma once

// #include <functional>
// #include <memory>

// #include <c10/util/strong_type.h>
// #include <torch/csrc/Export.h>

// ----------------------------------------------------------------------------
// -- Annotation --------------------------------------------------------------
// ----------------------------------------------------------------------------

 // namespace impl
 // namespace profiler
 // namespace torch


// Parsed from torch/csrc/profiler/util.h

// #pragma once

// #include <cstddef>
// #include <cstdint>
// #include <list>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include <ATen/record_function.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Optional.h>
// #include <c10/util/hash.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>

// #ifndef _WIN32
// #include <ctime>
// #endif
// #if defined(C10_IOS) && defined(C10_MOBILE)
// #include <sys/time.h> // for gettimeofday()
// #endif

// #if defined(__i386__) || defined(__x86_64__) || defined(__amd64__)
// #define C10_RDTSC
// #if defined(_MSC_VER)
// #elif defined(__CUDACC__) || defined(__HIPCC__)
// #elif defined(__clang__)
// `__rdtsc` is available by default.
// NB: This has to be first, because Clang will also define `__GNUC__`
// #elif defined(__GNUC__)
// #include <x86intrin.h>
// #else
// #undef C10_RDTSC
// #endif
// #endif

// TODO: replace with pytorch/rfcs#43 when it is ready.
// #define SOFT_ASSERT(cond, ...)
//   [&]() -> bool {
//     if (C10_UNLIKELY(!(cond))) {
//       torch::profiler::impl::logSoftAssert(
//           __func__,
//           __FILE__,
//           static_cast<uint32_t>(__LINE__),
//           #cond,
//           ::c10::str(__VA_ARGS__));
//       if (torch::profiler::impl::softAssertRaises()) {
//         TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__);
//       } else {
//         TORCH_WARN(__VA_ARGS__);
//       }
//       return false;
//     }
//     return true;
//   }()
@Namespace("torch::profiler::impl") public static native @Cast("bool") boolean softAssertRaises();
@Namespace("torch::profiler::impl") public static native void setSoftAssertRaises(@ByVal BoolOptional value);
@Namespace("torch::profiler::impl") public static native void logSoftAssert(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer cond,
    @Cast("const char*") BytePointer args);
@Namespace("torch::profiler::impl") public static native void logSoftAssert(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String cond,
    String args);
@Namespace("torch::profiler::impl") public static native void logSoftAssert(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer cond,
    @ByVal CompileTimeEmptyString args);
@Namespace("torch::profiler::impl") public static native void logSoftAssert(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String cond,
    @ByVal CompileTimeEmptyString args);

@Namespace("torch::profiler::impl") public static native @Cast("torch::profiler::impl::time_t") long getTimeSinceEpoch();

@Namespace("torch::profiler::impl") public static native @Cast("torch::profiler::impl::time_t") long getTime(@Cast("bool") boolean allow_monotonic/*=false*/);
@Namespace("torch::profiler::impl") public static native @Cast("torch::profiler::impl::time_t") long getTime();

// We often do not need to capture true wall times. If a fast mechanism such
// as TSC is available we can use that instead and convert back to epoch time
// during post processing. This greatly reduce the clock's contribution to
// profiling.
//   http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/
//   https://quick-bench.com/q/r8opkkGZSJMu9wM_XTbDouq-0Io
// TODO: We should use
// `https://github.com/google/benchmark/blob/main/src/cycleclock.h`

// Convert `getCount` results to Nanoseconds since unix epoch.


// Targeting ../FileLineFunc.java



@Namespace("torch::profiler::impl") public static native @StdVector FileLineFunc prepareCallstack(
    @StdVector StackEntry cs);
@Namespace("torch::profiler::impl") public static native @ByVal StringVector callstackStr(
    @StdVector FileLineFunc cs);
@Namespace("torch::profiler::impl") public static native @StdString BytePointer stacksToStr(
    @Const @ByRef StringVector stacks,
    @Cast("const char*") BytePointer delim);
@Namespace("torch::profiler::impl") public static native @StdString String stacksToStr(
    @Const @ByRef StringVector stacks,
    String delim);
@Namespace("torch::profiler::impl") public static native @Cast("std::vector<int64_t>*") @StdVector LongVector inputSizes(
    @Const @ByRef RecordFunction fn,
    @Cast("const bool") boolean flatten_list_enabled/*=false*/);
@Namespace("torch::profiler::impl") public static native @Cast("std::vector<int64_t>*") @StdVector LongVector inputSizes(
    @Const @ByRef RecordFunction fn);
@Namespace("torch::profiler::impl") public static native @StdString BytePointer shapesToStr(
    @Cast("std::vector<int64_t>*") @StdVector LongVector shapes);
@Namespace("torch::profiler::impl") public static native @StdString BytePointer strListToStr(@Const @ByRef StringVector types);
@Namespace("torch::profiler::impl") public static native @StdString BytePointer inputOpIdsToStr(
    @Const @ByRef RecordFunctionHandleIntList input_op_ids);
@Namespace("torch::profiler::impl") public static native @StdString BytePointer ivalueListToStr(@Const @ByRef IValueVector list);
@Namespace("torch::profiler::impl") public static native @ByVal StringVector inputTypes(@Const @ByRef RecordFunction fn);

@Namespace("torch::profiler::impl") public static native @ByVal StringIValueMap saveExtraArgs(@Const @ByRef RecordFunction fn);

@Namespace("torch::profiler::impl") public static native @Cast("uint64_t") long computeFlops(
    @StdString BytePointer op_name,
    @Const @ByRef StringIValueMap extra_args);
@Namespace("torch::profiler::impl") public static native @Cast("uint64_t") long computeFlops(
    @StdString String op_name,
    @Const @ByRef StringIValueMap extra_args);

 // namespace impl
 // namespace profiler
 // namespace torch
 // namespace profiler
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/profiler_kineto.h

// #pragma once

// #include <string>
// #include <vector>

// #include <torch/csrc/profiler/api.h>
// #include <torch/csrc/profiler/events.h>
// #include <torch/csrc/profiler/stubs/base.h>
// #include <torch/csrc/profiler/util.h>
// Targeting ../Result.java


 // namespace kineto
 // namespace impl
 // namespace profiler

// Consolidating events returned directly from Kineto
// with events manually created by us (e.g. start/stop marks,
// memory allocation events)

/*
 * This API is used by backends to record latency of events that
 * happened in the backend but were not visible to pytorch runtime.
 * For example, if part of the model is lowered to a dsp backend, then
 * the execution of that part of the model is delegated to the backend.
 * When backend finishes execution it has an option to provide profiling
 * information (latency only at the moment) corresponding to different operators
 * that were executed in the backend.
 * When such events are recorded by backend using this API, the event
 * records will be collected by active kineto profiler. If no kineto profiler
 * is active then the event is ignored.
 * This provides us with a way to generate all the profiling information
 * for a model regardless of where model (or part of it) executed.
 * @param start_time_us: start time in us of the event
 * @param end_time_us: end time in us of the event
 * @param debug_handle: debug handle to correlate this event/op with
 * model level module/source information
 * @param scope: scope of the event, e.g. LITE_INTERPRETER, RECORD_FN etc.
 * @param event_name: name of the event, e.g. op name
 * @param backend_name: name of the backend where the event took place.
 */
@Namespace("torch::autograd::profiler") public static native void reportBackendEventToActiveKinetoProfiler(
    @Cast("const int64_t") long start_time_us,
    @Cast("const int64_t") long end_time_us,
    @Cast("const int64_t") long debug_handle,
    RecordScope scope,
    @StdString BytePointer event_name,
    @StdString BytePointer backend_name);
@Namespace("torch::autograd::profiler") public static native void reportBackendEventToActiveKinetoProfiler(
    @Cast("const int64_t") long start_time_us,
    @Cast("const int64_t") long end_time_us,
    @Cast("const int64_t") long debug_handle,
    @Cast("at::RecordScope") byte scope,
    @StdString String event_name,
    @StdString String backend_name);



/*
 * Same as enableProfiler but with callback to do post-processing of
 * KinetoEvents.
 * enableProfilerWithEventPostProcess enables profiler to capture
 * specified activities, with specified RecordFunction scope, if any.
 * Additionally, it takes a functor that does in-place post processing of
 * events, e.g. populate stack trace or module hierarchy information lazily
 * using debug_handle.
 * Example usage is with lite interpreter that has recording scope of
 * LITE_INTERPRETER. In this case lite interpreter runtime, records debug
 * handles in RecordFunction, along with other information. Debug handles are
 * eventually passed down to KinetoEvent and recorded as part of the event.
 * KinetoEdgeCPUProfiler, in torch/csrc/jit/mobile/profiler_edge.cpp, enables
 * profiler using post-processing callback, via
 * enableProfilerWithEventPostProcess, that takes these debug handles and
 * generates stack trace and module hierarchy information, once profiling is
 * done.
 */


@Namespace("torch::autograd::profiler") public static native void prepareProfiler(
    @Const @ByRef ProfilerConfig config,
    @Const @ByRef ActivityTypeSet activities);

 // namespace profiler
 // namespace autograd

// Experimental.
@Namespace("torch::profiler::impl") public static native void _reportVulkanEventToProfiler(@ByVal @Cast("torch::profiler::impl::vulkan_id_t*") Pointer id);

 // namespace impl
 // namespace profiler

 // namespace torch


// Parsed from torch/csrc/autograd/profiler.h

// #pragma once

// #include <torch/csrc/autograd/profiler_kineto.h>
// #include <torch/csrc/autograd/profiler_legacy.h>


// Parsed from torch/csrc/api/include/torch/utils.h

// #pragma once

// #include <ATen/Parallel.h>
// #include <ATen/record_function.h>
// #include <torch/csrc/api/include/torch/types.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/profiler.h>
// #include <cstdint>

/** A RAII, thread-local guard that disabled gradient calculation.
 * 
 *  Disabling gradient calculation is useful for inference, when you are sure
 *  that you will not call {@code at::Tensor::backward}. It will reduce memory
 *  consumption for computations that would otherwise have {@code requires_grad() ==
 *  true}.
 * 
 *  In this mode, the result of every computation will have
 *  {@code requires_grad() == false}, even when the inputs have {@code requires_grad() ==
 *  true}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::NoGradGuard no_grad;
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  {
 *    auto doubler = [](torch::Tensor x) {
 *      torch::NoGradGuard no_grad;
 *      return x * 2;
 *    };
 *    auto z = doubler(x);
 *    std::cout << z.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

///
///
///
///

/** A RAII, thread-local guard that sets gradient calculation to on or off.
 * 
 *  {@code }AutoGradMode{@code } will enable or disable grads based on its argument
 *  {@code enabled}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  @param enabled: Flag whether to enable grad ({@code }true{@code }), or disable
 *               ({@code }false{@code }). This can be used to conditionally enable
 *               gradients.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::AutoGradMode enable_grad(true);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `true`
 *  }
 *  {
 *    torch::AutoGradMode enable_grad(false);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

/** Sets the global random seed for all newly created CPU and CUDA tensors. */

// Called during new thread initialization

// Returns the number of threads used in parallel region.

// Sets the number of threads to be used in parallel region.

// Returns the number of threads used for inter-op parallelism.

// Sets the number of threads to be used for inter-op parallelism.

// Returns true if both t1, t2 are undefined or both are defined and equal
@Namespace("torch") public static native @Cast("bool") boolean equal_if_defined(@ByVal Tensor t1, @ByVal Tensor t2);

// RecordFunction API

 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/cloneable.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/types.h>
// #include <torch/utils.h>

// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <utility>
// Targeting ../ModuleDictImplCloneable.java


// Targeting ../ModuleListImplCloneable.java


// Targeting ../SequentialImplCloneable.java


// Targeting ../ParameterDictImplCloneable.java


// Targeting ../ParameterListImplCloneable.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplCloneable.java


// Targeting ../BatchNorm1dImplCloneable.java


// Targeting ../InstanceNorm1dImplCloneable.java


// Targeting ../Conv1dImplCloneable.java


// Targeting ../ConvTranspose1dImplCloneable.java


// Targeting ../DropoutImplCloneable.java


// Targeting ../BatchNorm2dImplCloneable.java


// Targeting ../InstanceNorm2dImplCloneable.java


// Targeting ../Conv2dImplCloneable.java


// Targeting ../ConvTranspose2dImplCloneable.java


// Targeting ../Dropout2dImplCloneable.java


// Targeting ../BatchNorm3dImplCloneable.java


// Targeting ../InstanceNorm3dImplCloneable.java


// Targeting ../Conv3dImplCloneable.java


// Targeting ../ConvTranspose3dImplCloneable.java


// Targeting ../Dropout3dImplCloneable.java


// Targeting ../AlphaDropoutImplCloneable.java


// Targeting ../FeatureAlphaDropoutImplCloneable.java


// Targeting ../CosineSimilarityImplCloneable.java


// Targeting ../PairwiseDistanceImplCloneable.java


// Targeting ../EmbeddingImplCloneable.java


// Targeting ../EmbeddingBagImplCloneable.java


// Targeting ../FoldImplCloneable.java


// Targeting ../UnfoldImplCloneable.java


// Targeting ../IdentityImplCloneable.java


// Targeting ../LinearImplCloneable.java


// Targeting ../BilinearImplCloneable.java


// Targeting ../FlattenImplCloneable.java


// Targeting ../UnflattenImplCloneable.java


// Targeting ../L1LossImplCloneable.java


// Targeting ../KLDivLossImplCloneable.java


// Targeting ../MSELossImplCloneable.java


// Targeting ../BCELossImplCloneable.java


// Targeting ../HingeEmbeddingLossImplCloneable.java


// Targeting ../MultiMarginLossImplCloneable.java


// Targeting ../CosineEmbeddingLossImplCloneable.java


// Targeting ../SmoothL1LossImplCloneable.java


// Targeting ../HuberLossImplCloneable.java


// Targeting ../MultiLabelMarginLossImplCloneable.java


// Targeting ../SoftMarginLossImplCloneable.java


// Targeting ../MultiLabelSoftMarginLossImplCloneable.java


// Targeting ../TripletMarginLossImplCloneable.java


// Targeting ../TripletMarginWithDistanceLossImplCloneable.java


// Targeting ../CTCLossImplCloneable.java


// Targeting ../PoissonNLLLossImplCloneable.java


// Targeting ../MarginRankingLossImplCloneable.java


// Targeting ../NLLLossImplCloneable.java


// Targeting ../CrossEntropyLossImplCloneable.java


// Targeting ../BCEWithLogitsLossImplCloneable.java


// Targeting ../ReflectionPad1dImplCloneable.java


// Targeting ../ReplicationPad1dImplCloneable.java


// Targeting ../ConstantPad1dImplCloneable.java


// Targeting ../ZeroPad1dImplCloneable.java


// Targeting ../AvgPool1dImplCloneable.java


// Targeting ../MaxPool1dImplCloneable.java


// Targeting ../AdaptiveAvgPool1dImplCloneable.java


// Targeting ../AdaptiveMaxPool1dImplCloneable.java


// Targeting ../MaxUnpool1dImplCloneable.java


// Targeting ../LPPool1dImplCloneable.java


// Targeting ../ReflectionPad2dImplCloneable.java


// Targeting ../ReplicationPad2dImplCloneable.java


// Targeting ../ConstantPad2dImplCloneable.java


// Targeting ../ZeroPad2dImplCloneable.java


// Targeting ../AvgPool2dImplCloneable.java


// Targeting ../MaxPool2dImplCloneable.java


// Targeting ../AdaptiveAvgPool2dImplCloneable.java


// Targeting ../AdaptiveMaxPool2dImplCloneable.java


// Targeting ../MaxUnpool2dImplCloneable.java


// Targeting ../FractionalMaxPool2dImplCloneable.java


// Targeting ../LPPool2dImplCloneable.java


// Targeting ../ReflectionPad3dImplCloneable.java


// Targeting ../ReplicationPad3dImplCloneable.java


// Targeting ../ConstantPad3dImplCloneable.java


// Targeting ../ZeroPad3dImplCloneable.java


// Targeting ../AvgPool3dImplCloneable.java


// Targeting ../MaxPool3dImplCloneable.java


// Targeting ../AdaptiveAvgPool3dImplCloneable.java


// Targeting ../AdaptiveMaxPool3dImplCloneable.java


// Targeting ../MaxUnpool3dImplCloneable.java


// Targeting ../FractionalMaxPool3dImplCloneable.java


// Targeting ../RNNImplCloneable.java


// Targeting ../LSTMImplCloneable.java


// Targeting ../GRUImplCloneable.java


// Targeting ../RNNCellImplCloneable.java


// Targeting ../LSTMCellImplCloneable.java


// Targeting ../GRUCellImplCloneable.java


// Targeting ../PixelShuffleImplCloneable.java


// Targeting ../PixelUnshuffleImplCloneable.java


// Targeting ../UpsampleImplCloneable.java


// Targeting ../ELUImplCloneable.java


// Targeting ../SELUImplCloneable.java


// Targeting ../HardshrinkImplCloneable.java


// Targeting ../HardtanhImplCloneable.java


// Targeting ../LeakyReLUImplCloneable.java


// Targeting ../LogSigmoidImplCloneable.java


// Targeting ../SoftmaxImplCloneable.java


// Targeting ../SoftminImplCloneable.java


// Targeting ../LogSoftmaxImplCloneable.java


// Targeting ../Softmax2dImplCloneable.java


// Targeting ../PReLUImplCloneable.java


// Targeting ../ReLUImplCloneable.java


// Targeting ../ReLU6ImplCloneable.java


// Targeting ../RReLUImplCloneable.java


// Targeting ../CELUImplCloneable.java


// Targeting ../GLUImplCloneable.java


// Targeting ../GELUImplCloneable.java


// Targeting ../SiLUImplCloneable.java


// Targeting ../MishImplCloneable.java


// Targeting ../SigmoidImplCloneable.java


// Targeting ../SoftplusImplCloneable.java


// Targeting ../SoftshrinkImplCloneable.java


// Targeting ../SoftsignImplCloneable.java


// Targeting ../TanhImplCloneable.java


// Targeting ../TanhshrinkImplCloneable.java


// Targeting ../ThresholdImplCloneable.java


// Targeting ../MultiheadAttentionImplCloneable.java


// Targeting ../LayerNormImplCloneable.java


// Targeting ../LocalResponseNormImplCloneable.java


// Targeting ../CrossMapLRN2dImplCloneable.java


// Targeting ../GroupNormImplCloneable.java


// Targeting ../TransformerEncoderLayerImplCloneable.java


// Targeting ../TransformerDecoderLayerImplCloneable.java


// Targeting ../TransformerEncoderImplCloneable.java


// Targeting ../TransformerDecoderImplCloneable.java


// Targeting ../TransformerImplCloneable.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/batchnorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../BatchNormOptions.java



/** Options for the {@code BatchNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm1d
 *  model(BatchNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm2d
 *  model(BatchNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm3d
 *  model(BatchNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

// ============================================================================
// Targeting ../BatchNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/batchnorm.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @ByVal Tensor weight,
    @ByVal Tensor bias,
    @Cast("bool") boolean training,
    @ByVal DoubleOptional momentum,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.batch_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BatchNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::batch_norm(input, mean, variance,
/** F::BatchNormFuncOptions().weight(weight).bias(bias).momentum(0.1).eps(1e-05).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @Const @ByRef(nullValue = "torch::nn::functional::BatchNormFuncOptions{}") BatchNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/conv.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../DetailConv1dOptions.java


// Targeting ../DetailConv2dOptions.java


// Targeting ../DetailConv3dOptions.java




// Targeting ../Conv1dOptions.java


// Targeting ../Conv2dOptions.java


// Targeting ../Conv3dOptions.java



/** {@code ConvOptions} specialized for the {@code Conv1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv1d model(Conv1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv2d model(Conv2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv3d model(Conv3dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../Conv1dFuncOptions.java


// Targeting ../Conv2dFuncOptions.java


// Targeting ../Conv3dFuncOptions.java



/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
 *  }</pre> */


// Targeting ../ConvTranspose1dOptions.java


// Targeting ../ConvTranspose2dOptions.java


// Targeting ../ConvTranspose3dOptions.java



/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose1d model(ConvTranspose1dOptions(3, 2,
 *  3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose2d model(ConvTranspose2dOptions(3, 2,
 *  3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose3d model(ConvTranspose3dOptions(2, 2,
 *  2).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../ConvTranspose1dFuncOptions.java


// Targeting ../ConvTranspose2dFuncOptions.java


// Targeting ../ConvTranspose3dFuncOptions.java



/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
 *  }</pre> */

 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/conv.h

// #pragma once

// #include <torch/nn/options/conv.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kValid arg0);

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kSame arg0);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @Const @ByRef Conv1dPadding padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv1dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv1dFuncOptions{}") Conv1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @Const @ByRef Conv2dPadding padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv2dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv2dFuncOptions{}") Conv2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @Const @ByRef Conv3dPadding padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv3dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv3dFuncOptions{}") Conv3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal LongArrayRef stride,
    @ByVal LongArrayRef padding,
    @ByVal LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose1dFuncOptions{}") ConvTranspose1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal LongArrayRef stride,
    @ByVal LongArrayRef padding,
    @ByVal LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose2dFuncOptions{}") ConvTranspose2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal LongArrayRef stride,
    @ByVal LongArrayRef padding,
    @ByVal LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose3dFuncOptions{}") ConvTranspose3dFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/distance.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../CosineSimilarityOptions.java


/** Options for {@code torch::nn::functional::cosine_similarity}.
 * 
 *  See the documentation for {@code torch::nn::CosineSimilarityOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_similarity(input1, input2,
 *  F::CosineSimilarityFuncOptions().dim(1));
 *  }</pre> */

// Targeting ../PairwiseDistanceOptions.java


/** Options for {@code torch::nn::functional::pairwise_distance}.
 * 
 *  See the documentation for {@code torch::nn::PairwiseDistanceOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/distance.h

// #pragma once

// #include <torch/nn/options/distance.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_similarity
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::CosineSimilarityFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_similarity(input1, input2,
/** F::CosineSimilarityFuncOptions().dim(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_similarity(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::CosineSimilarityFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineSimilarityFuncOptions{}") CosineSimilarityOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pairwise_distance
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::PairwiseDistanceFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pairwise_distance(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::PairwiseDistanceFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PairwiseDistanceFuncOptions{}") PairwiseDistanceOptions options);

// ============================================================================

/** Computes the p-norm distance between every pair of row vectors in the input.
 *  This function will be faster if the rows are contiguous. */

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/dropout.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../DropoutOptions.java



/** Options for the {@code Dropout2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout2d model(Dropout2dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code Dropout3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout3d model(Dropout3dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code AlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AlphaDropout model(AlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */

///

/** Options for the {@code FeatureAlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FeatureAlphaDropout model(FeatureAlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */
// Targeting ../DropoutFuncOptions.java



/** Options for {@code torch::nn::functional::dropout2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
 *  }</pre> */

///

/** Options for {@code torch::nn::functional::dropout3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
 *  }</pre> */

///
// Targeting ../AlphaDropoutFuncOptions.java


// Targeting ../FeatureAlphaDropoutFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/dropout.h

// #pragma once

// #include <torch/nn/options/dropout.h>

// #include <utility>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::DropoutFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout(input, F::DropoutFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::DropoutFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout2d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout2dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout2dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout3d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout3dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout3dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input,
    double p,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AlphaDropoutFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::alpha_dropout(input,
/** F::AlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::AlphaDropoutFuncOptions{}") AlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input,
    double p,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.feature_alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::FeatureAlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::feature_alpha_dropout(input,
/** F::FeatureAlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::FeatureAlphaDropoutFuncOptions{}") FeatureAlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/embedding.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../EmbeddingOptions.java


// Targeting ../EmbeddingFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingFuncOptions.java



 // namespace functional

// ============================================================================


///
// Targeting ../EmbeddingBagOptions.java


// Targeting ../EmbeddingBagFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingBagFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/embedding.h

// #pragma once

// #include <torch/nn/options/embedding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native void _no_grad_embedding_renorm_(
    @ByVal Tensor weight,
    @Const @ByRef Tensor input,
    float max_norm,
    float norm_type);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @ByVal LongOptional padding_idx,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @Cast("bool") boolean sparse);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding(input, weight,
/** F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingFuncOptions{}") EmbeddingFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor offsets,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @ByVal EmbeddingBagMode mode,
    @Cast("bool") boolean sparse,
    @Const @ByRef Tensor per_sample_weights,
    @Cast("bool") boolean include_last_offset,
    @ByVal LongOptional padding_idx);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding_bag
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingBagFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding_bag(input, weight,
/** F::EmbeddingBagFuncOptions().mode(torch::kSum).offsets(offsets));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingBagFuncOptions{}") EmbeddingBagFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/fold.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../FoldOptions.java


/** Options for {@code torch::nn::functional::fold}.
 * 
 *  See the documentation for {@code torch::nn::FoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
 *  }</pre> */

// Targeting ../UnfoldOptions.java


/** Options for {@code torch::nn::functional::unfold}.
 * 
 *  See the documentation for {@code torch::nn::UnfoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/fold.h

// #pragma once

// #include <torch/nn/options/fold.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fold(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer output_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.fold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FoldFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::FoldFuncOptions*") @ByRef FoldOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor unfold(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.unfold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::UnfoldFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::UnfoldFuncOptions*") @ByRef UnfoldOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/instancenorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>
// Targeting ../InstanceNormOptions.java



/** Options for the {@code InstanceNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm1d
 *  model(InstanceNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm2d
 *  model(InstanceNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm3d
 *  model(InstanceNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */
// Targeting ../InstanceNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/instancenorm.h

// #pragma once

// #include <torch/nn/options/instancenorm.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @Cast("bool") boolean use_input_stats,
    double momentum,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.instance_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InstanceNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::instance_norm(input,
/** F::InstanceNormFuncOptions().running_mean(mean).running_var(variance).weight(weight).bias(bias).momentum(0.1).eps(1e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::InstanceNormFuncOptions{}") InstanceNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/linear.h

// #pragma once

// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::Tensor()") Tensor bias);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor linear(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor bias);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/activation.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../ELUOptions.java


/** Options for {@code torch::nn::functional::elu}.
 * 
 *  See the documentation for {@code torch::nn::ELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SELUOptions.java


/** Options for {@code torch::nn::functional::selu}.
 * 
 *  See the documentation for {@code torch::nn::SELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::selu(input, F::SELUFuncOptions(false));
 *  }</pre> */

// Targeting ../GLUOptions.java


/** Options for {@code torch::nn::functional::glu}.
 * 
 *  See the documentation for {@code torch::nn::GLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::glu(input, GLUFuncOptions(1));
 *  }</pre> */

// Targeting ../GELUOptions.java


/** Options for {@code torch::nn::functional::gelu}.
 * 
 *  See the documentation for {@code torch::nn::GELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::gelu(input, F::GELUFuncOptions().approximate("none"));
 *  }</pre> */

// Targeting ../HardshrinkOptions.java


/** Options for {@code torch::nn::functional::hardshrink}.
 * 
 *  See the documentation for {@code torch::nn::HardshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
 *  }</pre> */

// Targeting ../HardtanhOptions.java


/** Options for {@code torch::nn::functional::hardtanh}.
 * 
 *  See the documentation for {@code torch::nn::HardtanhOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardtanh(x,
 *  F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
 *  }</pre> */

// Targeting ../LeakyReLUOptions.java


/** Options for {@code torch::nn::functional::leaky_relu}.
 * 
 *  See the documentation for {@code torch::nn::LeakyReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::leaky_relu(x,
 *  F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftmaxOptions.java



// ============================================================================
// Targeting ../SoftmaxFuncOptions.java




// Targeting ../SoftminOptions.java



// ============================================================================
// Targeting ../SoftminFuncOptions.java




// Targeting ../LogSoftmaxOptions.java



// ============================================================================
// Targeting ../LogSoftmaxFuncOptions.java




// Targeting ../PReLUOptions.java


// Targeting ../ReLUOptions.java


/** Options for {@code torch::nn::functional::relu}.
 * 
 *  See the documentation for {@code torch::nn::ReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu(x, F::ReLUFuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../ReLU6Options.java


/** Options for {@code torch::nn::functional::relu6}.
 * 
 *  See the documentation for {@code torch::nn::ReLU6Options} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu6(x, F::ReLU6FuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../RReLUOptions.java



// ============================================================================
// Targeting ../RReLUFuncOptions.java




// Targeting ../CELUOptions.java


/** Options for {@code torch::nn::functional::celu}.
 * 
 *  See the documentation for {@code torch::nn::CELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftplusOptions.java


/** Options for {@code torch::nn::functional::softplus}.
 * 
 *  See the documentation for {@code torch::nn::SoftplusOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
 *  }</pre> */

// Targeting ../SoftshrinkOptions.java


/** Options for {@code torch::nn::functional::softshrink}.
 * 
 *  See the documentation for {@code torch::nn::SoftshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
 *  }</pre> */

// Targeting ../ThresholdOptions.java


/** Options for {@code torch::nn::functional::threshold}.
 * 
 *  See the documentation for {@code torch::nn::ThresholdOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
 *  }</pre> */
 // namespace functional

// ============================================================================
// Targeting ../GumbelSoftmaxFuncOptions.java




// Targeting ../MultiheadAttentionOptions.java



// ============================================================================
// Targeting ../MultiheadAttentionForwardFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/linear.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../LinearOptions.java


// Targeting ../FlattenOptions.java


// Targeting ../UnflattenOptions.java


// Targeting ../BilinearOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/activation.h

// #pragma once

// #include <ATen/Dispatch.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/options/activation.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/linear.h>
// #include <torch/types.h>
// #include <limits>
// #include <utility>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor elu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.elu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor elu(@ByVal Tensor input, @Cast("const torch::nn::functional::ELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ELUFuncOptions{}") ELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.selu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::selu(input, F::SELUFuncOptions(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("const torch::nn::functional::SELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SELUFuncOptions{}") SELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input, double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardshrinkFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardshrink(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::HardshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardshrinkFuncOptions{}") HardshrinkOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardtanh(
    @ByVal Tensor input,
    double min_val,
    double max_val,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardtanhFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardtanh(x,
/** F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardtanh(@ByVal Tensor input, @Cast("const torch::nn::functional::HardtanhFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardtanhFuncOptions{}") HardtanhOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input, double negative_slope, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.leaky_relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LeakyReLUFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::leaky_relu(x,
/** F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor leaky_relu(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::LeakyReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::LeakyReLUFuncOptions{}") LeakyReLUOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor logsigmoid(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits,
    double tau,
    @Cast("bool") boolean hard,
    int dim);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.gumbel_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GumbelSoftmaxFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::gumbel_softmax(logits, F::GumbelSoftmaxFuncOptions().hard(true).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits,
    @Const @ByRef(nullValue = "torch::nn::functional::GumbelSoftmaxFuncOptions{}") GumbelSoftmaxFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftmaxFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmax(input, F::SoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmax(@Const @ByRef Tensor input, @Const @ByRef SoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softmin(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long dim,
    @ByVal ScalarTypeOptional dtype);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmin
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftminFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmin(input, F::SoftminFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Const @ByRef SoftminFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.log_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LogSoftmaxFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::log_softmax(input, LogSoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor log_softmax(
    @Const @ByRef Tensor input,
    @Const @ByRef LogSoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::glu(input, GLUFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor glu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GLUFuncOptions{}") GLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal Tensor gelu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GELUFuncOptions{}") GELUOptions options);

// ============================================================================

// ============================================================================

// ============================================================================

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu(x, F::ReLUFuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLUFuncOptions{}") ReLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu6
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLU6FuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu6(x, F::ReLU6FuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLU6FuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLU6FuncOptions{}") ReLU6Options options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor rrelu(
    @ByVal Tensor input,
    double lower,
    double upper,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.rrelu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::RReLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::rrelu(x, F::RReLUFuncOptions().lower(0.1).upper(0.4).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor rrelu(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::RReLUFuncOptions{}") RReLUFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor celu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.celu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor celu(@ByVal Tensor input, @Cast("const torch::nn::functional::CELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CELUFuncOptions{}") CELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input, double beta, double threshold);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softplus
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftplusFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softplus(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::SoftplusFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftplusFuncOptions{}") SoftplusOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input, double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftshrinkFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softshrink(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::SoftshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftshrinkFuncOptions{}") SoftshrinkOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor softsign(@Const @ByRef Tensor input);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor tanhshrink(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor threshold(
    @ByVal Tensor input,
    double threshold,
    double value,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.threshold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ThresholdFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor threshold(@ByVal Tensor input, @Cast("const torch::nn::functional::ThresholdFuncOptions*") @ByRef ThresholdOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Cast("int64_t") long embed_dim_to_check,
    @Cast("int64_t") long num_heads,
    @Const @ByRef Tensor in_proj_weight,
    @Const @ByRef Tensor in_proj_bias,
    @Const @ByRef Tensor bias_k,
    @Const @ByRef Tensor bias_v,
    @Cast("bool") boolean add_zero_attn,
    double dropout_p,
    @Const @ByRef Tensor out_proj_weight,
    @Const @ByRef Tensor out_proj_bias,
    @Cast("bool") boolean training/*=true*/,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor key_padding_mask,
    @Cast("bool") boolean need_weights/*=true*/,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor attn_mask,
    @Cast("bool") boolean use_separate_proj_weight/*=false*/,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor q_proj_weight,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor k_proj_weight,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor v_proj_weight,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor static_k,
    @Const @ByRef(nullValue = "torch::Tensor{}") Tensor static_v,
    @Cast("bool") boolean average_attn_weights/*=true*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Cast("int64_t") long embed_dim_to_check,
    @Cast("int64_t") long num_heads,
    @Const @ByRef Tensor in_proj_weight,
    @Const @ByRef Tensor in_proj_bias,
    @Const @ByRef Tensor bias_k,
    @Const @ByRef Tensor bias_v,
    @Cast("bool") boolean add_zero_attn,
    double dropout_p,
    @Const @ByRef Tensor out_proj_weight,
    @Const @ByRef Tensor out_proj_bias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Const @ByRef MultiheadAttentionForwardFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/loss.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../L1LossOptions.java


/** Options for {@code torch::nn::functional::l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::L1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../KLDivLossOptions.java


/** Options for {@code torch::nn::functional::kl_div}.
 * 
 *  See the documentation for {@code torch::nn::KLDivLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::kl_div(input, target,
 *  F::KLDivFuncOptions().reduction(torch::kNone).log_target(false));
 *  }</pre> */

// Targeting ../MSELossOptions.java


/** Options for {@code torch::nn::functional::mse_loss}.
 * 
 *  See the documentation for {@code torch::nn::MSELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../BCELossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::BCELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy(input, target,
 *  F::BinaryCrossEntropyFuncOptions().weight(weight));
 *  }</pre> */

// Targeting ../HingeEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::hinge_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::HingeEmbeddingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hinge_embedding_loss(input, target,
 *  F::HingeEmbeddingLossFuncOptions().margin(2));
 *  }</pre> */

// Targeting ../MultiMarginLossOptions.java


/** Options for {@code torch::nn::functional::multi_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiMarginLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multi_margin_loss(input, target,
 *  F::MultiMarginLossFuncOptions().margin(2).weight(weight));
 *  }</pre> */

// Targeting ../CosineEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::cosine_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::CosineEmbeddingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_embedding_loss(input1, input2, target,
 *  F::CosineEmbeddingLossFuncOptions().margin(0.5));
 *  }</pre> */

// Targeting ../MultiLabelMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelMarginLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_margin_loss(input, target,
 *  F::MultilabelMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../SoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::SoftMarginLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::soft_margin_loss(input, target,
 *  F::SoftMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../MultiLabelSoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelSoftMarginLossOptions} class
 *  to learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_soft_margin_loss(input, target,
 *  F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
 *  }</pre> */

// Targeting ../TripletMarginLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_loss(anchor, positive, negative,
 *  F::TripletMarginLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../TripletMarginWithDistanceLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_with_distance_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginWithDistanceLossOptions}
 *  class to learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_with_distance_loss(anchor, positive, negative,
 *  F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../CTCLossOptions.java


/** Options for {@code torch::nn::functional::ctc_loss}.
 * 
 *  See the documentation for {@code torch::nn::CTCLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::ctc_loss(log_probs, targets, input_lengths, target_lengths,
 *  F::CTCLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../SmoothL1LossOptions.java


/** Options for {@code torch::nn::functional::smooth_l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::SmoothL1LossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../HuberLossOptions.java


/** Options for {@code torch::nn::functional::huber_loss}.
 * 
 *  See the documentation for {@code torch::nn::HuberLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::huber_loss(input, target, F::HuberLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../PoissonNLLLossOptions.java


/** Options for {@code torch::nn::functional::poisson_nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::PoissonNLLLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::poisson_nll_loss(input, target,
 *  F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../MarginRankingLossOptions.java


/** Options for {@code torch::nn::functional::margin_ranking_loss}.
 * 
 *  See the documentation for {@code torch::nn::MarginRankingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::margin_ranking_loss(input1, input2, target,
 *  F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
 *  }</pre> */

// Targeting ../NLLLossOptions.java


/** Options for {@code torch::nn::functional::nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::NLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::nll_loss(input, target,
 *  F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../CrossEntropyLossOptions.java


/** Options for {@code torch::nn::functional::cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::CrossEntropyLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cross_entropy(input, target,
 *  F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../BCEWithLogitsLossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy_with_logits}.
 * 
 *  See the documentation for {@code torch::nn::BCEWithLogitsLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy_with_logits(input, target,
 *  F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/loss.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/loss.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::L1LossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::L1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::L1LossFuncOptions{}") L1LossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal KLDivLossReduction reduction,
    @Cast("bool") boolean log_target/*=false*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal KLDivLossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::KLDivFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::kl_div(input, target,
/** F::KLDivFuncOptions.reduction(torch::kNone).log_target(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::KLDivFuncOptions*") @ByRef(nullValue = "torch::nn::functional::KLDivFuncOptions{}") KLDivLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.mse_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MSELossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MSELossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MSELossFuncOptions{}") MSELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::BinaryCrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy(input, target,
/** F::BinaryCrossEntropyFuncOptions().weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyFuncOptions{}") BCELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hinge_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::HingeEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hinge_embedding_loss(input, target,
/** F::HingeEmbeddingLossFuncOptions().margin(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HingeEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HingeEmbeddingLossFuncOptions{}") HingeEmbeddingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("int64_t") long p,
    double margin,
    @Const @ByRef Tensor weight,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multi_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultiMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multi_margin_loss(input, target,
/** F::MultiMarginLossFuncOptions().margin(2).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultiMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultiMarginLossFuncOptions{}") MultiMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::CosineEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_embedding_loss(input1, input2, target,
/** F::CosineEmbeddingLossFuncOptions().margin(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CosineEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineEmbeddingLossFuncOptions{}") CosineEmbeddingLossOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double beta/*=1.*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction,
    double beta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.smooth_l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SmoothL1LossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SmoothL1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SmoothL1LossFuncOptions{}") SmoothL1LossOptions options,
    double beta/*=1.*/);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction,
    double delta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.huber_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HuberLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::huber_loss(input, target,
/** F::HuberLossFuncOptions().reduction(torch::kNone).delta(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HuberLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HuberLossFuncOptions{}") HuberLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultilabelMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_margin_loss(input, target,
/** F::MultilabelMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelMarginLossFuncOptions{}") MultiLabelMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftMarginLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::soft_margin_loss(input, target,
/** F::SoftMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftMarginLossFuncOptions{}") SoftMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultilabelSoftMarginLossFuncOptions} class to learn
/** what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_soft_margin_loss(input, target,
/** F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelSoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelSoftMarginLossFuncOptions{}") MultiLabelSoftMarginLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    double margin,
    double p,
    double eps,
    @Cast("bool") boolean swap,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::TripletMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_loss(anchor, positive, negative,
/** F::TripletMarginLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginLossFuncOptions{}") TripletMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @ByVal @Cast("c10::optional<torch::nn::functional::TripletMarginWithDistanceLossFuncOptions::distance_function_t>*") Pointer distance_function,
    double margin,
    @Cast("bool") boolean swap,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_with_distance_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::TripletMarginWithDistanceLossFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_with_distance_loss(anchor, positive, negative,
/** F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginWithDistanceLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginWithDistanceLossFuncOptions{}") TripletMarginWithDistanceLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor ctc_loss(
    @Const @ByRef Tensor log_probs,
    @Const @ByRef Tensor targets,
    @Const @ByRef Tensor input_lengths,
    @Const @ByRef Tensor target_lengths,
    @Cast("int64_t") long blank,
    @ByVal LossReduction reduction,
    @Cast("bool") boolean zero_infinity);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.ctc_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CTCLossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::ctc_loss(log_probs, targets, input_lengths, target_lengths,
/** F::CTCLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor ctc_loss(
    @Const @ByRef Tensor log_probs,
    @Const @ByRef Tensor targets,
    @Const @ByRef Tensor input_lengths,
    @Const @ByRef Tensor target_lengths,
    @Cast("const torch::nn::functional::CTCLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CTCLossFuncOptions{}") CTCLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("bool") boolean log_input,
    @Cast("bool") boolean full,
    double eps,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.poisson_nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PoissonNLLLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::poisson_nll_loss(input, target,
/** F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::PoissonNLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PoissonNLLLossFuncOptions{}") PoissonNLLLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor margin_ranking_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.margin_ranking_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MarginRankingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::margin_ranking_loss(input1, input2, target,
/** F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor margin_ranking_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MarginRankingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MarginRankingLossFuncOptions{}") MarginRankingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @Const @ByVal LossReduction reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NLLLossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::nll_loss(input, target,
/** F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::NLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::NLLLossFuncOptions{}") NLLLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @ByVal LossReduction reduction,
    double label_smoothing);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CrossEntropyFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cross_entropy(input, target,
/** F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CrossEntropyFuncOptions{}") CrossEntropyLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy_with_logits(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal LossReduction reduction,
    @Const @ByRef Tensor pos_weight);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy_with_logits
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy_with_logits(input, target,
/** F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy_with_logits(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions{}") BCEWithLogitsLossOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from ATen/PadNd.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <c10/util/string_view.h>

@Namespace("at") public enum padding_mode {
  reflect(0),
  replicate(1),
  circular(2),
  constant(3);

    public final int value;
    private padding_mode(int v) { this.value = v; }
    private padding_mode(padding_mode e) { this.value = e.value; }
    public padding_mode intern() { for (padding_mode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public static native @StringView BytePointer padding_mode_string(padding_mode m);
@Namespace("at") public static native @StringView String padding_mode_string(@Cast("at::padding_mode") int m);

 // namespace at


// Parsed from torch/csrc/api/include/torch/nn/options/padding.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../ReflectionPad1dOptions.java


// Targeting ../ReflectionPad2dOptions.java


// Targeting ../ReflectionPad3dOptions.java



/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad1d model(ReflectionPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad2d model(ReflectionPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad3d model(ReflectionPad3dOptions({1, 1, 2, 0, 1, 1}));
 *  }</pre> */
// Targeting ../ReplicationPad1dOptions.java


// Targeting ../ReplicationPad2dOptions.java


// Targeting ../ReplicationPad3dOptions.java



/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad1d model(ReplicationPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad2d model(ReplicationPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad3d model(ReplicationPad3dOptions({1, 2, 1, 2, 1, 2}));
 *  }</pre> */
// Targeting ../ZeroPad1dOptions.java


// Targeting ../ZeroPad2dOptions.java


// Targeting ../ZeroPad3dOptions.java



/** {@code ZeroPadOptions} specialized for the {@code ZeroPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad1d model(ConstantPad1dOptions({3, 1});
 *  }</pre> */

///

/** {@code ZeroPadOptions} specialized for the {@code ZeroPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad2d model(ConstantPad2dOptions({1, 1, 2, 0});
 *  }</pre> */

///

/** {@code ZeroPadOptions} specialized for the {@code ZeroPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad3d model(ConstantPad3dOptions({1, 2, 1, 2, 1, 2});
 *  }</pre> */
// Targeting ../ConstantPad1dOptions.java


// Targeting ../ConstantPad2dOptions.java


// Targeting ../ConstantPad3dOptions.java



/** {@code ConstantPadOptions} specialized for the {@code ConstantPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad1d model(ConstantPad1dOptions({3, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad2d model(ConstantPad2dOptions({3, 0, 2, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad3d model(ConstantPad3dOptions({1, 2, 1, 2, 1, 2}, 3.5));
 *  }</pre> */

// ============================================================================
// Targeting ../PadFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/padding.h

// #pragma once

// #include <ATen/PadNd.h>
// #include <torch/nn/options/padding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(
    @Const @ByRef Tensor input,
    @ByVal LongArrayRef pad,
    @ByVal PaddingMode mode,
    double value);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(
    @Const @ByRef Tensor input,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] pad,
    @ByVal PaddingMode mode,
    double value);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pad
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PadFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pad(input, F::PadFuncOptions({1, 2, 2, 1, 1,
/** 2}).mode(torch::kReplicate));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pad(@Const @ByRef Tensor input, @Const @ByRef PadFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/utils.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #include <vector>

// Reverse the order of `t` and repeat each element for `n` times.
// This can be used to translate padding arg used by Conv and Pooling modules
// to the ones used by `F::pad`.
//
// This mirrors `_reverse_repeat_tuple` in `torch/nn/modules/utils.py`.
@Namespace("torch::nn::modules::utils") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _reverse_repeat_vector(
    @ByVal LongArrayRef t,
    @Cast("int64_t") long n);
@Namespace("torch::nn::modules::utils") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _reverse_repeat_vector(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] t,
    @Cast("int64_t") long n);

@Namespace("torch::nn::modules::utils") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _list_with_default(
    @ByVal LongOptionalArrayRef out_size,
    @ByVal LongArrayRef defaults);
@Namespace("torch::nn::modules::utils") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _list_with_default(
    @ByVal LongOptionalVector out_size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... defaults);

 // namespace utils
 // namespace modules
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/pooling.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../AvgPool1dOptions.java


// Targeting ../AvgPool2dOptions.java


// Targeting ../AvgPool3dOptions.java



/** {@code AvgPoolOptions} specialized for the {@code AvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool1d model(AvgPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool2d model(AvgPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool3d model(AvgPool3dOptions(5).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../MaxPool1dOptions.java


// Targeting ../MaxPool2dOptions.java


// Targeting ../MaxPool3dOptions.java



/** {@code MaxPoolOptions} specialized for the {@code MaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool1d model(MaxPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool2d model(MaxPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool3d model(MaxPool3dOptions(3).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::max_pool1d} and
 *  {@code torch::nn::functional::max_pool1d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool2d} and
 *  {@code torch::nn::functional::max_pool2d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool3d} and
 *  {@code torch::nn::functional::max_pool3d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../AdaptiveMaxPool1dOptions.java


// Targeting ../AdaptiveMaxPool2dOptions.java


// Targeting ../AdaptiveMaxPool3dOptions.java



/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool1d model(AdaptiveMaxPool1dOptions(3));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool2d model(AdaptiveMaxPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool3d model(AdaptiveMaxPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_max_pool1d} and
 *  {@code torch::nn::functional::adaptive_max_pool1d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool2d} and
 *  {@code torch::nn::functional::adaptive_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool3d} and
 *  {@code torch::nn::functional::adaptive_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../AdaptiveAvgPool1dOptions.java


// Targeting ../AdaptiveAvgPool2dOptions.java


// Targeting ../AdaptiveAvgPool3dOptions.java



/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool1d model(AdaptiveAvgPool1dOptions(5));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool2d model(AdaptiveAvgPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool3d model(AdaptiveAvgPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool1dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool2dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool3dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../MaxUnpool1dOptions.java


// Targeting ../MaxUnpool2dOptions.java


// Targeting ../MaxUnpool3dOptions.java



/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool1d model(MaxUnpool1dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool2d model(MaxUnpool2dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool3d model(MaxUnpool3dOptions(3).stride(2).padding(1));
 *  }</pre> */

// ============================================================================
// Targeting ../MaxUnpool1dFuncOptions.java


// Targeting ../MaxUnpool2dFuncOptions.java


// Targeting ../MaxUnpool3dFuncOptions.java



/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool1d(x, indices,
 *  F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool2d(x, indices,
 *  F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
 *  }</pre> */


// Targeting ../FractionalMaxPool1dOptions.java


// Targeting ../FractionalMaxPool2dOptions.java


// Targeting ../FractionalMaxPool3dOptions.java



/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool2d model(FractionalMaxPool2dOptions(5).output_size(1));
 *  }</pre> */

///

/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool3d model(FractionalMaxPool3dOptions(5).output_size(1));
 *  }</pre> */
/** Options for {@code torch::nn::functional::fractional_max_pool2d} and
 *  {@code torch::nn::functional::fractional_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool2d(x,
 *  F::FractionalMaxPool2dFuncOptions(3).output_size(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::fractional_max_pool3d} and
 *  {@code torch::nn::functional::fractional_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool3d(x,
 *  F::FractionalMaxPool3dFuncOptions(3).output_size(2));
 *  }</pre> */

// Targeting ../LPPool1dOptions.java


// Targeting ../LPPool2dOptions.java


// Targeting ../LPPool3dOptions.java



/** {@code LPPoolOptions} specialized for the {@code LPPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool1d model(LPPool1dOptions(1, 2).stride(5).ceil_mode(true));
 *  }</pre> */

///

/** {@code LPPoolOptions} specialized for the {@code LPPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool2d model(LPPool2dOptions(1, std::vector<int64_t>({3, 4})).stride({5,
 *  6}).ceil_mode(true));
 *  }</pre> */
/** Options for {@code torch::nn::functional::lp_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::lp_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/pooling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/pooling.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad,
    @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad,
    @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d_with_indices(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d_with_indices(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d_with_indices(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail

/** See the documentation for
 *  {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
 *  optional arguments are supported for this functional.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d_with_indices(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d_with_indices(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d_with_indices(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool3dOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(
    @Const @ByRef Tensor input,
    @Const @ByRef LongArrayRef kernel_size,
    @Const @ByRef LongArrayRef stride,
    @Const @ByRef LongArrayRef padding,
    @Const @ByRef LongVectorOptional output_size);
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(
    @Const @ByRef Tensor input,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] kernel_size,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] stride,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] padding,
    @Const @ByRef LongVectorOptional output_size);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool1d(x, indices,
/** F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool2d(x, indices,
/** F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<2>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<2> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<2,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d_with_indices(x,
/** F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<2> >*") LongExpandingArrayOptional output_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<2,double> >*") DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d(x,
/** F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<3>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<3> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<3,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d_with_indices(x,
/** F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal T_TensorTensor_T fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<3> >*") LongExpandingArrayOptional output_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<3,double> >*") DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d(x,
/** F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool1d(
    @Const @ByRef Tensor input,
    double norm_type,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool1dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef LPPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool2d(
    @Const @ByRef Tensor input,
    double norm_type,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool2dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef LPPool2dOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/normalization.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// #include <vector>
// Targeting ../LayerNormOptions.java



// ============================================================================
// Targeting ../LayerNormFuncOptions.java




// Targeting ../LocalResponseNormOptions.java


/** Options for {@code torch::nn::functional::local_response_norm}.
 * 
 *  See the documentation for {@code torch::nn::LocalResponseNormOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
 *  }</pre> */

// Targeting ../CrossMapLRN2dOptions.java



// ============================================================================
// Targeting ../NormalizeFuncOptions.java




// Targeting ../GroupNormOptions.java



// ============================================================================
// Targeting ../GroupNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/normalization.h

// #pragma once

// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    double p,
    @Cast("int64_t") long dim,
    double eps,
    @ByVal TensorOptional out);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.normalize
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NormalizeFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::normalize(input, F::NormalizeFuncOptions().p(1).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    @ByVal(nullValue = "torch::nn::functional::NormalizeFuncOptions{}") NormalizeFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor layer_norm(
    @Const @ByRef Tensor input,
    @Cast("const std::vector<int64_t>*") @ByRef LongVector normalized_shape,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.layer_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LayerNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::layer_norm(input, F::LayerNormFuncOptions({2, 2}).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor layer_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef LayerNormFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long size,
    double alpha,
    double beta,
    double k);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.local_response_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::LocalResponseNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::LocalResponseNormFuncOptions*") @ByRef LocalResponseNormOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long num_groups,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.group_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GroupNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::group_norm(input, F::GroupNormFuncOptions(2).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef GroupNormFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/pixelshuffle.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../PixelShuffleOptions.java


// Targeting ../PixelUnshuffleOptions.java


/** Options for {@code torch::nn::functional::pixel_shuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelShuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
 *  }</pre> */

///
///

/** Options for {@code torch::nn::functional::pixel_unshuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelUnshuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_unshuffle(x, F::PixelUnshuffleFuncOptions(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/pixelshuffle.h

// #pragma once

// #include <torch/nn/options/pixelshuffle.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pixel_shuffle
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PixelShuffleFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_shuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelShuffleFuncOptions*") @ByRef PixelShuffleOptions options);

@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_unshuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelUnshuffleFuncOptions*") @ByRef PixelUnshuffleOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/upsampling.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>

// #include <vector>
// Targeting ../UpsampleOptions.java


// Targeting ../InterpolateFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/upsampling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/upsampling.h>

// #include <cmath>
// #include <utility>

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _interp_output_size(
    @Cast("int64_t") long dim,
    @ByVal @Cast("std::tuple<torch::Tensor,c10::optional<std::vector<int64_t> >,c10::optional<std::vector<double> >,c10::optional<bool> >*") Pointer closed_over_args);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input,
    @Const @ByRef LongVectorOptional size,
    @Const @ByRef DoubleVectorOptional scale_factor,
    @ByVal InterpolateMode mode,
    @ByVal BoolOptional align_corners,
    @ByVal BoolOptional recompute_scale_factor,
    @Cast("bool") boolean antialias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.interpolate
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InterpolateFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::interpolate(input,
/** F::InterpolateFuncOptions().size({4}).mode(torch::kNearest));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::InterpolateFuncOptions{}") InterpolateFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/vision.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../GridSampleFuncOptions.java



 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional/vision.h

// #pragma once

// #include <torch/nn/options/vision.h>
// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @Const @ByRef LongArrayRef size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @Const @ByRef LongArrayRef size);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long[] size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector("int64_t") long... size);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @ByVal GridSampleMode mode,
    @ByVal GridSamplePaddingMode padding_mode,
    @ByVal BoolOptional align_corners);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.grid_sample
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GridSampleFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::grid_sample(input, grid,
/** F::GridSampleFuncOptions().mode(torch::kBilinear).padding_mode(torch::kZeros).align_corners(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @Const @ByRef(nullValue = "torch::nn::functional::GridSampleFuncOptions{}") GridSampleFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/functional.h

// #pragma once

// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/functional/conv.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/functional/instancenorm.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/functional/vision.h>


// Parsed from torch/csrc/api/include/torch/nn/init.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

 // namespace init
 // namespace nn

/** Return the recommended gain value for the given nonlinearity function. */
@Namespace("torch::nn::init") public static native double calculate_gain(
    @ByVal Nonlinearity nonlinearity,
    double param/*=0.01*/);
@Namespace("torch::nn::init") public static native double calculate_gain(
    @ByVal Nonlinearity nonlinearity);

/** Fills the given {@code tensor} with the provided {@code value} in-place, and returns it.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor constant_(@ByVal Tensor tensor, @ByVal Scalar value);

/** Fills the given {@code tensor} with the Dirac delta function in-place, and returns
 *  it. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor dirac_(@ByVal Tensor tensor);

/** Fills the given 2-dimensional {@code matrix} with an identity matrix.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor eye_(@ByVal Tensor matrix);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a normal
 *  distribution parameterized by {@code mean} and {@code std}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor, double mean/*=0*/, double std/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with ones.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor ones_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with a (semi) orthogonal matrix, as described in
 *  "Exact solutions to the nonlinear dynamics of learning in deep linear neural
 *  networks" - Saxe, A. et al. (2013). The input tensor must have at least 2
 *  dimensions, and for tensors with more than 2 dimensions the trailing
 *  dimensions are flattened.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor);

/** Fills the 2D input {@code Tensor} as a sparse matrix, where the
 *  non-zero elements will be drawn from a centered normal distribution
 *  with the given standard deviation {@code std}, as described in "Deep learning via
 *  Hessian-free optimization" - Martens, J. (2010). The {@code sparsity} is a real
 *  value between 0 and 1 that controls the fraction of elements in each column
 *  to be set to zero.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity, double std/*=0.01*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a uniform
 *  distribution parameterized by {@code low} and {@code high}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor, double low/*=0*/, double high/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  normal distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") Nonlinearity nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  uniform distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") Nonlinearity nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010). Values are scaled by the
 *  {@code gain} parameter. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010), using a uniform
 *  distribution. Values are scaled by the {@code gain} parameter
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with zeros.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor zeros_(@ByVal Tensor tensor);

@Namespace("torch::nn::init") public static native @ByVal T_LongLong_T _calculate_fan_in_and_fan_out(
    @Const @ByRef Tensor tensor);

 // namespace init
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/common.h


///
///
///
///
///
// #pragma once

/** This macro enables a module with default arguments in its forward method
 *  to be used in a Sequential module.
 * 
 *  Example usage:
 * 
 *  Let's say we have a module declared like this:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 * 
 *  If we try to use it in a Sequential module and run forward:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);
 *  }</pre>
 * 
 *  We will receive the following error message:
 *  <pre>{@code
 *  MImpl's forward() method expects 3 argument(s), but received 1.
 *  If MImpl's forward() method has default arguments, please make sure
 *  the forward() method is declared with a corresponding
 *  `FORWARD_HAS_DEFAULT_ARGS` macro.
 *  }</pre>
 * 
 *  The right way to fix this error is to use the {@code FORWARD_HAS_DEFAULT_ARGS}
 *  macro when declaring the module:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   protected:
 *    /*
 *    NOTE: looking at the argument list of `forward`:
 *    `forward(int a, int b = 2, double c = 3.0)`
 *    we saw the following default arguments:
 *    ----------------------------------------------------------------
 *    0-based index of default |         Default value of arg
 *    arg in forward arg list  |  (wrapped by `torch::nn::AnyValue()`)
 *    ----------------------------------------------------------------
 *                1            |       torch::nn::AnyValue(2)
 *                2            |       torch::nn::AnyValue(3.0)
 *    ----------------------------------------------------------------
 *    Thus we pass the following arguments to the `FORWARD_HAS_DEFAULT_ARGS`
 *    macro:
 *    * /
 *    FORWARD_HAS_DEFAULT_ARGS({1, torch::nn::AnyValue(2)}, {2,
 *    torch::nn::AnyValue(3.0)})
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 *  Now, running the following would work:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);  // This correctly populates the default arguments for
 *  `MImpl::forward`
 *  }</pre> */
// #define FORWARD_HAS_DEFAULT_ARGS(...)
//   template <typename ModuleType, typename... ArgumentTypes>
//   friend struct torch::nn::AnyModuleHolder;
//   bool _forward_has_default_args() override {
//     return true;
//   }
//   unsigned int _forward_num_required_args() override {
//     std::pair<unsigned int, torch::nn::AnyValue> args_info[] = {__VA_ARGS__};
//     return args_info[0].first;
//   }
//   std::vector<torch::nn::AnyValue> _forward_populate_default_args(
//       std::vector<torch::nn::AnyValue>&& arguments) override {
//     std::pair<unsigned int, torch::nn::AnyValue> args_info[] = {__VA_ARGS__};
//     unsigned int num_all_args = std::rbegin(args_info)->first + 1;
//     TORCH_INTERNAL_ASSERT(
//         arguments.size() >= _forward_num_required_args() &&
//         arguments.size() <= num_all_args);
//     std::vector<torch::nn::AnyValue> ret = std::move(arguments);
//     ret.reserve(num_all_args);
//     for (auto& arg_info : args_info) {
//       if (arg_info.first > ret.size() - 1)
//         ret.emplace_back(std::move(arg_info.second));
//     }
//     return ret;
//   }


// Parsed from torch/csrc/api/include/torch/nn/modules/container/any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../AnyModule.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModule ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

































// Private Methods







 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/moduledict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ModuleDictImpl.java



/** A {@code ModuleHolder} subclass for {@code ModuleDictImpl}.
 *  See the documentation for {@code ModuleDictImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/modulelist.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <utility>
// #include <vector>
// Targeting ../ModuleListImpl.java



/** A {@code ModuleHolder} subclass for {@code ModuleListImpl}.
 *  See the documentation for {@code ModuleListImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/named_any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <initializer_list>
// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>

/** Stores a type erased {@code Module} with name.
 * 
 *  The {@code NamedAnyModule} class enables the following API for constructing
 *  {@code nn::Sequential} with named submodules:
 *  \rst
 *  .. code-block:: cpp
 * 
 *    struct M : torch::nn::Module {
 *      explicit M(int value_) : value(value_) {}
 *      int value;
 *      int forward() {
 *        return value;
 *      }
 *    };
 * 
 *    Sequential sequential({
 *      {"m1", std::make_shared<M>(1)},  // shared pointer to {@code Module} is
 *      supported {std::string("m2"), M(2)},  // {@code Module} is supported
 *      {"linear1", Linear(10, 3)}  // {@code ModuleHolder} is supported
 *    });
 *  \endrst */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/parameterdict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <utility>
// #include <vector>
// Targeting ../ParameterDictImpl.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/parameterlist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ParameterListImpl.java


 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/container/sequential.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <ostream>
// #include <string>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../SequentialImpl.java



/** A {@code ModuleHolder} subclass for {@code SequentialImpl}.
 *  See the documentation for {@code SequentialImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/linear.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../IdentityImpl.java



/** A {@code ModuleHolder} subclass for {@code IdentityImpl}.
 *  See the documentation for {@code IdentityImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../LinearImpl.java



/** A {@code ModuleHolder} subclass for {@code LinearImpl}.
 *  See the documentation for {@code LinearImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Linear} with
 *  {@code torch::nn::LinearOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../FlattenImpl.java



/** A {@code ModuleHolder} subclass for {@code FlattenImpl}.
 *  See the documentation for {@code FlattenImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Flatten} with
 *  {@code torch::nn::FlattenOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../UnflattenImpl.java



/** A {@code ModuleHolder} subclass for {@code UnflattenImpl}.
 *  See the documentation for {@code UnflattenImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Unflatten} with
 *  {@code torch::nn::UnflattenOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../BilinearImpl.java



/** A {@code ModuleHolder} subclass for {@code BilinearImpl}.
 *  See the documentation for {@code BilinearImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Bilinear} with
 *  {@code torch::nn::BilinearOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/activation.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/options/activation.h>

// #include <torch/csrc/Export.h>
// Targeting ../ELUImpl.java



/** A {@code ModuleHolder} subclass for {@code ELUImpl}.
 *  See the documentation for {@code ELUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ELU} with {@code torch::nn::ELUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../SELUImpl.java



/** A {@code ModuleHolder} subclass for {@code SELUImpl}.
 *  See the documentation for {@code SELUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code SELU} with {@code torch::nn::SELUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../HardshrinkImpl.java



/** A {@code ModuleHolder} subclass for {@code HardshrinkImpl}.
 *  See the documentation for {@code HardshrinkImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Hardshrink} with
 *  {@code torch::nn::HardshrinkOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../HardtanhImpl.java



/** A {@code ModuleHolder} subclass for {@code HardtanhImpl}.
 *  See the documentation for {@code HardtanhImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Hardtanh} with
 *  {@code torch::nn::HardtanhOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LeakyReLUImpl.java



/** A {@code ModuleHolder} subclass for {@code LeakyReLUImpl}.
 *  See the documentation for {@code LeakyReLUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LeakyReLU} with
 *  {@code torch::nn::LeakyReLUOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LogSigmoidImpl.java



/** A {@code ModuleHolder} subclass for {@code LogSigmoidImpl}.
 *  See the documentation for {@code LogSigmoidImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../SoftmaxImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftmaxImpl}.
 *  See the documentation for {@code SoftmaxImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Softmax} with
 *  {@code torch::nn::SoftmaxOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../SoftminImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftminImpl}.
 *  See the documentation for {@code SoftminImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Softmin} with
 *  {@code torch::nn::SoftminOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LogSoftmaxImpl.java



/** A {@code ModuleHolder} subclass for {@code LogSoftmaxImpl}.
 *  See the documentation for {@code LogSoftmaxImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LogSoftmax} with
 *  {@code torch::nn::LogSoftmaxOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../Softmax2dImpl.java



/** A {@code ModuleHolder} subclass for {@code Softmax2dImpl}.
 *  See the documentation for {@code Softmax2dImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../PReLUImpl.java



/** A {@code ModuleHolder} subclass for {@code PReLUImpl}.
 *  See the documentation for {@code PReLUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code PReLU} with {@code torch::nn::PReLUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../ReLUImpl.java



/** A {@code ModuleHolder} subclass for {@code ReLUImpl}.
 *  See the documentation for {@code ReLUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ReLU} with {@code torch::nn::ReLUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../ReLU6Impl.java



/** A {@code ModuleHolder} subclass for {@code ReLU6Impl}.
 *  See the documentation for {@code ReLU6Impl} class to learn what methods it
 *  provides, and examples of how to use {@code ReLU6} with {@code torch::nn::ReLU6Options}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../RReLUImpl.java



/** A {@code ModuleHolder} subclass for {@code RReLUImpl}.
 *  See the documentation for {@code RReLUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code RReLU} with {@code torch::nn::RReLUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../CELUImpl.java



/** A {@code ModuleHolder} subclass for {@code CELUImpl}.
 *  See the documentation for {@code CELUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code CELU} with {@code torch::nn::CELUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../GLUImpl.java



/** A {@code ModuleHolder} subclass for {@code GLUImpl}.
 *  See the documentation for {@code GLUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code GLU} with {@code torch::nn::GLUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../GELUImpl.java



/** A {@code ModuleHolder} subclass for {@code GELUImpl}.
 *  See the documentation for {@code GELUImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../SiLUImpl.java



/** A {@code ModuleHolder} subclass for {@code SiLUImpl}.
 *  See the documentation for {@code SiLUImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../MishImpl.java



/** A {@code ModuleHolder} subclass for {@code MishImpl}.
 *  See the documentation for {@code MishImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../SigmoidImpl.java



/** A {@code ModuleHolder} subclass for {@code SigmoidImpl}.
 *  See the documentation for {@code SigmoidImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../SoftplusImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftplusImpl}.
 *  See the documentation for {@code SoftplusImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Softplus} with
 *  {@code torch::nn::SoftplusOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../SoftshrinkImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftshrinkImpl}.
 *  See the documentation for {@code SoftshrinkImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Softshrink} with
 *  {@code torch::nn::SoftshrinkOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../SoftsignImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftsignImpl}.
 *  See the documentation for {@code SoftsignImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../TanhImpl.java



/** A {@code ModuleHolder} subclass for {@code TanhImpl}.
 *  See the documentation for {@code TanhImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../TanhshrinkImpl.java



/** A {@code ModuleHolder} subclass for {@code TanhshrinkImpl}.
 *  See the documentation for {@code TanhshrinkImpl} class to learn what methods it
 *  provides, or the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../ThresholdImpl.java



/** A {@code ModuleHolder} subclass for {@code ThresholdImpl}.
 *  See the documentation for {@code ThresholdImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Threshold} with
 *  {@code torch::nn::ThresholdOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MultiheadAttentionImpl.java



/** A {@code ModuleHolder} subclass for {@code MultiheadAttentionImpl}.
 *  See the documentation for {@code MultiheadAttentionImpl} class to learn what
 *  methods it provides, and examples of how to use {@code MultiheadAttention} with
 *  {@code torch::nn::MultiheadAttentionOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/adaptive.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../AdaptiveLogSoftmaxWithLossOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/adaptive.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/options/adaptive.h>
// Targeting ../ASMoutput.java


// Targeting ../AdaptiveLogSoftmaxWithLossImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveLogSoftmaxWithLossImpl}.
 *  See the documentation for {@code AdaptiveLogSoftmaxWithLossImpl} class to learn
 *  what methods it provides, and examples of how to use
 *  {@code AdaptiveLogSoftmaxWithLoss} with
 *  {@code torch::nn::AdaptiveLogSoftmaxWithLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/batchnorm.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/init.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstdint>
// Targeting ../BatchNorm1dImplBaseBase.java


// Targeting ../InstanceNorm1dImplBaseBase.java


// Targeting ../BatchNorm2dImplBaseBase.java


// Targeting ../InstanceNorm2dImplBaseBase.java


// Targeting ../BatchNorm3dImplBaseBase.java


// Targeting ../InstanceNorm3dImplBaseBase.java


// Targeting ../BatchNorm1dImplBase.java


// Targeting ../BatchNorm2dImplBase.java


// Targeting ../BatchNorm3dImplBase.java


// Targeting ../BatchNorm1dImpl.java



/** A {@code ModuleHolder} subclass for {@code BatchNorm1dImpl}.
 *  See the documentation for {@code BatchNorm1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code BatchNorm1d} with
 *  {@code torch::nn::BatchNorm1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../BatchNorm2dImpl.java



/** A {@code ModuleHolder} subclass for {@code BatchNorm2dImpl}.
 *  See the documentation for {@code BatchNorm2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code BatchNorm2d} with
 *  {@code torch::nn::BatchNorm2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../BatchNorm3dImpl.java



/** A {@code ModuleHolder} subclass for {@code BatchNorm3dImpl}.
 *  See the documentation for {@code BatchNorm3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code BatchNorm3d} with
 *  {@code torch::nn::BatchNorm3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/conv.h

// #pragma once

// #include <c10/util/irange.h>
// #include <c10/util/overloaded.h>

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/init.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../Conv1dImplBase.java


// Targeting ../ConvTranspose1dImplBaseBase.java


// Targeting ../Conv2dImplBase.java


// Targeting ../ConvTranspose2dImplBaseBase.java


// Targeting ../Conv3dImplBase.java


// Targeting ../ConvTranspose3dImplBaseBase.java


// Targeting ../Conv1dImpl.java



/** A {@code ModuleHolder} subclass for {@code Conv1dImpl}.
 *  See the documentation for {@code Conv1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Conv1d} with
 *  {@code torch::nn::Conv1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../Conv2dImpl.java



/** A {@code ModuleHolder} subclass for {@code Conv2dImpl}.
 *  See the documentation for {@code Conv2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Conv2d} with
 *  {@code torch::nn::Conv2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../Conv3dImpl.java



/** A {@code ModuleHolder} subclass for {@code Conv3dImpl}.
 *  See the documentation for {@code Conv3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Conv3d} with
 *  {@code torch::nn::Conv3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../ConvTranspose1dImplBase.java


// Targeting ../ConvTranspose2dImplBase.java


// Targeting ../ConvTranspose3dImplBase.java


// Targeting ../ConvTranspose1dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConvTranspose1dImpl}.
 *  See the documentation for {@code ConvTranspose1dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ConvTranspose1d} with
 *  {@code torch::nn::ConvTranspose1dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ConvTranspose2dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConvTranspose2dImpl}.
 *  See the documentation for {@code ConvTranspose2dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ConvTranspose2d} with
 *  {@code torch::nn::ConvTranspose2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ConvTranspose3dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConvTranspose3dImpl}.
 *  See the documentation for {@code ConvTranspose3dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ConvTranspose3d} with
 *  {@code torch::nn::ConvTranspose3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/distance.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/options/distance.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>
// Targeting ../CosineSimilarityImpl.java



/** A {@code ModuleHolder} subclass for {@code CosineSimilarityImpl}.
 *  See the documentation for {@code CosineSimilarityImpl} class to learn what methods
 *  it provides, and examples of how to use {@code CosineSimilarity} with
 *  {@code torch::nn::CosineSimilarityOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../PairwiseDistanceImpl.java



/** A {@code ModuleHolder} subclass for {@code PairwiseDistanceImpl}.
 *  See the documentation for {@code PairwiseDistanceImpl} class to learn what methods
 *  it provides, and examples of how to use {@code PairwiseDistance} with
 *  {@code torch::nn::PairwiseDistanceOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/dropout.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../DropoutImplBase.java


// Targeting ../Dropout2dImplBase.java


// Targeting ../Dropout3dImplBase.java


// Targeting ../AlphaDropoutImplBase.java


// Targeting ../FeatureAlphaDropoutImplBase.java




// Targeting ../DropoutImpl.java



/** A {@code ModuleHolder} subclass for {@code DropoutImpl}.
 *  See the documentation for {@code DropoutImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Dropout} with
 *  {@code torch::nn::DropoutOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../Dropout2dImpl.java



/** A {@code ModuleHolder} subclass for {@code Dropout2dImpl}.
 *  See the documentation for {@code Dropout2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Dropout2d} with
 *  {@code torch::nn::Dropout2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../Dropout3dImpl.java



/** A {@code ModuleHolder} subclass for {@code Dropout3dImpl}.
 *  See the documentation for {@code Dropout3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Dropout3d} with
 *  {@code torch::nn::Dropout3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../AlphaDropoutImpl.java



/** A {@code ModuleHolder} subclass for {@code AlphaDropoutImpl}.
 *  See the documentation for {@code AlphaDropoutImpl} class to learn what methods it
 *  provides, and examples of how to use {@code AlphaDropout} with
 *  {@code torch::nn::AlphaDropoutOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../FeatureAlphaDropoutImpl.java



/** A {@code ModuleHolder} subclass for {@code FeatureAlphaDropoutImpl}.
 *  See the documentation for {@code FeatureAlphaDropoutImpl} class to learn what
 *  methods it provides, and examples of how to use {@code FeatureAlphaDropout} with
 *  {@code torch::nn::FeatureAlphaDropoutOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/embedding.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/embedding.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// Targeting ../EmbeddingImpl.java



/** A {@code ModuleHolder} subclass for {@code EmbeddingImpl}.
 *  See the documentation for {@code EmbeddingImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Embedding} with
 *  {@code torch::nn::EmbeddingOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../EmbeddingBagImpl.java



/** A {@code ModuleHolder} subclass for {@code EmbeddingBagImpl}.
 *  See the documentation for {@code EmbeddingBagImpl} class to learn what methods it
 *  provides, and examples of how to use {@code EmbeddingBag} with
 *  {@code torch::nn::EmbeddingBagOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/fold.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>
// Targeting ../FoldImpl.java



/** A {@code ModuleHolder} subclass for {@code FoldImpl}.
 *  See the documentation for {@code FoldImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Fold} with {@code torch::nn::FoldOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../UnfoldImpl.java



/** A {@code ModuleHolder} subclass for {@code UnfoldImpl}.
 *  See the documentation for {@code UnfoldImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Unfold} with
 *  {@code torch::nn::UnfoldOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/instancenorm.h

// #pragma once

// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/options/instancenorm.h>
// Targeting ../InstanceNorm1dImplBase.java


// Targeting ../InstanceNorm2dImplBase.java


// Targeting ../InstanceNorm3dImplBase.java


// Targeting ../InstanceNorm1dImpl.java



/** A {@code ModuleHolder} subclass for {@code InstanceNorm1dImpl}.
 *  See the documentation for {@code InstanceNorm1dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code InstanceNorm1d} with
 *  {@code torch::nn::InstanceNorm1dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../InstanceNorm2dImpl.java



/** A {@code ModuleHolder} subclass for {@code InstanceNorm2dImpl}.
 *  See the documentation for {@code InstanceNorm2dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code InstanceNorm2d} with
 *  {@code torch::nn::InstanceNorm2dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../InstanceNorm3dImpl.java



/** A {@code ModuleHolder} subclass for {@code InstanceNorm3dImpl}.
 *  See the documentation for {@code InstanceNorm3dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code InstanceNorm3d} with
 *  {@code torch::nn::InstanceNorm3dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/loss.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../L1LossImpl.java



/** A {@code ModuleHolder} subclass for {@code L1LossImpl}.
 *  See the documentation for {@code L1LossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code L1Loss} with
 *  {@code torch::nn::L1LossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../KLDivLossImpl.java



/** A {@code ModuleHolder} subclass for {@code KLDivLossImpl}.
 *  See the documentation for {@code KLDivLossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code KLDivLoss} with
 *  {@code torch::nn::KLDivLossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MSELossImpl.java



/** A {@code ModuleHolder} subclass for {@code MSELossImpl}.
 *  See the documentation for {@code MSELossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MSELoss} with
 *  {@code torch::nn::MSELossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../BCELossImpl.java



/** A {@code ModuleHolder} subclass for {@code BCELossImpl}.
 *  See the documentation for {@code BCELossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code BCELoss} with
 *  {@code torch::nn::BCELossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../HingeEmbeddingLossImpl.java



/** A {@code ModuleHolder} subclass for {@code HingeEmbeddingLossImpl}.
 *  See the documentation for {@code HingeEmbeddingLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code HingeEmbeddingLoss} with
 *  {@code torch::nn::HingeEmbeddingLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../MultiMarginLossImpl.java



/** A {@code ModuleHolder} subclass for {@code MultiMarginLossImpl}.
 *  See the documentation for {@code MultiMarginLossImpl} class to learn what methods
 *  it provides, and examples of how to use {@code MultiMarginLoss} with
 *  {@code torch::nn::MultiMarginLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../CosineEmbeddingLossImpl.java



/** A {@code ModuleHolder} subclass for {@code CosineEmbeddingLossImpl}.
 *  See the documentation for {@code CosineEmbeddingLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code CosineEmbeddingLoss} with
 *  {@code torch::nn::CosineEmbeddingLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../SmoothL1LossImpl.java



/** A {@code ModuleHolder} subclass for {@code SmoothL1LossImpl}.
 *  See the documentation for {@code SmoothL1LossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code SmoothL1Loss} with
 *  {@code torch::nn::SmoothL1LossOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../HuberLossImpl.java



/** A {@code ModuleHolder} subclass for {@code HuberLossImpl}.
 *  See the documentation for {@code HuberLossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code HuberLoss} with
 *  {@code torch::nn::HuberLossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MultiLabelMarginLossImpl.java



/** A {@code ModuleHolder} subclass for {@code MultiLabelMarginLossImpl}.
 *  See the documentation for {@code MultiLabelMarginLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code MultiLabelMarginLoss} with
 *  {@code torch::nn::MultiLabelMarginLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../SoftMarginLossImpl.java



/** A {@code ModuleHolder} subclass for {@code SoftMarginLossImpl}.
 *  See the documentation for {@code SoftMarginLossImpl} class to learn what methods
 *  it provides, and examples of how to use {@code SoftMarginLoss} with
 *  {@code torch::nn::SoftMarginLossOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../MultiLabelSoftMarginLossImpl.java



/** A {@code ModuleHolder} subclass for {@code MultiLabelSoftMarginLossImpl}.
 *  See the documentation for {@code MultiLabelSoftMarginLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code MultiLabelSoftMarginLoss}
 *  with {@code torch::nn::MultiLabelSoftMarginLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../TripletMarginLossImpl.java



/** A {@code ModuleHolder} subclass for {@code TripletMarginLossImpl}.
 *  See the documentation for {@code TripletMarginLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code TripletMarginLoss} with
 *  {@code torch::nn::TripletMarginLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../TripletMarginWithDistanceLossImpl.java



/** A {@code ModuleHolder} subclass for {@code TripletMarginWithDistanceLossImpl}.
 *  See the documentation for {@code TripletMarginWithDistanceLossImpl} class to learn
 *  what methods it provides, and examples of how to use
 *  {@code TripletMarginWithDistanceLoss} with
 *  {@code torch::nn::TripletMarginWithDistanceLossOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../CTCLossImpl.java



/** A {@code ModuleHolder} subclass for {@code CTCLossImpl}.
 *  See the documentation for {@code CTCLossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code CTCLoss} with
 *  {@code torch::nn::CTCLossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../PoissonNLLLossImpl.java



/** A {@code ModuleHolder} subclass for {@code PoissonNLLLossImpl}.
 *  See the documentation for {@code PoissonNLLLossImpl} class to learn what methods
 *  it provides, and examples of how to use {@code PoissonNLLLoss} with
 *  {@code torch::nn::PoissonNLLLossOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../MarginRankingLossImpl.java



/** A {@code ModuleHolder} subclass for {@code MarginRankingLossImpl}.
 *  See the documentation for {@code MarginRankingLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code MarginRankingLoss} with
 *  {@code torch::nn::MarginRankingLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../NLLLossImpl.java



/** A {@code ModuleHolder} subclass for {@code NLLLossImpl}.
 *  See the documentation for {@code NLLLossImpl} class to learn what methods it
 *  provides, and examples of how to use {@code NLLLoss} with
 *  {@code torch::nn::NLLLossOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../CrossEntropyLossImpl.java



/** A {@code ModuleHolder} subclass for {@code CrossEntropyLossImpl}.
 *  See the documentation for {@code CrossEntropyLossImpl} class to learn what methods
 *  it provides, and examples of how to use {@code CrossEntropyLoss} with
 *  {@code torch::nn::CrossEntropyLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../BCEWithLogitsLossImpl.java



/** A {@code ModuleHolder} subclass for {@code BCEWithLogitsLossImpl}.
 *  See the documentation for {@code BCEWithLogitsLossImpl} class to learn what
 *  methods it provides, and examples of how to use {@code BCEWithLogitsLoss} with
 *  {@code torch::nn::BCEWithLogitsLossOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/_functions.h

// #pragma once

// #include <torch/csrc/autograd/custom_function.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/types.h>

 // namespace functions
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/normalization.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/modules/_functions.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../LayerNormImpl.java



/** A {@code ModuleHolder} subclass for {@code LayerNormImpl}.
 *  See the documentation for {@code LayerNormImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LayerNorm} with
 *  {@code torch::nn::LayerNormOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LocalResponseNormImpl.java



/** A {@code ModuleHolder} subclass for {@code LocalResponseNormImpl}.
 *  See the documentation for {@code LocalResponseNormImpl} class to learn what
 *  methods it provides, and examples of how to use {@code LocalResponseNorm} with
 *  {@code torch::nn::LocalResponseNormOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../CrossMapLRN2dImpl.java



/** A {@code ModuleHolder} subclass for {@code CrossMapLRN2dImpl}.
 *  See the documentation for {@code CrossMapLRN2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code CrossMapLRN2d} with
 *  {@code torch::nn::CrossMapLRN2dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../GroupNormImpl.java



/** A {@code ModuleHolder} subclass for {@code GroupNormImpl}.
 *  See the documentation for {@code GroupNormImpl} class to learn what methods it
 *  provides, and examples of how to use {@code GroupNorm} with
 *  {@code torch::nn::GroupNormOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/padding.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/padding.h>

// #include <torch/csrc/Export.h>
// Targeting ../ReflectionPad1dImplBase.java


// Targeting ../ReflectionPad2dImplBase.java


// Targeting ../ReflectionPad3dImplBase.java


// Targeting ../ReflectionPad1dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReflectionPad1dImpl}.
 *  See the documentation for {@code ReflectionPad1dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReflectionPad1d} with
 *  {@code torch::nn::ReflectionPad1dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ReflectionPad2dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReflectionPad2dImpl}.
 *  See the documentation for {@code ReflectionPad2dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReflectionPad2d} with
 *  {@code torch::nn::ReflectionPad2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ReflectionPad3dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReflectionPad3dImpl}.
 *  See the documentation for {@code ReflectionPad3dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReflectionPad3d} with
 *  {@code torch::nn::ReflectionPad3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ReplicationPad1dImplBase.java


// Targeting ../ReplicationPad2dImplBase.java


// Targeting ../ReplicationPad3dImplBase.java


// Targeting ../ReplicationPad1dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReplicationPad1dImpl}.
 *  See the documentation for {@code ReplicationPad1dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReplicationPad1d} with
 *  {@code torch::nn::ReplicationPad1dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ReplicationPad2dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReplicationPad2dImpl}.
 *  See the documentation for {@code ReplicationPad2dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReplicationPad2d} with
 *  {@code torch::nn::ReplicationPad2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ReplicationPad3dImpl.java



/** A {@code ModuleHolder} subclass for {@code ReplicationPad3dImpl}.
 *  See the documentation for {@code ReplicationPad3dImpl} class to learn what methods
 *  it provides, and examples of how to use {@code ReplicationPad3d} with
 *  {@code torch::nn::ReplicationPad3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../ZeroPad1dImplBase.java


// Targeting ../ZeroPad2dImplBase.java


// Targeting ../ZeroPad3dImplBase.java


// Targeting ../ZeroPad1dImpl.java



/** A {@code ModuleHolder} subclass for {@code ZeroPad1dImpl}.
 *  See the documentation for {@code ZeroPad1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ZeroPad1d} with
 *  {@code torch::nn::ZeroPad1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../ZeroPad2dImpl.java



/** A {@code ModuleHolder} subclass for {@code ZeroPad2dImpl}.
 *  See the documentation for {@code ZeroPad2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ZeroPad2d} with
 *  {@code torch::nn::ZeroPad2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../ZeroPad3dImpl.java



/** A {@code ModuleHolder} subclass for {@code ZeroPad3dImpl}.
 *  See the documentation for {@code ZeroPad3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ZeroPad3d} with
 *  {@code torch::nn::ZeroPad3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../ConstantPad1dImplBase.java


// Targeting ../ConstantPad2dImplBase.java


// Targeting ../ConstantPad3dImplBase.java


// Targeting ../ConstantPad1dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConstantPad1dImpl}.
 *  See the documentation for {@code ConstantPad1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ConstantPad1d} with
 *  {@code torch::nn::ConstantPad1dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../ConstantPad2dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConstantPad2dImpl}.
 *  See the documentation for {@code ConstantPad2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ConstantPad2d} with
 *  {@code torch::nn::ConstantPad2dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../ConstantPad3dImpl.java



/** A {@code ModuleHolder} subclass for {@code ConstantPad3dImpl}.
 *  See the documentation for {@code ConstantPad3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code ConstantPad3d} with
 *  {@code torch::nn::ConstantPad3dOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/pixelshuffle.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/options/pixelshuffle.h>

// #include <torch/csrc/Export.h>
// Targeting ../PixelShuffleImpl.java



/** A {@code ModuleHolder} subclass for {@code PixelShuffleImpl}.
 *  See the documentation for {@code PixelShuffleImpl} class to learn what methods it
 *  provides, and examples of how to use {@code PixelShuffle} with
 *  {@code torch::nn::PixelShuffleOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */
// Targeting ../PixelUnshuffleImpl.java



/** A {@code ModuleHolder} subclass for {@code PixelUnshuffleImpl}.
 *  See the documentation for {@code PixelUnshuffleImpl} class to learn what methods
 *  it provides, and examples of how to use {@code PixelUnshuffle} with
 *  {@code torch::nn::PixelUnshuffleOptions}. See the documentation for {@code ModuleHolder}
 *  to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/pooling.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/pooling.h>

// #include <torch/csrc/Export.h>
// Targeting ../AvgPool1dImplBase.java


// Targeting ../AvgPool2dImplBase.java


// Targeting ../AvgPool3dImplBase.java


// Targeting ../AvgPool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code AvgPool1dImpl}.
 *  See the documentation for {@code AvgPool1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code AvgPool1d} with
 *  {@code torch::nn::AvgPool1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../AvgPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code AvgPool2dImpl}.
 *  See the documentation for {@code AvgPool2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code AvgPool2d} with
 *  {@code torch::nn::AvgPool2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../AvgPool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code AvgPool3dImpl}.
 *  See the documentation for {@code AvgPool3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code AvgPool3d} with
 *  {@code torch::nn::AvgPool3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MaxPool1dImplBase.java


// Targeting ../MaxPool2dImplBase.java


// Targeting ../MaxPool3dImplBase.java


// Targeting ../MaxPool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxPool1dImpl}.
 *  See the documentation for {@code MaxPool1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxPool1d} with
 *  {@code torch::nn::MaxPool1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MaxPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxPool2dImpl}.
 *  See the documentation for {@code MaxPool2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxPool2d} with
 *  {@code torch::nn::MaxPool2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MaxPool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxPool3dImpl}.
 *  See the documentation for {@code MaxPool3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxPool3d} with
 *  {@code torch::nn::MaxPool3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveMaxPool1dImplBase.java


// Targeting ../AdaptiveMaxPool2dImplBase.java


// Targeting ../AdaptiveMaxPool3dImplBase.java


// Targeting ../AdaptiveMaxPool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveMaxPool1dImpl}.
 *  See the documentation for {@code AdaptiveMaxPool1dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveMaxPool1d} with
 *  {@code torch::nn::AdaptiveMaxPool1dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveMaxPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveMaxPool2dImpl}.
 *  See the documentation for {@code AdaptiveMaxPool2dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveMaxPool2d} with
 *  {@code torch::nn::AdaptiveMaxPool2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveMaxPool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveMaxPool3dImpl}.
 *  See the documentation for {@code AdaptiveMaxPool3dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveMaxPool3d} with
 *  {@code torch::nn::AdaptiveMaxPool3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveAvgPool1dImplBase.java


// Targeting ../AdaptiveAvgPool2dImplBase.java


// Targeting ../AdaptiveAvgPool3dImplBase.java


// Targeting ../AdaptiveAvgPool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveAvgPool1dImpl}.
 *  See the documentation for {@code AdaptiveAvgPool1dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveAvgPool1d} with
 *  {@code torch::nn::AdaptiveAvgPool1dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveAvgPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveAvgPool2dImpl}.
 *  See the documentation for {@code AdaptiveAvgPool2dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveAvgPool2d} with
 *  {@code torch::nn::AdaptiveAvgPool2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../AdaptiveAvgPool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code AdaptiveAvgPool3dImpl}.
 *  See the documentation for {@code AdaptiveAvgPool3dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code AdaptiveAvgPool3d} with
 *  {@code torch::nn::AdaptiveAvgPool3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../MaxUnpool1dImplBase.java


// Targeting ../MaxUnpool2dImplBase.java


// Targeting ../MaxUnpool3dImplBase.java


// Targeting ../MaxUnpool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxUnpool1dImpl}.
 *  See the documentation for {@code MaxUnpool1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxUnpool1d} with
 *  {@code torch::nn::MaxUnpool1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MaxUnpool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxUnpool2dImpl}.
 *  See the documentation for {@code MaxUnpool2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxUnpool2d} with
 *  {@code torch::nn::MaxUnpool2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../MaxUnpool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code MaxUnpool3dImpl}.
 *  See the documentation for {@code MaxUnpool3dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code MaxUnpool3d} with
 *  {@code torch::nn::MaxUnpool3dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../FractionalMaxPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code FractionalMaxPool2dImpl}.
 *  See the documentation for {@code FractionalMaxPool2dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code FractionalMaxPool2d} with
 *  {@code torch::nn::FractionalMaxPool2dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../FractionalMaxPool3dImpl.java



/** A {@code ModuleHolder} subclass for {@code FractionalMaxPool3dImpl}.
 *  See the documentation for {@code FractionalMaxPool3dImpl} class to learn what
 *  methods it provides, and examples of how to use {@code FractionalMaxPool3d} with
 *  {@code torch::nn::FractionalMaxPool3dOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */
// Targeting ../LPPool1dImplBase.java


// Targeting ../LPPool2dImplBase.java


// Targeting ../LPPool1dImpl.java



/** A {@code ModuleHolder} subclass for {@code LPPool1dImpl}.
 *  See the documentation for {@code LPPool1dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LPPool1d} with
 *  {@code torch::nn::LPPool1dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LPPool2dImpl.java



/** A {@code ModuleHolder} subclass for {@code LPPool2dImpl}.
 *  See the documentation for {@code LPPool2dImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LPPool2d} with
 *  {@code torch::nn::LPPool2dOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/rnn.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../RNNOptionsBase.java




// Targeting ../RNNOptions.java


// Targeting ../LSTMOptions.java


// Targeting ../GRUOptions.java


// Targeting ../RNNCellOptionsBase.java




// Targeting ../RNNCellOptions.java


// Targeting ../LSTMCellOptions.java


// Targeting ../GRUCellOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/utils/rnn.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/types.h>

// #include <utility>


///
///
///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor invert_permutation(@Const @ByRef Tensor permutation);
// Targeting ../PackedSequence.java



/** Packs a Tensor containing padded sequences of variable length.
 * 
 *  {@code input} can be of size {@code }T x B x *{@code } where {@code T} is the length of the
 *  longest sequence (equal to {@code }lengths[0]{@code }), {@code }B{@code } is the batch size, and
 *  {@code }*{@code } is any number of dimensions (including 0). If {@code }batch_first{@code } is
 *  {@code }true{@code }, {@code }B x T x *{@code } {@code input} is expected.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code enforce_sorted} is
 *  {@code }true{@code }, the sequences should be sorted by length in a decreasing order,
 *  i.e.
 *  {@code }input[:,0]{@code } should be the longest sequence, and {@code }input[:,B-1]{@code } the
 *  shortest one.
 * 
 *  Note:
 *      This function accepts any input that has at least two dimensions. You
 *      can apply it to pack the labels, and use the output of the RNN with
 *      them to compute the loss directly. A Tensor can be retrieved from
 *      a {@code PackedSequence} object by calling its {@code }.data(){@code } function.
 * 
 *  Arguments:
 *      input (Tensor): padded batch of variable length sequences.
 *      lengths (Tensor): list of sequences lengths of each batch element.
 *      batch_first (bool, optional): if {@code }true{@code }, the input is expected in {@code }B
 *      x T x *{@code }
 *          format. Default: {@code }false{@code }.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, the input is expected to
 *          contain sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths,
    @Cast("bool") boolean batch_first/*=false*/,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths);

/** Pads a packed batch of variable length sequences.
 * 
 *  It is an inverse operation to {@code pack_padded_sequence}.
 * 
 *  The returned Tensor's data will be of size {@code }T x B x *{@code }, where {@code T} is the
 *  length of the longest sequence and {@code B} is the batch size. If {@code }batch_first{@code }
 *  is true, the data will be transposed into {@code }B x T x *{@code } format.
 * 
 *  Batch elements will be ordered decreasingly by their length.
 * 
 *  Arguments:
 *      sequence (PackedSequence): batch to pad
 *      batch_first (bool, optional): if {@code }true{@code }, the output will be in {@code }B x T
 *      x *{@code }
 *          format.
 *      padding_value (double, optional): values for padded elements.
 *      total_length (int64_t, optional): if specified, the output will be
 *      padded to
 *          have length {@code total_length}. This method will throw error
 *          if {@code total_length} is less than the max sequence length in
 *          {@code sequence}.
 * 
 *  Returns:
 *      Tuple of Tensor containing the padded sequence, and a Tensor
 *      containing the list of lengths of each sequence in the batch. */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal T_TensorTensor_T pad_packed_sequence(
    @ByVal PackedSequence sequence,
    @Cast("bool") boolean batch_first/*=false*/,
    double padding_value/*=0.0*/,
    @ByVal(nullValue = "c10::optional<int64_t>(torch::nullopt)") LongOptional total_length);
@Namespace("torch::nn::utils::rnn") public static native @ByVal T_TensorTensor_T pad_packed_sequence(
    @ByVal PackedSequence sequence);

/** Pad a list of variable length Tensors with {@code }padding_value{@code }
 * 
 *  {@code }pad_sequence{@code } stacks a list of Tensors along a new dimension,
 *  and pads them to equal length. For example, if the input is list of
 *  sequences with size {@code }L x *{@code } and if batch_first is false, and {@code }T x B x *{@code }
 *  otherwise.
 * 
 *  {@code B} is batch size. It is equal to the number of elements in {@code }sequences{@code }.
 *  {@code T} is length of the longest sequence.
 *  {@code L} is length of the sequence.
 *  {@code *} is any number of trailing dimensions, including none.
 * 
 *  Note:
 *      This function returns a Tensor of size {@code }T x B x *{@code } or {@code }B x T x *{@code }
 *      where {@code T} is the length of the longest sequence. This function assumes
 *      trailing dimensions and type of all the Tensors in sequences are same.
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): list of variable length sequences.
 *      batch_first (bool, optional): output will be in {@code }B x T x *{@code } if true,
 *      or in
 *          {@code }T x B x *{@code } otherwise
 *      padding_value (double, optional): value for padded elements. Default: 0.
 * 
 *  Returns:
 *      Tensor of size {@code }T x B x *{@code } if {@code batch_first} is {@code }false{@code }.
 *      Tensor of size {@code }B x T x *{@code } otherwise */

/** Packs a list of variable length Tensors
 * 
 *  {@code }sequences{@code } should be a list of Tensors of size {@code }L x *{@code }, where {@code L} is
 *  the length of a sequence and {@code *} is any number of trailing dimensions,
 *  including zero.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code }enforce_sorted{@code }
 *  is {@code }true{@code }, the sequences should be sorted in the order of decreasing
 *  length.
 * 
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): A list of sequences of decreasing
 *      length. enforce_sorted (bool, optional): if {@code }true{@code }, checks that the
 *      input
 *          contains sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorArrayRef sequences,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorArrayRef sequences);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorVector sequences,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorVector sequences);

 // namespace rnn
 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/rnn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils/rnn.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../RNNImplBase.java


// Targeting ../LSTMImplBase.java


// Targeting ../GRUImplBase.java



// Targeting ../RNNImpl.java



/** A {@code ModuleHolder} subclass for {@code RNNImpl}.
 *  See the documentation for {@code RNNImpl} class to learn what methods it
 *  provides, and examples of how to use {@code RNN} with {@code torch::nn::RNNOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../LSTMImpl.java



/** A {@code ModuleHolder} subclass for {@code LSTMImpl}.
 *  See the documentation for {@code LSTMImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LSTM} with {@code torch::nn::LSTMOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../GRUImpl.java



/** A {@code ModuleHolder} subclass for {@code GRUImpl}.
 *  See the documentation for {@code GRUImpl} class to learn what methods it
 *  provides, and examples of how to use {@code GRU} with {@code torch::nn::GRUOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RNNCellImplBase
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Targeting ../RNNCellImplBase.java


// Targeting ../LSTMCellImplBase.java


// Targeting ../GRUCellImplBase.java



// Targeting ../RNNCellImpl.java



/** A {@code ModuleHolder} subclass for {@code RNNCellImpl}.
 *  See the documentation for {@code RNNCellImpl} class to learn what methods it
 *  provides, and examples of how to use {@code RNNCell} with
 *  {@code torch::nn::RNNCellOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../LSTMCellImpl.java



/** A {@code ModuleHolder} subclass for {@code LSTMCellImpl}.
 *  See the documentation for {@code LSTMCellImpl} class to learn what methods it
 *  provides, and examples of how to use {@code LSTMCell} with
 *  {@code torch::nn::LSTMCellOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */
// Targeting ../GRUCellImpl.java



/** A {@code ModuleHolder} subclass for {@code GRUCellImpl}.
 *  See the documentation for {@code GRUCellImpl} class to learn what methods it
 *  provides, and examples of how to use {@code GRUCell} with
 *  {@code torch::nn::GRUCellOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/transformerlayer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>


///
// Targeting ../TransformerEncoderLayerOptions.java


// Targeting ../TransformerDecoderLayerOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/transformer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/options/transformerlayer.h>
// Targeting ../TransformerOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/transformer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerImpl.java



/** A {@code ModuleHolder} subclass for {@code TransformerImpl}.
 *  See the documentation for {@code TransformerImpl} class to learn what
 *  methods it provides, and examples of how to use {@code Transformer} with
 *  {@code torch::nn::TransformerOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/transformerlayer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderLayerImpl.java



/** A {@code ModuleHolder} subclass for {@code TransformerEncoderLayerImpl}{@code .
 *  See the documentation for }TransformerEncoderLayerImpl{@code  class to learn what
 *  methods it provides, and examples of how to use }TransformerEncoderLayer{@code 
 *  with }torch::nn::TransformerEncoderLayerOptions{@code . See the documentation for
 *  }ModuleHolder{@code  to learn about PyTorch's module storage semantics. */
// Targeting ../TransformerDecoderLayerImpl.java



/** A {@code ModuleHolder} subclass for {@code TransformerDecoderLayerImpl}.
 *  See the documentation for {@code TransformerDecoderLayerImpl} class to learn what
 *  methods it provides, and examples of how to use {@code TransformerDecoderLayer}
 *  with {@code torch::nn::TransformerDecoderLayerOptions}. See the documentation for
 *  {@code ModuleHolder} to learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/options/transformercoder.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/transformerlayer.h>
// Targeting ../TransformerEncoderOptions.java


// Targeting ../TransformerDecoderOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/transformercoder.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderImpl.java



/** A {@code ModuleHolder} subclass for {@code TransformerEncoderImpl}.
 *  See the documentation for {@code TransformerEncoderImpl} class to learn what
 *  methods it provides, and examples of how to use {@code TransformerEncoder} with
 *  {@code torch::nn::TransformerEncoderOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */
// Targeting ../TransformerDecoderImpl.java



/** A {@code ModuleHolder} subclass for {@code TransformerDecoderImpl}.
 *  See the documentation for {@code TransformerDecoderImpl} class to learn what
 *  methods it provides, and examples of how to use {@code TransformerDecoder} with
 *  {@code torch::nn::TransformerDecoderOptions}.
 *  See the documentation for {@code ModuleHolder} to learn about PyTorch's
 *  module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules/upsampling.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <ostream>
// Targeting ../UpsampleImpl.java



/** A {@code ModuleHolder} subclass for {@code UpsampleImpl}.
 *  See the documentation for {@code UpsampleImpl} class to learn what methods it
 *  provides, and examples of how to use {@code Upsample} with
 *  {@code torch::nn::UpsampleOptions}. See the documentation for {@code ModuleHolder} to
 *  learn about PyTorch's module storage semantics. */

 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/modules.h

// #pragma once

// Common
// #include <torch/nn/modules/common.h>

// Containers
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/functional.h>
// #include <torch/nn/modules/container/moduledict.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/modules/container/parameterdict.h>
// #include <torch/nn/modules/container/parameterlist.h>
// #include <torch/nn/modules/container/sequential.h>

// Layers
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/adaptive.h>
// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/modules/conv.h>
// #include <torch/nn/modules/distance.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/embedding.h>
// #include <torch/nn/modules/fold.h>
// #include <torch/nn/modules/instancenorm.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/loss.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/padding.h>
// #include <torch/nn/modules/pixelshuffle.h>
// #include <torch/nn/modules/pooling.h>
// #include <torch/nn/modules/rnn.h>
// #include <torch/nn/modules/transformer.h>
// #include <torch/nn/modules/transformercoder.h>
// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/upsampling.h>


// Parsed from torch/csrc/api/include/torch/nn/options.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/options/padding.h>
// #include <torch/nn/options/pixelshuffle.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/options/vision.h>


// Parsed from torch/csrc/api/include/torch/nn/utils/clip_grad.h

// #pragma once

// #include <torch/csrc/Export.h>

// #include <utility>

// Clips gradient norm of a vector of Tensors.
// See
// https://pytorch.org/docs/stable/nn.html?highlight=clip_grad_norm#torch.nn.utils.clip_grad_norm_
// for more details about this module.
//
// Difference with the python version: unlike the python version, even when
// skipping the finiteness checks (error_if_nonfinite = false), this function
// will introduce a device <=> CPU synchronization (for devices where that makes
// sense!) in order to return a CPU-side `double`. This C++ version therefore
// cannot be run fully asynchronously w.r.t. the device of the gradients.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm);

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm);

// Clips gradient of an iterable of parameters at specified value.
// Gradients are modified in-place.
// See https://pytorch.org/docs/stable/nn.html#clip-grad-value
// for more details about this module.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector parameters,
    double clip_value);

// A wrapper around clip_grad_value_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_value_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(@ByVal Tensor parameter, double clip_value);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/utils/convert_parameters.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/types.h>

// This helper function is to check if the parameters are located
// in the same device. Currently, the conversion between model parameters
// and single vector form is not supported for multiple allocations,
// e.g. parameters in different GPUs, or mixture of CPU/GPU.
@Namespace("torch::nn::utils") public static native @ByVal LongOptional _check_param_device(
    @Const @ByRef Tensor param,
    @ByVal LongOptional old_param_device);

// Convert parameters to one vector
@Namespace("torch::nn::utils") public static native @ByVal Tensor parameters_to_vector(
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector parameters);

// Convert one vector to the parameters
@Namespace("torch::nn::utils") public static native void vector_to_parameters(
    @Const @ByRef Tensor vec,
    @Cast({"", "std::vector<torch::Tensor>"}) @StdMove TensorVector parameters);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/csrc/api/include/torch/nn/utils.h

// #pragma once

// #include <torch/nn/utils/clip_grad.h>
// #include <torch/nn/utils/convert_parameters.h>
// #include <torch/nn/utils/rnn.h>


// Parsed from torch/csrc/api/include/torch/nn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional.h>
// #include <torch/nn/init.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules.h>
// #include <torch/nn/options.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils.h>


// Parsed from torch/csrc/api/include/torch/optim/optimizer.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/util/Exception.h>
// #include <c10/util/flat_hash_map.h>

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>

// #include <algorithm>
// #include <functional>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>

// Forward declarations confuse Doxygen
// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace at
 // namespace serialize

// Targeting ../OptimizerParamState.java


// Targeting ../OptimizerCloneableAdagradParamState.java


// Targeting ../OptimizerCloneableAdamParamState.java


// Targeting ../OptimizerCloneableAdamWParamState.java


// Targeting ../OptimizerCloneableLBFGSParamState.java


// Targeting ../OptimizerCloneableRMSpropParamState.java


// Targeting ../OptimizerCloneableSGDParamState.java


// Targeting ../OptimizerOptions.java


// Targeting ../OptimizerCloneableAdagradOptions.java


// Targeting ../OptimizerCloneableAdamOptions.java


// Targeting ../OptimizerCloneableAdamWOptions.java


// Targeting ../OptimizerCloneableLBFGSOptions.java


// Targeting ../OptimizerCloneableRMSpropOptions.java


// Targeting ../OptimizerCloneableSGDOptions.java


// Targeting ../OptimizerParamGroup.java


// Targeting ../Optimizer.java



/* How do we decide whether to serialize undefined tensors or
  c10::nullopt values into the output archive?
Answer: we strictly follow the behavior of Python API. To be more specific:

For optimizer options:
a) For undefined tensor: currently no tensor is used as an options argument in
Python API, so we don't need to worry about it now. b) For c10::nullopt value:
we serialize c10::nullopt values into the output archive, to follow the exact
same behavior as Python API.

For optimizer param state:
a) For undefined tensor: in param state, undefined tensor in C++ impl is
equivalent to missing key in Python impl. Since we don't serialize missing keys
in Python API, we skip undefined tensors when serializing the param state. b)
For c10::nullopt value: in param state, c10::nullopt value in C++ impl is
equivalent to missing key in Python impl. Since we don't serialize missing keys
in Python API, we skip c10::nullopt values when serializing the param state. */

/** Serializes an {@code Optimizer} into an {@code OutputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Optimizer optimizer);

/** Deserializes a {@code Tensor} from an {@code InputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Optimizer optimizer);

 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/optim/optimizer.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>
// #include <cstddef>
// #include <cstdint>
// #include <deque>
// #include <string>
// #include <vector>
// Utility function to save state


// Utility function to load state


// Utility function to save param_groups


// Utility function to load param_groups
// We take as input vector of pair of string and unique_ptr to optimizer options
// so that we can retain the state for each param by using the old tensor impl
// keys (saved during serialization) and map the new tensor impl keys to the
// correct state for each param

 // namespace detail

// Note: These functions are all called `serialize()` so they can be called
// inside a template where the archive type is a template type and can thus be
// passed such that the appropriate overload is selected.

/** Utility function to save a value of {@code int64_t} type. */


/** Utility function to load a value of {@code int64_t} type. */


/** Utility function to save a vector of step buffers. */


/** Utility function to load a vector of step buffers. */


// Utility function to save state and param_groups


// Utility function to load state and param_groups and update state


/** Utility function to save a vector of buffers. */


/** Utility function to load a vector of buffers. */


// #define _TORCH_OPTIM_SERIALIZE(name)
//   torch::optim::serialize(archive, #name, self.name)

// #define _TORCH_OPTIM_SERIALIZE_WITH_TEMPLATE_ARG(OptimizerName)
//   torch::optim::serialize<OptimizerName##ParamState, OptimizerName##Options>(
//       archive, self)

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG(name)
//   {
//     auto ivalue = torch::IValue(name());
//     /* do not serialize if name is an undefined tensor*/
//     if (!(ivalue.isTensor() &&
//           ivalue.nsafeToTensorImpl() ==
//               at::UndefinedTensorImpl::singleton())) {
//       archive.write(#name, ivalue);
//     }
//   }

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG_DEQUE(name)
//   {
//     c10::IValue ivalue = torch::IValue(deque_to_list(name()));
//     archive.write(#name, ivalue);
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG(T, name)
//   {
//     c10::IValue ivalue;
//     bool exists = archive.try_read(#name, ivalue);
//     if (exists) {
//       name(ivalue.to<T>());
//     } else {
//       bool is_tensor_type = std::is_base_of<torch::Tensor, T>::value;
//       TORCH_INTERNAL_ASSERT(is_tensor_type);
//     }
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_OPTIONAL(T, name)
//   {
//     c10::IValue ivalue;
//     bool exists = archive.try_read(#name, ivalue);
//     if (exists) {
//       name(ivalue.toOptional<T>());
//     }
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_DEQUE(T, name)
//   {
//     c10::IValue ivalue;
//     archive.read(#name, ivalue);
//     auto list = ivalue.to<c10::List<T::value_type>>();
//     name(list_to_deque(list));
//   }

 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/adagrad.h

// #pragma once

// #include <torch/nn/pimpl.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdagradOptions.java


// Targeting ../AdagradParamState.java


// Targeting ../Adagrad.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/adam.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamOptions.java


// Targeting ../AdamParamState.java


// Targeting ../Adam.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/adamw.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamWOptions.java


// Targeting ../AdamWParamState.java


// Targeting ../AdamW.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/lbfgs.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>

// #include <deque>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../LBFGSOptions.java


// Targeting ../LBFGSParamState.java


// Targeting ../LBFGS.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/rmsprop.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <functional>
// #include <memory>
// #include <string>
// #include <vector>
 // namespace serialize

// Targeting ../RMSpropOptions.java


// Targeting ../RMSpropParamState.java


// Targeting ../RMSprop.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/sgd.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../SGDOptions.java


// Targeting ../SGDParamState.java


// Targeting ../SGD.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h

// #pragma once

// #include <torch/optim/optimizer.h>

// #include <torch/csrc/Export.h>
// Targeting ../LRScheduler.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim/schedulers/step_lr.h

// #pragma once

// #include <torch/optim/schedulers/lr_scheduler.h>
// Targeting ../StepLR.java


 // namespace optim
 // namespace torch


// Parsed from torch/csrc/api/include/torch/optim.h

// #pragma once

// #include <torch/optim/adagrad.h>
// #include <torch/optim/adam.h>
// #include <torch/optim/adamw.h>
// #include <torch/optim/lbfgs.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/rmsprop.h>
// #include <torch/optim/sgd.h>

// #include <torch/optim/schedulers/lr_scheduler.h>
// #include <torch/optim/schedulers/step_lr.h>


// Parsed from torch/csrc/api/include/torch/sparse.h

// #pragma once

// #include <ATen/ATen.h>

 // namespace torch


// Parsed from torch/csrc/api/include/torch/special.h

// #pragma once

// #include <ATen/ATen.h>
// #include <torch/types.h>

/** Computes the natural logarithm of the absolute value of the gamma function
 *  See https://pytorch.org/docs/master/special.html#torch.special.gammaln.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::gammaln(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor gammaln(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor gammaln_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the regularized lower incomplete gamma function
 *  See https://pytorch.org/docs/master/special.html#torch.special.gammainc.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  auto s = torch::randn(128, dtype=kDouble);
 *  torch::special::gammainc(s, t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor gammainc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);


///
@Namespace("torch::special") public static native @ByRef Tensor gammainc_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

/** Computes the regularized upper incomplete gamma function
 *  See https://pytorch.org/docs/master/special.html#torch.special.gammainc.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  auto s = torch::randn(128, dtype=kDouble);
 *  torch::special::gammaincc(s, t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor gammaincc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);


///
@Namespace("torch::special") public static native @ByRef Tensor gammaincc_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

/** Computes the multivariate log-gamma function with dimension {@code p}, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.multigammaln.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::multigammaln(t, 1);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor multigammaln(@Const @ByRef Tensor self, @Cast("int64_t") long p);


///
@Namespace("torch::special") public static native @ByRef Tensor multigammaln_out(@ByRef Tensor result, @Const @ByRef Tensor self, @Cast("int64_t") long p);

/** Computes the nth derivative of the digamma function on the input.
 *  See https:://pytorch.org/docs/master/special.html#torch.special.polygamma.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::polygamma(2, t);
 *  }</pre> */

/** Computes the logarithmic derivative of the gamma function on input
 *  See https://pytorch.org/docs/master/special.html#torch.special.psi
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::psi(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor psi(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor psi_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the logarithmic derivative of the gamma function on input
 *  See https://pytorch.org/docs/master/special.html#torch.special.digamma
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::digamma(t);
 *  }</pre> */

/** Computes entropy of input, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.entr.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::entr(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor entr(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor entr_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the error function
 *  See https://pytorch.org/docs/master/special.html#torch.special.erf.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::erf(t);
 *  }</pre> */

/** Computes the complementary error function
 *  See https://pytorch.org/docs/master/special.html#torch.special.erfc.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::erfc(t);
 *  }</pre> */

/** Computes the scaled complementary error function
 *  See https://pytorch.org/docs/master/special.html#torch.special.erfcx.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::erfcx(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor erfcx(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor erfcx_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the inverse error function
 *  See https://pytorch.org/docs/master/special.html#torch.special.erfinv.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::erfinv(t);
 *  }</pre> */

/** Computes the log of summed exponentials of each row of input in the given
 *  dimension dim See
 *  https://pytorch.org/docs/master/special.html#torch.special.logsumexp.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(3, 3);
 *  torch::special::logsumexp(t, 1);
 *  }</pre> */

/** Computes the argument, x, for which the area under the Gaussian probability
 *  density function (integrated from minus infinity to x) is equal to input,
 *  elementwise. See
 *  https://pytorch.org/docs/master/special.html#torch.special.ndtri
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::rand(128, dtype=kDouble);
 *  torch::special::ndtri(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor ndtri(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor ndtri_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the log of area under the standard Gaussian probability density
 *  function, integrated from minus infinity to :attr:{@code input}, elementwise See
 *  https://pytorch.org/docs/master/special.html#torch.special.log_ndtr
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::log_ndtr(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor log_ndtr(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor log_ndtr_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the logit of input, elementwise.
 *  See https://pytorch.org/docs/master/special.html#torch.special.logit.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::logit(t);
 *  }</pre> */

/** Computes the expit (also known as the logistic sigmoid function) of input,
 *  elementwise See
 *  https://pytorch.org/docs/master/special.html#torch.special.expit.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::expit(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor expit(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor expit_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the base two exponential function of :attr:{@code input}, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.exp2.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::exp2(t);
 *  }</pre> */

/** Computes the exponential of the elements minus 1, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.expm1.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::expm1(t);
 *  }</pre> */

/** Computes x * log(y) for inputs, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.xlogy.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto y = torch::randn(128, dtype=kDouble);
 *  torch::special::xlogy(x, y);
 *  }</pre> */

/** Computes x * log1p(y) for inputs, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.xlog1py.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto y = torch::randn(128, dtype=kDouble);
 *  torch::special::xlog1py(x, y);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor xlog1py(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByVal Tensor xlog1py(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByVal Tensor xlog1py(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

@Namespace("torch::special") public static native @ByRef Tensor xlog1py_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByRef Tensor xlog1py_out(
    @ByRef Tensor result,
    @Const @ByRef Scalar self,
    @Const @ByRef Tensor other);


///
@Namespace("torch::special") public static native @ByRef Tensor xlog1py_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar other);

/** Computes Hurwitz Zeta function for inputs, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.zeta.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto y = torch::randn(128, dtype=kDouble);
 *  torch::special::zeta(x, y);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor zeta(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByVal Tensor zeta(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByVal Tensor zeta(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

@Namespace("torch::special") public static native @ByRef Tensor zeta_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("torch::special") public static native @ByRef Tensor zeta_out(
    @ByRef Tensor result,
    @Const @ByRef Scalar self,
    @Const @ByRef Tensor other);


///
@Namespace("torch::special") public static native @ByRef Tensor zeta_out(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Scalar other);

/** Computes the zeroth order modified Bessel function of the first kind of
 *  input, elementwise See
 *  https://pytorch.org/docs/master/special.html#torch.special.i0
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::i0(t);
 *  }</pre> */

/** Computes the area under the standard Gaussian probability density function,
 *  integrated from minus infinity to :attr:{@code input}, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.ndtr
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::ndtr(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor ndtr(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor ndtr_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the exponentially scaled zeroth order modified Bessel function of
 *  the first kind See
 *  https://pytorch.org/docs/master/special.html#torch.special.i0e.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::i0e(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor i0e(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor i0e_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the first order modified Bessel function of the first kind
 *  See https://pytorch.org/docs/master/special.html#torch.special.i1.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::i1(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor i1(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor i1_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the exponentially scaled first order modified Bessel function of
 *  the first kind See
 *  https://pytorch.org/docs/master/special.html#torch.special.i1e.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::i1e(t);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor i1e(@Const @ByRef Tensor self);


///
@Namespace("torch::special") public static native @ByRef Tensor i1e_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Computes the sinc of input, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.sinc.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::sinc(t);
 *  }</pre> */

/** Rounds the elements of the input
 *  See https://pytorch.org/docs/master/special.html#torch.special.round.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::round(t);
 *  }</pre> */

/** Computes log(1 + x) of the input, elementwise
 *  See https://pytorch.org/docs/master/special.html#torch.special.log1p.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, dtype=kDouble);
 *  torch::special::log1p(t);
 *  }</pre> */


/** Computes log followed by softmax(x) of the input
 *  See https://pytorch.org/docs/master/special.html#torch.special.log_softmax.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, 128, dtype=kDouble);
 *  torch::special::log_softmax(t, 0);
 *  }</pre> */

/** Computes softmax of the input along a given dimension
 *  See https://pytorch.org/docs/master/special.html#torch.special.softmax.
 * 
 *  Example:
 *  <pre>{@code
 *  auto t = torch::randn(128, 128, dtype=kDouble);
 *  torch::special::softmax(t, 0);
 *  }</pre> */

/** Airy function Ai.
 * 
 *  See https://pytorch.org/docs/master/special.html#torch.special.airy_ai.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::airy_ai(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor airy_ai(@Const @ByRef Tensor x);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor airy_ai_out(@ByRef Tensor y, @Const @ByRef Tensor x);

/** Bessel function of the first kind of order 0.
 * 
 *  See https://pytorch.org/docs/master/special.html#torch.special.bessel_j0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::bessel_j0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor bessel_j0(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor bessel_j0_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Bessel function of the first kind of order 1.
 * 
 *  See https://pytorch.org/docs/master/special.html#torch.special.bessel_j1.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::bessel_j1(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor bessel_j1(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor bessel_j1_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Bessel function of the second kind of order 0.
 * 
 *  See https://pytorch.org/docs/master/special.html#torch.special.bessel_y0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::bessel_y0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor bessel_y0(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor bessel_y0_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Bessel function of the second kind of order 1.
 * 
 *  See https://pytorch.org/docs/master/special.html#torch.special.bessel_y1.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::bessel_y1(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor bessel_y1(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor bessel_y1_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Chebyshev polynomial of the first kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.chebyshev_polynomial_t.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::chebyshev_polynomial_t(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Chebyshev polynomial of the second kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.chebyshev_polynomial_u.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::chebyshev_polynomial_u(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Chebyshev polynomial of the third kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.chebyshev_polynomial_v.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::chebyshev_polynomial_v(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Chebyshev polynomial of the fourth kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.chebyshev_polynomial_w.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::chebyshev_polynomial_w(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Physicist’s Hermite polynomial.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.hermite_polynomial_h.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::hermite_polynomial_h(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_h(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_h_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_h_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_h_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Probabilist’s Hermite polynomial.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.hermite_polynomial_he.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::hermite_polynomial_he(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_he(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_he_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_he_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor hermite_polynomial_he_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Laguerre polynomial.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.laguerre_polynomial_l.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::laguerre_polynomial_l(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor laguerre_polynomial_l(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor laguerre_polynomial_l_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor laguerre_polynomial_l_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor laguerre_polynomial_l_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Legendre polynomial.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.legendre_polynomial_p.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::legendre_polynomial_p(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor legendre_polynomial_p(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor legendre_polynomial_p_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor legendre_polynomial_p_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor legendre_polynomial_p_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Modified Bessel function of the first kind of order 0.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.modified_bessel_i0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::modified_bessel_i0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor modified_bessel_i0(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor modified_bessel_i0_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Modified Bessel function of the first kind of order 1.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.modified_bessel_i1.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::modified_bessel_i1(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor modified_bessel_i1(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor modified_bessel_i1_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Modified Bessel function of the second kind of order 0.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.modified_bessel_k0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::modified_bessel_k0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor modified_bessel_k0(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor modified_bessel_k0_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Modified Bessel function of the second kind of order 1.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.modified_bessel_k1.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::modified_bessel_k1(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor modified_bessel_k1(@Const @ByRef Tensor self);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor modified_bessel_k1_out(@ByRef Tensor result, @Const @ByRef Tensor self);

/** Scaled modified Bessel function of the second kind of order 0.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.scaled_modified_bessel_k0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::scaled_modified_bessel_k0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor scaled_modified_bessel_k0(@Const @ByRef Tensor x);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor scaled_modified_bessel_k0_out(@ByRef Tensor y, @Const @ByRef Tensor x);

/** Scaled modified Bessel function of the second kind of order 1.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.scaled_modified_bessel_k1.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::scaled_modified_bessel_k1(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor scaled_modified_bessel_k1(@Const @ByRef Tensor x);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor scaled_modified_bessel_k1_out(@ByRef Tensor y, @Const @ByRef Tensor x);

/** Shifted Chebyshev polynomial of the first kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.shifted_chebyshev_polynomial_t.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::shifted_chebyshev_polynomial_t(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_t_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Shifted Chebyshev polynomial of the second kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.shifted_chebyshev_polynomial_u.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::shifted_chebyshev_polynomial_u(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_u_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Shifted Chebyshev polynomial of the third kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.shifted_chebyshev_polynomial_v.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::shifted_chebyshev_polynomial_v(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_v_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Shifted Chebyshev polynomial of the fourth kind.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.shifted_chebyshev_polynomial_w.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 *  auto n = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::shifted_chebyshev_polynomial_w(x, n);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByVal Tensor shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Tensor n);

@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Scalar x,
    @Const @ByRef Tensor n);


///
///
///
///
@Namespace("torch::special") public static native @ByRef Tensor shifted_chebyshev_polynomial_w_out(
    @ByRef Tensor output,
    @Const @ByRef Tensor x,
    @Const @ByRef Scalar n);

/** Spherical Bessel function of the first kind of order 0.
 * 
 *  See
 *  https://pytorch.org/docs/master/special.html#torch.special.spherical_bessel_j0.
 * 
 *  Example:
 * 
 *  <pre>{@code
 *  auto x = torch::randn(128, dtype=kDouble);
 * 
 *  torch::special::spherical_bessel_j0(x);
 *  }</pre> */
@Namespace("torch::special") public static native @ByVal Tensor spherical_bessel_j0(@Const @ByRef Tensor x);

@Namespace("torch::special") public static native @ByRef Tensor spherical_bessel_j0_out(@ByRef Tensor y, @Const @ByRef Tensor x);
 // namespace special
 // namespace torch


// Parsed from torch/csrc/api/include/torch/version.h

// #pragma once

/** Indicates the major version of LibTorch. */
public static final int TORCH_VERSION_MAJOR = 2;

/** Indicates the minor version of LibTorch. */
public static final int TORCH_VERSION_MINOR = 1;

/** Indicates the patch version of LibTorch. */
public static final int TORCH_VERSION_PATCH = 2;

/** Indicates the version of LibTorch. */
public static final String TORCH_VERSION = 
  "2.1.2";


// Parsed from torch/csrc/autograd/InferenceMode.h

// #pragma once

// #include <c10/core/InferenceMode.h>
// #include <torch/csrc/Export.h>


 // namespace torch


// Parsed from caffe2/serialize/read_adapter_interface.h

// #pragma once

// #include <cstddef>
// #include <cstdint>

// #include "c10/macros/Macros.h"
// Targeting ../ReadAdapterInterface.java



 // namespace serialize
 // namespace caffe2


// Parsed from caffe2/serialize/istream_adapter.h

// #pragma once

// #include <istream>

// #include "c10/macros/Macros.h"
// #include "caffe2/serialize/read_adapter_interface.h"
// Targeting ../IStreamAdapter.java



 // namespace serialize
 // namespace caffe2


// Parsed from caffe2/serialize/versions.h

// #pragma once
// #include <cstdint>

@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kMinSupportedFileFormatVersion();

@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kMaxSupportedFileFormatVersion();

// Versions (i.e. why was the version number bumped?)

// Note [Dynamic Versions and torch.jit.save vs. torch.save]
//
// Our versioning scheme has a "produced file format version" which
// describes how an archive is to be read. The version written in an archive
// is at least this current produced file format version, but may be greater
// if it includes certain symbols. We refer to these conditional versions
// as "dynamic," since they are identified at runtime.
//
// Dynamic versioning is useful when an operator's semantics are updated.
// When using torch.jit.save we want those semantics to be preserved. If
// we bumped the produced file format version on every change, however,
// then older versions of PyTorch couldn't read even simple archives, like
// a single tensor, from newer versions of PyTorch. Instead, we
// assign dynamic versions to these changes that override the
// produced file format version as needed. That is, when the semantics
// of torch.div changed it was assigned dynamic version 4, and when
// torch.jit.saving modules that use torch.div those archives also have
// (at least) version 4. This prevents earlier versions of PyTorch
// from accidentally performing the wrong kind of division. Modules
// that don't use torch.div or other operators with dynamic versions
// can write the produced file format version, and these programs will
// run as expected on earlier versions of PyTorch.
//
// While torch.jit.save attempts to preserve operator semantics,
// torch.save does not. torch.save is analogous to pickling Python, so
// a function that uses torch.div will have different behavior if torch.saved
// and torch.loaded across PyTorch versions. From a technical perspective,
// torch.save ignores dynamic versioning.

// 1. Initial version
// 2. Removed op_version_set version numbers
// 3. Added type tags to pickle serialization of container types
// 4. (Dynamic) Stopped integer division using torch.div
//      (a versioned symbol preserves the historic behavior of versions 1--3)
// 5. (Dynamic) Stops torch.full inferring a floating point dtype
//      when given bool or integer fill values.
// 6. Write version string to `./data/version` instead of `version`.

// [12/15/2021]
// kProducedFileFormatVersion is set to 7 from 3 due to a different
// interpretation of what file format version is.
// Whenever there is new upgrader introduced,
// this number should be bumped.
// The reasons that version is bumped in the past:
//     1. aten::div is changed at version 4
//     2. aten::full is changed at version 5
//     3. torch.package uses version 6
//     4. Introduce new upgrader design and set the version number to 7
//        mark this change
// --------------------------------------------------
// We describe new operator version bump reasons here:
// 1) [01/24/2022]
//     We bump the version number to 8 to update aten::linspace
//     and aten::linspace.out to error out when steps is not
//     provided. (see: https://github.com/pytorch/pytorch/issues/55951)
// 2) [01/30/2022]
//     Bump the version number to 9 to update aten::logspace and
//     and aten::logspace.out to error out when steps is not
//     provided. (see: https://github.com/pytorch/pytorch/issues/55951)
// 3) [02/11/2022]
//     Bump the version number to 10 to update aten::gelu and
//     and aten::gelu.out to support the new approximate kwarg.
//     (see: https://github.com/pytorch/pytorch/pull/61439)
@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kProducedFileFormatVersion();

// Absolute minimum version we will write packages. This
// means that every package from now on will always be
// greater than this number.
@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kMinProducedFileFormatVersion();

// The version we write when the archive contains bytecode.
// It must be higher or eq to kProducedFileFormatVersion.
// Because torchscript changes is likely introduce bytecode change.
// If kProducedFileFormatVersion is increased, kProducedBytecodeVersion
// should be increased too. The relationship is:
// kMaxSupportedFileFormatVersion >= (most likely ==) kProducedBytecodeVersion
//   >= kProducedFileFormatVersion
// If a format change is forward compatible (still readable by older
// executables), we will not increment the version number, to minimize the
// risk of breaking existing clients. TODO: A better way would be to allow
// the caller that creates a model to specify a maximum version that its
// clients can accept.
// Versions:
//  0x1L: Initial version
//  0x2L: (Comment missing)
//  0x3L: (Comment missing)
//  0x4L: (update) Added schema to function tuple. Forward-compatible change.
//  0x5L: (update) Update bytecode is sharing constant tensor files from
//  torchscript, and only serialize extra tensors that are not in the
//  torchscript constant table. Also update tensor storage schema adapting to
//  the unify format, the root key of tensor storage is updated from {index} to
//  {the_pointer_value_the_tensor.storage}, for example:
//  `140245072983168.storage` Forward-compatibility change.
//  0x6L: Implicit opereator versioning using number of specified argument.
//  Refer to the summary of https://github.com/pytorch/pytorch/pull/56845 for
//  details.
//  0x7L: Enable support for operators with default arguments plus out
//  arguments. Refer. See https://github.com/pytorch/pytorch/pull/63651 for
//  details.
//  0x8L: Emit promoted operators as instructions. See
//  https://github.com/pytorch/pytorch/pull/71662 for details.
//  0x9L: Change serialization format from pickle to format This version is to
//  serve migration. v8 pickle and v9 flatbuffer are the same. Refer to the
//  summary of https://github.com/pytorch/pytorch/pull/75201 for more details.
@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kProducedBytecodeVersion();

// static_assert(
//     kProducedBytecodeVersion >= kProducedFileFormatVersion,
//     "kProducedBytecodeVersion must be higher or equal to
//     kProducedFileFormatVersion.");

// Introduce kMinSupportedBytecodeVersion and kMaxSupportedBytecodeVersion
// for limited backward/forward compatibility support of bytecode. If
// kMinSupportedBytecodeVersion <= model_version <= kMaxSupportedBytecodeVersion
// (in loader), we should support this model_version. For example, we provide a
// wrapper to handle an updated operator.
@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kMinSupportedBytecodeVersion();
@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const uint64_t") long kMaxSupportedBytecodeVersion();

 // namespace serialize
 // namespace caffe2


// Parsed from torch/csrc/jit/serialization/unpickler.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/script_type_parser.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// Targeting ../DeserializationStorageContext.java


// Targeting ../Unpickler.java





 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/script_type_parser.h

// #pragma once
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/resolver.h>
// #include <torch/csrc/jit/frontend/tree_views.h>
// Targeting ../ScriptTypeParser.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/resolver.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/jit/frontend/sugared_value.h>
// Targeting ../Resolver.java


// Targeting ../NativeResolver.java



@Namespace("torch::jit") public static native @SharedPtr NativeResolver nativeResolver();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/sugared_value.h

// #pragma once
// #include <c10/util/Optional.h>
// #include <functional>
// #include <memory>
// #include <string>
// #include <utility>

// #include <ATen/core/symbol.h>
// #include <caffe2/serialize/versions.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/schema_matching.h>
// #include <torch/csrc/jit/frontend/versioned_symbols.h>
// #include <torch/csrc/jit/ir/ir.h>
// Targeting ../SugaredValue.java


// Targeting ../SimpleValue.java


// Targeting ../BuiltinFunction.java


// Targeting ../SugaredTupleValue.java


// Targeting ../BuiltinModule.java


// Targeting ../ClassValue.java


// Targeting ../NamedTupleConstructor.java


// Targeting ../FunctionValue.java


// Targeting ../ClosureValue.java


// Targeting ../MethodValue.java


// Targeting ../PrintValue.java


// Targeting ../CastValue.java


// Targeting ../TensorCastValue.java


// Targeting ../MagicMethod.java


// Targeting ../SpecialFormValue.java


// Targeting ../LegacyTensorConstructor.java


// Targeting ../RangeValue.java



// Specialized Tree structure to matched against for special handling
// of builtin functions iterables expressions like zip(), enumerate(), etc.
// zip and enumerate can be modeled as a tree of SimpleValue/RangeValue:
//    zip(x, y) ->  (x, y) with tuple assignment to each loop target
//    enumerate(x) -> (range(0, math.inf, 1), x)
// So a complicated expression like zip(a, enumerate(b), range(0, 100)) will be:
// (a, (range(0, math.inf, 1), b), range(0, 100))
// We use those base iterables to fill in the loop information like
// max_trip_count and set the value table for loop targets
// Iterables can contain lists of SugaredValues like ModuleLists. If it
// does, then we emit it unrolled and require that all values it contains
// have a statically-determinable length.

@Namespace("torch::jit") public static native @ByVal ValueVector toValues(
    @ByRef Graph g,
    @ByVal NamedValueArrayRef nvs);
// Targeting ../SimpleSelf.java


// Targeting ../ExceptionMessageValue.java


// Targeting ../ExceptionValue.java


// Targeting ../SugaredEnumClass.java


// Targeting ../SliceValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/error_report.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/frontend/tree.h>
// Targeting ../Call.java


// Targeting ../ErrorReport.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree.h

// #pragma once

// #include <functional>
// #include <memory>
// #include <unordered_map>
// #include <vector>

// #include <c10/util/SmallVector.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/jit/frontend/lexer.h>

// Trees are used to represent all forms of TC IR, pre- and post-typechecking.
// Rather than have a full class hierarchy for all TC statements, trees are a
// slight variation of Lisp s-expressions. For instance, the expression a*b+1
// is represented as:
// (+ (* (ident a) (ident b)) (const 1))
// Atoms like 'a', 'b', and '1' are represented by subclasses of Tree which
// define stringValue(). Everything else is a Compound object, which has a
// 'kind' that is a token from lexer.h's TokenKind enum. Single-character
// operators like '+' are represented using the character itself (so, add.kind()
// would be '+'). Each Compound object also contains a list of subtrees and is
// associated with a SourceRange for error reporting.
// Memory management of trees is done using intrusive_ptr.
// Targeting ../Tree.java


// Targeting ../JitString.java



@Namespace("torch::jit") public static native @ByVal SourceRange mergeRanges(@ByVal SourceRange c, @Cast("const torch::jit::TreeList*") @ByRef SymDimVector others);
// Targeting ../Compound.java


// Targeting ../pretty_tree.java



@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal pretty_tree t_);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef TreeRef t);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/lexer.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/parser_constants.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <algorithm>
// #include <clocale>
// #include <cstdlib>
// #include <memory>
// #include <sstream>
// #include <string>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// single character tokens are just the character itself '+'
// multi-character tokens need an entry here
// if the third entry is not the empty string, it is used
// in the lexer to match this token.

// These kinds are also used in Tree.h as the kind of the AST node.
// Some kinds TK_APPLY, TK_LIST are only used in the AST and are not seen in the
// lexer.

// #define TC_FORALL_TOKEN_KINDS(_)
//   _(TK_EOF, "eof", "")
//   _(TK_WHITESPACE, "whitespace", "")
//   _(TK_WHITESPACE_EOF, "whitespace_eof", "")
//   _(TK_NUMBER, "number", "")
//   _(TK_NEWLINE, "newline", "")
//   _(TK_INDENT, "indent", "")
//   _(TK_DEDENT, "dedent", "")
//   _(TK_DEF, "def", "def")
//   _(TK_EQUIVALENT, "equivalent", "<=>")
//   _(TK_IDENT, "ident", "")
//   _(TK_STRING, "string", "")
//   _(TK_STRINGLITERAL, "string_literal", "")
//   _(TK_CONST, "const", "")
//   _(TK_LIST, "list", "")
//   _(TK_DICT, "dict", "")
//   _(TK_OPTION, "option", "")
//   _(TK_APPLY, "apply", "")
//   _(TK_COMPREHENSION, "comprehension", "")
//   _(TK_RANGE_CONSTRAINT, "range_constraint", "")
//   _(TK_PARAM, "param", "")
//   _(TK_INFERRED, "inferred", "")
//   _(TK_ACCESS, "access", "")
//   _(TK_ASSIGN, "assign", "")
//   _(TK_AUG_ASSIGN, "aug_assign", "")
//   _(TK_ATTRIBUTE, "attribute", "")
//   _(TK_IF, "if", "if")
//   _(TK_ELSE, "else", "else")
//   _(TK_ELIF, "elif", "elif")
//   _(TK_WHILE, "while", "while")
//   _(TK_EXPR_STMT, "expression statement", "")
//   _(TK_RETURN, "return", "return")
//   _(TK_IS, "is", "is")
//   _(TK_ISNOT, "is not", "is not")
//   _(TK_NE, "ne", "!=")
//   _(TK_EQ, "eq", "==")
//   _(TK_LE, "le", "<=")
//   _(TK_GE, "ge", ">=")
//   _(TK_FLOOR_DIV, "floordiv", "//")
//   _(TK_IF_EXPR, "if", "")
//   _(TK_TRUE, "True", "True")
//   _(TK_FALSE, "False", "False")
//   _(TK_NONE, "None", "None")
//   _(TK_AND, "and", "and")
//   _(TK_OR, "or", "or")
//   _(TK_NOT, "not", "not")
//   _(TK_LSHIFT, "<<", "<<")
//   _(TK_RSHIFT, ">>", ">>")
//   _(TK_CAST, "cast", "")
//   _(TK_PLUS_EQ, "+=", "+=")
//   _(TK_MINUS_EQ, "-=", "-=")
//   _(TK_TIMES_EQ, "*=", "*=")
//   _(TK_DIV_EQ, "/=", "/=")
//   _(TK_MOD_EQ, "%=", "%=")
//   _(TK_BIT_OR_EQ, "|=", "|=")
//   _(TK_BIT_AND_EQ, "&=", "&=")
//   _(TK_BIT_XOR_EQ, "^=", "^=")
//   _(TK_LSHIFT_EQ, "<<=", "<<=")
//   _(TK_RSHIFT_EQ, ">>=", ">>=")
//   _(TK_POW_EQ, "**=", "**=")
//   _(TK_GLOBAL, "global", "global")
//   _(TK_BUILT_IN, "built-in", "")
//   _(TK_SUBSCRIPT, "subscript", "")
//   _(TK_VAR, "variable", "")
//   _(TK_NOTHING, "nothing", "")
//   _(TK_DICT_LITERAL, "dict-literal", "")
//   _(TK_LIST_LITERAL, "list-literal", "")
//   _(TK_TUPLE_LITERAL, "tuple-literal", "")
//   _(TK_FOR, "for", "for")
//   _(TK_IN, "in", "in")
//   _(TK_NOTIN, "not in", "not in")
//   _(TK_STARRED, "starred", "")
//   _(TK_UNARY_MINUS, "unary minus", "")
//   _(TK_POW, "pow operator", "**")
//   _(TK_ARROW, "arrow", "->")
//   _(TK_DECL, "decl", "")
//   _(TK_SLICE_EXPR, "slice expr", "")
//   _(TK_TYPE_COMMENT, "type comment", "# type:")
//   _(TK_RAISE, "raise", "raise")
//   _(TK_ASSERT, "assert", "assert")
//   _(TK_DOTS, "dots", "...")
//   _(TK_LIST_COMP, "list comprehension", "")
//   _(TK_DICT_COMP, "dict comprehension", "")
//   _(TK_BREAK, "break", "break")
//   _(TK_CONTINUE, "continue", "continue")
//   _(TK_DELETE, "del", "del")
//   _(TK_PASS, "pass", "pass")
//   _(TK_CLASS_DEF, "class", "class")
//   _(TK_IMPORT, "import", "import")
//   _(TK_WITH, "with", "with")
//   _(TK_WITH_ITEM, "withitem", "")
//   _(TK_AS, "as", "as")
//   _(TK_PROP, "property", "")
//   _(TK_ELLIPSIS, "Ellipsis", "Ellipsis")
//   _(TK_NONE_TYPE, "NoneType", "NoneType")

@Namespace("torch::jit") public enum TokenKind {
  // we use characters to represent themselves so skip all valid characters
  // before
  // assigning enum values to multi-char tokens.
  TK_DUMMY_START(256),
  TK_EOF(257),
  TK_WHITESPACE(258),
  TK_WHITESPACE_EOF(259),
  TK_NUMBER(260),
  TK_NEWLINE(261),
  TK_INDENT(262),
  TK_DEDENT(263),
  TK_DEF(264),
  TK_EQUIVALENT(265),
  TK_IDENT(266),
  TK_STRING(267),
  TK_STRINGLITERAL(268),
  TK_CONST(269),
  TK_LIST(270),
  TK_DICT(271),
  TK_OPTION(272),
  TK_APPLY(273),
  TK_COMPREHENSION(274),
  TK_RANGE_CONSTRAINT(275),
  TK_PARAM(276),
  TK_INFERRED(277),
  TK_ACCESS(278),
  TK_ASSIGN(279),
  TK_AUG_ASSIGN(280),
  TK_ATTRIBUTE(281),
  TK_IF(282),
  TK_ELSE(283),
  TK_ELIF(284),
  TK_WHILE(285),
  TK_EXPR_STMT(286),
  TK_RETURN(287),
  TK_IS(288),
  TK_ISNOT(289),
  TK_NE(290),
  TK_EQ(291),
  TK_LE(292),
  TK_GE(293),
  TK_FLOOR_DIV(294),
  TK_IF_EXPR(295),
  TK_TRUE(296),
  TK_FALSE(297),
  TK_NONE(298),
  TK_AND(299),
  TK_OR(300),
  TK_NOT(301),
  TK_LSHIFT(302),
  TK_RSHIFT(303),
  TK_CAST(304),
  TK_PLUS_EQ(305),
  TK_MINUS_EQ(306),
  TK_TIMES_EQ(307),
  TK_DIV_EQ(308),
  TK_MOD_EQ(309),
  TK_BIT_OR_EQ(310),
  TK_BIT_AND_EQ(311),
  TK_BIT_XOR_EQ(312),
  TK_LSHIFT_EQ(313),
  TK_RSHIFT_EQ(314),
  TK_POW_EQ(315),
  TK_GLOBAL(316),
  TK_BUILT_IN(317),
  TK_SUBSCRIPT(318),
  TK_VAR(319),
  TK_NOTHING(320),
  TK_DICT_LITERAL(321),
  TK_LIST_LITERAL(322),
  TK_TUPLE_LITERAL(323),
  TK_FOR(324),
  TK_IN(325),
  TK_NOTIN(326),
  TK_STARRED(327),
  TK_UNARY_MINUS(328),
  TK_POW(329),
  TK_ARROW(330),
  TK_DECL(331),
  TK_SLICE_EXPR(332),
  TK_TYPE_COMMENT(333),
  TK_RAISE(334),
  TK_ASSERT(335),
  TK_DOTS(336),
  TK_LIST_COMP(337),
  TK_DICT_COMP(338),
  TK_BREAK(339),
  TK_CONTINUE(340),
  TK_DELETE(341),
  TK_PASS(342),
  TK_CLASS_DEF(343),
  TK_IMPORT(344),
  TK_WITH(345),
  TK_WITH_ITEM(346),
  TK_AS(347),
  TK_PROP(348),
  TK_ELLIPSIS(349),
  TK_NONE_TYPE(350);

    public final int value;
    private TokenKind(int v) { this.value = v; }
    private TokenKind(TokenKind e) { this.value = e.value; }
    public TokenKind intern() { for (TokenKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::jit") public static native @StdString BytePointer kindToString(int kind);
@Namespace("torch::jit") public static native int stringToKind(@StdString BytePointer str);
@Namespace("torch::jit") public static native int stringToKind(@StdString String str);

// nested hash tables that indicate char-by-char what is a valid token.
// Targeting ../SharedParserData.java



@Namespace("torch::jit") public static native @ByRef SharedParserData sharedParserData();
// Targeting ../Token.java


 // namespace jit
 // namespace torch



// Parsed from caffe2/serialize/inline_container.h

// #pragma once

// #include <cerrno>
// #include <cstdio>
// #include <cstring>
// #include <fstream>
// #include <istream>
// #include <mutex>
// #include <ostream>
// #include <unordered_set>

// #include <c10/core/Allocator.h>
// #include <c10/core/Backend.h>

// #include "caffe2/serialize/istream_adapter.h"
// #include "caffe2/serialize/read_adapter_interface.h"
// #include "caffe2/serialize/versions.h"

// PyTorch containers are a special zip archive with the following layout
// archive_name.zip contains:
//    archive_name/
//        version # a file with a single decimal number written in ascii,
//                # used to establish the version of the archive format
//        model.json # overall model description, this is a json output of
//                   # ModelDef from torch.proto
//        # the following names are by convention only, model.json will
//        # refer to these files by full names
//        tensors/
//          0 # flat storage for tensor data, meta-data about shapes, etc. is
//            # in model.json
//          1
//          ...
//        # code entries will only exist for modules that have methods attached
//        code/
//          archive_name.py # serialized torch script code (python syntax, using
//          PythonPrint) archive_name_my_submodule.py # submodules have separate
//          files
//
// The PyTorchStreamWriter also ensures additional useful properties for these
// files
// 1. All files are stored uncompressed.
// 2. All files in the archive are aligned to 64 byte boundaries such that
//    it is possible to mmap the entire file and get an aligned pointer to
//    tensor data.
// 3. We universally write in ZIP64 format for consistency.

// The PyTorchStreamReader also provides additional properties:
// 1. It can read zip files that are created with common
//    zip tools. This means that even though our writer doesn't compress files,
//    the reader can still read files that were compressed.
// 2. It provides a getRecordOffset function which returns the offset into the
//    raw file where file data lives. If the file was written with
//    PyTorchStreamWriter it is guaranteed to be 64 byte aligned.

// PyTorchReader/Writer handle checking the version number on the archive format
// and ensure that all files are written to a archive_name directory so they
// unzip cleanly.

// When developing this format we want to pay particular attention to the
// following use cases:
//
// -- Reading --
// 1) Reading with full random access
//   a) Reading with file api's such as fread()
//   b) mmaping the file and jumping around the mapped region
// 2) Reading with 1-pass sequential access
//      -> A reader will need to build up a data structure of parsed structures
//         as it reads
//
// -- Writing --
// 1) Writing with full random access
// 2) Writing with 1-pass sequential access
//      -> We must take care not to require updating values that have already
//         been written. We place the variable-length index at the end and do
//         not put any indicies into the header to fulfill this constraint.

// The model.json, which contains all the metadata information,
// should be written as the last file. One reason is that the size of tensor
// data is usually stable. As long as the shape and type of the tensor do not
// change, the size of the data won't change. On the other sied, the size of the
// serialized model is likely to change, so we store it as the last record, and
// we don't need to move previous records when updating the model data.

// The zip format is sufficiently flexible to handle the above use-case.
// it puts its central directory at the end of the archive and we write
// model.json as the last file when writing after we have accumulated all
// other information.

@Namespace("caffe2::serialize") @MemberGetter public static native @Cast("const char*") BytePointer kSerializationIdRecordName();
// Targeting ../PyTorchStreamReader.java


// Writer-specific constants
@Namespace("caffe2::serialize::detail") @MemberGetter public static native @Cast("const uint64_t") long kFieldAlignment();

// Returns a record to be appended to the local user extra data entry in order
// to make data beginning aligned at kFieldAlignment bytes boundary.

 // namespace detail

 // namespace serialize
 // namespace caffe2


// Parsed from torch/csrc/jit/serialization/import.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>

// #include <istream>
 // namespace serialize
 // namespace caffe2

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

// For reading unified serialization format from torch.Package
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString BytePointer ts_id);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString String ts_id);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given {@code istream}.
 * 
 *  The istream must contain a serialized {@code Module}, exported via
 *  {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given {@code filename}.
 * 
 *  The file stored at the location given in {@code filename} must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given shared_ptr {@code rai}.
 * 
 *  The reader adapter, which is for customized input stream, must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule jitModuleFromSourceAndConstants(
    @Const @ByRef IValue ivalue,
    @Const @ByRef ExtraFilesMap source,
    @Const @ByRef IValueVector constants,
    int version);

@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr BytePointer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr BytePointer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr ByteBuffer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr ByteBuffer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr byte[] data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr byte[] data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString BytePointer filename,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString BytePointer filename,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString String filename,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString String filename,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_stream(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_stream(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByRef ExtraFilesMap extra_files);

 // namespace jit
 // namespace torch


// Parsed from c10/util/FbcodeMaps.h

// #ifndef C10_UTIL_FBCODEMAPS_H_
// #define C10_UTIL_FBCODEMAPS_H_

// Map typedefs so that we can use folly's F14 maps in fbcode without
// taking a folly dependency.

// #ifdef FBCODE_CAFFE2
// #include <folly/container/F14Map.h>
// #include <folly/container/F14Set.h>
// #else
// #include <unordered_map>
// #include <unordered_set>
// #endif
// #ifdef FBCODE_CAFFE2
// #else
// #endif
 // namespace c10

// #endif // C10_UTIL_FBCODEMAPS_H_


// Parsed from torch/csrc/jit/serialization/pickler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <string>
// #include <utility>
// #include <vector>

// #include <ATen/Utils.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/FbcodeMaps.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/string_view.h>
// #include <torch/csrc/Export.h>

// See Python's pickletools.py for a detailed description of each of these codes
@Namespace("torch::jit") public enum PickleOpCode {
  MARK((byte)('(')),
  STOP((byte)('.')),
  POP((byte)('0')),
  POP_MARK((byte)('1')),
  DUP((byte)('2')),
  FLOAT((byte)('F')),
  INT((byte)('I')),
  BININT((byte)('J')),
  BININT1((byte)('K')),
  LONG((byte)('L')),
  BININT2((byte)('M')),
  NONE((byte)('N')),
  PERSID((byte)('P')),
  BINPERSID((byte)('Q')),
  REDUCE((byte)('R')),
  STRING((byte)('S')),
  BINSTRING((byte)('T')),
  SHORT_BINSTRING((byte)('U')),
  // NB: Avoid using UNICODE as it is a macro in the Windows API
  UNICODE_((byte)('V')),
  BINUNICODE((byte)('X')),
  APPEND((byte)('a')),
  BUILD((byte)('b')),
  GLOBAL((byte)('c')),
  DICT((byte)('d')),
  EMPTY_DICT((byte)('}')),
  APPENDS((byte)('e')),
  GET((byte)('g')),
  BINGET((byte)('h')),
  INST((byte)('i')),
  LONG_BINGET((byte)('j')),
  LIST((byte)('l')),
  EMPTY_LIST((byte)(']')),
  OBJ((byte)('o')),
  PUT((byte)('p')),
  BINPUT((byte)('q')),
  LONG_BINPUT((byte)('r')),
  SETITEM((byte)('s')),
  TUPLE((byte)('t')),
  EMPTY_TUPLE((byte)(')')),
  SETITEMS((byte)('u')),
  BINFLOAT((byte)('G')),

  // Protocol 2
  PROTO((byte)(0x80)),
  NEWOBJ((byte)(0x81)),
  EXT1((byte)(0x82)),
  EXT2((byte)(0x83)),
  EXT4((byte)(0x84)),
  TUPLE1((byte)(0x85)),
  TUPLE2((byte)(0x86)),
  TUPLE3((byte)(0x87)),
  NEWTRUE((byte)(0x88)),
  NEWFALSE((byte)(0x89)),
  LONG1((byte)(0x8a)),
  LONG4((byte)(0x8b)),

  // Protocol 3 (Python 3.x)
  BINBYTES((byte)('B')),
  SHORT_BINBYTES((byte)('C')),

  // Protocol 4
  SHORT_BINUNICODE((byte)(0x8c)),
  BINUNICODE8((byte)(0x8d)),
  BINBYTES8((byte)(0x8e)),
  EMPTY_SET((byte)(0x8f)),
  ADDITEMS((byte)(0x90)),
  FROZENSET((byte)(0x91)),
  NEWOBJ_EX((byte)(0x92)),
  STACK_GLOBAL((byte)(0x93)),
  MEMOIZE((byte)(0x94)),
  FRAME((byte)(0x95));

    public final byte value;
    private PickleOpCode(byte v) { this.value = v; }
    private PickleOpCode(PickleOpCode e) { this.value = e.value; }
    public PickleOpCode intern() { for (PickleOpCode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../WriteableTensorData.java





// Targeting ../Pickler.java



// returns a (tensor, record_size) for a tensor, converting it to a CPU tensor
// if it was CUDA and to_cpu is True.
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor, @Cast("bool") boolean to_cpu/*=true*/);
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor);

// return the value of the tensor's storage pointer


// if the cls has __getstate__/__setstate__
// assert they have the right schema and return true,
// otherwise return false


// Declare BackendMeta serialization and deserialization function pointer types.

// A allowlist of device type, currently available is PrivateUse1
@Namespace("torch::jit") public static native @ByRef DeviceTypeSet GetBackendMetaAllowlist();

// Dynamically obtain serialization function pairs
// that require the corresponding backend.
@Namespace("torch::jit") public static native @Cast("std::array<c10::optional<std::pair<torch::jit::BackendMetaPtr,torch::jit::BackendMetaPtr> >,at::COMPILE_TIME_MAX_DEVICE_TYPES>*") @ByRef PointerPairOptional GetBackendMetaSerialization();

// Register function pointer of Tensor BackendMetadata for serialization.
@Namespace("torch::jit") public static native void TensorBackendMetaRegistry(
    DeviceType t,
    @ByVal @Cast("torch::jit::BackendMetaPtr*") Pointer get_fptr,
    @ByVal @Cast("torch::jit::BackendMetaPtr*") Pointer set_fptr);
@Namespace("torch::jit") public static native void TensorBackendMetaRegistry(
    @Cast("c10::DeviceType") byte t,
    @ByVal @Cast("torch::jit::BackendMetaPtr*") Pointer get_fptr,
    @ByVal @Cast("torch::jit::BackendMetaPtr*") Pointer set_fptr);

// Return a map of Tensor Metadata which including BackendMetaData for
// serialization. For now, it only takes care of `conj` and `neg` bit.
@Namespace("torch::jit") public static native @ByVal StringBoolMap getTensorMetadata(
    @Const @ByRef Tensor t);

// set Tensor Metadata based on the map.
// Refer: getTensorMetadata
@Namespace("torch::jit") public static native void setTensorMetadata(
    @Const @ByRef Tensor t,
    @ByVal StringBoolMap metadata);

// set Tensor metadata based on the map.
// NOTE: This overload is required by unpickler.cpp
@Namespace("torch::jit") public static native void setTensorMetadata(
    @Const @ByRef Tensor t,
    @ByVal GenericDict metadata_idict);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/parser_constants.h

// #pragma once
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer valid_single_char_tokens(); public static native void valid_single_char_tokens(BytePointer setter);
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/strtod.h

// #pragma once

// #include <c10/macros/Macros.h>

@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/schema_matching.h

// #pragma once
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/ir/named_value.h>

// #include <ATen/core/function_schema.h>
// Targeting ../MatchedSchema.java



@Namespace("torch::jit") public static native @Cast("bool") boolean isBlockListedSchema(@Const @ByRef FunctionSchema schema);

@Namespace("torch::jit") public static native @ByVal MatchedSchema matchSchema(
    @Const @ByRef FunctionSchema schema,
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs,
    @Const @ByRef(nullValue = "c10::optional<torch::jit::NamedValue>(c10::nullopt)") NamedValueOptional self);
@Namespace("torch::jit") public static native @ByVal MatchedSchema matchSchema(
    @Const @ByRef FunctionSchema schema,
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs);

@Namespace("torch::jit") public static native @ByVal SizeTMatchedSchemaPair matchSchemas(
    @Const @ByRef FunctionSchemaVector schemas,
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs,
    @Const @ByRef(nullValue = "c10::optional<torch::jit::NamedValue>(c10::nullopt)") NamedValueOptional self,
    @Cast("bool") boolean render_errors/*=false*/);
@Namespace("torch::jit") public static native @ByVal SizeTMatchedSchemaPair matchSchemas(
    @Const @ByRef FunctionSchemaVector schemas,
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs);

@Namespace("torch::jit") public static native @Cast("bool") boolean convertibleToList(
    @Const @ByRef Type.TypePtr type,
    @Const @ByRef Type.TypePtr list_type_);

@Namespace("torch::jit") public static native @StdString BytePointer getFullSchemaName(@Const @ByRef FunctionSchema schema);

@Namespace("torch::jit") public static native Value emitBuiltinCall(
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal Symbol name,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs,
    @Const @ByRef(nullValue = "c10::optional<torch::jit::NamedValue>(c10::nullopt)") NamedValueOptional self);
@Namespace("torch::jit") public static native Value emitBuiltinCall(
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @ByVal Symbol name,
    @ByVal NamedValueArrayRef args,
    @ByVal NamedValueArrayRef kwargs);

@Namespace("torch::jit") public static native @ByVal SizeTOptional findInputWithName(
    @StdString BytePointer name,
    @ByVal NamedValueArrayRef kwargs,
    @Cast("bool") boolean is_aten/*=false*/);
@Namespace("torch::jit") public static native @ByVal SizeTOptional findInputWithName(
    @StdString BytePointer name,
    @ByVal NamedValueArrayRef kwargs);
@Namespace("torch::jit") public static native @ByVal SizeTOptional findInputWithName(
    @StdString String name,
    @ByVal NamedValueArrayRef kwargs,
    @Cast("bool") boolean is_aten/*=false*/);
@Namespace("torch::jit") public static native @ByVal SizeTOptional findInputWithName(
    @StdString String name,
    @ByVal NamedValueArrayRef kwargs);

// applies implicit conversion from value trying to turn it into type
// concrete_type it succeeds if the return_value->isSubtypeOf(concrete_type)
@Namespace("torch::jit") public static native Value tryConvertToType(
    @Const @ByRef SourceRange loc,
    @ByRef Graph graph,
    @Const @ByRef Type.TypePtr concrete_type,
    Value value,
    @Cast("bool") boolean allow_conversions);
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/versioned_symbols.h

// #pragma once

// #include <caffe2/serialize/versions.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>

// #include <cstdint>
// Maps the given symbol into an implementation of its behavior at the
// given version.
// See note [Versioned Symbols]
@Namespace("torch::jit") public static native @ByVal Symbol get_symbol_for_version(@Const @ByVal Symbol name, @Cast("const uint64_t") long version);

// Maps the given kind to the minimum version that supports it.
// See note [Dynamic Versions and torch.jit.save vs. torch.save]
@Namespace("torch::jit") public static native @Cast("uint64_t") long get_min_version_for_kind(@Cast("const torch::jit::NodeKind*") @ByRef Symbol kind);
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree_views.h

// #pragma once
// #include <c10/util/string_utils.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <torch/csrc/jit/frontend/tree.h>

// #include <c10/util/complex.h>
// #include <functional>
// #include <iostream>
// #include <string>
// #include <utility>
// Targeting ../TreeView.java


// Targeting ../ExprListIterator.java


// Targeting ../StmtListIterator.java


// Targeting ../WithItemListIterator.java


// Targeting ../PropertyListIterator.java


// Targeting ../AssignListIterator.java


// Targeting ../ParamListIterator.java


// Targeting ../IdentListIterator.java


// Targeting ../AttributeListIterator.java


// Targeting ../ExprList.java


// Targeting ../StmtList.java


// Targeting ../WithItemList.java


// Targeting ../PropertyList.java


// Targeting ../AssignList.java


// Targeting ../ParamList.java


// Targeting ../IdentList.java


// Targeting ../AttributeList.java


// Targeting ../DefMaybe.java


// Targeting ../ExprMaybe.java


// Targeting ../VarMaybe.java


// Targeting ../PropertyListMaybe.java


// Targeting ../AssignListMaybe.java


// Targeting ../Ident.java


// Targeting ../Stmt.java


// Targeting ../Expr.java


// Targeting ../Attribute.java


// Targeting ../Param.java


// Targeting ../Decl.java


// Targeting ../Def.java


// Targeting ../Property.java


// Targeting ../ClassDef.java




// Targeting ../If.java


// Targeting ../While.java


// Targeting ../For.java


// Targeting ../ListComp.java


// Targeting ../DictComp.java


// Targeting ../Global.java


// Targeting ../AugAssignKind.java


// Targeting ../AugAssign.java


// Targeting ../Assign.java


// Targeting ../Return.java


// Targeting ../Raise.java


// Targeting ../Assert.java


// Targeting ../Pass.java


// Targeting ../Dots.java


// Targeting ../Break.java


// Targeting ../Continue.java


// Targeting ../ExprStmt.java


// Targeting ../BinOp.java


// Targeting ../UnaryOp.java


// Targeting ../ConstExpr.java


// Targeting ../StringLiteral.java


// Targeting ../Apply.java


// Targeting ../Select.java


// Targeting ../SliceExpr.java


// Targeting ../Subscript.java


// Targeting ../Var.java


// Targeting ../WithItem.java


// Targeting ../With.java


// Targeting ../TernaryIf.java


// Targeting ../ListLiteral.java


// Targeting ../TupleLiteral.java


// Targeting ../DictLiteral.java


// Targeting ../Starred.java


// Targeting ../Delete.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/serialization/pickle.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// #include <torch/csrc/jit/serialization/unpickler.h>

/** Pickle an IValue by calling a function to handle writing the data.
 * 
 *  {@code writer} is a function that takes in a pointer to a chunk of memory and its
 *  size and consumes it.
 * 
 *  See {@code jit::pickle} for more details. */

///
///
///
///
///
///
///
///
@Namespace("torch::jit") public static native void pickle(
    @ByVal PickleWriter writer,
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native void pickle(
    @ByVal PickleWriter writer,
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format compatible with Python's {@code pickle} module
 * 
 *  If present, {@code tensor_table} is a pointer to a table in which tensors that
 *  are contained within {@code ivalue} are stored, and the bytes returned by the
 *  pickler will only include references to these tensors in the table. This can
 *  be used to keep the binary blob size small.
 *  If not provided, tensors are stored in the same byte stream as the pickle
 *  data, similar to {@code torch.save()} in eager Python.
 * 
 *  Pickled values can be loaded in Python and C++:
 *  \rst
 *  .. code-block:: cpp
 * 
 *   torch::IValue float_value(2.3);
 * 
 *   // TODO: when tensors are stored in the pickle, delete this
 *   std::vector<at::Tensor> tensor_table;
 *   auto data = torch::jit::pickle(float_value, &tensor_table);
 * 
 *   std::vector<torch::IValue> ivalues =
 *       torch::jit::unpickle(data.data(), data.size());
 * 
 *  .. code-block:: python
 * 
 *    values = torch.load('data.pkl')
 *    print(values)
 * 
 *  \endrst */
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format that can be loaded by both
 *  {@code torch::pickle_load} in C++ and {@code torch.load} in Python. */

/** Deserialize a {@code torch::IValue} from bytes produced by either
 *  {@code torch::pickle_save} in C++ or {@code torch.save} in Python */

/** {@code reader} is a function that takes in a size to read from some pickled
 *  binary. {@code reader} should remember where it last read, and return
 *  the number of bytes read.
 *  See {@code torch::pickle} for details.
 *  type_resolver is used to resolve any JIT type based on type str */

///
///
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal PickleReader reader,
    @ByVal TypeResolver type_resolver,
    @ByVal TensorArrayRef tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal PickleReader reader,
    @ByVal TypeResolver type_resolver,
    @ByVal TensorArrayRef tensor_table);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal PickleReader reader,
    @ByVal TypeResolver type_resolver,
    @ByVal TensorVector tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal PickleReader reader,
    @ByVal TypeResolver type_resolver,
    @ByVal TensorVector tensor_table);

/** Decode a chunk of memory containing pickled data into its {@code torch::IValue}s.
 * 
 *  If any {@code torch::IValue}s in the pickled data are {@code Object}s, then a
 *  {@code class_resolver} function must be provided.
 * 
 *  See {@code torch::pickle} for details. */
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") TypeResolver type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorArrayRef tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") TypeResolver type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorVector tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size);

 // namespace jit
 // namespace torch


// Parsed from datasets.h

/*
  I don't think we can directly virtualize Dataset<...> because of CRTP in Dataset.

  Because of issue #723, we cannot virtualize superclasses of javacpp::*Dataset, only javacpp::*Dataset.
  We must redeclare/redefine virtual functions of parents in these classes, so that the JavaCPP peer classes implement
  the wrappers that call the Java implementations.
*/
// Targeting ../JavaDataset.java


// Targeting ../JavaTensorDataset.java


// Targeting ../JavaStreamDataset.java


// Targeting ../JavaStreamTensorDataset.java


// Targeting ../JavaStatefulDataset.java


// Targeting ../JavaStatefulTensorDataset.java





}

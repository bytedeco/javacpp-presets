// Targeted by JavaCPP version 1.5.11-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.cuda;

import org.bytedeco.pytorch.*;
import org.bytedeco.cuda.cudart.*;
import org.bytedeco.cuda.cusparse.*;
import org.bytedeco.cuda.cublas.*;
import org.bytedeco.cuda.cusolver.*;
import org.bytedeco.cuda.cudnn.*;
import org.bytedeco.cuda.nccl.*;
import org.bytedeco.pytorch.functions.*;
import org.bytedeco.pytorch.cuda.functions.*;
import org.bytedeco.pytorch.chrono.*;
import org.bytedeco.pytorch.global.torch.DeviceType;
import org.bytedeco.pytorch.global.torch.ScalarType;
import org.bytedeco.pytorch.global.torch.MemoryFormat;
import org.bytedeco.pytorch.Allocator;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;
import org.bytedeco.pytorch.*;
import static org.bytedeco.pytorch.global.torch.*;

import static org.bytedeco.pytorch.global.torch_cuda.*;

// #endif

// ProcessGroupNCCL implements NCCL bindings for c10d.
//
// All functions of the class are expected to be called in the same order
// across all processes in the process group.  This is the only way that we
// can guarantee to match up the same calls among all processes.
//
// All NCCL functions provided by this class are asynchronous functions. More
// specifically, each NCCL call is scheduled on a separate CUDA stream that is
// different from the current CUDA stream. This is for the purpose of
// achieving potentially concurrency and better performance. As a result,
// it is the callers' responsibility to make sure that the CUDA stream their
// code works on needs to wait for the NCCL operation from
// this class.
//
// This can be done by calling:
//
// either WorkNCCL::wait() or WorkNCCL::synchronize(), both achieves the same
// functionality and are synonyms.
//
// Also note that WorkNCCL::finishedGPUExecution() is a helper function only
// provided by ProcessGroupNCCL to check if the NCCL operation of WorkNCCL has
// finished execution on the GPU (not just scheduled).
//
// Example on using the NCCL process group
//
//   ProcessGroupNCCL pg(store, rank, size);
//   std::shared_ptr<WorkNCCL> work = pg.allreduce(tensors);
//
//   // At this point, NCCL kernel has already by queued successfully
//   // Now, let current stream wait for the NCCL to finish, this function is
//   // async operation as well
//
//   work->wait()
//
//   // Now continue on other work in the current stream.
@Namespace("c10d") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch_cuda.class)
public class ProcessGroupNCCL extends DistributedBackend {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public ProcessGroupNCCL(Pointer p) { super(p); }

  @NoOffset public static class WorkNCCL extends Work {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public WorkNCCL(Pointer p) { super(p); }
  

    // Constructor takes a list of CUDA devices
    public WorkNCCL(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("const char*") BytePointer profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "c10::optional<std::vector<at::Tensor> >(c10::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean desyncDebug/*=false*/,
            @Cast("bool") boolean enableTiming/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel) { super((Pointer)null); allocate(device, rank, opType, seq, profilingTitle, inputs, desyncDebug, enableTiming, distDebugLevel); }
    private native void allocate(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("const char*") BytePointer profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "c10::optional<std::vector<at::Tensor> >(c10::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean desyncDebug/*=false*/,
            @Cast("bool") boolean enableTiming/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel);
    public WorkNCCL(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq) { super((Pointer)null); allocate(device, rank, opType, seq); }
    private native void allocate(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq);
    public WorkNCCL(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            String profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "c10::optional<std::vector<at::Tensor> >(c10::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean desyncDebug/*=false*/,
            @Cast("bool") boolean enableTiming/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel) { super((Pointer)null); allocate(device, rank, opType, seq, profilingTitle, inputs, desyncDebug, enableTiming, distDebugLevel); }
    private native void allocate(
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            String profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "c10::optional<std::vector<at::Tensor> >(c10::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean desyncDebug/*=false*/,
            @Cast("bool") boolean enableTiming/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel);
    // Copy constructor doing partial copy without outputs_. Cleanup thread
    // monitors and removes finished works. However it will deadlock when
    // destructs outputs_ tensors who are view tensors in autograd graph.
    public WorkNCCL(@Const @ByRef WorkNCCL w) { super((Pointer)null); allocate(w); }
    private native void allocate(@Const @ByRef WorkNCCL w);

    // Checks if the NCCL kernel has started to execute.
    public native @Cast("bool") boolean isStarted();

    // Checks if request has completed. In this specific case of NCCL, it checks
    // if the NCCL operation has completed on the GPU in its own NCCL stream.
    // Non-blocking operation.
    public native @Cast("bool") boolean isCompleted();

    public native @Cast("bool") boolean isSuccess();

    // Same as calling synchronize() for NCCL work.
    public native @Cast("bool") @Name("wait") boolean _wait(@ByVal(nullValue = "std::chrono::milliseconds(kNoTimeout)") Milliseconds timeout);
    public native @Cast("bool") @Name("wait") boolean _wait();

    public native void abort();

    // Let current stream wait on the completing of the NCCL work
    // Throws on exceptions. Blocking operation, which will wait for work
    // completion.
    public native void synchronize();

    // Synchronize streams by blocking each on the NCCL stream
    public native void synchronizeStream();

    // Helper function to handle exception (throw if needed).
    public native void handleException(ErrorHandlingMode asyncErrorHandling);
    public native void handleException(@Cast("c10d::ErrorHandlingMode") int asyncErrorHandling);

    // Helper function that checks if the NCCL kernels have finished
    // execution on the GPUs
    public native @Cast("bool") boolean finishedGPUExecution();

    // Get a Future object that will be marked as completed internally.
    public native @IntrusivePtr("c10::ivalue::Future") @Cast({"", "c10::intrusive_ptr<c10::ivalue::Future>&"}) Future getFuture();

    public native float getDuration();

    public native @Cast("uint64_t") long getSequencenumber();

    public native @StdString BytePointer logPrefix();

    // Helper function that sets an exception_ptr on the WorkNCCL object.
    public native void setException(@ByVal @Cast("std::exception_ptr*") Pointer exception_ptr);

    // Helper function that returns True if the WorkNCCL object has timed out
    // and False otherwise.
    // In case of timeout, set exception on the WorkNCCL object.
    public native @Cast("bool") boolean checkTimeout(
            @ByVal(nullValue = "c10::optional<std::chrono::milliseconds>(c10::nullopt)") @Cast("c10::optional<std::chrono::milliseconds>*") Pointer timeout);
    public native @Cast("bool") boolean checkTimeout();

    public native @ByVal TensorVector result();
  }

  @NoOffset public static class Options extends DistributedBackend.Options {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public Options(Pointer p) { super(p); }
  
    // NOTE: timeout in ProcessGroupNCCL::Options denote the timeout for
    // operations. This is only used when blockingWait_ is enabled.
    public Options(@Cast("bool") boolean is_high_priority_stream/*=false*/) { super((Pointer)null); allocate(is_high_priority_stream); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::Options>") private native void allocate(@Cast("bool") boolean is_high_priority_stream/*=false*/);
    public Options() { super((Pointer)null); allocate(); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::Options>") private native void allocate();

    // return intrusive_ptr of the object
    public static native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options create(
            @Cast("bool") boolean is_high_priority_stream/*=false*/);
    public static native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options create();

    // Schedule NCCL operations on high priority CUDA streams
    public native @Cast("bool") boolean is_high_priority_stream(); public native Options is_high_priority_stream(boolean setter);

// #ifdef NCCL_HAS_COMM_NONBLOCKING
// #endif

    // Optional "parent" backend and color to create communicators from
    // via `ncclCommSplit`
    public native @SharedPtr ProcessGroupNCCL split_from(); public native Options split_from(ProcessGroupNCCL setter);
    public native @Cast("int64_t") long split_color(); public native Options split_color(long setter);
    public native @Cast("uint64_t*") @StdVector LongPointer global_ranks_in_group(); public native Options global_ranks_in_group(LongPointer setter);
  }

  // If you wish to create multiple process groups, each with a potentially
  // different rank and size, you can do so by passing a new store instance
  // to each one. If you have only a single store object, you can
  // use the `c10d::PrefixStore` to derive scoped instances.
  // This is also what the Python API in torch.distributed does.
  //
  // The process group instance keeps a reference to the store because
  // it may be used long after the constructor runs. In fact, the constructor
  // doesn't create any NCCL communicators. A single NCCL communicator can
  // only be used on a specific set of devices, and are therefore created
  // on-demand when a collective runs. If another collective is executed later,
  // against a different set of devices, the process group creates another NCCL
  // communicator. These NCCL communicators are cached and reused if possible.
  //
  public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, options); }
  private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size) { super((Pointer)null); allocate(store, rank, size); }
  private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size);

  // This constructor includes the deprecated `groupName` argument.
  // If you have existing code that uses the `groupName`, you can replace
  // it by specifying a `c10d::PrefixStore(groupName, store)` for store.
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, groupName, options); }
  @Deprecated private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName) { super((Pointer)null); allocate(store, rank, size, groupName); }
  @Deprecated private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, groupName, options); }
  @Deprecated private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName) { super((Pointer)null); allocate(store, rank, size, groupName); }
  @Deprecated private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName);

  public native @Cast("uint64_t") long getUid();

  public native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options getOptions();

  public native @StdString BytePointer getBackendName();

  public native @Cast("bool") boolean supportsSplitting();

  public native void startCoalescing();

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work endCoalescing();

  // For specifying a composite optype, such as ALLGATHER and REDUCE_SCATTER
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work endCoalescing(@ByVal OpType optype);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work broadcast(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::BroadcastOptions()") BroadcastOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work broadcast(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _broadcast_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors,
        @Const @ByRef(nullValue = "c10d::BroadcastOptions()") BroadcastOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _broadcast_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_sparse(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceOptions()") AllreduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_sparse(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceOptions()") AllreduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_coalesced(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceCoalescedOptions()") AllreduceCoalescedOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_coalesced(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::ReduceOptions()") ReduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors,
        @Const @ByRef(nullValue = "c10d::ReduceOptions()") ReduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _allgather_base(
        @ByRef Tensor outputbuffer,
        @ByRef Tensor inputbuffer,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _allgather_base(
        @ByRef Tensor outputbuffer,
        @ByRef Tensor inputbuffer);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_coalesced(
        @StdVector TensorVector outputTensorLists,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_coalesced(
        @StdVector TensorVector outputTensorLists,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_into_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_into_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_scatter_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_scatter_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work barrier(
        @Const @ByRef(nullValue = "c10d::BarrierOptions()") BarrierOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work barrier();

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Cast("std::vector<int64_t>*") @ByRef LongVector outputSplitSizes,
        @Cast("std::vector<int64_t>*") @ByRef LongVector inputSplitSizes,
        @Const @ByRef(nullValue = "c10d::AllToAllOptions()") AllToAllOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Cast("std::vector<int64_t>*") @ByRef LongVector outputSplitSizes,
        @Cast("std::vector<int64_t>*") @ByRef LongVector inputSplitSizes);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall(
        @ByRef TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllToAllOptions()") AllToAllOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall(
        @ByRef TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work send(
        @ByRef TensorVector tensors,
        int dstRank,
        int tag);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work recv(
        @ByRef TensorVector tensors,
        int srcRank,
        int tag);

  public native void groupStart();

  public native void groupEnd();

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work gather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::GatherOptions()") GatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work gather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::ScatterOptions()") ScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors);

  // Unsupported Ops
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work recvAnysource(
        @ByRef TensorVector tensors,
        int tag);

  // Agrees on an initial sequence number for the whole group by having rank 0
  // create it and broadcast it to other ranks using the store.
  public native void setSequenceNumberForGroup();

  // Retrieves the current sequence number for the whole group, which should be
  // in sync. If the returned number is not consistent across the group, it
  // may indicate that there is some sort of collective desynchronization.
  public native @Cast("uint64_t") long getSequenceNumberForGroup();

  // Return the total number of splits the communicators held by this process
  // group have performed.
  public native @Cast("uint64_t") long getCommSplitCounter();

  public native void registerOnCompletionHook(
        @ByRef(true) WorkInfoConsumer hook);
  public native void waitForPendingWorks();

  public native void enableCollectivesTiming();

  // Helper function for iteratively aborting communicators in the provided map

  public native @IntrusivePtr("c10d::intra_node_comm::IntraNodeComm") @Cast({"", "c10::intrusive_ptr<c10d::intra_node_comm::IntraNodeComm>&"}) IntraNodeComm initIntraNodeComm();

  // Provides an API to abort the ProcessGroup (similar to ncclCommAbort)
  // instead of relying on ProcessGroupNCCL destructor.
  // return true if abort is successful, otherwise false
  public native @Cast("bool") boolean abort(@ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional abortReason);
  public native @Cast("bool") boolean abort();

  public native void shutdown(@ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional reason);
  public native void shutdown();

  public native void eagerConnectSingleDevice(@ByVal Device device);

  public native void performNocolorSplit(@ByVal Device device);
}

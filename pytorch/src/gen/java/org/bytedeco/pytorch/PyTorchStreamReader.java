// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.functions.*;
import org.bytedeco.pytorch.Module;
import org.bytedeco.javacpp.annotation.Cast;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

import static org.bytedeco.pytorch.global.torch.*;


// PyTorch containers are a special zip archive with the following layout
// archive_name.zip contains:
//    archive_name/
//        version # a file with a single decimal number written in ascii,
//                # used to establish the version of the archive format
//        model.json # overall model description, this is a json output of
//                   # ModelDef from torch.proto
//        # the following names are by convention only, model.json will
//        # refer to these files by full names
//        tensors/
//          0 # flat storage for tensor data, meta-data about shapes, etc. is
//            # in model.json
//          1
//          ...
//        # code entries will only exist for modules that have methods attached
//        code/
//          archive_name.py # serialized torch script code (python syntax, using
//          PythonPrint) archive_name_my_submodule.py # submodules have separate
//          files
//
// The PyTorchStreamWriter also ensures additional useful properties for these
// files
// 1. All files are stored uncompressed.
// 2. All files in the archive are aligned to 64 byte boundaries such that
//    it is possible to mmap the entire file and get an aligned pointer to
//    tensor data.
// 3. We universally write in ZIP64 format for consistency.

// The PyTorchStreamReader also provides additional properties:
// 1. It can read zip files that are created with common
//    zip tools. This means that even though our writer doesn't compress files,
//    the reader can still read files that were compressed.
// 2. It provides a getRecordOffset function which returns the offset into the
//    raw file where file data lives. If the file was written with
//    PyTorchStreamWriter it is guaranteed to be 64 byte aligned.

// PyTorchReader/Writer handle checking the version number on the archive format
// and ensure that all files are written to a archive_name directory so they
// unzip cleanly.

// When developing this format we want to pay particular attention to the
// following use cases:
//
// -- Reading --
// 1) Reading with full random access
//   a) Reading with file api's such as fread()
//   b) mmaping the file and jumping around the mapped region
// 2) Reading with 1-pass sequential access
//      -> A reader will need to build up a data structure of parsed structures
//         as it reads
//
// -- Writing --
// 1) Writing with full random access
// 2) Writing with 1-pass sequential access
//      -> We must take care not to require updating values that have already
//         been written. We place the variable-length index at the end and do
//         not put any indicies into the header to fulfill this constraint.

// The model.json, which contains all the metadata information,
// should be written as the last file. One reason is that the size of tensor
// data is usually stable. As long as the shape and type of the tensor do not
// change, the size of the data won't change. On the other sied, the size of the
// serialized model is likely to change, so we store it as the last record, and
// we don't need to move previous records when updating the model data.

// The zip format is sufficiently flexible to handle the above use-case.
// it puts its central directory at the end of the archive and we write
// model.json as the last file when writing after we have accumulated all
// other information.

@Namespace("caffe2::serialize") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch.class)
public class PyTorchStreamReader extends Pointer {
    static { Loader.load(); }

  public PyTorchStreamReader(@StdString BytePointer file_name) { super((Pointer)null); allocate(file_name); }
  private native void allocate(@StdString BytePointer file_name);
  public PyTorchStreamReader(@StdString String file_name) { super((Pointer)null); allocate(file_name); }
  private native void allocate(@StdString String file_name);
  public PyTorchStreamReader(@Cast("std::istream*") Pointer in) { super((Pointer)null); allocate(in); }
  private native void allocate(@Cast("std::istream*") Pointer in);
  public PyTorchStreamReader(@SharedPtr ReadAdapterInterface in) { super((Pointer)null); allocate(in); }
  private native void allocate(@SharedPtr ReadAdapterInterface in);

  // return dataptr, size
  public native @ByVal T_DataPtrSizeT_T getRecord(@StdString BytePointer name);
  public native @ByVal T_DataPtrSizeT_T getRecord(@StdString String name);
  public native @Cast("size_t") long getRecordOffset(@StdString BytePointer name);
  public native @Cast("size_t") long getRecordOffset(@StdString String name);
  public native @Cast("bool") boolean hasRecord(@StdString BytePointer name);
  public native @Cast("bool") boolean hasRecord(@StdString String name);
  public native @ByVal StringVector getAllRecords();
  public native @Cast("uint64_t") long version();

  public native void setShouldLoadDebugSymbol(@Cast("bool") boolean should_load_debug_symbol);
}

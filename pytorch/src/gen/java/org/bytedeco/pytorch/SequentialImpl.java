// Targeted by JavaCPP version 1.5.6-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

import static org.bytedeco.pytorch.global.torch.*;


/** A list of {@code Module}s that acts as a {@code Module} itself.
 * 
 *  A {@code Sequential} is fundamentally a list of {@code Module}s, each with a {@code forward()}
 *  method. {@code Sequential} provides a {@code forward()} method of its own, which accepts
 *  any input and forwards it to the first module it stores. It then "chains"
 *  outputs to inputs sequentially for each subsequent module, finally returning
 *  the output of the last module. For example:
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Sequential seq(
 *      torch::nn::Linear(3, 4),
 *      torch::nn::BatchNorm1d(4),
 *      torch::nn::Dropout(0.5)
 *    );
 * 
 *    auto output = seq->forward(torch::ones(3));
 * 
 *  \endrst
 * 
 *  This can conceptually be thought of as the following loop (using Python as
 *  pseudocode):
 * 
 *  \rst
 *  .. code-block:: python
 * 
 *    def forward(sequential, input):
 *      for module in sequential:
 *        input = module(input)
 *      return input
 * 
 *  \endrst
 * 
 *  Why should you use {@code Sequential} instead of a simple {@code std::vector}? The value
 *  a {@code Sequential} provides over manually calling a sequence of modules is that
 *  it allows treating the whole container *as a single module*, such that
 *  performing a transformation on the {@code Sequential} applies to each of the
 *  modules it stores (which are each a registered submodule of the
 *  {@code Sequential}). For example, calling
 *  {@code .to(torch::kCUDA)} on a {@code Sequential} will move each module in the list to
 *  CUDA memory. For example:
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Sequential seq(
 *      torch::nn::Linear(3, 4),
 *      torch::nn::BatchNorm1d(4),
 *      torch::nn::Dropout(0.5)
 *    );
 * 
 *    // Convert all modules to CUDA.
 *    seq->to(torch::kCUDA);
 * 
 *  \endrst
 * 
 *  Finally, {@code Sequential} provides a lightweight container API, such as allowing
 *  iteration over submodules, positional access, adding a new module after
 *  construction via {@code push_back}, as well as joining two {@code Sequential}s via
 *  {@code extend}.
 * 
 *  \rst
 *  .. attention::
 *    One current limitation of {@code Sequential} is that all except the first module
 *    must accept a single argument. If your modules need to take multiple
 *    arguments, you should define them to take and return tuples.
 *  \endrst */
@Namespace("torch::nn") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch.class)
public class SequentialImpl extends SequentialImplCloneable {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public SequentialImpl(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public SequentialImpl(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public SequentialImpl position(long position) {
        return (SequentialImpl)super.position(position);
    }
    @Override public SequentialImpl getPointer(long i) {
        return new SequentialImpl((Pointer)this).position(position + i);
    }


  public SequentialImpl() { super((Pointer)null); allocate(); }
  @NoDeallocator private native void allocate();

  /** Constructs the {@code Sequential} from a variadic list of modules. */

  /** Constructs the {@code Sequential} from an {@code OrderedDict} of named {@code AnyModule}s. */
  public SequentialImpl(@Cast({"", "torch::OrderedDict<std::string,torch::nn::AnyModule>&&"}) @StdMove StringAnyModuleDict ordered_dict) { super((Pointer)null); allocate(ordered_dict); }
  @NoDeallocator private native void allocate(@Cast({"", "torch::OrderedDict<std::string,torch::nn::AnyModule>&&"}) @StdMove StringAnyModuleDict ordered_dict);

  /** Constructs the {@code Sequential} from a braced-init-list of named {@code AnyModule}s.
   *  It enables the following use case:
   *  {@code Sequential sequential({{"m1", M(1)}, {"m2", M(2)}})} */

  /** Special cloning function for {@code Sequential} because it does not use
   *  {@code reset()}. */
  public native @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module clone(
        @Const @ByRef(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
  public native @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module clone();

  /** {@code reset()} is empty for {@code Sequential}, since it does not have parameters of
   *  its own. */
  public native void reset();

  /** Pretty prints the {@code Sequential} module into the given {@code stream}. */
  
  ///
  ///
  ///
  ///
  ///
  ///
  ///
  ///
  public native void pretty_print(@Cast("std::ostream*") @ByRef Pointer stream);

  /** Feeds {@code inputs} to the first module and then chains outputs to inputs,
   *  returning the last output.
   * 
   *  Conceptually the following loop in Python:
   * 
   *  \rst
   *  .. code-block:: python
   * 
   *    def forward(sequential, input):
   *      for module in sequential:
   *        input = module(input)
   *      return input
   * 
   *  \endrst
   * 
   *  The return type is taken as the first template parameter. It defaults to
   *  {@code Tensor}. If the last module in the {@code Sequential} returns another type {@code T},
   *  you should call {@code forward<T>(inputs)} instead of just {@code forward(inputs)}:
   * 
   *  \rst
   *  .. code-block:: cpp
   * 
   *    torch::Tensor tensor = sequential1->forward(inputs);
   *    int integer = sequential2->forward<int>(inputs);
   *    float value = sequential3->forward<float>(inputs);
   * 
   *  \endrst */

  /** Adds a new (boxed) {@code Module} to the {@code Sequential} container. */

  /** Adds a new named (boxed) {@code Module} to the {@code Sequential} container. */

  /** Adds a new {@code Module} to the {@code Sequential} container, moving or copying it
   *  into a {@code shared_ptr} internally. This method allows passing value types,
   *  and letting the container deal with the boxing. This means you can write
   *  {@code Sequential(Module(3, 4))} instead of
   *  {@code Sequential(std::make_shared<Module>(3, 4))}. */

  /** Adds a new named {@code Module} to the {@code Sequential} container, moving or copying it
   *  into a {@code shared_ptr} internally. This method allows passing value types,
   *  and letting the container deal with the boxing. */

  /** Unwraps the contained module of a {@code ModuleHolder} and adds it to the
   *  {@code Sequential}. */

  /** Unwraps the contained named module of a {@code ModuleHolder} and adds it to the
   *  {@code Sequential}. */

  /** Iterates over the container and calls {@code push_back()} on each value. */

  /** Adds a type-erased {@code AnyModule} to the {@code Sequential}. */
  public native void push_back(@ByVal AnyModule any_module);

  public native void push_back(@StdString BytePointer name, @ByVal AnyModule any_module);
  public native void push_back(@StdString String name, @ByVal AnyModule any_module);

  /** Returns an iterator to the start of the {@code Sequential}. */
  public native @ByVal @Cast("torch::nn::SequentialImpl::Iterator*") AnyModuleVector.Iterator begin();

  /** Returns a const iterator to the start of the {@code Sequential}. */

  /** Returns an iterator to the end of the {@code Sequential}. */
  public native @ByVal @Cast("torch::nn::SequentialImpl::Iterator*") AnyModuleVector.Iterator end();

  /** Returns a const iterator to the end of the {@code Sequential}. */

  /** Attempts to return the module at the given index as the requested type.
   *  Throws an exception if the index is out of bounds or the types do not
   *  match. */

  /** Attempts to return the module at the given index as the requested type.
   *  Throws an exception if the index is out of bounds or the types do not
   *  match. */

  /** Attempts to return a {@code std::shared_ptr} whose dynamic type is that of the
   *  underlying module at the given index. Throws an exception if the index is
   *  out of bounds. */
  public native @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module ptr(@Cast("size_t") long index);

  /** Attempts to return a {@code std::shared_ptr} whose type is the one provided.
   *  Throws an exception if the index is out of bounds or the types do not
   *  match. */

  /** Like {@code ptr(index)}. */
  public native @SharedPtr @Name("operator []") @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module get(@Cast("size_t") long index);

  /** The current size of the {@code Sequential} container. */
  public native @Cast("size_t") @NoException long size();

  /** True if there are no modules in the {@code Sequential}. */
  public native @Cast("bool") @NoException boolean is_empty();
}

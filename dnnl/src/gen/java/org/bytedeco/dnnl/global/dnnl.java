// Targeted by JavaCPP version 1.5.2: DO NOT EDIT THIS FILE

package org.bytedeco.dnnl.global;

import org.bytedeco.dnnl.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class dnnl extends org.bytedeco.dnnl.presets.dnnl {
    static { Loader.load(); }

// Targeting ../dnnl_primitive_desc_vector.java


// Targeting ../primitive_vector.java


// Targeting ../IntMemoryMap.java


// Parsed from dnnl_config.h

/*******************************************************************************
* Copyright 2019 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

// #ifndef DNNL_CONFIG_H
// #define DNNL_CONFIG_H

/** \cond DO_NOT_DOCUMENT_THIS */

// All symbols shall be internal unless marked as DNNL_API
// #if defined _WIN32 || defined __CYGWIN__
// #define DNNL_HELPER_DLL_IMPORT __declspec(dllimport)
// #define DNNL_HELPER_DLL_EXPORT __declspec(dllexport)
// #else
// #if __GNUC__ >= 4
// #define DNNL_HELPER_DLL_IMPORT __attribute__((visibility("default")))
// #define DNNL_HELPER_DLL_EXPORT __attribute__((visibility("default")))
// #else
// #define DNNL_HELPER_DLL_IMPORT
// #define DNNL_HELPER_DLL_EXPORT
// #endif
// #endif

// #ifdef DNNL_DLL
// #ifdef DNNL_DLL_EXPORTS
// #define DNNL_API DNNL_HELPER_DLL_EXPORT
// #else
// #define DNNL_API DNNL_HELPER_DLL_IMPORT
// #endif
// #else
// #define DNNL_API
// #endif

// #if defined(__GNUC__)
// #define DNNL_DEPRECATED __attribute__((deprecated))
// #elif defined(_MSC_VER)
// #define DNNL_DEPRECATED __declspec(deprecated)
// #else
// #define DNNL_DEPRECATED
// #endif

/** \endcond */

// No runtime (disabled)
public static final long DNNL_RUNTIME_NONE = 0L;
// Sequential runtime (CPU only)
public static final long DNNL_RUNTIME_SEQ = 1L;
// OpenMP runtime (CPU only)
public static final long DNNL_RUNTIME_OMP = 2L;
// TBB runtime (CPU only)
public static final long DNNL_RUNTIME_TBB = 4L;
// OpenCL runtime
public static final long DNNL_RUNTIME_OCL = 256L;

// clang-format off

// DNNL CPU threading runtime
public static final long DNNL_CPU_THREADING_RUNTIME = DNNL_RUNTIME_OMP;

// DNNL CPU engine runtime
public static final long DNNL_CPU_RUNTIME = DNNL_RUNTIME_OMP;

// DNNL GPU engine runtime
public static final long DNNL_GPU_RUNTIME = DNNL_RUNTIME_NONE;

// clang-format on

// #if defined(DNNL_CPU_RUNTIME) && defined(DNNL_GPU_RUNTIME)
// #if (DNNL_CPU_RUNTIME == DNNL_RUNTIME_NONE)
//         || (DNNL_CPU_RUNTIME == DNNL_RUNTIME_OCL)
// #error "Unexpected DNNL_CPU_RUNTIME"
// #endif
// #if (DNNL_GPU_RUNTIME != DNNL_RUNTIME_NONE)
//         && (DNNL_GPU_RUNTIME != DNNL_RUNTIME_OCL)
// #error "Unexpected DNNL_GPU_RUNTIME"
// #endif
// #else
// #error "BOTH DNNL_CPU_RUNTIME and DNNL_GPU_RUNTIME must be defined"
// #endif

// #endif


// Parsed from dnnl_types.h

/*******************************************************************************
* Copyright 2016-2019 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C API types definitions */

// #ifndef DNNL_TYPES_H
// #define DNNL_TYPES_H

// #ifdef __cplusplus
// #endif

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <stddef.h>

///
///
// #include <stdint.h>
// Targeting ../dnnl_version_t.java



/** Status values returned by the library functions. */
/** enum dnnl_status_t */
public static final int
    /** The operation was successful */
    dnnl_success = 0,
    /** The operation failed due to an out-of-memory condition */
    dnnl_out_of_memory = 1,
    /** The operation failed because of incorrect function arguments */
    dnnl_invalid_arguments = 2,
    /** The operation failed because requested functionality is not implemented */
    dnnl_unimplemented = 3,
    /** Primitive iterator passed over last primitive descriptor */
    dnnl_iterator_ends = 4,
    /** Primitive or engine failed on execution */
    dnnl_runtime_error = 5,
    /** Queried element is not required for given primitive */
    dnnl_not_required = 6;

/** Data type specification */
/** enum dnnl_data_type_t */
public static final int
    /** Undefined data type, used for empty memory descriptors. */
    dnnl_data_type_undef = 0,
    /** 16-bit/half-precision floating point. */
    dnnl_f16 = 1,
    /** non-standard 16-bit (bfloat16 w/ 7 bit mantissa) floating point. */
    dnnl_bf16 = 2,
    /** 32-bit/single-precision floating point. */
    dnnl_f32 = 3,
    /** 32-bit signed integer. */
    dnnl_s32 = 4,
    /** 8-bit signed integer. */
    dnnl_s8 = 5,
    /** 8-bit unsigned integer. */
    dnnl_u8 = 6;

/** Memory format kind */
/** enum dnnl_format_kind_t */
public static final int
    /** Undefined memory format kind, used for empty memory descriptors. */
    dnnl_format_kind_undef = 0,
    /** Unspecified format kind.
     *  The primitive selects a format automatically. */
    dnnl_format_kind_any = 1,
    /** A tensor in a generic format described by the stride and blocking
     *  values in each dimension. See \ref dnnl_blocking_desc_t for more
     *  information. */
    dnnl_blocked = 2,
    /** Weights format used in 8bit Winograd convolution */
    dnnl_format_kind_wino = 3,
    /** Packed weights format used in RNN */
    dnnl_format_kind_rnn_packed = 4;

/** Memory format tag specification.
 * 
 *  DNNL formats describe physical data layout. The physical layout
 *  is described as a sequence of the dimensions as they are laid out in the
 *  memory (from the outer-most to the inner-most). Note that this order
 *  doesn't affect the logical order of the dimensions that is kept in the
 *  {@code dims} field of the dnnl_memory_desc_t structure. The logical order of the
 *  dimensions is specified by the primitive that uses the tensor.
 * 
 *  For example, CNN 5D tensor always has its logical dimensions in the order
 *  {@code (batch, channels, depth, height, width)}, while the physical layout might be
 *  {@code NCDHW} (corresponds to #dnnl_ncdhw format tag) or
 *  {@code NDHWC} (corresponds to #dnnl_ndhwc format tag).
 * 
 *  ~~~cpp
 *  int batch = 2, channels = 16, depth = 13, height = 13, width = 13;
 * 
 *  int ndims = 5; // 5D tensor
 *  dnnl_dims_t dims = {batch, channels, depth, height, width};
 *  dnnl_memory_desc_t data_in_ncdhw;
 *  dnnl_memory_desc_init_by_tag(
 *       &data_in_ncdhw, 5, dims, dnnl_f32, dnnl_ncdhw);
 * 
 *  // note that in both cases dims passed are the same
 *  dnnl_memory_desc_t data_in_ndhwc;
 *  dnnl_memory_desc_init_by_tag(
 *       &data_in_ndhwc, 5, dims, dnnl_f32, dnnl_ndhwc);
 *  ~~~
 * 
 *  Memory format tags can be further divided into two categories:
 *   - Domain-agnostic names, i.e. names the do not depend on the tensor usage
 *     in the specific primitive. These names use letters from {@code a} to {@code l} to
 *     denote logical dimension from 1 to 12, and form the order in which the
 *     dimensions are laid in memory. For instance, #dnnl_ab is used to denote
 *     2D tensor where the second logical dimension (aka {@code b}) is the innermost,
 *     i.e. has stride = 1, and the first logical dimension ({@code a}) laid out in
 *     memory with stride equal to the size of second dimension. On the other
 *     hand, #dnnl_ba is just transposed version of the same tensor: the
 *     first dimension ({@code a}) becomes the innermost one.
 *   - Domain-specific names, i.e. names that make sense only in the context of
 *     a certain domain, such as CNN. This names are just aliases to the
 *     corresponding domain-agnostic tags and used mostly for the convenience.
 *     For example, #dnnl_nc is used to denote 2D CNN activations tensor
 *     memory format, where channels are the innermost dimension and batch is an
 *     outermost one. Moreover, #dnnl_nc is just an alias to #dnnl_ab,
 *     since for DNNL CNN primitives the logical dimensions of
 *     activations tensors come in order: batch, channels, spatial.
 *     In other words, batch corresponds to the first logical dimension ({@code a}),
 *     channels correspond to the second one ({@code b}).
 * 
 *  The following domain-specific notation applies to memory format tags:
 *   - \c 'n' denotes the mini-batch dimension
 *   - \c 'c' denotes a channels dimension
 *   - When there are multiple channel dimensions (for example, in convolution
 *     weights tensor), \c 'i' and \c 'o' denote dimensions of input and output
 *     channels
 *   - \c 'd', \c 'h', and \c 'w' denote spatial depth, height, and width
 *     respectively
 * 
 *  Upper-case letters indicate that the data is laid out in blocks for a
 *  particular dimension. In such cases, the format name contains both upper-
 *  and lower-case letters for that dimension with a lower-case letter preceded
 *  by the block size. For example: #dnnl_nChw8c describes a format where the
 *  outermost dimension is mini-batch, followed by the channel block number,
 *  followed by the spatial height and width, and finally followed by 8-element
 *  channel blocks.
 * 
 *  @see \ref dev_guide_understanding_memory_formats */
/** enum dnnl_format_tag_t */
public static final int
    /** Undefined memory format tag */
    dnnl_format_tag_undef = 0,
    /** Undefined memory format tag.
     *  The primitive selects a format automatically. */
    dnnl_format_tag_any = 1,

    // Semantic agnostic section
    // The physical order of dimensions is defined by the permutation of the
    // characters, assuming that ab..z defines the natural order.

    // Plain formats

    /** plain 1D tensor */
    dnnl_a = 2,
    /** plain 2D tensor */
    dnnl_ab = 3,
    /** plain 3D tensor */
    dnnl_abc = 4,
    /** plain 4D tensor */
    dnnl_abcd = 5,
    /** plain 5D tensor */
    dnnl_abcde = 6,
    /** plain 6D tensor */
    dnnl_abcdef = 7,

    // Permuted plain formats

    /** permuted 5D tensor */
    dnnl_abdec = 8,
    /** permuted 3D tensor */
    dnnl_acb = 9,
    /** permuted 5D tensor */
    dnnl_acbde = 10,
    /** permuted 4D tensor */
    dnnl_acdb = 11,
    /** permuted 5D tensor */
    dnnl_acdeb = 12,
    /** permuted 2D tensor */
    dnnl_ba = 13,
    /** permuted 3D tensor */
    dnnl_bac = 14,
    /** permuted 4D tensor */
    dnnl_bacd = 15,
    /** permuted 3D tensor */
    dnnl_bca = 16,
    /** permuted 4D tensor */
    dnnl_bcda = 17,
    /** permuted 5D tensor */
    dnnl_bcdea = 18,
    /** permuted 3D tensor */
    dnnl_cba = 19,
    /** permuted 4D tensor */
    dnnl_cdba = 20,
    /** permuted 5D tensor */
    dnnl_cdeba = 21,
    /** permuted 5D tensor */
    dnnl_decab = 22,

    // Opaque blocked formats

    dnnl_Abc16a = 23,
    dnnl_ABc16a16b = 24,
    /** 3D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBc16b = 25,
    dnnl_ABc16b16a = 26,
    dnnl_Abc4a = 27,
    /** 3D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBc4b = 28,
    dnnl_ABc4b16a4b = 29,
    dnnl_ABc4b4a = 30,
    dnnl_ABc8a16b2a = 31,
    dnnl_ABc8a8b = 32,
    /** 3D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBc8b = 33,
    dnnl_ABc8b16a2b = 34,
    dnnl_BAc8a16b2a = 35,
    dnnl_ABc8b8a = 36,
    dnnl_Abcd16a = 37,
    dnnl_ABcd16a16b = 38,
    dnnl_ABcd32a32b = 39,
    /** 4D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcd16b = 40,
    dnnl_ABcd16b16a = 41,
    dnnl_aBCd16b16c = 42,
    dnnl_aBCd16c16b = 43,
    dnnl_Abcd4a = 44,
    /** 4D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcd4b = 45,
    dnnl_ABcd4b16a4b = 46,
    dnnl_ABcd4b4a = 47,
    dnnl_aBCd4c16b4c = 48,
    dnnl_aBCd4c4b = 49,
    dnnl_ABcd8a16b2a = 50,
    dnnl_ABcd8a8b = 51,
    /** 4D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBcd8b = 52,
    dnnl_ABcd8b16a2b = 53,
    dnnl_aBCd8b16c2b = 54,
    dnnl_BAcd8a16b2a = 55,
    /** 4D tensor blocked by 1st and 2nd dimension with block size 8 */
    dnnl_ABcd8b8a = 56,
    dnnl_aBCd8b8c = 57,
    dnnl_aBCd8c16b2c = 58,
    dnnl_ABcde8a16b2a = 59,
    dnnl_aCBd8b16c2b = 60,
    dnnl_aBCd8c8b = 61,
    dnnl_Abcde16a = 62,
    dnnl_ABcde16a16b = 63,
    dnnl_BAcde8a16b2a = 64,
    /** 5D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcde16b = 65,
    dnnl_ABcde16b16a = 66,
    dnnl_aBCde16b16c = 67,
    dnnl_aBCde16c16b = 68,
    dnnl_aBCde2c8b4c = 69,
    dnnl_Abcde4a = 70,
    /** 5D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcde4b = 71,
    dnnl_ABcde4b4a = 72,
    dnnl_aBCde4b4c = 73,
    dnnl_aBCde4c16b4c = 74,
    dnnl_aBCde4c4b = 75,
    dnnl_Abcde8a = 76,
    dnnl_ABcde8a8b = 77,
    dnnl_BAcde16b16a = 78,
    /** 5D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBcde8b = 79,
    dnnl_ABcde8b16a2b = 80,
    dnnl_aBCde8b16c2b = 81,
    dnnl_aCBde8b16c2b = 82,
    dnnl_ABcde8b8a = 83,
    dnnl_aBCde8b8c = 84,
    dnnl_ABcd4a8b8a4b = 85,
    dnnl_ABcd2a8b8a2b = 86,
    dnnl_aBCde4b8c8b4c = 87,
    dnnl_aBCde2b8c8b2c = 88,
    dnnl_aBCde8c16b2c = 89,
    dnnl_aBCde8c8b = 90,
    /** 6D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcdef16b = 91,
    dnnl_aBCdef16b16c = 92,
    dnnl_aBCdef16c16b = 93,
    /** 6D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcdef4b = 94,
    dnnl_aBCdef4c4b = 95,
    dnnl_aBCdef8b8c = 96,
    dnnl_aBCdef8c16b2c = 97,
    dnnl_aBCdef8b16c2b = 98,
    dnnl_aCBdef8b16c2b = 99,
    dnnl_aBCdef8c8b = 100,
    dnnl_aBdc16b = 101,
    dnnl_aBdc4b = 102,
    dnnl_aBdc8b = 103,
    dnnl_aBdec16b = 104,
    dnnl_aBdec32b = 105,
    dnnl_aBdec4b = 106,
    dnnl_aBdec8b = 107,
    dnnl_aBdefc16b = 108,
    dnnl_aCBdef16c16b = 109,
    dnnl_aBdefc4b = 110,
    dnnl_aBdefc8b = 111,
    dnnl_Abcdef16a = 112,
    dnnl_Acb16a = 113,
    dnnl_Acb4a = 114,
    dnnl_Acb8a = 115,
    dnnl_aCBd16b16c = 116,
    dnnl_aCBd16c16b = 117,
    dnnl_aCBde16b16c = 118,
    dnnl_aCBde16c16b = 119,
    dnnl_Acdb16a = 120,
    dnnl_Acdb32a = 121,
    dnnl_Acdb4a = 122,
    dnnl_Acdb8a = 123,
    dnnl_Acdeb16a = 124,
    dnnl_Acdeb4a = 125,
    dnnl_Acdeb8a = 126,
    dnnl_BAc16a16b = 127,
    dnnl_BAc16b16a = 128,
    dnnl_BAcd16a16b = 129,
    dnnl_BAcd16b16a = 130,

    /** Just a sentinel, not real memory format tag. Must be changed after new
     *  format tag is added. */
    dnnl_format_tag_last = 131,

    // Aliases

    /** 1D tensor, an alias to #dnnl_a */
    dnnl_x = dnnl_a,
    /** 2D CNN activations tensor, an alias to #dnnl_ab */
    dnnl_nc = dnnl_ab,
    /** 2D CNN activations tensor, an alias to #dnnl_ba */
    dnnl_cn = dnnl_ba,
    /** 2D RNN statistics tensor, an alias to #dnnl_ab */
    dnnl_tn = dnnl_ab,
    /** 2D RNN statistics tensor, an alias to #dnnl_ba */
    dnnl_nt = dnnl_ba,
    /** 3D CNN activations tensor, an alias to #dnnl_abc */
    dnnl_ncw = dnnl_abc,
    /** 3D CNN activations tensor, an alias to #dnnl_acb */
    dnnl_nwc = dnnl_acb,
    /** 4D CNN activations tensor, an alias to #dnnl_abcd */
    dnnl_nchw = dnnl_abcd,
    /** 4D CNN activations tensor, an alias to #dnnl_acdb */
    dnnl_nhwc = dnnl_acdb,
    /** 4D CNN activations tensor, an alias to #dnnl_bcda */
    dnnl_chwn = dnnl_bcda,
    /** 5D CNN activations tensor, an alias to #dnnl_abcde */
    dnnl_ncdhw = dnnl_abcde,
    /** 5D CNN activations tensor, an alias to #dnnl_acdeb */
    dnnl_ndhwc = dnnl_acdeb,

    /** 2D CNN weights tensor, an alias to #dnnl_ab */
    dnnl_oi = dnnl_ab,
    /** 2D CNN weights tensor, an alias to #dnnl_ba */
    dnnl_io = dnnl_ba,
    /** 3D CNN weights tensor, an alias to #dnnl_abc */
    dnnl_oiw = dnnl_abc,
    /** 3D CNN weights tensor, an alias to #dnnl_acb */
    dnnl_owi = dnnl_acb,
    /** 3D CNN weights tensor, an alias to #dnnl_cba */
    dnnl_wio = dnnl_cba,
    /** 3D CNN weights tensor, an alias to #dnnl_bca */
    dnnl_iwo = dnnl_bca,
    /** 4D CNN weights tensor, an alias to #dnnl_abcd */
    dnnl_oihw = dnnl_abcd,
    /** 4D CNN weights tensor, an alias to #dnnl_cdba */
    dnnl_hwio = dnnl_cdba,
    /** 4D CNN weights tensor, an alias to #dnnl_acdb */
    dnnl_ohwi = dnnl_acdb,
    /** 4D CNN weights tensor, an alias to #dnnl_bcda */
    dnnl_ihwo = dnnl_bcda,
    /** 4D CNN weights tensor, an alias to #dnnl_bacd */
    dnnl_iohw = dnnl_bacd,
    /** 5D CNN weights tensor, an alias to #dnnl_abcde */
    dnnl_oidhw = dnnl_abcde,
    /** 5D CNN weights tensor, an alias to #dnnl_cdeba */
    dnnl_dhwio = dnnl_cdeba,
    /** 5D CNN weights tensor, an alias to #dnnl_acdeb */
    dnnl_odhwi = dnnl_acdeb,
    /** 5D CNN weights tensor, an alias to #dnnl_bcdea */
    dnnl_idhwo = dnnl_bcdea,

    /** 4D CNN weights tensor (incl. groups), an alias to #dnnl_abcd */
    dnnl_goiw = dnnl_abcd,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_abcde */
    dnnl_goihw = dnnl_abcde,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_decab */
    dnnl_hwigo = dnnl_decab,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_acbde */
    dnnl_giohw = dnnl_acbde,
    /** 6D CNN weights tensor (incl. groups), an alias to #dnnl_abcdef */
    dnnl_goidhw = dnnl_abcdef,

    /** 3D RNN data tensor in the format (seq_length, batch, input channels). */
    dnnl_tnc = dnnl_abc,
    /** 3D RNN data tensor in the format (batch, seq_length, input channels). */
    dnnl_ntc = dnnl_bac,
    /** 4D RNN states tensor in the format (num_layers, num_directions,
     *  batch, state channels). */
    
///
    dnnl_ldnc = dnnl_abcd,
    /** 5D RNN weights tensor in the format (num_layers, num_directions,
     *   input_channels, num_gates, output_channels).
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    
///
    dnnl_ldigo = dnnl_abcde,
    /** 5D RNN weights tensor in the format (num_layers, num_directions,
     *  num_gates, output_channels, input_channels).
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    
///
    dnnl_ldgoi = dnnl_abdec,
    /** 4D RNN bias tensor in the format (num_layers, num_directions,
     *  num_gates, output_channels).
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    dnnl_ldgo = dnnl_abcd,

    // Opaque data types, are not to be used explicitly

    // data

    /** 5D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBcde16b */
    dnnl_nCdhw16c = dnnl_aBcde16b,
    /** 5D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBcde4b */
    dnnl_nCdhw4c = dnnl_aBcde4b,
    /** 5D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBcde8b */
    dnnl_nCdhw8c = dnnl_aBcde8b,
    /** 4D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBcd16b */
    dnnl_nChw16c = dnnl_aBcd16b,
    /** 4D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBcd4b */
    dnnl_nChw4c = dnnl_aBcd4b,
    /** 4D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBcd8b */
    dnnl_nChw8c = dnnl_aBcd8b,
    /** 3D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBc16b */
    dnnl_nCw16c = dnnl_aBc16b,
    /** 3D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBc4b */
    dnnl_nCw4c = dnnl_aBc4b,
    /** 3D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBc8b */
    dnnl_nCw8c = dnnl_aBc8b,
    dnnl_NCw16n16c = dnnl_ABc16a16b,
    dnnl_NCdhw16n16c = dnnl_ABcde16a16b,
    dnnl_NChw16n16c = dnnl_ABcd16a16b,
    dnnl_NChw32n32c = dnnl_ABcd32a32b,

    // weights, 3D
    dnnl_IOw16o16i = dnnl_BAc16a16b,
    dnnl_IOw16i16o = dnnl_BAc16b16a,
    dnnl_OIw16i16o = dnnl_ABc16b16a,
    dnnl_OIw16o16i = dnnl_ABc16a16b,
    dnnl_Oiw16o = dnnl_Abc16a,
    dnnl_OIw4i16o4i = dnnl_ABc4b16a4b,
    dnnl_OIw4i4o = dnnl_ABc4b4a,
    dnnl_Oiw4o = dnnl_Abc4a,
    dnnl_OIw8i16o2i = dnnl_ABc8b16a2b,
    dnnl_OIw8i8o = dnnl_ABc8b8a,
    dnnl_OIw8o16i2o = dnnl_ABc8a16b2a,
    dnnl_IOw8o16i2o = dnnl_BAc8a16b2a,
    dnnl_OIw8o8i = dnnl_ABc8a8b,
    dnnl_Owi16o = dnnl_Acb16a,
    dnnl_Owi4o = dnnl_Acb4a,
    dnnl_Owi8o = dnnl_Acb8a,

    // weights, 4D
    dnnl_IOhw16i16o = dnnl_BAcd16b16a,
    dnnl_IOhw16o16i = dnnl_BAcd16a16b,
    dnnl_Ohwi16o = dnnl_Acdb16a,
    dnnl_Ohwi32o = dnnl_Acdb32a,
    dnnl_Ohwi4o = dnnl_Acdb4a,
    dnnl_Ohwi8o = dnnl_Acdb8a,
    dnnl_OIhw16i16o = dnnl_ABcd16b16a,
    dnnl_OIhw16o16i = dnnl_ABcd16a16b,
    dnnl_Oihw16o = dnnl_Abcd16a,
    dnnl_OIhw4i16o4i = dnnl_ABcd4b16a4b,
    dnnl_OIhw4i4o = dnnl_ABcd4b4a,
    dnnl_Oihw4o = dnnl_Abcd4a,
    dnnl_OIhw8i16o2i = dnnl_ABcd8b16a2b,
    dnnl_OIhw8i8o = dnnl_ABcd8b8a,
    dnnl_OIhw8o16i2o = dnnl_ABcd8a16b2a,
    dnnl_IOhw8o16i2o = dnnl_BAcd8a16b2a,
    dnnl_OIhw8o8i = dnnl_ABcd8a8b,

    // weights, 5D
    dnnl_Odhwi16o = dnnl_Acdeb16a,
    dnnl_Odhwi4o = dnnl_Acdeb4a,
    dnnl_Odhwi8o = dnnl_Acdeb8a,
    dnnl_OIdhw16i16o = dnnl_ABcde16b16a,
    dnnl_OIdhw16o16i = dnnl_ABcde16a16b,
    dnnl_Oidhw16o = dnnl_Abcde16a,
    dnnl_OIdhw4i4o = dnnl_ABcde4b4a,
    dnnl_Oidhw4o = dnnl_Abcde4a,
    dnnl_OIdhw8i16o2i = dnnl_ABcde8b16a2b,
    dnnl_OIdhw8i8o = dnnl_ABcde8b8a,
    dnnl_OIdhw8o16i2o = dnnl_ABcde8a16b2a,
    dnnl_IOdhw8o16i2o = dnnl_BAcde8a16b2a,
    dnnl_OIdhw8o8i = dnnl_ABcde8a8b,
    dnnl_IOdhw16i16o = dnnl_BAcde16b16a,

    // weights w/ groups, 3D
    dnnl_Goiw16g = dnnl_Abcd16a,
    dnnl_gIOw16o16i = dnnl_aCBd16b16c,
    dnnl_gIOw16i16o = dnnl_aCBd16c16b,
    dnnl_gOIw16i16o = dnnl_aBCd16c16b,
    dnnl_gOIw16o16i = dnnl_aBCd16b16c,
    dnnl_gOiw16o = dnnl_aBcd16b,
    dnnl_gOIw4i16o4i = dnnl_aBCd4c16b4c,
    dnnl_gOIw4i4o = dnnl_aBCd4c4b,
    dnnl_gOiw4o = dnnl_aBcd4b,
    dnnl_gOIw8i16o2i = dnnl_aBCd8c16b2c,
    dnnl_gOIw8i8o = dnnl_aBCd8c8b,
    dnnl_gOIw8o16i2o = dnnl_aBCd8b16c2b,
    dnnl_gIOw8o16i2o = dnnl_aCBd8b16c2b,
    dnnl_gOIw8o8i = dnnl_aBCd8b8c,
    dnnl_gOwi16o = dnnl_aBdc16b,
    dnnl_gOwi4o = dnnl_aBdc4b,
    dnnl_gOwi8o = dnnl_aBdc8b,

    // weights w/ groups, 4D
    dnnl_gIOhw16i16o = dnnl_aCBde16c16b,
    dnnl_gIOhw16o16i = dnnl_aCBde16b16c,
    dnnl_gOhwi16o = dnnl_aBdec16b,
    dnnl_gOhwi32o = dnnl_aBdec32b,
    dnnl_gOhwi4o = dnnl_aBdec4b,
    dnnl_gOhwi8o = dnnl_aBdec8b,
    dnnl_Goihw16g = dnnl_Abcde16a,
    dnnl_gOIhw16i16o = dnnl_aBCde16c16b,
    dnnl_gOIhw16o16i = dnnl_aBCde16b16c,
    dnnl_gOihw16o = dnnl_aBcde16b,
    dnnl_gOIhw2i8o4i = dnnl_aBCde2c8b4c,
    dnnl_gOIhw4i16o4i = dnnl_aBCde4c16b4c,
    dnnl_gOIhw4i4o = dnnl_aBCde4c4b,
    dnnl_gOIhw4o4i = dnnl_aBCde4b4c,
    dnnl_gOihw4o = dnnl_aBcde4b,
    dnnl_Goihw8g = dnnl_Abcde8a,
    dnnl_gOIhw8i16o2i = dnnl_aBCde8c16b2c,
    dnnl_gOIhw8i8o = dnnl_aBCde8c8b,
    dnnl_gOIhw8o16i2o = dnnl_aBCde8b16c2b,
    dnnl_gIOhw8o16i2o = dnnl_aCBde8b16c2b,
    dnnl_gOIhw8o8i = dnnl_aBCde8b8c,

    dnnl_OIhw4o8i8o4i = dnnl_ABcd4a8b8a4b,
    dnnl_OIhw2o8i8o2i = dnnl_ABcd2a8b8a2b,
    dnnl_gOIhw4o8i8o4i = dnnl_aBCde4b8c8b4c,
    dnnl_gOIhw2o8i8o2i = dnnl_aBCde2b8c8b2c,

    // weights w/ groups, 6D
    dnnl_gIOdhw16i16o = dnnl_aCBdef16c16b,
    dnnl_gOdhwi16o = dnnl_aBdefc16b,
    dnnl_gOdhwi4o = dnnl_aBdefc4b,
    dnnl_gOdhwi8o = dnnl_aBdefc8b,
    dnnl_gOIdhw16i16o = dnnl_aBCdef16c16b,
    dnnl_gOIdhw16o16i = dnnl_aBCdef16b16c,
    dnnl_gOidhw16o = dnnl_aBcdef16b,
    dnnl_gOIdhw4i4o = dnnl_aBCdef4c4b,
    dnnl_gOidhw4o = dnnl_aBcdef4b,
    dnnl_gOIdhw8i16o2i = dnnl_aBCdef8c16b2c,
    dnnl_gOIdhw8i8o = dnnl_aBCdef8c8b,
    dnnl_gOIdhw8o16i2o = dnnl_aBCdef8b16c2b,
    dnnl_gIOdhw8o16i2o = dnnl_aCBdef8b16c2b,
    dnnl_gOIdhw8o8i = dnnl_aBCdef8b8c,
    dnnl_Goidhw16g = dnnl_Abcdef16a;

/** Kinds of propagation. */
/** enum dnnl_prop_kind_t */
public static final int
    // TODO: suggest renames
    /** Undefined propagation type. */
    dnnl_prop_kind_undef = 0,
    /** Forward data propagation (training mode). In this mode primitives
     *  perform computations necessary for subsequent backward propagation. */
    dnnl_forward_training = 64,
    /** Forward data propagation (inference mode). In this mode primitives
     *  perform only computations that are necessary for inference and omit
     *  computations that are necessary only for backward propagation. */
    dnnl_forward_inference = 96,
    /** Forward data propagation (alias for \c dnnl_forward_inference). */
    dnnl_forward_scoring = dnnl_forward_inference,
    /** Forward data propagation (alias for \c dnnl_forward_training). */
    dnnl_forward = dnnl_forward_training,
    /** Backward propagation (with respect to all parameters). */
    dnnl_backward = 128,
    /** Backward data propagation. */
    dnnl_backward_data = 160,
    /** Backward weights propagation. */
    dnnl_backward_weights = 192,
    /** Backward bias propagation. */
    dnnl_backward_bias = 193;

/** Kinds of primitives. Used to implement a way to extend the library with new
 *  primitives without changing the ABI. */
/** enum dnnl_primitive_kind_t */
public static final int
    /** Undefined primitive */
    dnnl_undefined_primitive = 0,
    /** A reorder primitive. */
    dnnl_reorder = 1,
    /** A shuffle primitive. */
    dnnl_shuffle = 2,
    /** A (out-of-place) concat primitive. */
    dnnl_concat = 3,
    /** A sum primitive. */
    dnnl_sum = 4,
    /** A convolution primitive. */
    dnnl_convolution = 5,
    /** A deconvolution primitive. */
    dnnl_deconvolution = 6,
    /** An element-wise primitive. */
    dnnl_eltwise = 7,
    /** A softmax primitive. */
    dnnl_softmax = 8,
    /** A pooling primitive. */
    dnnl_pooling = 9,
    /** An LRN primitive. */
    dnnl_lrn = 10,
    /** A batch normalization primitive. */
    dnnl_batch_normalization = 11,
    /** A layer normalization primitive. */
    dnnl_layer_normalization = 12,
    /** An inner product primitive. */
    dnnl_inner_product = 13,
    /** A rnn primitive. */
    dnnl_rnn = 14,
    /** A matrix multiplication primitive. */
    dnnl_gemm = 15,
    /** A binary primitive. */
    dnnl_binary = 16;

/** Kinds of algorithms. */
/** enum dnnl_alg_kind_t */
public static final int
    dnnl_alg_kind_undef = 0,
    /** Direct convolution */
    dnnl_convolution_direct = 0x1,
    /** Winograd convolution */
    dnnl_convolution_winograd = 0x2,
    /** Convolution algorithm(either direct or Winograd) is chosen just in time */
    dnnl_convolution_auto = 0x3,
    /** Direct deconvolution */
    dnnl_deconvolution_direct = 0xa,
    /** Winograd deconvolution */
    dnnl_deconvolution_winograd = 0xb,
    /** Eltwise: ReLU */
    dnnl_eltwise_relu = 0x1f,
    /** Eltwise: hyperbolic tangent non-linearity (tanh) */
    dnnl_eltwise_tanh = 0x2f,
    /** Eltwise: parametric exponential linear unit (elu) */
    dnnl_eltwise_elu = 0x3f,
    /** Eltwise: square */
    dnnl_eltwise_square = 0x4f,
    /** Eltwise: abs */
    dnnl_eltwise_abs = 0x5f,
    /** Eltwise: square root */
    dnnl_eltwise_sqrt = 0x6f,
    /** Eltwise: linear */
    dnnl_eltwise_linear = 0x7f,
    /** Eltwise: bounded_relu */
    dnnl_eltwise_bounded_relu = 0x8f,
    /** Eltwise: soft_relu */
    dnnl_eltwise_soft_relu = 0x9f,
    /** Eltwise: logistic */
    dnnl_eltwise_logistic = 0xaf,
    /** Eltwise: exponent */
    
///
    dnnl_eltwise_exp = 0xbf,
    /** Eltwise: gelu
     * 
     *  \note Tanh approximation formula is used to approximate
     *  cumulative distribution function of a Gaussian */
    dnnl_eltwise_gelu = 0xcf,
    /** Eltwise: swish */
    dnnl_eltwise_swish = 0xdf,
    /** Max pooling */
    dnnl_pooling_max = 0x1ff,
    /** Average pooling include padding */
    dnnl_pooling_avg_include_padding = 0x2ff,
    /** Average pooling exclude padding */
    dnnl_pooling_avg_exclude_padding = 0x3ff,
    dnnl_pooling_avg = dnnl_pooling_avg_exclude_padding,
    /** Local response normalization (LRN) across multiple channels */
    dnnl_lrn_across_channels = 0xaff,
    /** LRN within a single channel */
    dnnl_lrn_within_channel = 0xbff,
    /** RNN cell */
    dnnl_vanilla_rnn = 0x1fff,
    /** LSTM cell */
    dnnl_vanilla_lstm = 0x2fff,
    /** GRU cell */
    
///
    dnnl_vanilla_gru = 0x3fff,
    /** GRU cell with linear before reset
     * 
     *  Modification of original GRU cell. Differs from #dnnl_vanilla_gru
     *  in how the new memory gate is calculated:
     *  <pre>{@code \[ c_t = tanh(W_c*x_t + b_{c_x} + r_t*(U_c*h_{t-1}+b_{c_h})) \]}</pre>
     *  Primitive expects 4 biases on input:
     *  {@code [b_{u}, b_{r}, b_{c_x}, b_{c_h}]} */
    dnnl_lbr_gru = 0x4fff,
    /** Binary add */
    dnnl_binary_add = 0x1fff0,
    /** Binary mul */
    dnnl_binary_mul = 0x1fff1;

/** Flags for batch normalization primitive. */
/** enum dnnl_normalization_flags_t */
public static final int
    /** Use global statistics
     * 
     *  If specified
     *   - on forward propagation use mean and variance provided by user (input)
     *   - on backward propagation reduces the amount of computations, since
     *     mean and variance are considered as constants
     * 
     *   If not specified:
     *    - on forward propagation mean and variance are computed and stored in
     *      output
     *    - on backward propagation compute full derivative wrt to data */
    
///
///
    dnnl_use_global_stats = 0x1,

    /** Use scale and shift parameters
     * 
     *  If specified:
     *   - on forward propagation use scale and shift (aka scale and bias) for
     *     the batch normalization results
     *   - on backward propagation (for prop_kind == #dnnl_backward) compute
     *     diff wrt to scale and shift (hence one extra output used)
     * 
     *  If no specified:
     *   - on backward propagation prop_kind == #dnnl_backward_data has the
     *     same behavior as prop_kind == #dnnl_backward */
    
///
///
    dnnl_use_scaleshift = 0x2,

    /** Fuse with ReLU
     * 
     *  The flag implies negative slope being 0. On training this is the only
     *  configuration supported. For inference, to use non-zero negative slope
     *  consider using \ref dev_guide_attributes_post_ops.
     * 
     *  If specified:
     *   - on inference this option behaves the same as if the primitive were
     *     fused with ReLU using post ops API with zero negative slope.
     *   - on training primitive requires workspace (required to be able to
     *     perform backward pass) */
    dnnl_fuse_norm_relu = 0x4;

/** \}
 <p>
 *  \addtogroup c_api_types_memory Memory
 *  \{
 <p>
 *  Maximum number of dimensions a tensor can have. Only restricts the amount
 *  of space used for the tensor description. Individual computational
 *  primitives may support only tensors of certain dimensions. */
public static final int DNNL_MAX_NDIMS = 12;

/** A type to describe tensor dimension. */

/** A type to describe tensor dimensions. */

///
// Targeting ../dnnl_blocking_desc_t.java



/** Winograd-specific formats */
/** enum dnnl_wino_memory_format_t */
public static final int
    /** Undefined memory format, used for empty memory descriptors. */
    dnnl_wino_undef = 0,
    // Tensors of weights for 2x3 winograd convolutions.
    /** Internal weights format for 2x3 Winograd */
    dnnl_wino_wei_aaOIoi = 1,
    /** Internal weights format for 2x3 Winograd */
    dnnl_wino_wei_aaOio = 2,
    /** Internal weights format for 2x3 Winograd */
    dnnl_wino_wei_aaOBiOo = 3,
    // Tensor of weights for 4x3 convolution.
    /** Internal weights format for 4x3 Winograd */
    dnnl_wino_wei_OBaaIBOIio = 4;
// Targeting ../dnnl_wino_desc_t.java



/** enum dnnl_rnn_packed_memory_format_t */
public static final int
    dnnl_packed_format_undef = 0,
    dnnl_ldigo_p = 1,
    dnnl_ldgoi_p = 2;

/** Maximum number of parts of RNN weights tensor that require separate
 *  computation. */
public static final int DNNL_RNN_MAX_N_PARTS = 4;
// Targeting ../dnnl_rnn_packed_desc_t.java



/** Flags for memory special features */
/** enum dnnl_memory_extra_flags_t */
public static final int
    
///
    dnnl_memory_extra_flag_none = 0x0,
    /** Indicates the weights have an additional buffer, that depends on the
     *  \p compensation_mask.
     * 
     *  For instance, in 4D case with the compensation mask equals (1 << 0)
     *  the additional buffer would consist of OC values:
     *  O[oc : 0,OC] =
     *   -128 * SUM(ic : 0,IC; kh : 0,KH; kw : 0,KW){ weights(oc, ic, kh, kw) } */
    dnnl_memory_extra_flag_compensation_conv_s8s8 = 0x1,
    dnnl_memory_extra_flag_scale_adjust = 0x2,
    dnnl_memory_extra_flag_gpu_rnn_u8s8_compensation = 0x4;
// Targeting ../dnnl_memory_extra_desc_t.java


// Targeting ../dnnl_memory_desc_t.java


// Targeting ../dnnl_memory.java



/** A memory handle. */

/** A constant memory handle. */

// #define DNNL_MEMORY_NONE (NULL)
// #define DNNL_MEMORY_ALLOCATE ((void *)(size_t)-1)
// Targeting ../dnnl_op_desc_t.java


// Targeting ../const_dnnl_op_desc_t.java


// Targeting ../dnnl_convolution_desc_t.java



/** A descriptor of a deconvolution operation. */
// Targeting ../dnnl_shuffle_desc_t.java


// Targeting ../dnnl_eltwise_desc_t.java


// Targeting ../dnnl_softmax_desc_t.java


// Targeting ../dnnl_pooling_desc_t.java


// Targeting ../dnnl_lrn_desc_t.java


// Targeting ../dnnl_batch_normalization_desc_t.java


// Targeting ../dnnl_layer_normalization_desc_t.java


// Targeting ../dnnl_inner_product_desc_t.java



/** Flags for RNN cell. */
/** enum dnnl_rnn_flags_t */
public static final int dnnl_rnn_flags_undef = 0x0;

/** A direction of RNN primitive execution. */
/** enum dnnl_rnn_direction_t */
public static final int
    /** Unidirectional execution of RNN primitive from left to right. */
    dnnl_unidirectional_left2right = 0,
    /** Unidirectional execution of RNN primitive from right to left. */
    dnnl_unidirectional_right2left = 1,
    /** Bidirectional execution of RNN primitive with concatenation of the
     *  results. */
    dnnl_bidirectional_concat = 2,
    /** Bidirectional execution of RNN primitive with summation of the
     *  results. */
    dnnl_bidirectional_sum = 3,
    dnnl_unidirectional = dnnl_unidirectional_left2right;
// Targeting ../dnnl_rnn_desc_t.java


// Targeting ../dnnl_binary_desc_t.java



/** \}
 <p>
 *  \addtogroup c_api_engine_types Engine
 *  \{
 <p>
 *  \brief Kinds of engines. */
/** enum dnnl_engine_kind_t */
public static final int
    /** An unspecified engine. */
    dnnl_any_engine = 0,
    /** CPU engine. */
    dnnl_cpu = 1,
    /** GPU engine. */
    dnnl_gpu = 2;
// Targeting ../dnnl_engine.java


/** \brief An engine handle. */
// Targeting ../dnnl_primitive_desc_iterator.java



/** \brief A primitive descriptor iterator handle. */

/** \brief A constant primitive descriptor iterator handle. */
// Targeting ../dnnl_primitive_desc.java



/** \brief A primitive descriptor handle. */

/** \brief A constant primitive descriptor handle. */

/** \}
 <p>
 *  \addtogroup c_api_primitive_attr Primitive descriptor attributes
 *  \{
 <p>
 *  Scratchpad mode */
/** enum dnnl_scratchpad_mode_t */
public static final int
    /** The library manages scratchpad (default) */
    dnnl_scratchpad_mode_library = 0,
    /** A user shall query and provide the scratchpad memory to primitives */
    dnnl_scratchpad_mode_user = 1;
// Targeting ../dnnl_primitive_attr.java



/** \brief A primitive descriptor attributes handle that controls primitive
 *  behavior. */

/** \brief A constant primitive descriptor attributes handle. */
// Targeting ../dnnl_post_ops.java



/** \brief A post operation chain handle. */

/** \brief A constant post operation chain handle. */
// Targeting ../dnnl_primitive.java


/** A primitive handle. */
/** A constant primitive handle. */

/** \addtogroup c_api_types_arguments Argument indices
 *  \{ */

public static final int DNNL_ARG_SRC_0 = 1;
public static final int DNNL_ARG_SRC = DNNL_ARG_SRC_0;
public static final int DNNL_ARG_SRC_LAYER = DNNL_ARG_SRC_0;
public static final int DNNL_ARG_FROM = DNNL_ARG_SRC_0;

public static final int DNNL_ARG_SRC_1 = 2;
public static final int DNNL_ARG_SRC_ITER = DNNL_ARG_SRC_1;

public static final int DNNL_ARG_SRC_2 = 3;
public static final int DNNL_ARG_SRC_ITER_C = DNNL_ARG_SRC_2;

public static final int DNNL_ARG_DST_0 = 17;
public static final int DNNL_ARG_DST = DNNL_ARG_DST_0;
public static final int DNNL_ARG_TO = DNNL_ARG_DST_0;
public static final int DNNL_ARG_DST_LAYER = DNNL_ARG_DST_0;

public static final int DNNL_ARG_DST_1 = 18;
public static final int DNNL_ARG_DST_ITER = DNNL_ARG_DST_1;

public static final int DNNL_ARG_DST_2 = 19;
public static final int DNNL_ARG_DST_ITER_C = DNNL_ARG_DST_2;

public static final int DNNL_ARG_WEIGHTS_0 = 33;
public static final int DNNL_ARG_WEIGHTS = DNNL_ARG_WEIGHTS_0;
public static final int DNNL_ARG_SCALE_SHIFT = DNNL_ARG_WEIGHTS_0;
public static final int DNNL_ARG_WEIGHTS_LAYER = DNNL_ARG_WEIGHTS_0;

public static final int DNNL_ARG_WEIGHTS_1 = 34;
public static final int DNNL_ARG_WEIGHTS_ITER = DNNL_ARG_WEIGHTS_1;

public static final int DNNL_ARG_BIAS = 41;

public static final int DNNL_ARG_MEAN = 49;
public static final int DNNL_ARG_VARIANCE = 50;

public static final int DNNL_ARG_WORKSPACE = 64;
public static final int DNNL_ARG_SCRATCHPAD = 80;

public static final int DNNL_ARG_DIFF_SRC_0 = 129;
public static final int DNNL_ARG_DIFF_SRC = DNNL_ARG_DIFF_SRC_0;
public static final int DNNL_ARG_DIFF_SRC_LAYER = DNNL_ARG_DIFF_SRC_0;

public static final int DNNL_ARG_DIFF_SRC_1 = 130;
public static final int DNNL_ARG_DIFF_SRC_ITER = DNNL_ARG_DIFF_SRC_1;

public static final int DNNL_ARG_DIFF_SRC_2 = 131;
public static final int DNNL_ARG_DIFF_SRC_ITER_C = DNNL_ARG_DIFF_SRC_2;

public static final int DNNL_ARG_DIFF_DST_0 = 145;
public static final int DNNL_ARG_DIFF_DST = DNNL_ARG_DIFF_DST_0;
public static final int DNNL_ARG_DIFF_DST_LAYER = DNNL_ARG_DIFF_DST_0;

public static final int DNNL_ARG_DIFF_DST_1 = 146;
public static final int DNNL_ARG_DIFF_DST_ITER = DNNL_ARG_DIFF_DST_1;

public static final int DNNL_ARG_DIFF_DST_2 = 147;
public static final int DNNL_ARG_DIFF_DST_ITER_C = DNNL_ARG_DIFF_DST_2;

public static final int DNNL_ARG_DIFF_WEIGHTS_0 = 161;
public static final int DNNL_ARG_DIFF_WEIGHTS = DNNL_ARG_DIFF_WEIGHTS_0;
public static final int DNNL_ARG_DIFF_SCALE_SHIFT = DNNL_ARG_DIFF_WEIGHTS_0;
public static final int DNNL_ARG_DIFF_WEIGHTS_LAYER = DNNL_ARG_DIFF_WEIGHTS_0;

public static final int DNNL_ARG_DIFF_WEIGHTS_1 = 162;
public static final int DNNL_ARG_DIFF_WEIGHTS_ITER = DNNL_ARG_DIFF_WEIGHTS_1;

public static final int DNNL_ARG_DIFF_BIAS = 169;

public static final int DNNL_ARG_MULTIPLE_SRC = 1024;

///
public static final int DNNL_ARG_MULTIPLE_DST = 2048;
// Targeting ../dnnl_exec_arg_t.java



/** \}
 <p>
 *  \addtogroup c_api_types_query Queries
 *  \{
 <p>
 *  Primitive descriptor query specification
 * 
 *  For generic function dnnl_primitive_desc_query(), the type of result must
 *  agree with the queried argument. The correspondence table:
 *       Query                           | type of result
 *       --------------------------------------------------------------
 *       #dnnl_query_engine              | dnnl_engine_t *
 *       #dnnl_query_scratchpad_engine   | dnnl_engine_t *
 *       #dnnl_query_primitive_kind      | dnnl_primitive_kind_t *
 *       *_s32                           | int *
 *       *_s64                           | dnnl_dim_t * (same as int64_t *)
 *       *_f64                           | double *
 *       *_str                           | const char **
 *       #dnnl_query_op_d                | const_dnnl_op_desc_t *
 *       *_md                            | const dnnl_memory_desc_t **
 *       *_${op}_d                       | const dnnl_${op}_desc_t **
 *       *_pd                            | const_dnnl_primitive_desc_t *
 * 
 *  \note
 *      Rule of thumb: all opaque types and structures are returned by
 *      reference. All numbers are returned by value.
 * 
 *  \warning
 *      All returned references point to constant objects and are valid only
 *      during the lifetime of the queried primitive descriptor. Returned objects
 *      must not be destroyed by the user. If you need to keep the object longer
 *      than the lifetime of the queried primitive descriptor, use
 *      dnnl_primitive_desc_clone() to make a copy. */
/** enum dnnl_query_t */
public static final int
    /** no query */
    dnnl_query_undef = 0,

    /** execution engine */
    dnnl_query_engine = 1,
    /** primitive kind */
    dnnl_query_primitive_kind = 2,

    /** number of inputs expected */
    dnnl_query_num_of_inputs_s32 = 3,
    /** number of outputs expected */
    dnnl_query_num_of_outputs_s32 = 4,

    /** runtime estimation (seconds) */
    dnnl_query_time_estimate_f64 = 5,
    /** memory consumption -- extra */
    dnnl_query_memory_consumption_s64 = 6,
    /**  (scratch) memory, additional to
     *   all inputs and outputs memory
     *   (bytes) */

    /** scratchpad engine -- engine to be used */
    dnnl_query_scratchpad_engine = 7,
    /**  for creating scratchpad memory */

    /** implementation name */
    dnnl_query_impl_info_str = 8,

    /** source engine */
    dnnl_query_reorder_src_engine = 9,
    /** destination engine */
    dnnl_query_reorder_dst_engine = 10,

    /** propagation kind */
    dnnl_query_prop_kind = 11,

    // memory and op descriptor section
    /** stub */
    dnnl_query_some_d = 64,
    /** op descriptor */
    dnnl_query_op_d = 65,
    /** convolution descriptor */
    dnnl_query_convolution_d = 66,
    /** deconvolution descriptor */
    dnnl_query_deconvolution_d = 67,
    /** shuffle descriptor */
    dnnl_query_shuffle_d = 68,
    /** eltwise descriptor */
    dnnl_query_eltwise_d = 69,
    /** softmax descriptor */
    dnnl_query_softmax_d = 70,
    /** pooling descriptor */
    dnnl_query_pooling_d = 71,
    /** lrn descriptor */
    dnnl_query_lrn_d = 72,
    /** batch normalization descriptor */
    dnnl_query_batch_normalization_d = 73,
    /** layer normalization descriptor */
    dnnl_query_layer_normalization_d = 74,
    /** inner product descriptor */
    dnnl_query_inner_product_d = 75,
    /** rnn descriptor */
    dnnl_query_rnn_d = 76,
    /** GEMM descriptor */
    dnnl_query_gemm_d = 77,
    /** binary descriptor */
    dnnl_query_binary_d = 78,

    // memory descriptor section
    /** stub */
    dnnl_query_some_md = 128,
    /** source memory desc */
    dnnl_query_src_md = 129,
    /** source gradient memory desc */
    dnnl_query_diff_src_md = 130,
    /** weights memory descriptor desc */
    dnnl_query_weights_md = 131,
    /** weights grad. memory desc */
    dnnl_query_diff_weights_md = 132,
    /** destination memory desc */
    dnnl_query_dst_md = 133,
    /** destination grad. memory desc */
    dnnl_query_diff_dst_md = 134,
    /** workspace memory desc */
    dnnl_query_workspace_md = 135,
    /** scratchpad memory desc */
    dnnl_query_scratchpad_md = 136;

/** \}
 <p>
 *  \addtogroup c_api_types_stream Execution stream
 *  \{
 <p>
 *  \brief Stream flags. */
/** enum dnnl_stream_flags_t */
public static final int
    /** Default order execution. Either in-order or out-of-order depending on
     *  the runtime. */
    dnnl_stream_default_order = 0x1,
    /** In-order execution. */
    dnnl_stream_in_order = 0x2,
    /** Out-of-order execution. */
    dnnl_stream_out_of_order = 0x4,
    /** Default stream configuration. */
    dnnl_stream_default_flags = dnnl_stream_default_order;
// Targeting ../dnnl_stream.java


/** An execution stream handle. */
/** A constant execution stream handle. */

/** \}
 *  \}
 *  \} */

// #ifdef __cplusplus
// #endif

// #endif


// Parsed from dnnl_version.h

/*******************************************************************************
* Copyright 2019 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

// #ifndef DNNL_VERSION_H
// #define DNNL_VERSION_H

// clang-format off

/** Major version */
public static final int DNNL_VERSION_MAJOR = 1;

/** Minor version */
public static final int DNNL_VERSION_MINOR = 1;

/** Patch version */
public static final int DNNL_VERSION_PATCH = 1;

/** Git commit hash */
public static native @MemberGetter String DNNL_VERSION_HASH();
public static final String DNNL_VERSION_HASH = DNNL_VERSION_HASH();

// clang-format on

// #endif


// Parsed from dnnl.h

/*******************************************************************************
* Copyright 2016-2019 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C API */

// #ifndef DNNL_H
// #define DNNL_H

// #include "dnnl_config.h"
// #include "dnnl_types.h"
// #include "dnnl_version.h"

/** \cond DO_NOT_DOCUMENT_THIS */
// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif
/** \endcond */

// #ifdef __cplusplus
// #endif

/** \addtogroup c_api C API
 *  \{
 <p>
 *  \addtogroup c_api_primitive Primitive operations
 *  \{
 <p>
 *  \addtogroup c_api_primitive_common Common primitive operations
 *  \{
 <p>
 *  Creates a primitive descriptor \p iterator for given \p op_desc, \p attr,
 *  \p engine, and optionally a hint primitive descriptor from forward
 *  propagation (required for backward propagation). Pass \c NULL for forward
 *  propagation. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_iterator_create(
        @ByPtrPtr dnnl_primitive_desc_iterator iterator, const_dnnl_op_desc_t op_desc,
        @Const dnnl_primitive_attr attr, dnnl_engine engine,
        @Const dnnl_primitive_desc hint_forward_primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_iterator_create(
        @Cast("dnnl_primitive_desc_iterator_t*") PointerPointer iterator, const_dnnl_op_desc_t op_desc,
        @Const dnnl_primitive_attr attr, dnnl_engine engine,
        @Const dnnl_primitive_desc hint_forward_primitive_desc);

/** Iterates over primitive descriptors. Returns #dnnl_iterator_ends if no
 *  more primitive descriptors are available. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_iterator_next(
        dnnl_primitive_desc_iterator iterator);

/** Fetches the current primitive descriptor.
 * 
 *  \note
 *      The user should delete the fetched primitive descriptor using
 *      dnnl_primitive_desc_destroy() once it is no longer needed. */
public static native dnnl_primitive_desc dnnl_primitive_desc_iterator_fetch(
        @Const dnnl_primitive_desc_iterator iterator);

/** Deletes a primitive descriptor \p iterator */
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_iterator_destroy(
        dnnl_primitive_desc_iterator iterator);

/** Creates a \p primitive_desc using \p op_desc, \p attr, \p engine, and
 *  optionally a hint primitive descriptor from forward propagation. The call is
 *  equivalent to creating a primitive descriptor iterator, immediately fetching
 *  a primitive descriptor, and then destroying the iterator. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, const_dnnl_op_desc_t op_desc,
        @Const dnnl_primitive_attr attr, dnnl_engine engine,
        @Const dnnl_primitive_desc hint_forward_primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, const_dnnl_op_desc_t op_desc,
        @Const dnnl_primitive_attr attr, dnnl_engine engine,
        @Const dnnl_primitive_desc hint_forward_primitive_desc);

/** Makes a copy of a \p primitive_desc. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_clone(
        @ByPtrPtr dnnl_primitive_desc primitive_desc,
        @Const dnnl_primitive_desc existing_primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_clone(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc,
        @Const dnnl_primitive_desc existing_primitive_desc);

/** Returns a constant reference to the attribute of a \p primitive_desc.
 * 
 *  \warning
 *       The user should not destroy the obtained \p attr.
 * 
 *  \warning
 *       The lifetime of an \p attr is the same as that of a \p primitive_desc,
 *       so it is illegal to use the \p attr once \p primitive_desc has been
 *       destroyed. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_get_attr(
        @Const dnnl_primitive_desc primitive_desc,
        @Const @ByPtrPtr dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_get_attr(
        @Const dnnl_primitive_desc primitive_desc,
        @Cast("const_dnnl_primitive_attr_t*") PointerPointer attr);

/** Deletes a \p primitive_desc. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_destroy(
        dnnl_primitive_desc primitive_desc);

/** Queries primitive descriptor
 * 
 *  One of the most typical use cases is to query a primitive descriptor
 *  created with source, weights, and destination formats equal
 *  to #dnnl_format_tag_any about the corresponding memory descriptors
 *  (\p what equals #dnnl_query_src_md, #dnnl_query_weights_md, and
 *  #dnnl_query_dst_md respectively) to be able to prepare memory and
 *  create reorders if required.
 * 
 *  Another quite typical use case is to query an operation primitive
 *  descriptor for a workspace (\p what equals #dnnl_query_workspace_md).
 *  The returned status #dnnl_not_required indicates that a workspace is
 *  not required.
 * 
 *  \note When querying a memory descriptor for a scratchpad, a
 *  workspace, or an optional parameter, the query will return a
 *  zero_md if the parameter is not needed.
 * 
 *  A few other possibilities:
 *   - query an operation primitive descriptor for the underlying operation
 *     descriptor (#dnnl_query_convolution_d, #dnnl_query_eltwise_d,
 *     #dnnl_query_rnn_d, etc.)
 *   - query an operation primitive descriptor for the implementation
 *     information string (#dnnl_query_impl_info_str)
 *   - query an operation primitive descriptor for the number of inputs and
 *     outputs (#dnnl_query_num_of_inputs_s32 and
 *     #dnnl_query_num_of_outputs_s32 respectively)
 * 
 *  @see dnnl_query_t for more options */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_query(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index, Pointer result);

/** Queries primitive descriptor for memory descriptor
 * 
 *  @return NULL in case of any error.
 * 
 *  This is just a specialized version of dnnl_primitive_desc_query
 *  used for convenience. */

///
///
public static native @Const dnnl_memory_desc_t dnnl_primitive_desc_query_md(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index);

/** Queries primitive descriptor for signed 32bit int
 * 
 *  @return 0 in case of any error (in particular if the queried entity is
 *  not of type int32_t). Note that 0 might also be the actual returned
 *  value.
 * 
 *  This is just a specialized version of dnnl_primitive_desc_query
 *  used for convenience. */
public static native int dnnl_primitive_desc_query_s32(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index);

/** Creates a \p primitive using a \p primitive_desc descriptor. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_create(@ByPtrPtr dnnl_primitive primitive,
        @Const dnnl_primitive_desc primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create(@Cast("dnnl_primitive_t*") PointerPointer primitive,
        @Const dnnl_primitive_desc primitive_desc);

/** Executes a \p primitive using a \p stream, and \p nargs arguments
 *  \p args. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_execute(@Const dnnl_primitive primitive,
        dnnl_stream stream, int nargs, @Const dnnl_exec_arg_t args);

/** Retrieves a reference to the \p primitive_desc descriptor of given \p
 *  primitive.
 * 
 *  \warning
 *      The returned object must not be destroyed by the user. The \c const
 *      qualifier of the returned object prevents such attempts. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_primitive_desc(
        @Const dnnl_primitive primitive,
        @Const @ByPtrPtr dnnl_primitive_desc primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_primitive_desc(
        @Const dnnl_primitive primitive,
        @Cast("const_dnnl_primitive_desc_t*") PointerPointer primitive_desc);

/** Deletes a \p primitive. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_destroy(dnnl_primitive primitive);

/** \}
 <p>
 *  \addtogroup c_api_attributes Attributes
 *  An extension for controlling primitive behavior.
 *  \{
 <p>
 *  Creates an empty (default) \p attr attribute. All the parameters are set to
 *  default values.
 * 
 *  An empty attribute is used in primitive descriptor creation whenever it
 *  is not passed explicitly, e.g. in dnnl_primitive_desc_create. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_create(@ByPtrPtr dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_create(@Cast("dnnl_primitive_attr_t*") PointerPointer attr);

/** Makes a copy of an \p existing_attr. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_clone(
        @ByPtrPtr dnnl_primitive_attr attr, @Const dnnl_primitive_attr existing_attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_clone(
        @Cast("dnnl_primitive_attr_t*") PointerPointer attr, @Const dnnl_primitive_attr existing_attr);

/** Deletes an \p attr. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_destroy(dnnl_primitive_attr attr);

/** Returns the scratchpad \p mode set in the attribute \p attr */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") IntPointer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") IntBuffer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") int[] mode);

/** Sets scratchpad \p mode.
 * 
 *  The possible values are: #dnnl_scratchpad_mode_library (default) and
 *  #dnnl_scratchpad_mode_user. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_scratchpad_mode(
        dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t") int mode);

/** Returns \p count, correspondence scale \p mask, and a pointer to a constant
 *  floating point array of output \p scales for given \p attr, previously set
 *  by dnnl_primitive_attr_set_output_scales.
 * 
 *  \warning
 *       The \p scales array points to the internal \p attr field, so the user
 *       should not modify or destroy \p scales.
 * 
 *  \warning
 *       The lifetime of \p scales is the same as that of the \p attr to which it
 *       belongs, so it is illegal to use \p scales after \p attr is destroyed. */

///
///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_output_scales(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Cast("const float**") PointerPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_output_scales(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Const @ByPtrPtr FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_output_scales(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongBuffer count, IntBuffer mask,
        @Const @ByPtrPtr FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_output_scales(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") long[] count, int[] mask,
        @Const @ByPtrPtr float[] scales);

/** Sets output \p scales for primitive operations. The number of elements \p
 *  count and correspondence scale \p mask are stored for future use.
 * 
 *  The \p mask argument defines the correspondence between the output tensor
 *  dimensions and the \p scales array. Set the i-th bit of \p mask to 1 to use a
 *  dedicated scaling factor for each slice of the output tensor over the i-th
 *  dimension. Set \p mask to 0 to use a common scaling factor for the whole
 *  output tensor.
 * 
 *  \note
 *       The dimension order is always native and does not depend on the actual
 *       layout used. Examples:
 *        - 2D dimensional data the order of dimensions is always: (n, c)
 *        - 4D dimensional data the order is always: (n, c, h, w)
 *        - 5D dimensional weights the order is always: (g, oc, ic, kh, kw)
 * 
 *  Example usage:
 *  <pre>{@code
 *       int mb = 32, oc = 32, oh = 14, ow = 14; // convolution output params
 *       float scales[oc] = { ... }; // unique output scales per output channel
 *       int oc_dim = 1; // mb_dim = 0, channel_dim = 1, height_dim = 2, ...
 * 
 *       dnnl_convolution_desc_t cd; // create & configure convolution op_desc
 * 
 *       dnnl_primitive_attr_t attr;
 *       dnnl_primitive_attr_create(&attr);  // create default attributes
 *       dnnl_primitive_attr_set_output_scales(attr, oc, 1 << oc_dim, scales);
 * 
 *       dnnl_primitive_desc_t cpd;
 *       dnnl_primitive_desc_create(&cpd, &cd, attr, NULL);
 *  }</pre>
 * 
 *  \note
 *       There is no way to check that \p count corresponds to \p mask until an
 *       actual primitive descriptor is created, so it is the user's
 *       responsibility to set proper values. The following formula must hold:
 * 
 *       <pre>{@code \[count = \prod\limits_{d \in mask} output.dims[d]\]}</pre> */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_output_scales(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_output_scales(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_output_scales(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const float[] scales);

/** Returns \p post_ops for given \p attr.
 * 
 *  \warning
 *       \p post_ops points to the internal \p attr field, so the user should not
 *       modify or destroy \p post_ops. Also, the lifetime of \p post_ops is the
 *       same as that of the \p attr it belongs to, so it is illegal to use \p
 *       post_ops after \p attr has been destroyed. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_post_ops(
        @Const dnnl_primitive_attr attr, @Const @ByPtrPtr dnnl_post_ops post_ops);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_post_ops(
        @Const dnnl_primitive_attr attr, @Cast("const_dnnl_post_ops_t*") PointerPointer post_ops);

/** Sets configured \p post_ops to an attribute \p attr for future use (when
 *  primitive descriptor is being created).
 * 
 *  \note
 *       At this point in time, there is no way to check whether the primitive
 *       descriptor does or does not support a given sequence of post operations.
 *       Therefore the user should handle an error that might occur at the
 *       dnnl_primitive_desc_create call. */
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_post_ops(
        dnnl_primitive_attr attr, @Const dnnl_post_ops post_ops);

/** \addtogroup c_api_attributes_post_ops Sequence of post operations
 *  An extension for performing extra operations after a base operation.
 *  \{
 <p>
 *  Creates an empty sequence of post operations \p post_ops. */
public static native @Cast("dnnl_status_t") int dnnl_post_ops_create(@ByPtrPtr dnnl_post_ops post_ops);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_create(@Cast("dnnl_post_ops_t*") PointerPointer post_ops);

/** Deletes a \p post_ops sequence. */
public static native @Cast("dnnl_status_t") int dnnl_post_ops_destroy(dnnl_post_ops post_ops);

/** Returns the \p length of post operations for given \p post_ops. */
public static native int dnnl_post_ops_len(@Const dnnl_post_ops post_ops);

/** Returns the kind of post operation with index \p index in given
 *  \p post_ops. In case of error, returns #dnnl_undefined_primitive. */

///
///
///
///
public static native @Cast("dnnl_primitive_kind_t") int dnnl_post_ops_get_kind(
        @Const dnnl_post_ops post_ops, int index);

/** Appends accumulation (sum) post operation to the \p post_ops. Prior to
 *  accumulating the result, the previous value would be multiplied by \p scale.
 * 
 *  The kind of this post operation is #dnnl_sum.
 * 
 *  This feature might improve performance for cases like residual learning
 *  blocks, where the result of convolution is accumulated to the previously
 *  computed activations. The parameter \p scale might be extreme for the
 *  integer-based computations when the result and previous activations have
 *  different logical scaling factors.
 * 
 *  In the simplest case when the accumulation is the only post operation, the
 *  computations would be:
 *  dst[] <- scale * dst[] + op(...) // instead of dst[] <- op(...)
 * 
 *  \note
 *       This post operation (as well as all the others) disregards the original
 *       layout of the destination; that is, the layout of the original
 *       destination is expected to be the same as the layout of the stored
 *       destination. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_sum(
        dnnl_post_ops post_ops, float scale);

/** Gets the parameters of the accumulation (sum) post operation with index
 *  \p index in the sequence of \p post_ops.
 * 
 *  \note
 *       If index \p index would not correspond to the accumulation post
 *       operation, the function returns #dnnl_invalid_arguments. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, FloatPointer scale);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, FloatBuffer scale);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, float[] scale);

/** Appends eltwise post operation to the \p post_ops with given parameters
 *  \p kind, \p alpha, and \p beta (@see dnnl_eltwise_forward_desc_init and
 *  dnnl_eltwise_desc_t).
 * 
 *  The kind of this post operation is #dnnl_eltwise.
 * 
 *  In the simplest case when the eltwise is the only post operation, the
 *  computations would be:
 *  dst[] <- scale * eltwise_op ( op(...) ) // instead of dst[] <- op(...)
 *  where eltwise_op is configured with the given parameters. */
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_eltwise(dnnl_post_ops post_ops,
        float scale, @Cast("dnnl_alg_kind_t") int alg, float alpha, float beta);

/** Gets the eltwise parameters of the post operation with index \p index in
 *  the sequence of \p post_ops. */

///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, FloatPointer scale,
        @Cast("dnnl_alg_kind_t*") IntPointer alg, FloatPointer alpha, FloatPointer beta);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, FloatBuffer scale,
        @Cast("dnnl_alg_kind_t*") IntBuffer alg, FloatBuffer alpha, FloatBuffer beta);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, float[] scale,
        @Cast("dnnl_alg_kind_t*") int[] alg, float[] alpha, float[] beta);

/** \}
 <p>
 *  \}
 <p>
 *  \addtogroup c_api_memory Memory
 *  A primitive to describe and store data.
 * 
 *  The library supports various data types and formats. Memory hierarchy
 *  consists of three levels of abstraction:
 *  1. **Memory descriptor** -- engine agnostic logical description of data
 *       (number of dimensions, dimensions themselves, and data type), and
 *       optionally the format/layout that describes the physical representation
 *       of data in memory. If the format is not known yet, one can pass
 *       #dnnl_format_tag_any. This approach is used to allow compute-intensive
 *       primitives to specify the most appropriate format on their own with
 *       users required to reorder the data if the incoming format doesn't match
 *       the primitive's selection. Memory descriptor can be initialized with
 *       dnnl_memory_desc_init_by_tag() or dnnl_memory_desc_init_by_strides()
 *       functions, or by directly filling the dnnl_memory_desc_t structure.
 *       The latter requires deep knowledge of how the physical data
 *       representation is mapped to the structure.
 *       The \ref dev_guide_understanding_memory_formats topic should shed some
 *       light on that.
 *       For the fully defined memory descriptors (i.e. where the format kind is
 *       not equal to #dnnl_format_kind_any) a user can the size, using the
 *       dnnl_memory_desc_get_size() function. As described in
 *       \ref dev_guide_understanding_memory_formats, the size of data sometimes
 *       cannot be computed as the product of dimensions times the size
 *       of the data type. So users are encouraged to use this function
 *       for better code portability.
 *       Two memory descriptors can be compared with dnnl_memory_desc_equal().
 *       The comparison is especially useful when checking whether a primitive
 *       requires reorder from the user's data format to the primitive's format.
 *  2. **Memory** -- an engine-specific object that handles the data and its
 *       description (a memory descriptor). For CPU enigne, the data handle is
 *       simply a pointer to \c void. The data handle can be queried using
 *       dnnl_memory_get_data_handle() and set using
 *       dnnl_memory_set_data_handle(). The latter function always sets the
 *       memory in the padding region to zero, which is the invariant maintained
 *       by all the primitives in DNNL.
 *       See \ref dev_guide_understanding_memory_formats for more details.
 *       A memory can be created using dnnl_memory_create() function.
 *       A memory can also be queried for the underlying memory descriptor and
 *       engine using dnnl_memory_get_memory_desc() and
 *       dnnl_memory_get_engine() functions.
 * 
 *  Along with ordinary memory with all dimensions being positive, Intel
 *  DNNL supports *zero-volume* memory with one or more dimensions set to
 *  zero. This is to support the NumPy\* convention.
 *  If a *zero-volume* memory is passed to a primitive, the primitive does
 *  not perform any computations on this memory. For example:
 *   - Convolution with {@code (0 batch, 3 input channels, 13 height, 13 width)}
 *     source and {@code (16 output channels, 3 input channels, 3 height, 3 width)}
 *     weights would produce {@code (0 batch, 16 output channels, 11 height, 11 width)}
 *     destination (assuming strides are {@code 1} and paddings are zero) and perform
 *     zero multiply-add operations.
 *   - Concatenation of three memories of shapes {@code (3, 4, 13, 13)},
 *     {@code (3, 0, 13, 13)}, and {@code (3, 1, 13, 13)} along the second axis would produce
 *     the output of the shape {@code (3, 5, 13, 13)}, effectively ignoring the second
 *     input (however, if the user created a concatenation primitive descriptor
 *     with three inputs they should also provide all three memories to the
 *     concatenation primitive, including the one with zero second dimension).
 *   - However, DNNL would return an error when attempting to create a
 *     convolution with *zero-volume* memory passed for weights because such a
 *     convolution is not well-defined:
 *     ~~~
 *     dst(1, 16, 11, 11) <-- src(1, 0, 13, 13) (*) wei(16, 0, 3, 3)
 *     ~~~
 *     Should the values in the destination be zeroes or just not accessed at
 *     all? Moreover, backward pass w.r.t. weights in such cases is also not
 *     well-defined.
 * 
 *   Data handle of *zero-volume* memory is never accessed and hence can be
 *   unset (NULL in case of CPU engine).
 * 
 *  @see \ref dev_guide_understanding_memory_formats
 *  \{
 <p>
 *  Initializes a \p memory_desc memory descriptor using \p ndims, \p dims, \p
 *  data_type, and \p strides.
 * 
 *  The \p strides might be NULL, which means the order of physical dimensions
 *  is the same as the order of logical ones.
 * 
 *  \note The logical order of dimensions is defined by a primitive that
 *        consumes the memory. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_strides(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongPointer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_strides(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongBuffer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_strides(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") long[] strides);

/** Initializes a \p memory_desc memory descriptor using \p ndims, \p dims, \p
 *  data_type, and format \p tag.
 * 
 *  \p tag can be #dnnl_format_tag_any, which allows a primitive to define
 *  the appropriate memory format. In this case, the \p format_kind would be set
 *  to #dnnl_format_kind_any */
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_tag(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_tag(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_by_tag(
        dnnl_memory_desc_t memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);

/** Initializes a \p memory_desc for a given \p parent_memory_desc, with
 *  \p dims sizes and \p offsets. May fail if layout used does not allow
 *  obtain desired submemory. In this case consider using {@code extract} or {@code insert}
 *  primitive */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_submemory(
        dnnl_memory_desc_t memory_desc,
        @Const dnnl_memory_desc_t parent_memory_desc, @Cast("const int64_t*") LongPointer dims,
        @Cast("const int64_t*") LongPointer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_submemory(
        dnnl_memory_desc_t memory_desc,
        @Const dnnl_memory_desc_t parent_memory_desc, @Cast("const int64_t*") LongBuffer dims,
        @Cast("const int64_t*") LongBuffer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_init_submemory(
        dnnl_memory_desc_t memory_desc,
        @Const dnnl_memory_desc_t parent_memory_desc, @Cast("const int64_t*") long[] dims,
        @Cast("const int64_t*") long[] offsets);

/** Initializes an \p out_memory_desc with new \p ndims and \p dims from a \p
 *  in_memory_desc. If \p in_memory_desc and \p out_memory_desc point to the
 *  same memory descriptor, then \p in_memory_desc is re-initialized with new
 *  values. \p out_memory_desc inherits data type from \p in_memory_desc and has
 *  same blocked format_kind. Others format_kind's are not supported.
 * 
 *  \note Limitation: the only currently supported reshape operation is
 *                    appending 1-sized dimensions to the end. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        dnnl_memory_desc_t out_memory_desc,
        @Const dnnl_memory_desc_t in_memory_desc, int ndims,
        @Cast("const int64_t*") LongPointer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        dnnl_memory_desc_t out_memory_desc,
        @Const dnnl_memory_desc_t in_memory_desc, int ndims,
        @Cast("const int64_t*") LongBuffer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        dnnl_memory_desc_t out_memory_desc,
        @Const dnnl_memory_desc_t in_memory_desc, int ndims,
        @Cast("const int64_t*") long[] dims);

/** Compares two memory descriptors.
 *  @return 1 if the descriptors are the same.
 *  @return 0 if the descriptors are different.
 * 
 *  Use this function to identify whether a reorder is required between the
 *  two memories */
public static native int dnnl_memory_desc_equal(
        @Const dnnl_memory_desc_t lhs, @Const dnnl_memory_desc_t rhs);

/** Returns the size (in bytes) that is required for given \p memory_desc */
public static native @Cast("size_t") long dnnl_memory_desc_get_size(
        @Const dnnl_memory_desc_t memory_desc);

/** Creates a memory for given \p memory_desc and \p engine. Also sets \p
 *  handle to one of the following:
 *  - pointer to the user allocated memory, i.e. valid handle. In this case the
 *    library doesn't own allocated memory.
 *  - DNNL_MEMORY_ALLOCATE to ask the library to allocate and
 *    attach memory. In this case the library owns allocated memory.
 *  - DNNL_MEMORY_NONE to create dnnl_memory w/o attached memory. */
public static native @Cast("dnnl_status_t") int dnnl_memory_create(@ByPtrPtr dnnl_memory memory,
        @Const dnnl_memory_desc_t memory_desc, dnnl_engine engine,
        Pointer handle);
public static native @Cast("dnnl_status_t") int dnnl_memory_create(@Cast("dnnl_memory_t*") PointerPointer memory,
        @Const dnnl_memory_desc_t memory_desc, dnnl_engine engine,
        Pointer handle);

/** Returns a \p memory_desc associated with \p memory. */
public static native @Cast("dnnl_status_t") int dnnl_memory_get_memory_desc(
        @Const dnnl_memory memory, @Cast("const dnnl_memory_desc_t**") PointerPointer memory_desc);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_memory_desc(
        @Const dnnl_memory memory, @Const @ByPtrPtr dnnl_memory_desc_t memory_desc);

/** Returns an \p engine associated with \p memory. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_get_engine(
        @Const dnnl_memory memory, @ByPtrPtr dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_engine(
        @Const dnnl_memory memory, @Cast("dnnl_engine_t*") PointerPointer engine);

/** For a \p memory, maps the data of the memory to \p mapped_ptr.
 * 
 *  Mapping allows to read/write directly from/to the memory contents for
 *  engines that do not support direct memory access.
 * 
 *  Mapping is an exclusive operation - a memory object cannot be used in other
 *  operations until this memory object is unmapped.
 * 
 *  \note Any primitives working with \p memory should be completed before
 *        mapping the memory. Use dnnl_stream_wait to synchronize the
 *        corresponding execution stream.
 * 
 *  \note Map/unmap API is provided mainly for debug/testing purposes and its
 *        performance may be suboptimal. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_map_data(
        @Const dnnl_memory memory, @Cast("void**") PointerPointer mapped_ptr);
public static native @Cast("dnnl_status_t") int dnnl_memory_map_data(
        @Const dnnl_memory memory, @Cast("void**") @ByPtrPtr Pointer mapped_ptr);

/** For a \p memory, unmaps a mapped pointer to the data of the memory.
 * 
 *  Any changes of the mapped data are synchronized back to the memory after the
 *  call is complete. The mapped pointer must be obtained through a
 *  dnnl_memory_map_data call.
 * 
 *  \note Map/unmap API is provided mainly for debug/testing purposes and its
 *        performance may be suboptimal. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_unmap_data(
        @Const dnnl_memory memory, Pointer mapped_ptr);

/** For a \p memory, returns the data \p handle.
 * 
 *  For the CPU engine, the data handle is a pointer to the actual data. */
public static native @Cast("dnnl_status_t") int dnnl_memory_get_data_handle(
        @Const dnnl_memory memory, @Cast("void**") PointerPointer handle);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_data_handle(
        @Const dnnl_memory memory, @Cast("void**") @ByPtrPtr Pointer handle);

/** For a \p memory, sets the data \p handle. */
public static native @Cast("dnnl_status_t") int dnnl_memory_set_data_handle(
        dnnl_memory memory, Pointer handle);

// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif

/** Deletes a \p memory. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_destroy(dnnl_memory memory);

/** \}
 <p>
 *  \addtogroup c_api_reorder Reorder
 *  A primitive to copy data between memory formats.
 * 
 *  @see \ref dev_guide_reorder in developer guide
 *  @see \ref cpp_api_reorder in \ref cpp_api
 *  \{
 <p>
 *  Initializes a \p reorder_primitive_desc using the description of the source
 *  (\p src_engine and \p src_md) and destination (\p dst_engine and \p dst_md)
 *  memory, and an \p attr attribute.
 * 
 *  Inputs:
 *   - input (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - output (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_reorder_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc reorder_primitive_desc,
        @Const dnnl_memory_desc_t src_md, dnnl_engine src_engine,
        @Const dnnl_memory_desc_t dst_md, dnnl_engine dst_engine,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_reorder_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer reorder_primitive_desc,
        @Const dnnl_memory_desc_t src_md, dnnl_engine src_engine,
        @Const dnnl_memory_desc_t dst_md, dnnl_engine dst_engine,
        @Const dnnl_primitive_attr attr);

/** \}
 <p>
 *  \addtogroup c_api_concat Concat
 *  A primitive to concatenate data by arbitrary dimension.
 * 
 *  @see \ref dev_guide_concat in developer guide
 *  @see \ref cpp_api_concat in \ref cpp_api
 *  \{
 <p>
 *  Creates out-of-place \p concat_primitive_desc for concatenation of \p n
 *  inputs by \p concat_dimension with resulting \p output_desc memory
 *  descriptor. \p output_desc can be NULL or specified with the
 *  #dnnl_format_kind_any format kind -- in this case, the appropriate memory
 *  format would be chosen automatically.
 * 
 *  Inputs:
 *   - input 0 (#dnnl_query_src_md, 0)
 *   - input 1 (#dnnl_query_src_md, 1)
 *   - ...
 *   - input \p n - 1 (#dnnl_query_src_md, \p n - 1)
 * 
 *  Outputs:
 *   - output (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_concat_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc concat_primitive_desc,
        @Const dnnl_memory_desc_t dst_md, int n, int concat_dimension,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_concat_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer concat_primitive_desc,
        @Const dnnl_memory_desc_t dst_md, int n, int concat_dimension,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);

/** \}
 <p>
 *  \addtogroup c_api_sum Sum
 *  A primitive to sum data.
 * 
 *  @see \ref dev_guide_sum in developer guide
 *  @see \ref cpp_api_sum in \ref cpp_api
 *  \{
 <p>
 *  Creates out-of-place \p sum_primitive_desc for sum of \p n
 *  inputs multiplied by scale with resulting \p output_desc memory
 *  descriptor. \p output_desc can be NULL or specified with the
 *  #dnnl_format_kind_any format kind -- in this case, the appropriate memory
 *  format would be chosen automatically.
 * 
 *  Inputs:
 *   - src 0 (#dnnl_query_src_md, 0)
 *   - src 1 (#dnnl_query_src_md, 1)
 *   - ...
 *   - src \p n - 1 (#dnnl_query_src_md, \p n - 1)
 * 
 *  Outputs:
 *   - output (#dnnl_query_dst_md, 0) */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const FloatPointer scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const FloatBuffer scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const float[] scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const FloatPointer scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const FloatBuffer scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc,
        @Const dnnl_memory_desc_t dst_mds, int n, @Const float[] scales,
        @Const dnnl_memory_desc_t src_mds, @Const dnnl_primitive_attr attr,
        dnnl_engine engine);

/** \}
 <p>
 *  \addtogroup c_api_binary Binary
 *  A primitive to perform tensor operations over two tensors.
 * 
 *   @see \ref dev_guide_binary in developer guide
 *   @see \ref cpp_api_binary in \ref cpp_api
 *  \{
 <p>
 *  Initializes a binary descriptor \p binary_desc, \p alg_kind (possible
 *  values are #dnnl_binary_add and #dnnl_binary_mul), and memory descriptors.
 * 
 *  \note Memory descriptor \p dst_desc can have \p format_kind set to
 *        #dnnl_format_kind_any.
 * 
 *  \note Both memory descriptors must have the same number of dimensions.
 *        Element broadcasting is supported for memory descriptor \p src1_desc
 *        and are applied to dimensions for which \ src1_desc has size equal
 *        to 1.
 * 
 *  Inputs:
 *   - src0 (#dnnl_query_src_md, 0)
 *   - src1 (#dnnl_query_src_md, 1)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_binary_desc_init(dnnl_binary_desc_t binary_desc,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src0_desc,
        @Const dnnl_memory_desc_t src1_desc,
        @Const dnnl_memory_desc_t dst_desc);

/** \}
 <p>
 *  \addtogroup c_api_convolution Convolution
 *  The convolution primitive computes a forward, backward, or weight update for
 *  a batched convolution operation on 1D, 2D, or 3D spatial data with bias.
 * 
 *   @see \ref dev_guide_convolution in developer guide
 *   @see \ref cpp_api_convolution in \ref cpp_api
 *  \{
 <p>
 *  Initializes a convolution descriptor \p conv_desc for forward propagation
 *  using \p prop_kind (possible values are #dnnl_forward_training and
 *  #dnnl_forward_inference), \p alg_kind, memory descriptors, \p strides, \p
 *  padding_l, and \p padding_r. In order to create a
 *  convolution without bias, \p bias_desc should either be \c NULL or point to
 *  a descriptor with memory format kind equal to #dnnl_format_kind_undef.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 *   - bias (#dnnl_query_weights_md, 1), if created with bias
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** Initializes a dilated convolution descriptor \p conv_desc for forward
 *  propagation using \p prop_kind (possible values are #dnnl_forward_training
 *  and #dnnl_forward_inference), \p alg_kind, memory descriptors, \p strides,
 *  \p dilates, \p padding_l, and \p padding_r.
 *  In order to create a dilated convolution without bias, \p bias_desc
 *  should either be \c NULL or point to a descriptor with memory format kind
 *  equals #dnnl_format_kind_undef.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 *   - bias (#dnnl_query_weights_md, 1), if created with bias
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_forward_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a convolution descriptor \p conv_desc for backward propagation
 *  with respect to data using \p alg_kind, memory descriptors, \p strides, \p
 *  padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a dilated convolution descriptor \p conv_desc for backward
 *  propagation with respect to data using \p alg_kind, memory descriptors, \p
 *  strides, \p dilates \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_data_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** Initializes a convolution descriptor \p conv_desc for backward propagation
 *  with respect to weights using \p alg_kind, memory descriptors, \p strides,
 *  \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_weights (#dnnl_query_diff_weights_md, 0)
 *   - diff_bias (#dnnl_query_diff_weights_md, 1), if created with bias */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a convolution descriptor \p conv_desc for backward propagation
 *  with respect to weights using \p alg_kind, memory descriptors, \p strides,
 *  \p dilates \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_weights (#dnnl_query_diff_weights_md, 0)
 *   - diff_bias (#dnnl_query_diff_weights_md, 1), if created with bias */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_convolution_backward_weights_desc_init(
        dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** \}
 <p>
 *  \addtogroup c_api_deconvolution Deconvolution
 *  A primitive to compute deconvolution using different algorithms.
 * 
 *  \{
 <p>
 *  Initializes a deconvolution descriptor \p deconv_desc for forward
 *  propagation using \p prop_kind (possible values are #dnnl_forward_training
 *  and #dnnl_forward_inference), \p alg_kind, memory descriptors, \p strides,
 *  \p padding_l, and \p padding_r. In order to create a
 *  deconvolution without bias, \p bias_desc should either be \c NULL or point to
 *  a descriptor with memory format kind equals #dnnl_format_kind_undef.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 *   - bias (#dnnl_query_weights_md, 1), if created with bias
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** Initializes a dilated deconvolution descriptor \p deconv_desc for forward
 *  propagation using \p prop_kind (possible values are #dnnl_forward_training
 *  and #dnnl_forward_inference), \p alg_kind, memory descriptors, \p strides,
 *  \p dilates, \p padding_l, and \p padding_r. In order to
 *  create a dilated deconvolution without bias, \p bias_desc should either be
 *  \c NULL or point to a descriptor with memory format kind equal
 *  #dnnl_format_kind_undef.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 *   - bias (#dnnl_query_weights_md, 1), if created with bias
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_forward_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc, @Const dnnl_memory_desc_t dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a deconvolution descriptor \p conv_desc for backward propagation
 *  with respect to data using \p alg_kind, memory descriptors, \p strides, \p
 *  padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a dilated deconvolution descriptor \p conv_desc for backward
 *  propagation with respect to data using \p alg_kind, memory descriptors, \p
 *  strides, \p dilates, \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_data_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** Initializes a deconvolution descriptor \p conv_desc for backward propagation
 *  with respect to weights using \p alg_kind, memory descriptors, \p strides,
 *  \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_weights (#dnnl_query_diff_weights_md, 0)
 *   - diff_bias (#dnnl_query_diff_weights_md, 1), if created with bias */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r);

/** Initializes a dilated deconvolution descriptor \p conv_desc for backward
 *  propagation with respect to weights using \p alg_kind, memory descriptors,
 *  \p strides, \p dilates, \p padding_l, and \p padding_r.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_weights (#dnnl_query_diff_weights_md, 0)
 *   - diff_bias (#dnnl_query_diff_weights_md, 1), if created with bias */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_dilated_deconvolution_backward_weights_desc_init(
        @Cast("dnnl_deconvolution_desc_t*") dnnl_convolution_desc_t conv_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** \}
 <p>
 *  \addtogroup c_api_shuffle Shuffle
 *  A primitive to shuffle data along the axis.
 * 
 *  @see \ref dev_guide_shuffle in developer guide
 *  @see \ref cpp_api_shuffle in \ref cpp_api
 *  \{
 <p>
 *  Initializes a \p shuffle_desc for forward propagation using \p prop_kind,
 *  memory descriptor \p data_desc, \p axis, and \p group_size.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *  */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_shuffle_forward_desc_init(
        dnnl_shuffle_desc_t shuffle_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t data_desc, int axis, @Cast("dnnl_dim_t") long group_size);

/** Initializes a \p shuffle_desc for backward propagation using memory
 *  descriptor \p diff_data_desc, \p axis, and \p group_size.
 * 
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0)
 *  */

///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_shuffle_backward_desc_init(
        dnnl_shuffle_desc_t shuffle_desc,
        @Const dnnl_memory_desc_t diff_data_desc, int axis,
        @Cast("dnnl_dim_t") long group_size);

/** \}
 <p>
 *  \addtogroup c_api_eltwise Eltwise
 *  A primitive to compute element-wise operations such as parametric rectifier
 *  linear unit (ReLU).
 * 
 *  Both forward and backward passes support in-place operation; that is, src
 *  and dst point to the same memory for forward pass, and diff_dst and diff_src
 *  point to the same memory for backward pass.
 * 
 *  \warning Because the original src is required for backward pass, in-place
 *  forward pass in general cannot be applied during training. However, for some
 *  kinds of element-wise operations (namely ReLU with alpha parameter equals 0),
 *  dst and src can be interchangeable for the backward pass, which enables
 *  performance of in-place forward even for training.
 * 
 *  @see \ref dev_guide_eltwise in developer guide
 *  @see \ref cpp_api_eltwise in \ref cpp_api
 * 
 *  \{
 <p>
 *  Initializes an \p eltwise_desc for forward propagation using \p prop_kind
 *  (possible values are #dnnl_forward_training and #dnnl_forward_inference),
 *  \p alg_kind algorithm, memory descriptor \p data_desc, \p alpha, and
 *  \p beta parameters.
 * 
 *  @see dnnl_eltwise_desc_t for details.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_eltwise_forward_desc_init(
        dnnl_eltwise_desc_t eltwise_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t data_desc,
        float alpha, float beta);

/** Initializes an \p eltwise_desc for backward propagation using \p alg_kind
 *  algorithm memory descriptors \p diff_data_desc and \p data_desc, and the
 *  \p alpha and \p beta parameters.
 * 
 *  @see dnnl_eltwise_desc_t for details.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_eltwise_backward_desc_init(
        dnnl_eltwise_desc_t eltwise_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_data_desc,
        @Const dnnl_memory_desc_t data_desc, float alpha, float beta);

/** \}
 <p>
 *  \addtogroup c_api_softmax Softmax
 *  A primitive to perform softmax.
 * 
 *  @see \ref dev_guide_softmax in developer guide
 *  @see \ref cpp_api_softmax in \ref cpp_api
 *  \{
 <p>
 *  Initializes a \p softmax_desc for forward propagation using \p prop_kind
 *  (possible values are #dnnl_forward_training and #dnnl_forward_inference)
 *  and memory descriptor \p data_desc.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
public static native @Cast("dnnl_status_t") int dnnl_softmax_forward_desc_init(
        dnnl_softmax_desc_t softmax_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t data_desc, int softmax_axis);

/** Initializes a \p softmax_desc for backward propagation using memory
 *  descriptors \p diff_desc and \p data_desc.
 * 
 *  Inputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_softmax_backward_desc_init(
        dnnl_softmax_desc_t softmax_desc, @Const dnnl_memory_desc_t diff_desc,
        @Const dnnl_memory_desc_t data_desc, int softmax_axis);

/** \}
 <p>
 *  \addtogroup c_api_pooling Pooling
 *  A primitive to perform max or average pooling.
 * 
 *  @see \ref dev_guide_pooling in developer guide
 *  @see \ref cpp_api_pooling in \ref cpp_api
 * 
 *  \{
 <p>
 *  Initializes a pooling descriptor \p pool_desc for forward propagation using
 *  \p prop_kind (possible values are #dnnl_forward_training and
 *  #dnnl_forward_inference), \p alg_kind, memory descriptors, and pooling
 *  parameters in the spatial domain: \p strides, \p kernel sizes, \p padding_l,
 *  and \p padding_r.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p alg_kind = #dnnl_pooling_max and
 *       \p prop_kind = #dnnl_forward_training */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer kernel, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer kernel, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] kernel, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** Initializes a pooling descriptor \p pool_desc for backward propagation
 *  using \p alg_kind, memory descriptors, and pooling parameters in the spatial
 *  domain: \p strides, \p kernel sizes, \p padding_l, and \p padding_r.
 * 
 *  \note If \p padding_r is \c NULL, the padding is supposed to be symmetric.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p alg_kind = #dnnl_pooling_max
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer kernel, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer kernel, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_desc_init(
        dnnl_pooling_desc_t pool_desc, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] kernel, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r);

/** \}
 <p>
 *  \addtogroup c_api_lrn LRN
 *  A primitive to perform local response normalization (LRN) across or within
 *  channels.
 * 
 *  @see dnnl_primitive_desc_query and dnnl_primitive_desc_query_pd
 * 
 *  @see \ref dev_guide_lrn in developer guide
 *  @see \ref cpp_api_lrn in \ref cpp_api
 * 
 *  \{
 <p>
 *  Initializes an \p lrn_desc for forward propagation using \p prop_kind
 *  (possible values are #dnnl_forward_training and #dnnl_forward_inference),
 *  \p alg_kind, memory descriptor \p data_desc, and regularization
 *  parameters \p local_size, \p alpha, \p beta, and \p k.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if the underlying implementation requires */

///
///
public static native @Cast("dnnl_status_t") int dnnl_lrn_forward_desc_init(dnnl_lrn_desc_t lrn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc_t data_desc, @Cast("dnnl_dim_t") long local_size, float alpha,
        float beta, float k);

/** Initializes an \p lrn_desc for backward propagation using \p alg_kind,
 *  memory descriptors \p data_desc and \p diff_data_desc, and regularization
 *  parameters \p local_size, \p alpha, \p beta, and \p k.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if the underlying implementation requires
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lrn_backward_desc_init(dnnl_lrn_desc_t lrn_desc,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc_t diff_data_desc,
        @Const dnnl_memory_desc_t data_desc, @Cast("dnnl_dim_t") long local_size, float alpha,
        float beta, float k);

/** \}
 <p>
 *  \addtogroup c_api_batch_normalization Batch Normalization
 *  A primitive to perform batch normalization.
 * 
 *  Both forward and backward passes support in-place operation; that is, src
 *  and dst point to the same memory for forward pass, and diff_dst and diff_src
 *  point to the same memory for backward pass.
 * 
 *  Batch normalization supports different flavors controlled by
 *  dnnl_batch_normalization_desc_t. For example, batch normalization can
 *  compute the mean and variance on its own or take them as inputs. It can
 *  either perform scaling and shifting using gamma and beta parameters or not.
 *  Optionally, it can also perform a fused ReLU, which in case of training would
 *  also require a workspace.
 * 
 *  @see dnnl_batch_normalization_desc_t
 * 
 *  @see \ref dev_guide_batch_normalization in developer guide
 *  @see \ref cpp_api_batch_normalization in \ref cpp_api
 *  \{
 <p>
 *  Initializes a batch normalization descriptor \p bnrm_desc for forward
 *  propagation using \p prop_kind (possible values are
 *  #dnnl_forward_training and #dnnl_forward_inference), memory descriptor
 *  \p data_desc, normalization parameter \p epsilon, and \p flags set using bit
 *  flags of type dnnl_batch_normalization_desc_t.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - mean (#dnnl_query_src_md, 1),
 *       if #dnnl_use_global_stats bit-flags is set in \p flags
 *   - variance (#dnnl_query_src_md, 2),
 *       if #dnnl_use_global_stats bit-flags is set in \p flags
 *   - scale_and_shift (#dnnl_query_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *   - mean (#dnnl_query_dst_md, 1),
 *       if #dnnl_use_global_stats bit-flags is not set in \p flags
 *       \p prop_kind = #dnnl_forward_training
 *   - variance (#dnnl_query_dst_md, 2),
 *       if #dnnl_use_global_stats bit-flags is not set in \p flags
 *       and \p prop_kind = #dnnl_forward_training
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if #dnnl_fuse_norm_relu bit-flags is set in \p flags
 *       and \p prop_kind = #dnnl_forward_training
 * 
 *  \note In-place operation is supported; that is, dst points to the same memory
 *        as src.
 * 
 *  @see dnnl_batch_normalization_desc_t */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_forward_desc_init(
        dnnl_batch_normalization_desc_t bnrm_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t data_desc, float epsilon, @Cast("unsigned") int flags);

/** Initializes a batch normalization descriptor \p bnrm_desc for backward
 *  propagation with respect to data and scale-shift parameters using memory
 *  descriptors \p data_desc and \p diff_data_desc, normalization parameter
 *  \p epsilon, and \p flags set using bit flags of type
 *  dnnl_batch_normalization_desc_t.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - mean (#dnnl_query_src_md, 1)
 *   - variance (#dnnl_query_src_md, 2)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - scale_and_shift (#dnnl_query_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if #dnnl_fuse_norm_relu bit-flags is set in \p flags
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0)
 *   - diff_scale_and_shift (#dnnl_query_diff_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 *       and \p prop_kind = #dnnl_backward
 * 
 *  \note in-place operation is supported,
 *        i.e. diff_src points to the same memory as diff_dst.
 * 
 *  @see dnnl_batch_normalization_desc_t */

///
///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_backward_desc_init(
        dnnl_batch_normalization_desc_t bnrm_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t diff_data_desc,
        @Const dnnl_memory_desc_t data_desc, float epsilon, @Cast("unsigned") int flags);

/** \}
 <p>
 *  \addtogroup c_api_layer_normalization Layer Normalization
 *  A primitive to perform layer normalization. Normalization is performed over
 *  the last logical axis of data tensor.
 * 
 *  Both forward and backward passes support in-place operation; that is, src
 *  and dst point to the same memory for forward pass, and diff_dst and diff_src
 *  point to the same memory for backward pass.
 * 
 *  Layer normalization supports different flavors controlled by
 *  dnnl_layer_normalization_desc_t. For example, layer normalization can
 *  compute the mean and variance on its own or take them as inputs. It can
 *  either perform scaling and shifting using gamma and beta parameters or not.
 * 
 *  @see dnnl_layer_normalization_desc_t
 * 
 *  @see \ref dev_guide_layer_normalization in developer guide
 *  @see \ref cpp_api_layer_normalization in \ref cpp_api
 *  \{
 <p>
 *  Initializes a layer normalization descriptor \p lnrm_desc for forward
 *  propagation using \p prop_kind (possible values are
 *  #dnnl_forward_training and #dnnl_forward_inference), memory descriptor
 *  \p data_desc, normalization parameter \p epsilon, and \p flags set using bit
 *  flags of type dnnl_layer_normalization_desc_t.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - mean (#dnnl_query_src_md, 1),
 *       if #dnnl_use_global_stats bit-flags is set in \p flags
 *   - variance (#dnnl_query_src_md, 2),
 *       if #dnnl_use_global_stats bit-flags is set in \p flags
 *   - scale_and_shift (#dnnl_query_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0)
 *   - mean (#dnnl_query_dst_md, 1),
 *       if #dnnl_use_global_stats bit-flags is not set in \p flags
 *       \p prop_kind = #dnnl_forward_training
 *   - variance (#dnnl_query_dst_md, 2),
 *       if #dnnl_use_global_stats bit-flags is not set in \p flags
 *       and \p prop_kind = #dnnl_forward_training
 * 
 *  \note In-place operation is supported; that is, dst points to the same memory
 *        as src.
 * 
 *  @see dnnl_layer_normalization_desc_t */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_forward_desc_init(
        dnnl_layer_normalization_desc_t lnrm_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t data_desc,
        @Const dnnl_memory_desc_t stat_desc, float epsilon, @Cast("unsigned") int flags);

/** Initializes a layer normalization descriptor \p lnrm_desc for backward
 *  propagation with respect to data and scale-shift parameters using memory
 *  descriptors \p data_desc and \p diff_data_desc, normalization parameter
 *  \p epsilon, and \p flags set using bit flags of type
 *  dnnl_layer_normalization_desc_t.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - mean (#dnnl_query_src_md, 1)
 *   - variance (#dnnl_query_src_md, 2)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - scale_and_shift (#dnnl_query_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0)
 *   - diff_scale_and_shift (#dnnl_query_diff_weights_md, 0),
 *       if #dnnl_use_scaleshift bit-flags is set in \p flags
 *       and \p prop_kind = #dnnl_backward
 * 
 *  \note in-place operation is supported,
 *        i.e. diff_src points to the same memory as diff_dst.
 * 
 *  @see dnnl_layer_normalization_desc_t */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_backward_desc_init(
        dnnl_layer_normalization_desc_t lnrm_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t diff_data_desc,
        @Const dnnl_memory_desc_t data_desc,
        @Const dnnl_memory_desc_t stat_desc, float epsilon, @Cast("unsigned") int flags);

/** \}
 <p>
 *  \addtogroup c_api_inner_product Inner product
 *  A primitive to compute an inner product.
 * 
 *  @see \ref dev_guide_inner_product in developer guide
 *  @see \ref cpp_api_inner_product in \ref cpp_api
 *  \{
 <p>
 *  Initializes an inner product descriptor \p ip_desc for forward propagation
 *  using \p prop_kind (possible values are #dnnl_forward_training and
 *  #dnnl_forward_inference) and memory descriptors. In order to create an
 *  inner product without bias, \p bias_desc should be either \c NULL or a
 *  pointer to a descriptor with memory format kind equals
 *  #dnnl_format_kind_undef.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 *   - bias (#dnnl_query_weights_md, 1), if created with bias
 * 
 *  Outputs:
 *   - dst (#dnnl_query_dst_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_forward_desc_init(
        dnnl_inner_product_desc_t ip_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_desc);

/** Initializes an inner product descriptor \p ip_desc for backward propagation
 *  with respect to data using memory descriptors.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 *   - weights (#dnnl_query_weights_md, 0)
 * 
 *  Outputs:
 *   - diff_src (#dnnl_query_diff_src_md, 0) */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_data_desc_init(
        dnnl_inner_product_desc_t ip_desc,
        @Const dnnl_memory_desc_t diff_src_desc,
        @Const dnnl_memory_desc_t weights_desc,
        @Const dnnl_memory_desc_t diff_dst_desc);

/** Initializes an inner product descriptor \p ip_desc for backward propagation
 *  with respect to weights using memory descriptors.
 * 
 *  \note Memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Inputs:
 *   - src (#dnnl_query_src_md, 0)
 *   - diff_dst (#dnnl_query_diff_dst_md, 0)
 * 
 *  Outputs:
 *   - diff_weights (#dnnl_query_diff_weights_md, 0)
 *   - diff_bias (#dnnl_query_diff_weights_md, 1), if created with bias */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_weights_desc_init(
        dnnl_inner_product_desc_t ip_desc, @Const dnnl_memory_desc_t src_desc,
        @Const dnnl_memory_desc_t diff_weights_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_desc);

/** \}
 <p>
 *  \addtogroup c_api_rnn RNN
 *  A primitive to compute the common recurrent layer.
 * 
 *  @see \ref dev_guide_rnn in developer guide
 *  @see \ref cpp_api_rnn in \ref cpp_api
 *  \{
 <p>
 *  Sets quantization \p scale and \p shift for RNN data tensors.
 *  For performance reasons, low precision configuration of RNN primitive
 *  expects input activations to have unsigned int8 data type. Scale and shift
 *  used to quantize floating point data to unsigned integer must be passed to
 *  RNN primitive using attributes.
 *  Example usage:
 *  <pre>{@code
 *      // rnn parameters
 *      int l = 2, t = 2, mb = 32, sic = 32, slc = 32, dic = 32, dlc = 32;
 *      // activations quantization parameters
 *      float scale = ..., shift = ..;
 * 
 *      dnnl_primitive_attr_t rnn_attr;
 *      // create default attributes
 *      dnnl_primitive_attr_create(&rnn_attr);
 * 
 *      // set scale and shift for int8 quantization of activation
 *      dnnl_primitive_attr_set_rnn_data_qparams(rnn_attr, scale, shift);
 * 
 *      // create & configure rnn op_desc
 *      dnnl_rnn_desc_t rnn_d;
 *      dnnl_primitive_desc_t rnn_pd;
 *      dnnl_primitive_desc_create(&rnn_pd, &rnn_d, attr, engine, NULL);
 *  }</pre>
 *  \note
 *      Quantization scale and shift are common for src_layer, src_iter,
 *      dst_iter and dst_layer. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_data_qparams(
        dnnl_primitive_attr attr, float scale, float shift);

/** Sets quantization scales \p weights_scales for RNN weights tensors.
 *  Low precision configuration of RNN primitive expects input weights to have
 *  signed int8 data type. Scales used to quantize floating point data
 *  to signed integer must be passed to RNN primitive using attributes.
 *  The \p mask argument defines correspondence between output tensor dimensions
 *  and the \p weights_scales array. Set i-th bit of \p mask to 1 to use
 *  dedicated scaling factor for each slice of the output tensor over i-th
 *  dimension. Set \p mask to 0 to use common scaling factor for the whole output
 *  tensor. Example usage:
 *  <pre>{@code
 *       // rnn parameters
 *       int l = 2, t = 2, mb = 32, sic = 32, slc = 32, dic = 32, dlc = 32;
 *       // unique output scales per output channel
 *       float weights_scales[dic * n_gates] = { ... };
 *       // mask that specifies last two dimensions of ldigo format
 *       int mask = 0x3;
 * 
 *       dnnl_primitive_attr_t attr;
 *       // create default attributes
 *       dnnl_primitive_attr_create(&attr);
 * 
 *       // set output channel-wise weights scales
 *       dnnl_primitive_attr_set_rnn_weights_qparams(attr, dic * n_gates, mask,
 *               weights_scales);
 * 
 *       // create & configure rnn op_desc
 *       dnnl_rnn_desc_t rnn_d;
 *       dnnl_primitive_desc_t rnn_pd;
 *       dnnl_primitive_desc_create(&rnn_pd, &rnn_d, attr, engine, NULL);
 *  }</pre>
 *  \note
 *       The dimension order is always native and does not depend on the actual
 *       layout used. For example, 5 dimensional weights always have
 *       (l, d, i, g, o) logical dimension ordering.
 *  \note
 *       Quantization sales are common for weights_layer and weights_iteration
 *  \note
 *       There is no way to check that \p count corresponds to \p mask until an
 *       actual primitive descriptor is created, so it is user's responsibility
 *       to set proper values. The following formula must be held:
 * 
 *       <pre>{@code \[count = \prod\limits_{d \in mask} output.dims[d]\]}</pre> */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatPointer weights_scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatBuffer weights_scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const float[] weights_scales);

/** Initializes an RNN descriptor \p rnn_desc for forward propagation
 *  using \p prop_kind, \p activation, \p direction, and memory descriptors.
 *  \note If \p prop_kind equals #dnnl_forward_training, you must query a
 *  workspace memory descriptor before creating the primitive.
 * 
 *  \p src_iter_desc, \p bias_desc, and \p dst_iter_desc are allowed to either be
 *  \c NULL or point to a zero memory descriptor, which would indicate that the
 *  RNN primitive should not use them and will default to zero values.
 * 
 *  \note All memory descriptorsare allowed to be initialized with
 *  #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Parameters:
 *   - activation (#dnnl_eltwise_relu, #dnnl_eltwise_tanh or #dnnl_eltwise_logistic)
 *   - alpha (negative slope if activation is #dnnl_eltwise_relu)
 *   - beta (unused for now)
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 * 
 *  Outputs:
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p prop_kind equals #dnnl_forward_training */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_forward_desc_init(
        dnnl_rnn_desc_t rnn_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("const dnnl_alg_kind_t") int activation, @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc, @Cast("unsigned") int flags, float alpha,
        float beta);

/** Initializes an RNN descriptor \p rnn_desc for backward propagation
 *  using \p prop_kind, \p activation, \p direction, and memory descriptors.
 * 
 *  \note All memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  \p src_iter_desc (simultaneously with \p diff_src_iter_desc),
 *  \p bias_desc (simultaneously with \p diff_bias_desc), and
 *  \p dst_iter_desc (simultaneously with \p diff_src_iter_desc) are allowed to
 *  either be \c NULL or point to a zero memory descriptor, which would indicate
 *  that the RNN primitive should not use them and will default to zero values.
 * 
 *  Parameters:
 *   - activation (#dnnl_eltwise_relu, #dnnl_eltwise_tanh or #dnnl_eltwise_logistic)
 *   - alpha (negative slope if activation is #dnnl_eltwise_relu)
 *   - beta (unused for now)
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - diff_dst_layer (#dnnl_query_diff_dst_md, 0)
 *   - diff_dst_iter (#dnnl_query_diff_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0)
 * 
 *  Outputs:
 *   - diff_src_layer (#dnnl_query_diff_src_md, 0)
 *   - diff_src_iter (#dnnl_query_diff_src_md, 1), if used
 *   - diff_weights_layer (#dnnl_query_diff_weights_md, 0)
 *   - diff_weights_iter (#dnnl_query_diff_weights_md, 1)
 *   - diff_bias (#dnnl_query_diff_weights_md, 2), if used */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_backward_desc_init(
        dnnl_rnn_desc_t rnn_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("const dnnl_alg_kind_t") int activation, @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc,
        @Const dnnl_memory_desc_t diff_src_layer_desc,
        @Const dnnl_memory_desc_t diff_src_iter_desc,
        @Const dnnl_memory_desc_t diff_weights_layer_desc,
        @Const dnnl_memory_desc_t diff_weights_iter_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_layer_desc,
        @Const dnnl_memory_desc_t diff_dst_iter_desc, @Cast("unsigned") int flags,
        float alpha, float beta);

/** Initializes an LSTM descriptor \p rnn_desc for forward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 *  \note If \p prop_kind equals #dnnl_forward_training, you must query a
 *  workspace memory descriptor before creating the primitive.
 * 
 *  \p src_iter_desc, \p bias_desc, and \p dst_iter_desc are allowed to either be
 *  \c NULL or point to a zero memory descriptor, which would indicate that the
 *  RNN primitive should not use them and will default to zero values.
 * 
 *  \note All memory descriptors except \p src_iter_desc are allowed to be
 *        initialized with #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - src_iter_c (#dnnl_query_src_md, 2), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 * 
 *  Outputs:
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - dst_iter_c (#dnnl_query_dst_md, 2), if used
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p prop_kind equals #dnnl_forward_training */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lstm_forward_desc_init(dnnl_rnn_desc_t rnn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t src_iter_c_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc,
        @Const dnnl_memory_desc_t dst_iter_c_desc, @Cast("unsigned") int flags);

/** Initializes an LSTM descriptor \p rnn_desc for backward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 * 
 *  \note All memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  \p src_iter_desc (simultaneously with \p diff_src_iter_desc),
 *  \p bias_desc (simultaneously with \p diff_bias_desc), and
 *  \p dst_iter_desc (simultaneously with \p diff_src_iter_desc) are allowed to
 *  either be \c NULL or point to a zero memory descriptor, which would indicate
 *  that the RNN primitive should not use them and will default to zero values.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - src_iter_c (#dnnl_query_src_md, 2), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - dst_iter_c (#dnnl_query_dst_md, 2), if used
 *   - diff_dst_layer (#dnnl_query_diff_dst_md, 0)
 *   - diff_dst_iter (#dnnl_query_diff_dst_md, 1), if used
 *   - diff_dst_iter_c (#dnnl_query_diff_dst_md, 2), if used
 *   - workspace (#dnnl_query_workspace_md, 0)
 * 
 *  Outputs:
 *   - diff_src_layer (#dnnl_query_diff_src_md, 0)
 *   - diff_src_iter (#dnnl_query_diff_src_md, 1), if used
 *   - diff_src_iter_c (#dnnl_query_diff_src_md, 2), if used
 *   - diff_weights_layer (#dnnl_query_diff_weights_md, 0)
 *   - diff_weights_iter (#dnnl_query_diff_weights_md, 1)
 *   - diff_bias (#dnnl_query_diff_weights_md, 2), if used */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lstm_backward_desc_init(dnnl_rnn_desc_t rnn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t src_iter_c_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc,
        @Const dnnl_memory_desc_t dst_iter_c_desc,
        @Const dnnl_memory_desc_t diff_src_layer_desc,
        @Const dnnl_memory_desc_t diff_src_iter_desc,
        @Const dnnl_memory_desc_t diff_src_iter_c_desc,
        @Const dnnl_memory_desc_t diff_weights_layer_desc,
        @Const dnnl_memory_desc_t diff_weights_iter_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_layer_desc,
        @Const dnnl_memory_desc_t diff_dst_iter_desc,
        @Const dnnl_memory_desc_t diff_dst_iter_c_desc, @Cast("unsigned") int flags);

/** Initializes a GRU descriptor \p rnn_desc for forward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 *  \note If \p prop_kind equals #dnnl_forward_training, you must query a
 *  workspace memory descriptor before creating the primitive.
 * 
 *  \p src_iter_desc, \p bias_desc, and \p dst_iter_desc are allowed to either be
 *  \c NULL or point to a zero memory descriptor, which would indicate that the
 *  RNN primitive should not use them and will default to zero values.
 * 
 *  \note All memory descriptors except \p src_iter_desc are allowed to be
 *        initialized with #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 * 
 *  Outputs:
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p prop_kind equals #dnnl_forward_training */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_gru_forward_desc_init(dnnl_rnn_desc_t rnn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc, @Cast("unsigned") int flags);

/** Initializes a GRU descriptor \p rnn_desc for backward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 * 
 *  \note All memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  \p src_iter_desc (simultaneously with \p diff_src_iter_desc),
 *  \p bias_desc (simultaneously with \p diff_bias_desc), and
 *  \p dst_iter_desc (simultaneously with \p diff_src_iter_desc) are allowed to
 *  either be \c NULL or point to a zero memory descriptor, which would indicate
 *  that the RNN primitive should not use them and will default to zero values.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - diff_dst_layer (#dnnl_query_diff_dst_md, 0)
 *   - diff_dst_iter (#dnnl_query_diff_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0)
 * 
 *  Outputs:
 *   - diff_src_layer (#dnnl_query_diff_src_md, 0)
 *   - diff_src_iter (#dnnl_query_diff_src_md, 1), if used
 *   - diff_weights_layer (#dnnl_query_diff_weights_md, 0)
 *   - diff_weights_iter (#dnnl_query_diff_weights_md, 1)
 *   - diff_bias (#dnnl_query_diff_weights_md, 2), if used */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_gru_backward_desc_init(dnnl_rnn_desc_t rnn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc,
        @Const dnnl_memory_desc_t diff_src_layer_desc,
        @Const dnnl_memory_desc_t diff_src_iter_desc,
        @Const dnnl_memory_desc_t diff_weights_layer_desc,
        @Const dnnl_memory_desc_t diff_weights_iter_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_layer_desc,
        @Const dnnl_memory_desc_t diff_dst_iter_desc, @Cast("unsigned") int flags);

/** Initializes an LBR GRU descriptor \p rnn_desc for forward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 *  \note If \p prop_kind equals #dnnl_forward_training, you must query a
 *  workspace memory descriptor before creating the primitive.
 * 
 *  \p src_iter_desc, \p bias_desc, and \p dst_iter_desc are allowed to either be
 *  \c NULL or point to a zero memory descriptor, which would indicate that the
 *  RNN primitive should not use them and will default to zero values.
 * 
 *  \note All memory descriptors except \p src_iter_desc are allowed to be
 *        initialized with #dnnl_format_kind_any value of \p format_kind.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 * 
 *  Outputs:
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0),
 *       if \p prop_kind equals #dnnl_forward_training */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_forward_desc_init(dnnl_rnn_desc_t rnn_desc,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc, @Cast("unsigned") int flags);

/** Initializes an LBR GRU descriptor \p rnn_desc for backward propagation
 *  using \p prop_kind, \p direction, and memory descriptors.
 * 
 *  \note All memory descriptors are allowed to be initialized with
 *        #dnnl_format_kind_any value of \p format_kind.
 * 
 *  \p src_iter_desc (simultaneously with \p diff_src_iter_desc),
 *  \p bias_desc (simultaneously with \p diff_bias_desc), and
 *  \p dst_iter_desc (simultaneously with \p diff_src_iter_desc) are allowed to
 *  either be \c NULL or point to a zero memory descriptor, which would indicate
 *  that the RNN primitive should not use them and will default to zero values.
 * 
 *  Parameters:
 *   - flags (unused for now)
 * 
 *  Inputs:
 *   - src_layer (#dnnl_query_src_md, 0)
 *   - src_iter (#dnnl_query_src_md, 1), if used
 *   - weights_layer (#dnnl_query_weights_md, 0)
 *   - weights_iter (#dnnl_query_weights_md, 1)
 *   - bias (#dnnl_query_weights_md, 2), if used
 *   - dst_layer (#dnnl_query_dst_md, 0)
 *   - dst_iter (#dnnl_query_dst_md, 1), if used
 *   - diff_dst_layer (#dnnl_query_diff_dst_md, 0)
 *   - diff_dst_iter (#dnnl_query_diff_dst_md, 1), if used
 *   - workspace (#dnnl_query_workspace_md, 0)
 * 
 *  Outputs:
 *   - diff_src_layer (#dnnl_query_diff_src_md, 0)
 *   - diff_src_iter (#dnnl_query_diff_src_md, 1), if used
 *   - diff_weights_layer (#dnnl_query_diff_weights_md, 0)
 *   - diff_weights_iter (#dnnl_query_diff_weights_md, 1)
 *   - diff_bias (#dnnl_query_diff_weights_md, 2), if used */
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_backward_desc_init(
        dnnl_rnn_desc_t rnn_desc, @Cast("dnnl_prop_kind_t") int prop_kind,
        @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc_t src_layer_desc,
        @Const dnnl_memory_desc_t src_iter_desc,
        @Const dnnl_memory_desc_t weights_layer_desc,
        @Const dnnl_memory_desc_t weights_iter_desc,
        @Const dnnl_memory_desc_t bias_desc,
        @Const dnnl_memory_desc_t dst_layer_desc,
        @Const dnnl_memory_desc_t dst_iter_desc,
        @Const dnnl_memory_desc_t diff_src_layer_desc,
        @Const dnnl_memory_desc_t diff_src_iter_desc,
        @Const dnnl_memory_desc_t diff_weights_layer_desc,
        @Const dnnl_memory_desc_t diff_weights_iter_desc,
        @Const dnnl_memory_desc_t diff_bias_desc,
        @Const dnnl_memory_desc_t diff_dst_layer_desc,
        @Const dnnl_memory_desc_t diff_dst_iter_desc, @Cast("unsigned") int flags);

/** \}
 <p>
 *  \}
 <p>
 *  \addtogroup c_api_engine Engine operations
 *  \{
 <p>
 *  Returns the number of engines of a particular \p kind. */
public static native @Cast("size_t") long dnnl_engine_get_count(@Cast("dnnl_engine_kind_t") int kind);

/** Creates an \p engine of particular \p kind and \p index. */
public static native @Cast("dnnl_status_t") int dnnl_engine_create(
        @ByPtrPtr dnnl_engine engine, @Cast("dnnl_engine_kind_t") int kind, @Cast("size_t") long index);
public static native @Cast("dnnl_status_t") int dnnl_engine_create(
        @Cast("dnnl_engine_t*") PointerPointer engine, @Cast("dnnl_engine_kind_t") int kind, @Cast("size_t") long index);

// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif

/** Returns the kind of an \p engine. */
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") IntPointer kind);
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") IntBuffer kind);
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") int[] kind);

// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif

/** Returns the kind of an \p engine. */

/** Destroys an \p engine. */
public static native @Cast("dnnl_status_t") int dnnl_engine_destroy(dnnl_engine engine);

/** \}
 <p>
 *  \addtogroup c_api_stream Execution stream operations
 *  \{
 <p>
 *  Creates an execution \p stream for \p engine and with \p flags. */
public static native @Cast("dnnl_status_t") int dnnl_stream_create(
        @ByPtrPtr dnnl_stream stream, dnnl_engine engine, @Cast("unsigned") int flags);
public static native @Cast("dnnl_status_t") int dnnl_stream_create(
        @Cast("dnnl_stream_t*") PointerPointer stream, dnnl_engine engine, @Cast("unsigned") int flags);

// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif

/** Waits for all primitives in the execution \p stream to finish. */
public static native @Cast("dnnl_status_t") int dnnl_stream_wait(dnnl_stream stream);

/** Destroys an execution \p stream. */

///
public static native @Cast("dnnl_status_t") int dnnl_stream_destroy(dnnl_stream stream);

/** \}
 <p>
 *  \addtogroup c_api_service Service functions
 *  \{
 <p>
 *  Sets verbosity level (print information to stdout).
 *  Possible levels are:
 *   - 0 -- no verbose output (default)
 *   - 1 -- primitive information at execution
 *   - 2 -- primitive information at creation and execution
 * 
 *  \note
 *      Dumping information might affect performance.
 *      This setting overrides the DNNL_VERBOSE environment variable. */

///
public static native @Cast("dnnl_status_t") int dnnl_set_verbose(int level);

/** Enables or disables dumping of JIT-generated code.
 *  The enable parameter can be:
 *   - 0 -- disable
 *   - any other value -- enable
 * 
 *  \note
 *      This setting overrides the DNNL_JIT_DUMP environment variable. */
public static native @Cast("dnnl_status_t") int dnnl_set_jit_dump(int enable);

/** Gets library version information.
 *  Version information includes:
 *   - major -- major version number
 *   - minor -- minor version number
 *   - patch -- patch release number
 *   - hash -- git commit hash */

///
///
///
///
public static native @Const dnnl_version_t dnnl_version();

/** \}
 <p>
 *  \addtogroup c_api_blas BLAS functions
 *  A subset of Basic Linear ALgebra (BLAS) functions to perform
 *  matrix-matrix multiplication.
 *  \{
 <p>
 *  SGEMM performs a matrix-matrix multiplication operation defined as
 * 
 *  C := alpha*op( A )*op( B ) + beta*C
 * 
 *  where
 *   - op( X ) is one of op( X ) = X or op( X ) = X**T,
 *   - alpha and beta are scalars,
 *   - A, B and C are matrices, with op( A ) an m by k matrix, op( B ) a k by n matrix
 *     and C an m by n matrix.
 * 
 *  The matrices are assumed to be stored in row-major order (the elements
 *  in a matrix rows are contiguous in memory).
 * 
 *  \note
 *       The API is different from the standard BLAS routine
 *       because it returns dnnl_status_t for error handling.
 *       XERBLA is not supported: no error message will be printed
 *       in case of incorrect parameters. */

///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const FloatPointer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatPointer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatPointer C, @Cast("dnnl_dim_t") long ldc);
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const FloatBuffer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatBuffer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatBuffer C, @Cast("dnnl_dim_t") long ldc);
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const float[] A, @Cast("dnnl_dim_t") long lda,
        @Const float[] B, @Cast("dnnl_dim_t") long ldb, float beta, float[] C, @Cast("dnnl_dim_t") long ldc);

/** dnnl_gemm_u8s8s32() and dnnl_gemm_s8s8s32() perform a matrix-matrix
 *  multiplication operation and add the result to a scalar-matrix product.
 *  For the final result, a vector is added to each row or column of the output
 *  matrix.
 * 
 *  The operation is defined as:
 * 
 *  - {@code C := alpha*(op(A) - A_offset) * (op(B) - B_offset) + beta*C + C_offset}
 * 
 *  where
 *   - {@code op( X ) = X} or {@code op( X ) = X**T},
 *   - {@code A_offset} is an m-by-k matrix with every element
 *                equal to the value {@code ao},
 *   - {@code B_offset} is an k-by-n matrix with every element
 *                equal to the value {@code bo},
 *   - {@code C_offset} is an m-by-n matrix defined by the {@code co} array of size {@code len}:
 *     - if {@code offsetc = F}: {@code len} must be at least {@code 1},
 *     - if {@code offsetc = C}: {@code len} must be at least {@code max(1, m)},
 *     - if {@code offsetc = R}: {@code len} must be at least {@code max(1, n)},
 *   - {@code alpha} and {@code beta} are scalars, and
 *   - {@code A}, {@code B} and {@code C} are matrices, with {@code op( A )} an m-by-k matrix,
 *     {@code op( B )} a k-by-n matrix and {@code C} an m-by-n matrix.
 * 
 *  The matrices are assumed to be stored in row-major order (the elements
 *  in a matrix rows are contiguous in memory).
 * 
 *  \note
 *       The API is different compared with the standard BLAS routine.
 *       In particular, the function returns \ref dnnl_status_t for
 *       error handling.
 *       XERBLA is not supported: no error message will be printed
 *       in case of incorrect parameters.
 * 
 *  \warning
 *       On some architectures the intermediate saturation might happen,
 *       which would lead to unexpected results. For more details, refer to
 *       \ref dev_guide_int8_computations. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") BytePointer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") byte[] A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);

/** dnnl_gemm_u8s8s32() and dnnl_gemm_s8s8s32() perform a matrix-matrix
 *  multiplication operation and add the result to a scalar-matrix product.
 *  For the final result, a vector is added to each row or column of the output
 *  matrix.
 * 
 *  For full description, see dnnl_gemm_u8s8s32().
 * 
 *  \warning
 *       On some architectures the intermediate saturation might happen,
 *       which would lead to unexpected results. For more details, refer to
 *       \ref dev_guide_int8_computations. */
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const BytePointer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const byte[] A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);
/** \}
 <p>
 *  \} */

// #ifdef __cplusplus
// #endif

// #endif


// Parsed from dnnl.hpp

/*******************************************************************************
* Copyright 2016-2019 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C++ API */

// #ifndef DNNL_HPP
// #define DNNL_HPP

// #include "dnnl_config.h"

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <algorithm>
// #include <cstdlib>
// #include <iterator>
// #include <memory>
// #include <vector>
// #include <unordered_map>

// #include "dnnl.h"

// #if DNNL_GPU_RUNTIME == DNNL_RUNTIME_OCL
// #endif
/** \endcond */
// Targeting ../error.java



/** A class that provides the destructor for an DNNL C handle */
// Targeting ../dnnl_engine_handle.java


// Targeting ../dnnl_memory_handle.java


// Targeting ../dnnl_primitive_desc_handle.java


// Targeting ../dnnl_primitive_attr_handle.java


// Targeting ../dnnl_post_ops_handle.java


// Targeting ../dnnl_primitive_handle.java


// Targeting ../dnnl_stream_handle.java



/** \cond DO_NOT_DOCUMENT_THIS */
/** \endcond */
// Targeting ../primitive.java



@Namespace("dnnl") public static native @Cast("dnnl_primitive_kind_t") int convert_to_c(@ByVal primitive.kind akind);


/** \}
 <p>
 *  \addtogroup cpp_api_enums Common data types and enumerations
 *  A proxy to \ref c_api_types in \ref c_api.
 * 
 *  \{
 <p>
 *  Scratchpad mode */
@Namespace("dnnl") public enum scratchpad_mode {
    /** The library manages scratchpad (default) */
    library(dnnl_scratchpad_mode_library),
    /** A user shall query and provide the scratchpad memory to primitives */
    user(dnnl_scratchpad_mode_user);

    public final int value;
    private scratchpad_mode(int v) { this.value = v; }
    private scratchpad_mode(scratchpad_mode e) { this.value = e.value; }
    public scratchpad_mode intern() { for (scratchpad_mode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("dnnl") public static native @Cast("dnnl_scratchpad_mode_t") int convert_to_c(scratchpad_mode mode);
@Namespace("dnnl") public static native @Cast("dnnl_scratchpad_mode_t") int convert_to_c(@Cast("dnnl::scratchpad_mode") int mode);

/** Propagation kind */
@Namespace("dnnl") public enum prop_kind {
    /** Undefined propagation kind */
    undef(dnnl_prop_kind_undef),
    /** Forward data propagation (training mode). In this mode primitives
     *  perform computations necessary for subsequent backward propagation. */
    forward_training(dnnl_forward_training),
    /** Forward data propagation (inference mode). In this mode primitives
     *  perform only computations that are necessary for inference and omit
     *  computations that are necessary only for backward propagation. */
    forward_inference(dnnl_forward_inference),
    /** Forward data propagation,
     *  alias for #dnnl::prop_kind::forward_inference */
    forward_scoring(dnnl_forward_scoring),
    /** Forward data propagation,
     *  alias for #dnnl::prop_kind::forward_training */
    forward(dnnl_forward),
    /** Backward propagation (with respect to all parameters). */
    backward(dnnl_backward),
    /** Backward data propagation. */
    backward_data(dnnl_backward_data),
    /** Backward weights propagation. */
    backward_weights(dnnl_backward_weights),
    /** Backward bias propagation. */
    backward_bias(dnnl_backward_bias);

    public final int value;
    private prop_kind(int v) { this.value = v; }
    private prop_kind(prop_kind e) { this.value = e.value; }
    public prop_kind intern() { for (prop_kind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("dnnl") public static native @Cast("dnnl_prop_kind_t") int convert_to_c(prop_kind kind);

/** Kinds of algorithms. */
@Namespace("dnnl") public enum algorithm {
    undef(dnnl_alg_kind_undef),
    /** Convolution algorithm(either direct or Winograd) is chosen just in time */
    convolution_auto(dnnl_convolution_auto),
    /** Direct convolution */
    convolution_direct(dnnl_convolution_direct),
    /** Winograd convolution */
    convolution_winograd(dnnl_convolution_winograd),
    /** Direct deconvolution */
    deconvolution_direct(dnnl_deconvolution_direct),
    /** Winograd deconvolution */
    deconvolution_winograd(dnnl_deconvolution_winograd),
    /** Eltwise: ReLU */
    eltwise_relu(dnnl_eltwise_relu),
    /** Eltwise: hyperbolic tangent non-linearity (tanh) */
    eltwise_tanh(dnnl_eltwise_tanh),
    /** Eltwise: parametric exponential linear unit (elu) */
    eltwise_elu(dnnl_eltwise_elu),
    /** Eltwise: square */
    eltwise_square(dnnl_eltwise_square),
    /** Eltwise: abs */
    eltwise_abs(dnnl_eltwise_abs),
    /** Eltwise: square root */
    eltwise_sqrt(dnnl_eltwise_sqrt),
    /** Eltwise: x*sigmoid(a*x) */
    eltwise_swish(dnnl_eltwise_swish),
    /** Eltwise: linear */
    eltwise_linear(dnnl_eltwise_linear),
    /** Eltwise: bounded_relu */
    eltwise_bounded_relu(dnnl_eltwise_bounded_relu),
    /** Eltwise: soft_relu */
    eltwise_soft_relu(dnnl_eltwise_soft_relu),
    /** Eltwise: logistic */
    eltwise_logistic(dnnl_eltwise_logistic),
    /** Eltwise: exponent */
    eltwise_exp(dnnl_eltwise_exp),
    /** Eltwise: gelu */
    eltwise_gelu(dnnl_eltwise_gelu),
    /** Local response normalization (LRN) across multiple channels */
    lrn_across_channels(dnnl_lrn_across_channels),
    /** LRN within a single channel */
    lrn_within_channel(dnnl_lrn_within_channel),
    /** Max pooling */
    pooling_max(dnnl_pooling_max),
    /** Average pooling exclude padding,
     *  alias for #dnnl::algorithm::pooling_avg_include_padding */
    pooling_avg(dnnl_pooling_avg),
    /** Average pooling include padding */
    pooling_avg_include_padding(dnnl_pooling_avg_include_padding),
    /** Average pooling exclude padding */
    pooling_avg_exclude_padding(dnnl_pooling_avg_exclude_padding),
    /** RNN cell */
    vanilla_rnn(dnnl_vanilla_rnn),
    /** LSTM cell */
    vanilla_lstm(dnnl_vanilla_lstm),
    /** GRU cell */
    
///
    vanilla_gru(dnnl_vanilla_gru),
    /** GRU cell with linear before reset
     * 
     *  Modification of original GRU cell. Differs from #dnnl_vanilla_gru
     *  in how the new memory gate is calculated:
     *  <pre>{@code \[ c_t = tanh(W_c*x_t + b_{c_x} + r_t*(U_c*h_{t-1}+b_{c_h})) \]}</pre>
     *  Primitive expects 4 biases on input:
     *  {@code [b_{u}, b_{r}, b_{c_x}, b_{c_h}]} */
    lbr_gru(dnnl_lbr_gru),
    /** Binary add */
    binary_add(dnnl_binary_add),
    /** Binary mul */
    binary_mul(dnnl_binary_mul);

    public final int value;
    private algorithm(int v) { this.value = v; }
    private algorithm(algorithm e) { this.value = e.value; }
    public algorithm intern() { for (algorithm e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("dnnl") public static native @Cast("dnnl_alg_kind_t") int convert_to_c(algorithm aalgorithm);

/** Flags for batch normalization primitive. */
@Namespace("dnnl") public enum normalization_flags {
    /** Use global statistics
     * 
     *  If specified
     *   - on forward propagation use mean and variance provided by user (input)
     *   - on backward propagation reduces the amount of computations, since
     *     mean and variance are considered as constants
     * 
     *   If not specified:
     *    - on forward propagation mean and variance are computed and stored in
     *      output
     *    - on backward propagation compute full derivative wrt to data */
    
///
///
    use_global_stats(dnnl_use_global_stats),

    /** Use scale and shift parameters
     * 
     *  If specified:
     *   - on forward propagation use scale and shift (aka scale and bias) for
     *     the batch normalization results
     *   - on backward propagation
     *     (for prop_kind == #dnnl::prop_kind::backward) compute
     *     diff wrt to scale and shift (hence one extra output used)
     * 
     *  If not specified:
     *   - on backward propagation
     *     prop_kind == #dnnl::prop_kind::backward_data has the
     *     same behavior as prop_kind == #dnnl::prop_kind::backward */
    
///
    use_scale_shift(dnnl_use_scaleshift),

    /** Fuse with ReLU
     * 
     *  If specified:
     *   - on inference this option behaves the same as if the primitive were
     *     fused with ReLU via post ops API
     *   - on training primitive requires workspace (required to be able to
     *     perform backward pass) */
    fuse_norm_relu(dnnl_fuse_norm_relu);

    public final int value;
    private normalization_flags(int v) { this.value = v; }
    private normalization_flags(normalization_flags e) { this.value = e.value; }
    public normalization_flags intern() { for (normalization_flags e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("dnnl") public static native @Cast("dnnl_normalization_flags_t") int convert_to_c(normalization_flags aflag);

@Namespace("dnnl") public enum rnn_flags { undef(dnnl_rnn_flags_undef);

    public final int value;
    private rnn_flags(int v) { this.value = v; }
    private rnn_flags(rnn_flags e) { this.value = e.value; }
    public rnn_flags intern() { for (rnn_flags e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("dnnl") public static native @Cast("dnnl_rnn_flags_t") int convert_to_c(rnn_flags aflag);

// #define DNNL_DEFINE_BITMASK_OPS(enum_name)
//     inline enum_name operator|(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) | static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name operator&(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) & static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name operator^(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) ^ static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name &operator|=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) | static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name &operator&=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) & static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name &operator^=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) ^ static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name operator~(enum_name rhs) {
//         return static_cast<enum_name>(~static_cast<unsigned>(rhs));
//     }

@Namespace("dnnl") public static native @Name("operator |") normalization_flags or(normalization_flags lhs, normalization_flags rhs);
@Namespace("dnnl") public static native @Name("operator |") @Cast("dnnl::normalization_flags") int or(@Cast("dnnl::normalization_flags") int lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @Name("operator &") normalization_flags and(normalization_flags lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @Name("operator &") @Cast("dnnl::normalization_flags") int and(@Cast("dnnl::normalization_flags") int lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @Name("operator ^") normalization_flags xor(normalization_flags lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @Name("operator ^") @Cast("dnnl::normalization_flags") int xor(@Cast("dnnl::normalization_flags") int lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") IntPointer orPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") IntBuffer orPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") int[] orPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") IntPointer orPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") IntBuffer orPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::normalization_flags*") int[] orPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") IntPointer andPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") IntBuffer andPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") int[] andPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") IntPointer andPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") IntBuffer andPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::normalization_flags*") int[] andPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") IntPointer xorPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") IntBuffer xorPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") int[] xorPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") IntPointer xorPut(@ByRef @Cast("dnnl::normalization_flags*") IntPointer lhs, @Cast("dnnl::normalization_flags") int rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") IntBuffer xorPut(@ByRef @Cast("dnnl::normalization_flags*") IntBuffer lhs, normalization_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::normalization_flags*") int[] xorPut(@ByRef @Cast("dnnl::normalization_flags*") int[] lhs, @Cast("dnnl::normalization_flags") int rhs);

    @Namespace("dnnl") public static native @Name("operator ~") normalization_flags not(normalization_flags rhs);
    @Namespace("dnnl") public static native @Name("operator ~") @Cast("dnnl::normalization_flags") int not(@Cast("dnnl::normalization_flags") int rhs);
@Namespace("dnnl") public static native @Name("operator |") rnn_flags or(rnn_flags lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @Name("operator &") rnn_flags and(rnn_flags lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @Name("operator ^") rnn_flags xor(rnn_flags lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::rnn_flags*") IntPointer orPut(@ByRef @Cast("dnnl::rnn_flags*") IntPointer lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::rnn_flags*") int[] orPut(@ByRef @Cast("dnnl::rnn_flags*") int[] lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator |=") @Cast("dnnl::rnn_flags*") IntBuffer orPut(@ByRef @Cast("dnnl::rnn_flags*") IntBuffer lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::rnn_flags*") IntPointer andPut(@ByRef @Cast("dnnl::rnn_flags*") IntPointer lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::rnn_flags*") int[] andPut(@ByRef @Cast("dnnl::rnn_flags*") int[] lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator &=") @Cast("dnnl::rnn_flags*") IntBuffer andPut(@ByRef @Cast("dnnl::rnn_flags*") IntBuffer lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::rnn_flags*") IntPointer xorPut(@ByRef @Cast("dnnl::rnn_flags*") IntPointer lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::rnn_flags*") int[] xorPut(@ByRef @Cast("dnnl::rnn_flags*") int[] lhs, rnn_flags rhs);
    @Namespace("dnnl") public static native @ByRef @Name("operator ^=") @Cast("dnnl::rnn_flags*") IntBuffer xorPut(@ByRef @Cast("dnnl::rnn_flags*") IntBuffer lhs, rnn_flags rhs);

    @Namespace("dnnl") public static native @Name("operator ~") rnn_flags not(rnn_flags rhs);

// #undef DNNL_DEFINE_BITMASK_OPS

@Namespace("dnnl") public enum rnn_direction {
    unidirectional_left2right(dnnl_unidirectional_left2right),
    unidirectional_right2left(dnnl_unidirectional_right2left),
    unidirectional(dnnl_unidirectional),
    bidirectional_concat(dnnl_bidirectional_concat),
    bidirectional_sum(dnnl_bidirectional_sum);

    public final int value;
    private rnn_direction(int v) { this.value = v; }
    private rnn_direction(rnn_direction e) { this.value = e.value; }
    public rnn_direction intern() { for (rnn_direction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}


///
///
@Namespace("dnnl") public static native @Cast("dnnl_rnn_direction_t") int convert_to_c(rnn_direction adir);

/** Primitive descriptor query specification
 * 
 *  In general should be used from C++ API since required queries are directly
 *  implemented as class members (for instance, a query for source memory
 *  descriptor).
 * 
 *  For more information see \ref dnnl_query_t. */
@Namespace("dnnl") public enum query {
    /** no query */
    undef(dnnl_query_undef),

    /** execution engine */
    engine(dnnl_query_engine),
    /** primitive kind */
    primitive_kind(dnnl_query_primitive_kind),

    /** number of inputs expected */
    num_of_inputs_s32(dnnl_query_num_of_inputs_s32),
    /** number of outputs expected */
    num_of_outputs_s32(dnnl_query_num_of_outputs_s32),

    /** runtime estimation (seconds), unimplemented */
    
///
///
    time_estimate_f64(dnnl_query_time_estimate_f64),
    /** memory consumption (bytes)
     * 
     *  extra (scratch) memory, additional to all inputs and outputs memory
     * 
     *  @see \ref dev_guide_attributes_scratchpad */
    
///
    memory_consumption_s64(dnnl_query_memory_consumption_s64),

    /** scratchpad engine
     * 
     *  engine to be used for creating scratchpad memory */
    scratchpad_engine(dnnl_query_scratchpad_engine),

    /** reorder source engine */
    reorder_src_engine(dnnl_query_reorder_src_engine),
    /** reorder destination engine */
    reorder_dst_engine(dnnl_query_reorder_dst_engine),

    /** implementation name */
    impl_info_str(dnnl_query_impl_info_str),

    /** op descriptor */
    op_d(dnnl_query_op_d),
    /** convolution descriptor */
    convolution_d(dnnl_query_convolution_d),
    /** deconvolution descriptor */
    deconvolution_d(dnnl_query_deconvolution_d),
    /** shuffle descriptor */
    shuffle_d(dnnl_query_shuffle_d),
    /** eltwise descriptor */
    eltwise_d(dnnl_query_eltwise_d),
    /** softmax descriptor */
    softmax_d(dnnl_query_softmax_d),
    /** pooling descriptor */
    pooling_d(dnnl_query_pooling_d),
    /** lrn descriptor */
    lrn_d(dnnl_query_lrn_d),
    /** batch normalization descriptor */
    batch_normalization_d(dnnl_query_batch_normalization_d),
    /** layer normalization descriptor */
    layer_normalization_d(dnnl_query_layer_normalization_d),
    /** inner product descriptor */
    inner_product_d(dnnl_query_inner_product_d),
    /** rnn descriptor */
    rnn_d(dnnl_query_rnn_d),
    /** binary descriptor */
    binary_d(dnnl_query_binary_d),

    /** source memory desc */
    src_md(dnnl_query_src_md),
    /** source gradient memory desc */
    diff_src_md(dnnl_query_diff_src_md),
    /** weights memory descriptor desc */
    weights_md(dnnl_query_weights_md),
    /** weights grad. memory desc */
    diff_weights_md(dnnl_query_diff_weights_md),
    /** destination memory desc */
    dst_md(dnnl_query_dst_md),
    /** destination grad. memory desc */
    diff_dst_md(dnnl_query_diff_dst_md),
    /** workspace memory desc */
    workspace_md(dnnl_query_workspace_md),
    /** scratchpad memory desc */
    scratchpad_md(dnnl_query_scratchpad_md);

    public final int value;
    private query(int v) { this.value = v; }
    private query(query e) { this.value = e.value; }
    public query intern() { for (query e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}


///
@Namespace("dnnl") public static native @Cast("dnnl_query_t") int convert_to_c(query aquery);

/** \}
 <p>
 *  \addtogroup cpp_api_attr Attributes
 *  An extension for controlling primitive behavior.
 * 
 *  @see \ref c_api_attributes in \ref c_api
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../post_ops.java



/** \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../primitive_attr.java



/** \}
 <p>
 *  \addtogroup cpp_api_engine Engine
 *  Engine operations.
 * 
 *  @see \ref c_api_engine in \ref c_api
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../engine.java



/** \}
 <p>
 *  \addtogroup cpp_api_stream Stream
 *  Execution stream operations
 * 
 *  @see \ref c_api_stream in \ref c_api
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../stream.java



@Namespace("dnnl") public static native @ByVal @Name("operator |") stream.flags or(@ByVal stream.flags lhs, @ByVal stream.flags rhs);

@Namespace("dnnl") public static native @ByVal @Name("operator &") stream.flags and(@ByVal stream.flags lhs, @ByVal stream.flags rhs);

@Namespace("dnnl") public static native @ByVal @Name("operator ^") stream.flags xor(@ByVal stream.flags lhs, @ByVal stream.flags rhs);


///
@Namespace("dnnl") public static native @ByVal @Name("operator ~") stream.flags not(@ByVal stream.flags rhs);
// Targeting ../memory.java



@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@Cast("dnnl_data_type_t") int a, @ByVal memory.data_type b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Cast("dnnl_data_type_t") int a, @ByVal memory.data_type b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@ByVal memory.data_type a, @Cast("dnnl_data_type_t") int b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@ByVal memory.data_type a, @Cast("dnnl_data_type_t") int b);

@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@Cast("dnnl_format_tag_t") int a, @ByVal memory.format_tag b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Cast("dnnl_format_tag_t") int a, @ByVal memory.format_tag b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@ByVal memory.format_tag a, @Cast("dnnl_format_tag_t") int b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@ByVal memory.format_tag a, @Cast("dnnl_format_tag_t") int b);
// Targeting ../primitive_desc_base.java


// Targeting ../reorder.java



/** \}
 <p>
 *  \addtogroup cpp_api_concat Concat
 *  A primitive to concatenate data by arbitrary dimension.
 * 
 *  @see \ref dev_guide_concat in developer guide
 *  @see \ref c_api_concat in \ref c_api
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */

///
@Namespace("dnnl") public static native @StdVector dnnl_memory_desc_t convert_to_c(
        @StdVector memory.desc mems);
// Targeting ../concat.java


// Targeting ../sum.java


// Targeting ../primitive_desc.java


// Targeting ../convolution_forward.java


// Targeting ../convolution_backward_data.java


// Targeting ../convolution_backward_weights.java


// Targeting ../deconvolution_forward.java


// Targeting ../deconvolution_backward_data.java


// Targeting ../deconvolution_backward_weights.java


// Targeting ../lrn_forward.java


// Targeting ../lrn_backward.java


// Targeting ../pooling_forward.java


// Targeting ../pooling_backward.java


// Targeting ../eltwise_forward.java


// Targeting ../eltwise_backward.java


// Targeting ../softmax_forward.java


// Targeting ../softmax_backward.java


// Targeting ../batch_normalization_forward.java


// Targeting ../batch_normalization_backward.java


// Targeting ../layer_normalization_forward.java


// Targeting ../layer_normalization_backward.java


// Targeting ../inner_product_forward.java


// Targeting ../inner_product_backward_data.java


// Targeting ../inner_product_backward_weights.java


// Targeting ../rnn_primitive_desc_base.java


// Targeting ../vanilla_rnn_forward.java


// Targeting ../vanilla_rnn_backward.java


// Targeting ../lstm_forward.java


// Targeting ../lstm_backward.java


// Targeting ../gru_forward.java


// Targeting ../gru_backward.java


// Targeting ../lbr_gru_forward.java


// Targeting ../lbr_gru_backward.java


// Targeting ../shuffle_forward.java


// Targeting ../shuffle_backward.java


// Targeting ../binary.java



/** \}
 <p>
 *  \} Primitives
 <p>
 *  \} C++ API */

// implementation section

/** \cond DO_NOT_DOCUMENT_THIS */





/** \endcond */

 // namespace dnnl

// #endif


}

// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.bytedeco.dnnl.global;

import org.bytedeco.dnnl.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.opencl.*;
import static org.bytedeco.opencl.global.OpenCL.*;

public class dnnl extends org.bytedeco.dnnl.presets.dnnl {
    static { Loader.load(); }

// Targeting ../dnnl_memory_desc_vector.java


// Targeting ../memory_desc_vector.java


// Targeting ../dnnl_primitive_desc_vector.java


// Targeting ../primitive_vector.java


// Targeting ../IntMemoryMap.java


// Parsed from oneapi/dnnl/dnnl_common_types.h

/*******************************************************************************
* Copyright 2022-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C API common types definitions */

// #ifndef ONEAPI_DNNL_DNNL_COMMON_TYPES_H
// #define ONEAPI_DNNL_DNNL_COMMON_TYPES_H

// #ifdef __cplusplus
// #endif

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <stddef.h>
// #include <stdint.h>

// #include "oneapi/dnnl/dnnl_config.h"

/** \endcond
 <p>
 *  \addtogroup dnnl_api oneDNN API
 *  \{
 <p>
 *  \addtogroup dnnl_api_common Common API
 *  \{
 <p>
 *  \addtogroup dnnl_api_utils
 *  \{
 <p>
 *  Status values returned by the library functions. */
/** enum dnnl_status_t */
public static final int
    /** The operation was successful */
    dnnl_success = 0,
    /** The operation failed due to an out-of-memory condition */
    dnnl_out_of_memory = 1,
    /** The operation failed because of incorrect function arguments */
    dnnl_invalid_arguments = 2,
    /** The operation failed because requested functionality is not implemented */
    dnnl_unimplemented = 3,
    /** The last available implementation is reached */
    dnnl_last_impl_reached = 4,
    /** Primitive or engine failed on execution */
    dnnl_runtime_error = 5,
    /** Queried element is not required for given primitive */
    dnnl_not_required = 6,
    /** The graph is not legitimate */
    dnnl_invalid_graph = 7,
    /** The operation is not legitimate according to op schema */
    dnnl_invalid_graph_op = 8,
    /** The shape cannot be inferred or compiled */
    dnnl_invalid_shape = 9,
    /** The data type cannot be inferred or compiled */
    dnnl_invalid_data_type = 10;

/** \} dnnl_api_utils
 <p>
 *  \addtogroup dnnl_api_data_types Data types
 *  \{
 <p>
 *  Data type specification */
/** enum dnnl_data_type_t */
public static final int
    /** Undefined data type, used for empty memory descriptors. */
    dnnl_data_type_undef = 0,
    /** 16-bit/half-precision floating point. */
    dnnl_f16 = 1,
    /** non-standard 16-bit (bfloat16 w/ 7 bit mantissa) floating point. */
    dnnl_bf16 = 2,
    /** 32-bit/single-precision floating point. */
    dnnl_f32 = 3,
    /** 32-bit signed integer. */
    dnnl_s32 = 4,
    /** 8-bit signed integer. */
    dnnl_s8 = 5,
    /** 8-bit unsigned integer. */
    dnnl_u8 = 6,
    /** 64-bit/double-precision floating point. */
    dnnl_f64 = 7,
    /** Boolean data type. Size is C++ implementation defined. */
    dnnl_boolean = 8,

    /** Parameter to allow internal only data_types without undefined behavior.
     *  This parameter is chosen to be valid for so long as sizeof(int) >= 2. */
    dnnl_data_type_max = 0x7fff;

/** Maximum number of dimensions a tensor can have. Only restricts the amount
 *  of space used for the tensor description. Individual computational
 *  primitives may support only tensors of certain dimensions. */
public static final int DNNL_MAX_NDIMS = 12;

/** A type to describe tensor dimension. */

/** A type to describe tensor dimensions. */

/** \} dnnl_api_data_types
 <p>
 *  \addtogroup dnnl_api_fpmath_mode Floating-point Math Mode
 *  \{
 <p>
 *  Floating-point math mode */
/** enum dnnl_fpmath_mode_t */
public static final int
    /** Default behavior, no downconversions allowed */
    dnnl_fpmath_mode_strict = 0,
    /** Implicit f32->bf16 conversions allowed */
    dnnl_fpmath_mode_bf16 = 1,
    /** Implicit f32->f16 conversions allowed */
    dnnl_fpmath_mode_f16 = 2,
    /** Implicit f32->f16, f32->tf32 or f32->bf16 conversions allowed */
    dnnl_fpmath_mode_any = 3,
    /** Implicit f32->tf32 conversions allowed */
    dnnl_fpmath_mode_tf32 = 4;

/** \} dnnl_api_fpmath_mode
 <p>
 *  \addtogroup dnnl_api_engine Engine
 *  \{
 <p>
 *  \brief Kinds of engines. */
/** enum dnnl_engine_kind_t */
public static final int
    /** An unspecified engine. */
    dnnl_any_engine = 0,
    /** CPU engine. */
    dnnl_cpu = 1,
    /** GPU engine. */
    dnnl_gpu = 2;
// Targeting ../dnnl_engine.java


/** \brief An engine handle. */
// #if 0
// #endif

/** \} dnnl_api_engine
 <p>
 *  \addtogroup dnnl_api_stream Stream
 *  \{
 <p>
 *  \brief Stream flags. */
/** enum dnnl_stream_flags_t */
public static final int
    // In-order execution.
    dnnl_stream_in_order = 0x1,
    /** Out-of-order execution. */
    dnnl_stream_out_of_order = 0x2,
    /** Default stream configuration. */
    dnnl_stream_default_flags = dnnl_stream_in_order;
// #ifdef DNNL_EXPERIMENTAL_PROFILING
// #endif
// Targeting ../dnnl_stream.java


/** An execution stream handle. */
/** A constant execution stream handle. */
// Targeting ../dnnl_version_t.java



/** \} dnnl_api_service
 <p>
 *  \} dnnl_api_common
 <p>
 *  \} dnnl_api */

// #ifdef __cplusplus
// #endif

// #endif


// Parsed from oneapi/dnnl/dnnl_types.h

/*******************************************************************************
* Copyright 2016-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C API types definitions */

// #ifndef ONEAPI_DNNL_DNNL_TYPES_H
// #define ONEAPI_DNNL_DNNL_TYPES_H

// #ifdef __cplusplus
// #endif

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <stddef.h>
// #include <stdint.h>
/** \endcond */

// #include "oneapi/dnnl/dnnl_config.h"

// #include "oneapi/dnnl/dnnl_common_types.h"

/** \addtogroup dnnl_api
 *  \{
 <p>
 *  \addtogroup dnnl_api_memory
 *  \{
 <p>
 *  Memory format kind */
/** enum dnnl_format_kind_t */
public static final int
    /** Undefined memory format kind, used for empty memory descriptors. */
    dnnl_format_kind_undef = 0,
    /** A special format kind that indicates that the actual format will be
     *  selected by a primitive automatically. */
    dnnl_format_kind_any = 1,
    /** A tensor in a generic format described by the stride and blocking
     *  values in each dimension. */
    dnnl_blocked = 2,
    /** A special format kind that indicates that tensor format is opaque. */
    dnnl_format_kind_opaque = 3,
// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif
    /** Parameter to allow internal only format kinds without undefined
     *  behavior. This parameter is chosen to be valid for so long as
     *  sizeof(int) >= 2. */
    dnnl_format_kind_max = 0x7fff;

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

// #ifdef DNNL_EXPERIMENTAL_PROFILING

// #endif

/** Memory format tag specification.
 * 
 *  oneDNN formats describe physical data layout. The physical layout
 *  is described as a sequence of the dimensions as they are laid out in the
 *  memory (from the outer-most to the inner-most). Note that this order
 *  doesn't affect the logical order of the dimensions that is kept in the
 *  {@code dims} field of the dnnl_memory_desc_t structure. The logical order of the
 *  dimensions is specified by the primitive that uses the tensor.
 * 
 *  For example, CNN 5D tensor always has its logical dimensions in the order
 *  {@code (batch, channels, depth, height, width)}, while the physical layout might be
 *  {@code NCDHW} (corresponds to #dnnl_ncdhw format tag) or
 *  {@code NDHWC} (corresponds to #dnnl_ndhwc format tag).
 * 
 *  ~~~cpp
 *  int batch = 2, channels = 16, depth = 13, height = 13, width = 13;
 * 
 *  int ndims = 5; // 5D tensor
 *  dnnl_dims_t dims = {batch, channels, depth, height, width};
 *  dnnl_memory_desc_t data_in_ncdhw;
 *  dnnl_memory_desc_create_with_tag(
 *       &data_in_ncdhw, 5, dims, dnnl_f32, dnnl_ncdhw);
 * 
 *  // note that in both cases dims passed are the same
 *  dnnl_memory_desc_t data_in_ndhwc;
 *  dnnl_memory_desc_create_with_tag(
 *       &data_in_ndhwc, 5, dims, dnnl_f32, dnnl_ndhwc);
 * 
 *  dnnl_memory_desc_destroy(data_in_ncdhw);
 *  dnnl_memory_desc_destroy(data_in_ndhwc);
 *  ~~~
 * 
 *  Memory format tags can be further divided into two categories:
 *   - Domain-agnostic names, i.e. names the do not depend on the tensor usage
 *     in the specific primitive. These names use letters from {@code a} to {@code l} to
 *     denote logical dimension from 1 to 12, and form the order in which the
 *     dimensions are laid in memory. For instance, #dnnl_ab is used to denote
 *     2D tensor where the second logical dimension (aka {@code b}) is the innermost,
 *     i.e. has stride = 1, and the first logical dimension ({@code a}) laid out in
 *     memory with stride equal to the size of second dimension. On the other
 *     hand, #dnnl_ba is just transposed version of the same tensor: the
 *     first dimension ({@code a}) becomes the innermost one.
 *   - Domain-specific names, i.e. names that make sense only in the context of
 *     a certain domain, such as CNN. This names are just aliases to the
 *     corresponding domain-agnostic tags and used mostly for the convenience.
 *     For example, #dnnl_nc is used to denote 2D CNN activations tensor
 *     memory format, where channels are the innermost dimension and batch is an
 *     outermost one. Moreover, #dnnl_nc is just an alias to #dnnl_ab,
 *     since for oneDNN CNN primitives the logical dimensions of
 *     activations tensors come in order: batch, channels, spatial.
 *     In other words, batch corresponds to the first logical dimension ({@code a}),
 *     channels correspond to the second one ({@code b}).
 * 
 *  The following domain-specific notation applies to memory format tags:
 *   - \c 'n' denotes the mini-batch dimension
 *   - \c 'c' denotes a channels dimension
 *   - When there are multiple channel dimensions (for example, in convolution
 *     weights tensor), \c 'i' and \c 'o' denote dimensions of input and output
 *     channels
 *   - \c 'd', \c 'h', and \c 'w' denote spatial depth, height, and width
 *     respectively
 * 
 *  Upper-case letters indicate that the data is laid out in blocks for a
 *  particular dimension. In such cases, the format name contains both upper-
 *  and lower-case letters for that dimension with a lower-case letter preceded
 *  by the block size. For example: #dnnl_nChw8c describes a format where the
 *  outermost dimension is mini-batch, followed by the channel block number,
 *  followed by the spatial height and width, and finally followed by 8-element
 *  channel blocks.
 * 
 *  @see \ref dev_guide_understanding_memory_formats */
/** enum dnnl_format_tag_t */
public static final int
    /** Undefined memory format tag */
    dnnl_format_tag_undef = 0,
    /** Undefined memory format tag.
     *  The primitive selects a format automatically. */
    dnnl_format_tag_any = 1,

    // Semantic agnostic section
    // The physical order of dimensions is defined by the permutation of the
    // characters, assuming that ab..z defines the natural order.

    // Plain formats

    /** plain 1D tensor */
    dnnl_a = 2,
    /** plain 2D tensor */
    dnnl_ab = 3,
    /** plain 3D tensor */
    dnnl_abc = 4,
    /** plain 4D tensor */
    dnnl_abcd = 5,
    /** plain 5D tensor */
    dnnl_abcde = 6,
    /** plain 6D tensor */
    dnnl_abcdef = 7,
    /** plain 7D tensor */
    dnnl_abcdefg = 8,
    /** plain 8D tensor */
    dnnl_abcdefgh = 9,
    /** plain 9D tensor */
    dnnl_abcdefghi = 10,
    /** plain 10D tensor */
    dnnl_abcdefghij = 11,
    /** plain 11D tensor */
    dnnl_abcdefghijk = 12,
    /** plain 12D tensor */
    dnnl_abcdefghijkl = 13,

    // Permuted plain formats

    /** permuted 2D tensor */
    dnnl_ba = 14,
    /** permuted 3D tensor */
    dnnl_acb = 15,
    /** permuted 3D tensor */
    dnnl_bac = 16,
    /** permuted 3D tensor */
    dnnl_bca = 17,
    /** permuted 3D tensor */
    dnnl_cab = 18,
    /** permuted 3D tensor */
    dnnl_cba = 19,
    /** permuted 4D tensor */
    dnnl_abdc = 20,
    /** permuted 4D tensor */
    dnnl_acbd = 21,
    /** permuted 4D tensor */
    dnnl_acdb = 22,
    /** permuted 4D tensor */
    dnnl_adbc = 23,
    /** permuted 4D tensor */
    dnnl_adcb = 24,
    /** permuted 4D tensor */
    dnnl_bacd = 25,
    /** permuted 4D tensor */
    dnnl_bcda = 26,
    /** permuted 4D tensor */
    dnnl_cdab = 27,
    /** permuted 4D tensor */
    dnnl_cdba = 28,
    /** permuted 4D tensor */
    dnnl_dcab = 29,
    /** permuted 5D tensor */
    dnnl_abced = 30,
    /** permuted 5D tensor */
    dnnl_abdec = 31,
    /** permuted 5D tensor */
    dnnl_acbde = 32,
    /** permuted 5D tensor */
    dnnl_acdeb = 33,
    /** permuted 5D tensor */
    dnnl_adecb = 34,
    /** permuted 5D tensor */
    dnnl_bacde = 35,
    /** permuted 5D tensor */
    dnnl_bcdea = 36,
    /** permuted 5D tensor */
    dnnl_cdeab = 37,
    /** permuted 5D tensor */
    dnnl_cdeba = 38,
    /** permuted 5D tensor */
    dnnl_decab = 39,
    /** permuted 6D tensor */
    dnnl_abcdfe = 40,
    /** permuted 6D tensor */
    dnnl_abdefc = 41,
    /** permuted 6D tensor */
    dnnl_abdfce = 42,
    /** permuted 6D tensor */
    dnnl_acbdef = 43,
    /** permuted 6D tensor */
    dnnl_adefcb = 44,
    /** permuted 6D tensor */
    dnnl_defcab = 45,
    /** permuted 7D tensor */
    dnnl_abcdegf = 46,
    /** permuted 8D tensor */
    dnnl_abcdefhg = 47,
    /** permuted 9D tensor */
    dnnl_abcdefgih = 48,
    /** permuted 10D tensor */
    dnnl_abcdefghji = 49,
    /** permuted 11D tensor */
    dnnl_abcdefghikj = 50,
    /** permuted 12D tensor */
    dnnl_abcdefghijlk = 51,

    // Opaque blocked formats

    dnnl_Abc16a = 52,
    dnnl_ABc16a16b = 53,
    dnnl_ABc32a32b = 54,
    dnnl_ABc4a4b = 55,
    /** 3D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBc16b = 56,
    dnnl_ABc16b16a = 57,
    dnnl_Abc4a = 58,
    /** 3D tensor blocked by 2nd dimension with block size 32 */
    dnnl_aBc32b = 59,
    /** 3D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBc4b = 60,
    dnnl_ABc4b16a4b = 61,
    dnnl_ABc2b8a4b = 62,
    dnnl_ABc16b16a4b = 63,
    dnnl_ABc16b16a2b = 64,
    dnnl_ABc4b4a = 65,
    dnnl_ABc8a16b2a = 66,
    dnnl_ABc8a8b = 67,
    dnnl_ABc8a4b = 68,
    /** 3D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBc8b = 69,
    dnnl_ABc8b16a2b = 70,
    dnnl_BAc8a16b2a = 71,
    dnnl_ABc8b8a = 72,
    dnnl_Abcd16a = 73,
    dnnl_Abcd8a = 74,
    dnnl_ABcd16a16b = 75,
    dnnl_Abcd32a = 76,
    dnnl_ABcd32a32b = 77,
    /** 4D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcd16b = 78,
    dnnl_ABcd16b16a = 79,
    dnnl_aBCd16b16c = 80,
    dnnl_aBCd16c16b = 81,
    dnnl_Abcd4a = 82,
    /** 4D tensor blocked by 2nd dimension with block size 32 */
    dnnl_aBcd32b = 83,
    /** 4D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcd4b = 84,
    dnnl_ABcd4b16a4b = 85,
    dnnl_ABcd16b16a4b = 86,
    dnnl_ABcd16b16a2b = 87,
    dnnl_ABcd4b4a = 88,
    dnnl_ABcd4a4b = 89,
    dnnl_aBCd2c4b2c = 90,
    dnnl_aBCd4b8c2b = 91,
    dnnl_aBCd4c16b4c = 92,
    dnnl_aBCd2c8b4c = 93,
    dnnl_aBCd16c16b4c = 94,
    dnnl_aBCd16c16b2c = 95,
    dnnl_aBCd4c4b = 96,
    dnnl_aBCd4b4c = 97,
    dnnl_ABcd8a16b2a = 98,
    dnnl_ABcd2b8a4b = 99,
    dnnl_ABcd8a8b = 100,
    dnnl_ABcd8a4b = 101,
    /** 4D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBcd8b = 102,
    dnnl_aBCd4c8b2c = 103,
    dnnl_ABcd8b16a2b = 104,
    dnnl_aBCd8b16c2b = 105,
    dnnl_BAcd8a16b2a = 106,
    /** 4D tensor blocked by 1st and 2nd dimension with block size 8 */
    dnnl_ABcd8b8a = 107,
    dnnl_aBCd8b8c = 108,
    dnnl_aBCd8b4c = 109,
    dnnl_aBCd8c16b2c = 110,
    dnnl_ABcde8a16b2a = 111,
    dnnl_aCBd8b16c2b = 112,
    dnnl_aBCd8c8b = 113,
    dnnl_Abcde16a = 114,
    dnnl_Abcde32a = 115,
    dnnl_ABcde16a16b = 116,
    dnnl_BAcde8a16b2a = 117,
    /** 4D tensor blocked by 3rd dimension with block size 4 */
    dnnl_aBCd2b4c2b = 118,
    /** 5D tensor blocked by 1st dimension with block size 16 */
    dnnl_ABcde4b16a4b = 119,
    /** 5D tensor blocked by 1st dimension with block size 8 */
    dnnl_ABcde2b8a4b = 120,
    /** 5D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcde16b = 121,
    dnnl_ABcde16b16a = 122,
    dnnl_aBCde16b16c = 123,
    dnnl_aBCde16c16b = 124,
    dnnl_aBCde2c8b4c = 125,
    dnnl_Abcde4a = 126,
    /** 5D tensor blocked by 2nd dimension with block size 32 */
    dnnl_aBcde32b = 127,
    /** 5D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcde4b = 128,
    dnnl_ABcde4b4a = 129,
    dnnl_ABcde4a4b = 130,
    dnnl_aBCde4b4c = 131,
    dnnl_aBCde2c4b2c = 132,
    dnnl_aBCde4b8c2b = 133,
    dnnl_aBCde4c16b4c = 134,
    dnnl_aBCde16c16b4c = 135,
    dnnl_aBCde16c16b2c = 136,
    dnnl_aBCde4c4b = 137,
    dnnl_Abcde8a = 138,
    dnnl_ABcde8a8b = 139,
    dnnl_ABcde8a4b = 140,
    dnnl_BAcde16b16a = 141,
    /** 5D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBcde8b = 142,
    dnnl_ABcde8b16a2b = 143,
    dnnl_aBCde8b16c2b = 144,
    dnnl_aBCde4c8b2c = 145,
    dnnl_aCBde8b16c2b = 146,
    dnnl_ABcde8b8a = 147,
    dnnl_ABcde32a32b = 148,
    dnnl_aBCde8b8c = 149,
    dnnl_aBCde8b4c = 150,
    dnnl_ABc4a8b8a4b = 151,
    dnnl_ABcd4a8b8a4b = 152,
    dnnl_ABcde4a8b8a4b = 153,
    dnnl_BAc4b8a8b4a = 154,
    dnnl_BAcd4b8a8b4a = 155,
    dnnl_BAcde4b8a8b4a = 156,
    dnnl_ABcd2a8b8a2b = 157,
    dnnl_aBCd4b8c8b4c = 158,
    dnnl_aBCde4b8c8b4c = 159,
    dnnl_aBCde2b8c8b2c = 160,
    dnnl_aBCde8c16b2c = 161,
    dnnl_aBCde8c8b = 162,
    /** 5D tensor blocked by 3rd dimension with block size 4 */
    dnnl_aBCde2b4c2b = 163,
    /** 6D tensor blocked by 2nd dimension with block size 16 */
    dnnl_aBcdef16b = 164,
    dnnl_aBCdef16b16c = 165,
    dnnl_aBCdef16c16b = 166,
    dnnl_aBCdef4c16b4c = 167,
    /** 6D tensor blocked by 2nd dimension with block size 8 */
    dnnl_aBCdef2c8b4c = 168,
    dnnl_aBCdef4c8b2c = 169,
    /** 6D tensor blocked by 3rd dimension with block size 4 */
    dnnl_aBCdef2b4c2b = 170,
    /** 6D tensor blocked by 2nd dimension with block size 4 */
    dnnl_aBcdef4b = 171,
    dnnl_aBCdef4c4b = 172,
    dnnl_aBCdef4b4c = 173,
    dnnl_aBCdef2c4b2c = 174,
    dnnl_aBCdef4b8c2b = 175,
    dnnl_aBCdef8b8c = 176,
    dnnl_aBCdef8b4c = 177,
    dnnl_aBCdef8c16b2c = 178,
    dnnl_aBCdef4b8c8b4c = 179,
    dnnl_aBCdef8b16c2b = 180,
    dnnl_aCBdef8b16c2b = 181,
    dnnl_aBCdef8c8b = 182,
    dnnl_aBdc16b = 183,
    dnnl_aBdC16b2c = 184,
    dnnl_aBdC16b4c = 185,
    dnnl_aBdc4b = 186,
    dnnl_aBdc8b = 187,
    dnnl_aBdec16b = 188,
    dnnl_aBdeC16b2c = 189,
    dnnl_aBdeC16b4c = 190,
    dnnl_aBdec32b = 191,
    dnnl_aBdec4b = 192,
    dnnl_aBdec8b = 193,
    dnnl_aBdefc16b = 194,
    dnnl_aBdefC16b2c = 195,
    dnnl_aCBdef16c16b = 196,
    dnnl_aBdefc4b = 197,
    dnnl_aBdefc8b = 198,
    dnnl_Abcdef16a = 199,
    dnnl_Abcdef32a = 200,
    dnnl_aBedc16b = 201,
    dnnl_Acb16a = 202,
    dnnl_AcB16a2b = 203,
    dnnl_AcB16a4b = 204,
    dnnl_Acb4a = 205,
    dnnl_Acb8a = 206,
    dnnl_aCBd16b16c = 207,
    dnnl_aCBd16c16b = 208,
    dnnl_aCBde16b16c = 209,
    dnnl_aCBde16c16b = 210,
    dnnl_Acdb16a = 211,
    dnnl_AcdB16a2b = 212,
    dnnl_AcdB16a4b = 213,
    dnnl_Acdb32a = 214,
    dnnl_Acdb4a = 215,
    dnnl_Acdb8a = 216,
    dnnl_Acdeb16a = 217,
    dnnl_AcdeB16a2b = 218,
    dnnl_Acdeb4a = 219,
    dnnl_Acdeb8a = 220,
    dnnl_Adcb16a = 221,
    dnnl_BAc16a16b = 222,
    dnnl_BAc16b16a = 223,
    dnnl_BAcd16a16b = 224,
    dnnl_BAcd16b16a = 225,
    dnnl_aCBd4c8b8c4b = 226,
    dnnl_aCBde4c8b8c4b = 227,
    dnnl_aCBdef4c8b8c4b = 228,
    dnnl_BAcde16a16b = 229,
    dnnl_aCBdef16b16c = 230,
    dnnl_ABc16b32a = 231,
    dnnl_ABc16b64a = 232,
    dnnl_ABc4b32a4b = 233,
    dnnl_ABc4b64a4b = 234,
    dnnl_ABc8b32a2b = 235,
    dnnl_ABc8b64a2b = 236,
    dnnl_AB16b16a = 237,
    dnnl_AB16b32a = 238,
    dnnl_AB16b64a = 239,
    dnnl_AB8b16a2b = 240,
    dnnl_AB8b32a2b = 241,
    dnnl_AB8b64a2b = 242,
    dnnl_AB4b16a4b = 243,
    dnnl_AB4b32a4b = 244,
    dnnl_AB4b64a4b = 245,
    dnnl_AB16b16a4b = 246,
    dnnl_ABcd16b32a = 247,
    dnnl_ABcd16b64a = 248,
    dnnl_ABcd4b32a4b = 249,
    dnnl_ABcd4b64a4b = 250,
    dnnl_ABcd8b32a2b = 251,
    dnnl_ABcd8b64a2b = 252,
    dnnl_ABcde4b32a4b = 253,
    dnnl_ABcde4b64a4b = 254,
    dnnl_ABcde16b16a4b = 255,
    dnnl_ABcde16b16a2b = 256,
    dnnl_ABcde16b32a = 257,
    dnnl_ABcde16b64a = 258,
    dnnl_ABcde8b32a2b = 259,
    dnnl_ABcde8b64a2b = 260,
    dnnl_aBCdef16c16b4c = 261,
    dnnl_aBCdef16c16b2c = 262,
    dnnl_AB32a32b8a4b = 263,
    dnnl_AB8a4b = 264,
    dnnl_AB32a32b8a2b = 265,
    dnnl_AB8a2b = 266,
    dnnl_abDc32d = 267,
    dnnl_abDC32d4c = 268,
    dnnl_abdEc32e = 269,
    dnnl_abdEC32e2c = 270,
    dnnl_abdEC32e4c = 271,
    dnnl_aBdefC16b4c = 272,
    dnnl_AcdeB16a4b = 273,
    dnnl_ABcd16a16b2a = 274,
    dnnl_ABc16a16b2a = 275,
    dnnl_aBCd16b16c2b = 276,
    dnnl_aBCde16b16c2b = 277,
    dnnl_Acb32a = 278,
    dnnl_AcB32a2b = 279,
    dnnl_AcB32a4b = 280,
    dnnl_Acb48a = 281,
    dnnl_AcB48a2b = 282,
    dnnl_AcB48a4b = 283,
    dnnl_Acb64a = 284,
    dnnl_AcB64a2b = 285,
    dnnl_AcB64a4b = 286,
    dnnl_cBa2b = 287,
    dnnl_cBa4b = 288,
    dnnl_aBdc32b = 289,
    dnnl_aBdC32b2c = 290,
    dnnl_aBdC32b4c = 291,
    dnnl_aBdc48b = 292,
    dnnl_aBdC48b2c = 293,
    dnnl_aBdC48b4c = 294,
    dnnl_aBdc64b = 295,
    dnnl_aBdC64b2c = 296,
    dnnl_aBdC64b4c = 297,
    dnnl_adCb2c = 298,
    dnnl_adCb4c = 299,
    dnnl_AcdB32a2b = 300,
    dnnl_AcdB32a4b = 301,
    dnnl_Acdb48a = 302,
    dnnl_AcdB48a2b = 303,
    dnnl_AcdB48a4b = 304,
    dnnl_Acdb64a = 305,
    dnnl_AcdB64a2b = 306,
    dnnl_AcdB64a4b = 307,
    dnnl_cdBa2b = 308,
    dnnl_cdBa4b = 309,
    dnnl_aBdeC32b2c = 310,
    dnnl_aBdeC32b4c = 311,
    dnnl_aBdec48b = 312,
    dnnl_aBdeC48b2c = 313,
    dnnl_aBdeC48b4c = 314,
    dnnl_aBdec64b = 315,
    dnnl_aBdeC64b2c = 316,
    dnnl_aBdeC64b4c = 317,
    dnnl_adeCb2c = 318,
    dnnl_adeCb4c = 319,
    dnnl_Acdeb32a = 320,
    dnnl_AcdeB32a2b = 321,
    dnnl_AcdeB32a4b = 322,
    dnnl_Acdeb48a = 323,
    dnnl_AcdeB48a2b = 324,
    dnnl_AcdeB48a4b = 325,
    dnnl_Acdeb64a = 326,
    dnnl_AcdeB64a2b = 327,
    dnnl_AcdeB64a4b = 328,
    dnnl_cdeBa2b = 329,
    dnnl_cdeBa4b = 330,
    dnnl_aBdefc32b = 331,
    dnnl_aBdefC32b2c = 332,
    dnnl_aBdefC32b4c = 333,
    dnnl_aBdefc48b = 334,
    dnnl_aBdefC48b2c = 335,
    dnnl_aBdefC48b4c = 336,
    dnnl_aBdefc64b = 337,
    dnnl_aBdefC64b2c = 338,
    dnnl_aBdefC64b4c = 339,
    dnnl_adefCb2c = 340,
    dnnl_adefCb4c = 341,
    dnnl_AB16b32a4b = 342,
    dnnl_AB16b48a4b = 343,
    dnnl_AB16b64a4b = 344,
    dnnl_AB16b16a2b = 345,
    dnnl_AB16b32a2b = 346,
    dnnl_AB16b48a2b = 347,
    dnnl_AB16b64a2b = 348,
    dnnl_ABc16b32a4b = 349,
    dnnl_ABc16b48a4b = 350,
    dnnl_ABc16b64a4b = 351,
    dnnl_ABc16b32a2b = 352,
    dnnl_ABc16b48a2b = 353,
    dnnl_ABc16b64a2b = 354,
    dnnl_ABcd16b32a4b = 355,
    dnnl_ABcd16b48a4b = 356,
    dnnl_ABcd16b64a4b = 357,
    dnnl_ABcd16b32a2b = 358,
    dnnl_ABcd16b48a2b = 359,
    dnnl_ABcd16b64a2b = 360,
    dnnl_ABcde16b32a4b = 361,
    dnnl_ABcde16b48a4b = 362,
    dnnl_ABcde16b64a4b = 363,
    dnnl_ABcde16b32a2b = 364,
    dnnl_ABcde16b48a2b = 365,
    dnnl_ABcde16b64a2b = 366,
    dnnl_ABc32a16b = 367,
    dnnl_ABcd32a16b = 368,
    dnnl_ABcde32a16b = 369,
    dnnl_AB48a16b = 370,
    dnnl_AB48a32b = 371,
    dnnl_ABc40a16b = 372,
    dnnl_ABc40a32b = 373,
    dnnl_aBC48b16c = 374,
    dnnl_aBC48b32c = 375,
    dnnl_ABcd40a16b = 376,
    dnnl_ABcd40a32b = 377,
    dnnl_abCd32c = 378,
    dnnl_abdCe32c = 379,
    dnnl_abdCE32c2e = 380,
    dnnl_BA16a16b2a = 381,
    dnnl_BA16a32b2a = 382,
    dnnl_BA16a48b2a = 383,
    dnnl_BA16a64b2a = 384,
    dnnl_BA16a16b4a = 385,
    dnnl_BA16a32b4a = 386,
    dnnl_BA16a48b4a = 387,
    dnnl_BA16a64b4a = 388,
    dnnl_ABcd8a2b = 389,
    dnnl_aBdeC16c16b2c = 390,
    dnnl_aBdeC16c16b4c = 391,
    dnnl_aBdefC16c16b2c = 392,
    dnnl_AcB16b16a2b = 393,
    dnnl_AcB16b16a4b = 394,
    dnnl_AcdB16b16a2b = 395,
    dnnl_AcdB16b16a4b = 396,
    dnnl_AcdeB16b16a2b = 397,
    dnnl_aBdefC16c16b4c = 398,
    dnnl_AcdeB16b16a4b = 399,
    dnnl_AcB16b32a2b = 400,
    dnnl_AcB16b32a4b = 401,
    dnnl_AcB16b48a2b = 402,
    dnnl_AcB16b48a4b = 403,
    dnnl_AcB16b64a2b = 404,
    dnnl_AcB16b64a4b = 405,
    dnnl_aBdC16c16b2c = 406,
    dnnl_aBdC16c16b4c = 407,
    dnnl_aBdC16c32b2c = 408,
    dnnl_aBdC16c32b4c = 409,
    dnnl_aBdC16c48b2c = 410,
    dnnl_aBdC16c48b4c = 411,
    dnnl_aBdC16c64b2c = 412,
    dnnl_aBdC16c64b4c = 413,
    dnnl_AcdB16b32a2b = 414,
    dnnl_AcdB16b32a4b = 415,
    dnnl_AcdB16b48a2b = 416,
    dnnl_AcdB16b48a4b = 417,
    dnnl_AcdB16b64a2b = 418,
    dnnl_AcdB16b64a4b = 419,
    dnnl_aBdeC16c32b2c = 420,
    dnnl_aBdeC16c32b4c = 421,
    dnnl_aBdeC16c48b2c = 422,
    dnnl_aBdeC16c48b4c = 423,
    dnnl_aBdeC16c64b2c = 424,
    dnnl_aBdeC16c64b4c = 425,
    dnnl_AcdeB16b32a2b = 426,
    dnnl_AcdeB16b32a4b = 427,
    dnnl_AcdeB16b48a2b = 428,
    dnnl_AcdeB16b48a4b = 429,
    dnnl_AcdeB16b64a2b = 430,
    dnnl_AcdeB16b64a4b = 431,
    dnnl_aBdefC16c32b2c = 432,
    dnnl_aBdefC16c32b4c = 433,
    dnnl_aBdefC16c48b2c = 434,
    dnnl_aBdefC16c48b4c = 435,
    dnnl_aBdefC16c64b2c = 436,
    dnnl_aBdefC16c64b4c = 437,
    dnnl_decbA16a = 438,
    dnnl_ABc4a2b = 439,
    dnnl_ABc8a2b = 440,
    dnnl_aBCd8b2c = 441,
    dnnl_ABcde4a2b = 442,
    dnnl_ABcde8a2b = 443,
    dnnl_ABcde40a16b = 444,
    dnnl_ABcde40a32b = 445,
    dnnl_aBCde8b2c = 446,
    dnnl_ABcde4a8b8a2b = 447,
    dnnl_ABcd4a8b8a2b = 448,
    dnnl_ABc4a8b8a2b = 449,
    dnnl_aBCdef4b8c8b2c = 450,
    dnnl_aBCde4b8c8b2c = 451,
    dnnl_aBCd4b8c8b2c = 452,
    dnnl_BAcde4b8a8b2a = 453,
    dnnl_BAcd4b8a8b2a = 454,
    dnnl_BAc4b8a8b2a = 455,
    dnnl_aCBdef4c8b8c2b = 456,
    dnnl_aCBde4c8b8c2b = 457,
    dnnl_aCBd4c8b8c2b = 458,
    dnnl_aBCdef8b2c = 459,
    dnnl_AB32a16b = 460,
    dnnl_AB32a32b = 461,
    dnnl_BA4b8a8b2a = 462,
    dnnl_BA4b8a8b4a = 463,
    dnnl_aBC32b16c = 464,
    dnnl_aBC32b32c = 465,
    dnnl_aCB4c8b8c2b = 466,
    dnnl_aCB4c8b8c4b = 467,
    dnnl_ABcd4a2b = 468,
    dnnl_ABc2b8a16b4a = 469,
    dnnl_ABcd2b8a16b4a = 470,
    dnnl_ABcde2b8a16b4a = 471,
    dnnl_ABc2a8b16a4b = 472,
    dnnl_ABc2a8b16a2b = 473,
    dnnl_ABc2b32a8b = 474,
    dnnl_ABcd2a8b16a4b = 475,
    dnnl_ABcd2a8b16a2b = 476,
    dnnl_aCBd2c8b16c2b = 477,
    dnnl_ABcd2b32a8b = 478,
    dnnl_aBCd2c8b16c2b = 479,
    dnnl_ABcde2a8b16a4b = 480,
    dnnl_ABcde2a8b16a2b = 481,
    dnnl_aCBde2c8b16c2b = 482,
    dnnl_ABcde2b32a8b = 483,
    dnnl_aBC2b8c16b2c = 484,
    dnnl_aBCd2b8c16b2c = 485,
    dnnl_aBCde2b8c16b2c = 486,
    dnnl_aBCdef2b8c16b2c = 487,
    dnnl_BAcde2b8a16b4a = 488,
    dnnl_BAcd2b8a16b4a = 489,
    dnnl_BAc2b8a16b4a = 490,
    dnnl_BAcde2b8a16b2a = 491,
    dnnl_BAcd2b8a16b2a = 492,
    dnnl_BAc2b8a16b2a = 493,
    dnnl_aBCde2c8b16c2b = 494,
    dnnl_aBCdef2c8b16c2b = 495,
    dnnl_aCBdef2c8b16c2b = 496,
    dnnl_aBCd2b8c16b4c = 497,
    dnnl_aBCde2b8c16b4c = 498,
    dnnl_BA4b8a16b2a = 499,
    dnnl_BA4b8a16b4a = 500,
    dnnl_aCB4c8b16c2b = 501,
    dnnl_aCB4c8b16c4b = 502,
    dnnl_BA16a16b = 503,
    dnnl_BA16a32b = 504,
    dnnl_BA16a48b = 505,
    dnnl_BA16a64b = 506,
    dnnl_aCB16c2b = 507,
    dnnl_aCB16c4b = 508,
    dnnl_BA16b2a = 509,
    dnnl_BA16b4a = 510,
    dnnl_aBC16b16c = 511,
    dnnl_aBC16b32c = 512,
    dnnl_AB16a16b = 513,
    dnnl_AB16a32b = 514,
    dnnl_ABcde16a16b2a = 515,
    dnnl_aBCdef16b16c2b = 516,
    dnnl_Acedb16a = 517,
    dnnl_aBdfec16b = 518,
    dnnl_abdEC64e2c = 519,
    dnnl_abdEC64e4c = 520,
    dnnl_aCB16b16c = 521,
    dnnl_aCB16b32c = 522,
    dnnl_aCB16b48c = 523,
    dnnl_aCB16b64c = 524,
    dnnl_aCB16b16c2b = 525,
    dnnl_aCB16b32c2b = 526,
    dnnl_aCB16b48c2b = 527,
    dnnl_aCB16b64c2b = 528,
    dnnl_aCB16b16c4b = 529,
    dnnl_aCB16b32c4b = 530,
    dnnl_aCB16b48c4b = 531,
    dnnl_aCB16b64c4b = 532,
    dnnl_abCd4c = 533,
    dnnl_abCde4c = 534,
    dnnl_abCdef4c = 535,
    dnnl_abCde32c = 536,
    dnnl_abCdef32c = 537,
    dnnl_ABcd16a32b = 538,
    dnnl_decbA8a = 539,
    dnnl_aCdefB16b32c2b = 540,
    dnnl_aCdefB16b32c4b = 541,
    dnnl_aCdefB16b48c2b = 542,
    dnnl_aCdefB16b48c4b = 543,
    dnnl_aCdefB16b64c2b = 544,
    dnnl_aCdefB16b64c4b = 545,
    dnnl_BcdeA16a32b2a = 546,
    dnnl_BcdeA16a32b4a = 547,
    dnnl_BcdeA16a48b2a = 548,
    dnnl_BcdeA16a48b4a = 549,
    dnnl_BcdeA16a64b2a = 550,
    dnnl_BcdeA16a64b4a = 551,
    dnnl_aCdefb32c = 552,
    dnnl_aCdefB32c2b = 553,
    dnnl_aCdefB32c4b = 554,
    dnnl_aCdefb48c = 555,
    dnnl_aCdefB48c2b = 556,
    dnnl_aCdefB48c4b = 557,
    dnnl_aCdefb64c = 558,
    dnnl_aCdefB64c2b = 559,
    dnnl_aCdefB64c4b = 560,
    dnnl_Bcdea32b = 561,
    dnnl_BcdeA32b2a = 562,
    dnnl_BcdeA32b4a = 563,
    dnnl_Bcdea48b = 564,
    dnnl_BcdeA48b2a = 565,
    dnnl_BcdeA48b4a = 566,
    dnnl_Bcdea64b = 567,
    dnnl_BcdeA64b2a = 568,
    dnnl_BcdeA64b4a = 569,
    dnnl_Bca32b = 570,
    dnnl_BcA32b2a = 571,
    dnnl_BcA32b4a = 572,
    dnnl_Bca48b = 573,
    dnnl_BcA48b2a = 574,
    dnnl_BcA48b4a = 575,
    dnnl_Bca64b = 576,
    dnnl_BcA64b2a = 577,
    dnnl_BcA64b4a = 578,
    dnnl_aCdb32c = 579,
    dnnl_aCdB32c2b = 580,
    dnnl_aCdB32c4b = 581,
    dnnl_aCdb48c = 582,
    dnnl_aCdB48c2b = 583,
    dnnl_aCdB48c4b = 584,
    dnnl_aCdb64c = 585,
    dnnl_aCdB64c2b = 586,
    dnnl_aCdB64c4b = 587,
    dnnl_BcA16a16b2a = 588,
    dnnl_BcA16a16b4a = 589,
    dnnl_BcdA16a16b2a = 590,
    dnnl_BcdA16a16b4a = 591,
    dnnl_BcdeA16a16b2a = 592,
    dnnl_BcdeA16a16b4a = 593,
    dnnl_aCdB16b16c2b = 594,
    dnnl_aCdB16b16c4b = 595,
    dnnl_aCdeB16b16c2b = 596,
    dnnl_aCdeB16b16c4b = 597,
    dnnl_aCdefB16b16c2b = 598,
    dnnl_aCdefB16b16c4b = 599,
    dnnl_BcA16a32b2a = 600,
    dnnl_BcA16a32b4a = 601,
    dnnl_BcA16a48b2a = 602,
    dnnl_BcA16a48b4a = 603,
    dnnl_BcA16a64b2a = 604,
    dnnl_BcA16a64b4a = 605,
    dnnl_aCdB16b32c2b = 606,
    dnnl_aCdB16b32c4b = 607,
    dnnl_aCdB16b48c2b = 608,
    dnnl_aCdB16b48c4b = 609,
    dnnl_aCdB16b64c2b = 610,
    dnnl_aCdB16b64c4b = 611,
    dnnl_BcdA16a32b2a = 612,
    dnnl_BcdA16a32b4a = 613,
    dnnl_BcdA16a48b2a = 614,
    dnnl_BcdA16a48b4a = 615,
    dnnl_BcdA16a64b2a = 616,
    dnnl_BcdA16a64b4a = 617,
    dnnl_aCdeB16b32c2b = 618,
    dnnl_aCdeB16b32c4b = 619,
    dnnl_aCdeB16b48c2b = 620,
    dnnl_aCdeB16b48c4b = 621,
    dnnl_aCdeB16b64c2b = 622,
    dnnl_aCdeB16b64c4b = 623,
    dnnl_Bca16b = 624,
    dnnl_BcA16b2a = 625,
    dnnl_BcA16b4a = 626,
    dnnl_Bcda16b = 627,
    dnnl_BcdA16b2a = 628,
    dnnl_BcdA16b4a = 629,
    dnnl_Bcdea16b = 630,
    dnnl_BcdeA16b2a = 631,
    dnnl_BcdeA16b4a = 632,
    dnnl_aCdb16c = 633,
    dnnl_aCdB16c2b = 634,
    dnnl_aCdB16c4b = 635,
    dnnl_aCdeb16c = 636,
    dnnl_aCdeB16c2b = 637,
    dnnl_aCdeB16c4b = 638,
    dnnl_aCdefb16c = 639,
    dnnl_aCdefB16c2b = 640,
    dnnl_aCdefB16c4b = 641,
    dnnl_Bcda32b = 642,
    dnnl_BcdA32b2a = 643,
    dnnl_BcdA32b4a = 644,
    dnnl_Bcda48b = 645,
    dnnl_BcdA48b2a = 646,
    dnnl_BcdA48b4a = 647,
    dnnl_Bcda64b = 648,
    dnnl_BcdA64b2a = 649,
    dnnl_BcdA64b4a = 650,
    dnnl_aCdeb32c = 651,
    dnnl_aCdeB32c2b = 652,
    dnnl_aCdeB32c4b = 653,
    dnnl_aCdeb48c = 654,
    dnnl_aCdeB48c2b = 655,
    dnnl_aCdeB48c4b = 656,
    dnnl_aCdeb64c = 657,
    dnnl_aCdeB64c2b = 658,
    dnnl_aCdeB64c4b = 659,
    dnnl_Acb24a = 660,
    dnnl_Acdb24a = 661,
    dnnl_Acdeb24a = 662,
    dnnl_aBdc24b = 663,
    dnnl_aBdec24b = 664,
    dnnl_aBdefc24b = 665,
    dnnl_abDc16d = 666,
    dnnl_abdEc16e = 667,
    dnnl_abdCe16c = 668,
    dnnl_AcB24a2b = 669,
    dnnl_AcdB24a2b = 670,
    dnnl_AcdeB24a2b = 671,
    dnnl_aBdC24b2c = 672,
    dnnl_aBdeC24b2c = 673,
    dnnl_aBdefC24b2c = 674,
    dnnl_AcB8a2b = 675,
    dnnl_AcdB8a2b = 676,
    dnnl_AcdeB8a2b = 677,
    dnnl_aBdC8b2c = 678,
    dnnl_aBdeC8b2c = 679,
    dnnl_aBdefC8b2c = 680,
    dnnl_AB8b32a = 681,
    dnnl_ABc8b32a = 682,
    dnnl_ABcd8b32a = 683,
    dnnl_ABcde8b32a = 684,
    dnnl_AB8b24a = 685,
    dnnl_ABc8b24a = 686,
    dnnl_ABcd8b24a = 687,
    dnnl_ABcde8b24a = 688,
    dnnl_AB8b16a = 689,
    dnnl_ABc8b16a = 690,
    dnnl_ABcd8b16a = 691,
    dnnl_ABcde8b16a = 692,
    dnnl_AB8b8a = 693,
    dnnl_AB4b8a4b = 694,
    dnnl_AB4b24a4b = 695,
    dnnl_ABc4b8a4b = 696,
    dnnl_ABc4b24a4b = 697,
    dnnl_ABcd4b8a4b = 698,
    dnnl_ABcd4b24a4b = 699,
    dnnl_ABcde4b8a4b = 700,
    dnnl_ABcde4b24a4b = 701,
    dnnl_AB8b24a2b = 702,
    dnnl_ABc8b24a2b = 703,
    dnnl_ABcd8b24a2b = 704,
    dnnl_ABcde8b24a2b = 705,
    dnnl_AB8b8a2b = 706,
    dnnl_ABc8b8a2b = 707,
    dnnl_ABcd8b8a2b = 708,
    dnnl_ABcde8b8a2b = 709,
    dnnl_AcB24a4b = 710,
    dnnl_AcdB24a4b = 711,
    dnnl_AcdeB24a4b = 712,
    dnnl_aBdC24b4c = 713,
    dnnl_aBdeC24b4c = 714,
    dnnl_aBdefC24b4c = 715,
    dnnl_AcB8a4b = 716,
    dnnl_AcdB8a4b = 717,
    dnnl_AcdeB8a4b = 718,
    dnnl_aBdC8b4c = 719,
    dnnl_aBdeC8b4c = 720,
    dnnl_aBdefC8b4c = 721,
    dnnl_Bca8b = 722,
    dnnl_BcA8b2a = 723,
    dnnl_Bcda8b = 724,
    dnnl_BcdA8b2a = 725,
    dnnl_Bcdea8b = 726,
    dnnl_BcdeA8b2a = 727,
    dnnl_aCdb8c = 728,
    dnnl_aCdB8c2b = 729,
    dnnl_aCdeb8c = 730,
    dnnl_aCdeB8c2b = 731,
    dnnl_aCdefb8c = 732,
    dnnl_aCdefB8c2b = 733,
    dnnl_Bca24b = 734,
    dnnl_BcA24b2a = 735,
    dnnl_Bcda24b = 736,
    dnnl_BcdA24b2a = 737,
    dnnl_Bcdea24b = 738,
    dnnl_BcdeA24b2a = 739,
    dnnl_aCdb24c = 740,
    dnnl_aCdB24c2b = 741,
    dnnl_aCdeb24c = 742,
    dnnl_aCdeB24c2b = 743,
    dnnl_aCdefb24c = 744,
    dnnl_aCdefB24c2b = 745,
    dnnl_BcA8b4a = 746,
    dnnl_BcdA8b4a = 747,
    dnnl_BcdeA8b4a = 748,
    dnnl_aCdB8c4b = 749,
    dnnl_aCdeB8c4b = 750,
    dnnl_aCdefB8c4b = 751,
    dnnl_BcA24b4a = 752,
    dnnl_BcdA24b4a = 753,
    dnnl_BcdeA24b4a = 754,
    dnnl_aCdB24c4b = 755,
    dnnl_aCdeB24c4b = 756,
    dnnl_aCdefB24c4b = 757,
    dnnl_AB16b48a = 758,
    dnnl_ABc16b48a = 759,
    dnnl_ABcd16b48a = 760,
    dnnl_ABcde16b48a = 761,
    dnnl_ABc16a4b = 762,
    dnnl_ABcd16a4b = 763,
    dnnl_ABcde16a4b = 764,
    dnnl_defcbA16a = 765,
    dnnl_defcbA8a = 766,

    /** Just a sentinel, not real memory format tag. Must be changed after new
     *  format tag is added. */
    dnnl_format_tag_last = 767,

    // Aliases

    /** 1D tensor, an alias to #dnnl_a */
    dnnl_x = dnnl_a,
    /** 2D CNN activations tensor, an alias to #dnnl_ab */
    dnnl_nc = dnnl_ab,
    /** 2D CNN activations tensor, an alias to #dnnl_ba */
    dnnl_cn = dnnl_ba,
    /** 2D RNN statistics tensor, an alias to #dnnl_ab */
    dnnl_tn = dnnl_ab,
    /** 2D RNN statistics tensor, an alias to #dnnl_ba */
    dnnl_nt = dnnl_ba,
    /** 3D CNN activations tensor, an alias to #dnnl_abc */
    dnnl_ncw = dnnl_abc,
    /** 3D CNN activations tensor, an alias to #dnnl_acb */
    dnnl_nwc = dnnl_acb,
    /** 4D CNN activations tensor, an alias to #dnnl_abcd */
    dnnl_nchw = dnnl_abcd,
    /** 4D CNN activations tensor, an alias to #dnnl_acdb */
    dnnl_nhwc = dnnl_acdb,
    /** 4D CNN activations tensor, an alias to #dnnl_bcda */
    dnnl_chwn = dnnl_bcda,
    /** 5D CNN activations tensor, an alias to #dnnl_abcde */
    dnnl_ncdhw = dnnl_abcde,
    /** 5D CNN activations tensor, an alias to #dnnl_acdeb */
    dnnl_ndhwc = dnnl_acdeb,

    /** 2D CNN weights tensor, an alias to #dnnl_ab */
    dnnl_oi = dnnl_ab,
    /** 2D CNN weights tensor, an alias to #dnnl_ba */
    dnnl_io = dnnl_ba,
    /** 3D CNN weights tensor, an alias to #dnnl_abc */
    dnnl_oiw = dnnl_abc,
    /** 3D CNN weights tensor, an alias to #dnnl_acb */
    dnnl_owi = dnnl_acb,
    /** 3D CNN weights tensor, an alias to #dnnl_cba */
    dnnl_wio = dnnl_cba,
    /** 3D CNN weights tensor, an alias to #dnnl_cab */
    dnnl_woi = dnnl_cab,
    /** 3D CNN weights tensor, an alias to #dnnl_bca */
    dnnl_iwo = dnnl_bca,
    /** 4D CNN weights tensor, an alias to #dnnl_abcd */
    dnnl_oihw = dnnl_abcd,
    /** 4D CNN weights tensor, an alias to #dnnl_cdba */
    dnnl_hwio = dnnl_cdba,
    /** 4D CNN weights tensor, an alias to #dnnl_cdab */
    dnnl_hwoi = dnnl_cdab,
    /** 4D CNN weights tensor, an alias to #dnnl_acdb */
    dnnl_ohwi = dnnl_acdb,
    /** 4D CNN weights tensor, an alias to #dnnl_bcda */
    dnnl_ihwo = dnnl_bcda,
    /** 4D CNN weights tensor, an alias to #dnnl_bacd */
    dnnl_iohw = dnnl_bacd,
    /** 5D CNN weights tensor, an alias to #dnnl_abcde */
    dnnl_oidhw = dnnl_abcde,
    /** 5D CNN weights tensor, an alias to #dnnl_bacde */
    dnnl_iodhw = dnnl_bacde,
    /** 5D CNN weights tensor, an alias to #dnnl_cdeba */
    dnnl_dhwio = dnnl_cdeba,
    /** 5D CNN weights tensor, an alias to #dnnl_cdeab */
    dnnl_dhwoi = dnnl_cdeab,
    /** 5D CNN weights tensor, an alias to #dnnl_acdeb */
    dnnl_odhwi = dnnl_acdeb,
    /** 5D CNN weights tensor, an alias to #dnnl_bcdea */
    dnnl_idhwo = dnnl_bcdea,

    /** 4D CNN weights tensor (incl. groups), an alias to #dnnl_abcd */
    dnnl_goiw = dnnl_abcd,
    /** 4D CNN weights tensor (incl. groups), an alias to #dnnl_abdc */
    dnnl_gowi = dnnl_abdc,
    /** 4D CNN weights tensor (incl. groups), an alias to #dnnl_dcab */
    dnnl_wigo = dnnl_dcab,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_abcde */
    dnnl_goihw = dnnl_abcde,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_abdec */
    dnnl_gohwi = dnnl_abdec,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_decab */
    dnnl_hwigo = dnnl_decab,
    /** 5D CNN weights tensor (incl. groups), an alias to #dnnl_acbde */
    dnnl_giohw = dnnl_acbde,
    /** 6D CNN weights tensor (incl. groups), an alias to #dnnl_abcdef */
    dnnl_goidhw = dnnl_abcdef,
    /** 6D CNN weights tensor (incl. groups), an alias to #dnnl_abdefc */
    dnnl_godhwi = dnnl_abdefc,
    /** 6D CNN weights tensor (incl. groups), an alias to #dnnl_acbdef */
    dnnl_giodhw = dnnl_acbdef,
    /** 6D CNN weights tensor (incl. groups), an alias to #dnnl_defcab */
    dnnl_dhwigo = dnnl_defcab,

    /** 3D RNN data tensor in the format (seq_length, batch, input channels),
     *  an alias to #dnnl_abc. */
    dnnl_tnc = dnnl_abc,
    /** 3D RNN data tensor in the format (batch, seq_length, input channels),
     *  an alias to #dnnl_bac. */
    dnnl_ntc = dnnl_bac,
    /** 4D RNN states tensor in the format (num_layers, num_directions,
     *  batch, state channels), an alias to #dnnl_abcd. */
    
///
    dnnl_ldnc = dnnl_abcd,
    /** 5D RNN weights tensor in the format (num_layers, num_directions,
     *  input_channels, num_gates, output_channels), an alias to #dnnl_abcde.
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    
///
    dnnl_ldigo = dnnl_abcde,
    /** 5D RNN weights tensor in the format (num_layers, num_directions,
     *  num_gates, output_channels, input_channels), an alias to #dnnl_abdec.
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    dnnl_ldgoi = dnnl_abdec,
    /** 4D LSTM projection tensor in the format (num_layers, num_directions,
     *  num_channels_in_hidden_state, num_channels_in_recurrent_projection),
     *  an alias to #dnnl_abcd. */
    dnnl_ldio = dnnl_abcd,
    /** 4D LSTM projection tensor in the format (num_layers, num_directions,
     *  num_channels_in_recurrent_projection, num_channels_in_hidden_state),
     *  an alias to #dnnl_abdc. */
    
///
    dnnl_ldoi = dnnl_abdc,
    /** 4D RNN bias tensor in the format (num_layers, num_directions,
     *  num_gates, output_channels), an alias to #dnnl_abcd.
     * 
     *   - For LSTM cells, the gates order is input, forget, candidate
     *     and output gate.
     *   - For GRU cells, the gates order is update, reset and output gate. */
    dnnl_ldgo = dnnl_abcd,
    /** 5D LSTM projection tensor */
    dnnl_ldOi16o = dnnl_abDc16d,
    dnnl_ldOi32o = dnnl_abDc32d,
    dnnl_ldOI32o4i = dnnl_abDC32d4c,
    dnnl_ldIo32i = dnnl_abCd32c,
    /** 6D RNN weights tensor */
    dnnl_ldgOi16o = dnnl_abdEc16e,
    dnnl_ldgOi32o = dnnl_abdEc32e,
    dnnl_ldgOI32o2i = dnnl_abdEC32e2c,
    dnnl_ldgOI32o4i = dnnl_abdEC32e4c,
    dnnl_ldgOI64o2i = dnnl_abdEC64e2c,
    dnnl_ldgOI64o4i = dnnl_abdEC64e4c,
    dnnl_ldgIo16i = dnnl_abdCe16c,
    dnnl_ldgIo32i = dnnl_abdCe32c,
    dnnl_ldgIO32i2o = dnnl_abdCE32c2e,

    // Opaque data types, are not to be used explicitly

    // data
    /** 5D CNN activations tensor blocked by channels with block size 32,
     *  an alias to #dnnl_aBcde32b */
    dnnl_nCdhw32c = dnnl_aBcde32b,
    /** 5D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBcde16b */
    dnnl_nCdhw16c = dnnl_aBcde16b,
    /** 5D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBcde4b */
    dnnl_nCdhw4c = dnnl_aBcde4b,
    /** 5D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBcde8b */
    dnnl_nCdhw8c = dnnl_aBcde8b,
    /** 4D CNN activations tensor blocked by channels with block size 32,
     *  an alias to #dnnl_aBcd32b */
    dnnl_nChw32c = dnnl_aBcd32b,
    /** 4D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBcd16b */
    dnnl_nChw16c = dnnl_aBcd16b,
    /** 4D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBcd4b */
    dnnl_nChw4c = dnnl_aBcd4b,
    /** 4D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBcd8b */
    dnnl_nChw8c = dnnl_aBcd8b,
    /** 3D CNN activations tensor blocked by channels with block size 32,
     *  an alias to #dnnl_aBc32b */
    dnnl_nCw32c = dnnl_aBc32b,
    /** 3D CNN activations tensor blocked by channels with block size 16,
     *  an alias to #dnnl_aBc16b */
    dnnl_nCw16c = dnnl_aBc16b,
    /** 3D CNN activations tensor blocked by channels with block size 4,
     *  an alias to #dnnl_aBc4b */
    dnnl_nCw4c = dnnl_aBc4b,
    /** 3D CNN activations tensor blocked by channels with block size 8,
     *  an alias to #dnnl_aBc8b */
    dnnl_nCw8c = dnnl_aBc8b,
    dnnl_NCw16n16c = dnnl_ABc16a16b,
    dnnl_NCdhw16n16c = dnnl_ABcde16a16b,
    dnnl_NChw16n16c = dnnl_ABcd16a16b,
    dnnl_NCw32n16c = dnnl_ABc32a16b,
    dnnl_NChw32n16c = dnnl_ABcd32a16b,
    dnnl_NChw16n32c = dnnl_ABcd16a32b,
    dnnl_NCdhw32n16c = dnnl_ABcde32a16b,
    dnnl_NCw32n32c = dnnl_ABc32a32b,
    dnnl_NChw32n32c = dnnl_ABcd32a32b,
    dnnl_NCdhw32n32c = dnnl_ABcde32a32b,

    // weights, 2D
    dnnl_OI16i16o = dnnl_AB16b16a,
    dnnl_OI16i32o = dnnl_AB16b32a,
    dnnl_OI16i48o = dnnl_AB16b48a,
    dnnl_OI16i64o = dnnl_AB16b64a,
    dnnl_OI8i8o2i = dnnl_AB8b8a2b,
    dnnl_OI8i16o2i = dnnl_AB8b16a2b,
    dnnl_OI8i24o2i = dnnl_AB8b24a2b,
    dnnl_OI8i32o2i = dnnl_AB8b32a2b,
    dnnl_OI8i64o2i = dnnl_AB8b64a2b,
    dnnl_OI4i8o4i = dnnl_AB4b8a4b,
    dnnl_OI4i16o4i = dnnl_AB4b16a4b,
    dnnl_OI4i24o4i = dnnl_AB4b24a4b,
    dnnl_OI4i32o4i = dnnl_AB4b32a4b,
    dnnl_OI4i64o4i = dnnl_AB4b64a4b,
    dnnl_OI16i16o4i = dnnl_AB16b16a4b,
    dnnl_OI8i32o = dnnl_AB8b32a,
    dnnl_OI8i24o = dnnl_AB8b24a,
    dnnl_OI8i16o = dnnl_AB8b16a,
    dnnl_OI8i8o = dnnl_AB8b8a,

    // weights, 3D
    dnnl_IOw16o16i = dnnl_BAc16a16b,
    dnnl_IOw16i16o = dnnl_BAc16b16a,
    dnnl_OIw16i16o = dnnl_ABc16b16a,
    dnnl_OIw16i32o = dnnl_ABc16b32a,
    dnnl_OIw16i48o = dnnl_ABc16b48a,
    dnnl_OIw16i64o = dnnl_ABc16b64a,
    dnnl_OIw16o16i = dnnl_ABc16a16b,
    dnnl_Oiw16o = dnnl_Abc16a,
    dnnl_OIw4i8o4i = dnnl_ABc4b8a4b,
    dnnl_OIw4i16o4i = dnnl_ABc4b16a4b,
    dnnl_OIw4i24o4i = dnnl_ABc4b24a4b,
    dnnl_OIw4i32o4i = dnnl_ABc4b32a4b,
    dnnl_OIw4i64o4i = dnnl_ABc4b64a4b,
    dnnl_OIw2i8o4i = dnnl_ABc2b8a4b,
    dnnl_OIw16i16o4i = dnnl_ABc16b16a4b,
    dnnl_OIw16i16o2i = dnnl_ABc16b16a2b,
    dnnl_OIw16o16i2o = dnnl_ABc16a16b2a,
    dnnl_OIw4i4o = dnnl_ABc4b4a,
    dnnl_OIw4o4i = dnnl_ABc4a4b,
    dnnl_Oiw4o = dnnl_Abc4a,
    dnnl_OIw8i8o2i = dnnl_ABc8b8a2b,
    dnnl_OIw8i16o2i = dnnl_ABc8b16a2b,
    dnnl_OIw8i24o2i = dnnl_ABc8b24a2b,
    dnnl_OIw8i32o2i = dnnl_ABc8b32a2b,
    dnnl_OIw8i64o2i = dnnl_ABc8b64a2b,
    dnnl_OIw8i8o = dnnl_ABc8b8a,
    dnnl_OIw8o16i2o = dnnl_ABc8a16b2a,
    dnnl_IOw8o16i2o = dnnl_BAc8a16b2a,
    dnnl_OIw8o8i = dnnl_ABc8a8b,
    dnnl_OIw8o4i = dnnl_ABc8a4b,
    dnnl_Owi16o = dnnl_Acb16a,
    dnnl_OwI16o2i = dnnl_AcB16a2b,
    dnnl_OwI16o4i = dnnl_AcB16a4b,
    dnnl_Iwo8i = dnnl_Bca8b,
    dnnl_IwO8i2o = dnnl_BcA8b2a,
    dnnl_IwO8i4o = dnnl_BcA8b4a,
    dnnl_Iwo16i = dnnl_Bca16b,
    dnnl_IwO16i2o = dnnl_BcA16b2a,
    dnnl_IwO16i4o = dnnl_BcA16b4a,
    dnnl_Iwo24i = dnnl_Bca24b,
    dnnl_IwO24i2o = dnnl_BcA24b2a,
    dnnl_IwO24i4o = dnnl_BcA24b4a,
    dnnl_Owi4o = dnnl_Acb4a,
    dnnl_Owi8o = dnnl_Acb8a,
    dnnl_OwI8o2i = dnnl_AcB8a2b,
    dnnl_OIw8i32o = dnnl_ABc8b32a,
    dnnl_OIw8i24o = dnnl_ABc8b24a,
    dnnl_OIw8i16o = dnnl_ABc8b16a,
    dnnl_OwI8o4i = dnnl_AcB8a4b,

    // weights, 4D
    dnnl_IOhw16i16o = dnnl_BAcd16b16a,
    dnnl_IOhw16o16i = dnnl_BAcd16a16b,
    dnnl_Ohwi16o = dnnl_Acdb16a,
    dnnl_OhwI16o2i = dnnl_AcdB16a2b,
    dnnl_OhwI16o4i = dnnl_AcdB16a4b,
    dnnl_Ihwo8i = dnnl_Bcda8b,
    dnnl_IhwO8i2o = dnnl_BcdA8b2a,
    dnnl_IhwO8i4o = dnnl_BcdA8b4a,
    dnnl_Ihwo16i = dnnl_Bcda16b,
    dnnl_IhwO16i2o = dnnl_BcdA16b2a,
    dnnl_IhwO16i4o = dnnl_BcdA16b4a,
    dnnl_Ihwo24i = dnnl_Bcda24b,
    dnnl_IhwO24i2o = dnnl_BcdA24b2a,
    dnnl_IhwO24i4o = dnnl_BcdA24b4a,
    dnnl_Ohwi24o = dnnl_Acdb24a,
    dnnl_Ohwi32o = dnnl_Acdb32a,
    dnnl_Ohwi4o = dnnl_Acdb4a,
    dnnl_Ohwi8o = dnnl_Acdb8a,
    dnnl_OhwI8o2i = dnnl_AcdB8a2b,
    dnnl_OhwI8o4i = dnnl_AcdB8a4b,
    dnnl_OIhw16i16o = dnnl_ABcd16b16a,
    dnnl_OIhw16i32o = dnnl_ABcd16b32a,
    dnnl_OIhw16i48o = dnnl_ABcd16b48a,
    dnnl_OIhw16i64o = dnnl_ABcd16b64a,
    dnnl_OIhw16o16i = dnnl_ABcd16a16b,
    dnnl_Oihw16o = dnnl_Abcd16a,
    dnnl_OIhw4i8o4i = dnnl_ABcd4b8a4b,
    dnnl_OIhw4i16o4i = dnnl_ABcd4b16a4b,
    dnnl_OIhw4i24o4i = dnnl_ABcd4b24a4b,
    dnnl_OIhw4i32o4i = dnnl_ABcd4b32a4b,
    dnnl_OIhw4i64o4i = dnnl_ABcd4b64a4b,
    dnnl_OIhw16i16o4i = dnnl_ABcd16b16a4b,
    dnnl_OIhw16i16o2i = dnnl_ABcd16b16a2b,
    dnnl_OIhw16o16i2o = dnnl_ABcd16a16b2a,
    dnnl_OIhw4i4o = dnnl_ABcd4b4a,
    dnnl_OIhw4o4i = dnnl_ABcd4a4b,
    dnnl_Oihw4o = dnnl_Abcd4a,
    dnnl_OIhw8i8o2i = dnnl_ABcd8b8a2b,
    dnnl_OIhw8i16o2i = dnnl_ABcd8b16a2b,
    dnnl_OIhw8i32o2i = dnnl_ABcd8b32a2b,
    dnnl_OIhw8i24o2i = dnnl_ABcd8b24a2b,
    dnnl_OIhw8i64o2i = dnnl_ABcd8b64a2b,
    dnnl_OIhw8i8o = dnnl_ABcd8b8a,
    dnnl_OIhw8o16i2o = dnnl_ABcd8a16b2a,
    dnnl_OIhw2i8o4i = dnnl_ABcd2b8a4b,
    dnnl_IOhw8o16i2o = dnnl_BAcd8a16b2a,
    dnnl_OIhw8o8i = dnnl_ABcd8a8b,
    dnnl_OIhw8o4i = dnnl_ABcd8a4b,
    dnnl_Owhi16o = dnnl_Adcb16a,
    dnnl_OIhw8i32o = dnnl_ABcd8b32a,
    dnnl_OIhw8i24o = dnnl_ABcd8b24a,
    dnnl_OIhw8i16o = dnnl_ABcd8b16a,

    // weights, 5D
    dnnl_Odhwi16o = dnnl_Acdeb16a,
    dnnl_OdhwI16o2i = dnnl_AcdeB16a2b,
    dnnl_OdhwI16o4i = dnnl_AcdeB16a4b,
    dnnl_Idhwo8i = dnnl_Bcdea8b,
    dnnl_IdhwO8i2o = dnnl_BcdeA8b2a,
    dnnl_IdhwO8i4o = dnnl_BcdeA8b4a,
    dnnl_Idhwo16i = dnnl_Bcdea16b,
    dnnl_IdhwO16i2o = dnnl_BcdeA16b2a,
    dnnl_IdhwO16i4o = dnnl_BcdeA16b4a,
    dnnl_Idhwo24i = dnnl_Bcdea24b,
    dnnl_IdhwO24i2o = dnnl_BcdeA24b2a,
    dnnl_IdhwO24i4o = dnnl_BcdeA24b4a,
    dnnl_Odhwi4o = dnnl_Acdeb4a,
    dnnl_Odhwi8o = dnnl_Acdeb8a,
    dnnl_OdhwI8o2i = dnnl_AcdeB8a2b,
    dnnl_OdhwI8o4i = dnnl_AcdeB8a4b,
    dnnl_Odwhi16o = dnnl_Acedb16a,
    dnnl_OIdhw16i16o = dnnl_ABcde16b16a,
    dnnl_OIdhw16i32o = dnnl_ABcde16b32a,
    dnnl_OIdhw16i48o = dnnl_ABcde16b48a,
    dnnl_OIdhw16i64o = dnnl_ABcde16b64a,
    dnnl_OIdhw16o16i = dnnl_ABcde16a16b,
    dnnl_Oidhw16o = dnnl_Abcde16a,
    dnnl_OIdhw4i4o = dnnl_ABcde4b4a,
    dnnl_OIdhw4o4i = dnnl_ABcde4a4b,
    dnnl_Oidhw4o = dnnl_Abcde4a,
    dnnl_OIdhw8i8o2i = dnnl_ABcde8b8a2b,
    dnnl_OIdhw8i16o2i = dnnl_ABcde8b16a2b,
    dnnl_OIdhw8i32o2i = dnnl_ABcde8b32a2b,
    dnnl_OIdhw8i24o2i = dnnl_ABcde8b24a2b,
    dnnl_OIdhw8i64o2i = dnnl_ABcde8b64a2b,
    dnnl_OIdhw8i8o = dnnl_ABcde8b8a,
    dnnl_OIdhw8o16i2o = dnnl_ABcde8a16b2a,
    dnnl_IOdhw8o16i2o = dnnl_BAcde8a16b2a,
    dnnl_OIdhw4i8o4i = dnnl_ABcde4b8a4b,
    dnnl_OIdhw4i16o4i = dnnl_ABcde4b16a4b,
    dnnl_OIdhw4i24o4i = dnnl_ABcde4b24a4b,
    dnnl_OIdhw4i32o4i = dnnl_ABcde4b32a4b,
    dnnl_OIdhw4i64o4i = dnnl_ABcde4b64a4b,
    dnnl_OIdhw16i16o4i = dnnl_ABcde16b16a4b,
    dnnl_OIdhw16i16o2i = dnnl_ABcde16b16a2b,
    dnnl_OIdhw2i8o4i = dnnl_ABcde2b8a4b,
    dnnl_OIdhw8o8i = dnnl_ABcde8a8b,
    dnnl_OIdhw8o4i = dnnl_ABcde8a4b,
    dnnl_IOdhw16i16o = dnnl_BAcde16b16a,
    dnnl_OIdhw4o8i8o4i = dnnl_ABcde4a8b8a4b,
    dnnl_IOdhw16o16i = dnnl_BAcde16a16b,
    dnnl_OIdhw16o16i2o = dnnl_ABcde16a16b2a,
    dnnl_OIdhw8i32o = dnnl_ABcde8b32a,
    dnnl_OIdhw8i24o = dnnl_ABcde8b24a,
    dnnl_OIdhw8i16o = dnnl_ABcde8b16a,

    // weights w/ groups, 3D
    dnnl_Goiw16g = dnnl_Abcd16a,
    dnnl_Goiw8g = dnnl_Abcd8a,
    dnnl_Goiw4g = dnnl_Abcd4a,
    dnnl_gIOw16o16i = dnnl_aCBd16b16c,
    dnnl_gIOw16i16o = dnnl_aCBd16c16b,
    dnnl_gOIw16i16o = dnnl_aBCd16c16b,
    dnnl_gOIw16o16i = dnnl_aBCd16b16c,
    dnnl_gOiw16o = dnnl_aBcd16b,
    dnnl_gOIw4i16o4i = dnnl_aBCd4c16b4c,
    dnnl_gOIw2i8o4i = dnnl_aBCd2c8b4c,
    dnnl_gOIw16i16o4i = dnnl_aBCd16c16b4c,
    dnnl_gOIw16i16o2i = dnnl_aBCd16c16b2c,
    dnnl_gOIw16o16i2o = dnnl_aBCd16b16c2b,
    dnnl_gOIw4i4o = dnnl_aBCd4c4b,
    dnnl_gOIw4o4i = dnnl_aBCd4b4c,
    dnnl_gOiw4o = dnnl_aBcd4b,
    dnnl_gOIw8i16o2i = dnnl_aBCd8c16b2c,
    dnnl_gOIw8i8o = dnnl_aBCd8c8b,
    dnnl_gOIw8o16i2o = dnnl_aBCd8b16c2b,
    dnnl_gIOw8o16i2o = dnnl_aCBd8b16c2b,
    dnnl_gOIw8o8i = dnnl_aBCd8b8c,
    dnnl_gOIw8o4i = dnnl_aBCd8b4c,
    dnnl_gOwi16o = dnnl_aBdc16b,
    dnnl_gOwI16o2i = dnnl_aBdC16b2c,
    dnnl_gOwI16o4i = dnnl_aBdC16b4c,
    dnnl_gIwo8i = dnnl_aCdb8c,
    dnnl_gIwO8i2o = dnnl_aCdB8c2b,
    dnnl_gIwO8i4o = dnnl_aCdB8c4b,
    dnnl_gIwo16i = dnnl_aCdb16c,
    dnnl_gIwO16i2o = dnnl_aCdB16c2b,
    dnnl_gIwO16i4o = dnnl_aCdB16c4b,
    dnnl_gIwo24i = dnnl_aCdb24c,
    dnnl_gIwO24i2o = dnnl_aCdB24c2b,
    dnnl_gIwO24i4o = dnnl_aCdB24c4b,
    dnnl_gOwi4o = dnnl_aBdc4b,
    dnnl_gOwi8o = dnnl_aBdc8b,
    dnnl_gOwI8o2i = dnnl_aBdC8b2c,
    dnnl_gOwI8o4i = dnnl_aBdC8b4c,
    dnnl_Goiw32g = dnnl_Abcd32a,
    dnnl_gOIw2i4o2i = dnnl_aBCd2c4b2c,
    dnnl_gOIw2o4i2o = dnnl_aBCd2b4c2b,
    dnnl_gOIw4i8o2i = dnnl_aBCd4c8b2c,
    dnnl_gOIw4o8i2o = dnnl_aBCd4b8c2b,
    dnnl_goIw4i = dnnl_abCd4c,
    dnnl_goIw32i = dnnl_abCd32c,

    // weights w/ groups, 4D
    dnnl_gIOhw16i16o = dnnl_aCBde16c16b,
    dnnl_gIOhw16o16i = dnnl_aCBde16b16c,
    dnnl_gOhwi16o = dnnl_aBdec16b,
    dnnl_gOhwI16o2i = dnnl_aBdeC16b2c,
    dnnl_gOhwI16o4i = dnnl_aBdeC16b4c,
    dnnl_gIhwo8i = dnnl_aCdeb8c,
    dnnl_gIhwO8i2o = dnnl_aCdeB8c2b,
    dnnl_gIhwO8i4o = dnnl_aCdeB8c4b,
    dnnl_gIhwo16i = dnnl_aCdeb16c,
    dnnl_gIhwO16i2o = dnnl_aCdeB16c2b,
    dnnl_gIhwO16i4o = dnnl_aCdeB16c4b,
    dnnl_gIhwo24i = dnnl_aCdeb24c,
    dnnl_gIhwO24i2o = dnnl_aCdeB24c2b,
    dnnl_gIhwO24i4o = dnnl_aCdeB24c4b,
    dnnl_gOhwi32o = dnnl_aBdec32b,
    dnnl_gOhwi24o = dnnl_aBdec24b,
    dnnl_gOhwI24o2i = dnnl_aBdeC24b2c,
    dnnl_gOhwI24o4i = dnnl_aBdeC24b4c,
    dnnl_gOhwi4o = dnnl_aBdec4b,
    dnnl_gOhwi8o = dnnl_aBdec8b,
    dnnl_gOhwI8o2i = dnnl_aBdeC8b2c,
    dnnl_gOhwI8o4i = dnnl_aBdeC8b4c,
    dnnl_Goihw16g = dnnl_Abcde16a,
    dnnl_gOIhw16i16o = dnnl_aBCde16c16b,
    dnnl_gOIhw16o16i = dnnl_aBCde16b16c,
    dnnl_gOihw16o = dnnl_aBcde16b,
    dnnl_gOIhw2i8o4i = dnnl_aBCde2c8b4c,
    dnnl_gOIhw4i16o4i = dnnl_aBCde4c16b4c,
    dnnl_gOIhw16i16o4i = dnnl_aBCde16c16b4c,
    dnnl_gOIhw16i16o2i = dnnl_aBCde16c16b2c,
    dnnl_gOIhw16o16i2o = dnnl_aBCde16b16c2b,
    dnnl_gOIhw4i4o = dnnl_aBCde4c4b,
    dnnl_gOIhw4o4i = dnnl_aBCde4b4c,
    dnnl_gOihw4o = dnnl_aBcde4b,
    dnnl_Goihw8g = dnnl_Abcde8a,
    dnnl_Goihw4g = dnnl_Abcde4a,
    dnnl_gOIhw8i16o2i = dnnl_aBCde8c16b2c,
    dnnl_gOIhw8i8o = dnnl_aBCde8c8b,
    dnnl_gOIhw8o16i2o = dnnl_aBCde8b16c2b,
    dnnl_gIOhw8o16i2o = dnnl_aCBde8b16c2b,
    dnnl_gOIhw8o8i = dnnl_aBCde8b8c,
    dnnl_gOIhw8o4i = dnnl_aBCde8b4c,
    dnnl_Goihw32g = dnnl_Abcde32a,
    dnnl_gOwhi16o = dnnl_aBedc16b,
    dnnl_goIhw4i = dnnl_abCde4c,
    dnnl_goIhw32i = dnnl_abCde32c,

    dnnl_OIw4o8i8o4i = dnnl_ABc4a8b8a4b,
    dnnl_OIhw4o8i8o4i = dnnl_ABcd4a8b8a4b,
    dnnl_IOw4i8o8i4o = dnnl_BAc4b8a8b4a,
    dnnl_IOhw4i8o8i4o = dnnl_BAcd4b8a8b4a,
    dnnl_IOdhw4i8o8i4o = dnnl_BAcde4b8a8b4a,

    dnnl_OIhw2o8i8o2i = dnnl_ABcd2a8b8a2b,
    dnnl_gOIw4o8i8o4i = dnnl_aBCd4b8c8b4c,
    dnnl_gOIhw4o8i8o4i = dnnl_aBCde4b8c8b4c,
    dnnl_gOIdhw4o8i8o4i = dnnl_aBCdef4b8c8b4c,
    dnnl_gIOw4i8o8i4o = dnnl_aCBd4c8b8c4b,
    dnnl_gIOhw4i8o8i4o = dnnl_aCBde4c8b8c4b,
    dnnl_gIOdhw4i8o8i4o = dnnl_aCBdef4c8b8c4b,
    dnnl_gOIhw2o8i8o2i = dnnl_aBCde2b8c8b2c,
    dnnl_gOIhw2i4o2i = dnnl_aBCde2c4b2c,
    dnnl_gOIhw2o4i2o = dnnl_aBCde2b4c2b,
    dnnl_gOIhw4i8o2i = dnnl_aBCde4c8b2c,
    dnnl_gOIhw4o8i2o = dnnl_aBCde4b8c2b,

    // weights w/ groups, 6D
    dnnl_gIOdhw16i16o = dnnl_aCBdef16c16b,
    dnnl_gIOdhw16o16i = dnnl_aCBdef16b16c,
    dnnl_gOdhwi16o = dnnl_aBdefc16b,
    dnnl_gOdhwI16o2i = dnnl_aBdefC16b2c,
    dnnl_gOdhwI16o4i = dnnl_aBdefC16b4c,
    dnnl_gIdhwo8i = dnnl_aCdefb8c,
    dnnl_gIdhwO8i2o = dnnl_aCdefB8c2b,
    dnnl_gIdhwO8i4o = dnnl_aCdefB8c4b,
    dnnl_gIdhwo16i = dnnl_aCdefb16c,
    dnnl_gIdhwO16i2o = dnnl_aCdefB16c2b,
    dnnl_gIdhwO16i4o = dnnl_aCdefB16c4b,
    dnnl_gIdhwo24i = dnnl_aCdefb24c,
    dnnl_gIdhwO24i2o = dnnl_aCdefB24c2b,
    dnnl_gIdhwO24i4o = dnnl_aCdefB24c4b,
    dnnl_gOdhwi4o = dnnl_aBdefc4b,
    dnnl_gOdhwi8o = dnnl_aBdefc8b,
    dnnl_gOdhwI8o2i = dnnl_aBdefC8b2c,
    dnnl_gOdhwI8o4i = dnnl_aBdefC8b4c,
    dnnl_gOdwhi16o = dnnl_aBdfec16b,
    dnnl_gOIdhw16i16o = dnnl_aBCdef16c16b,
    dnnl_gOIdhw4i16o4i = dnnl_aBCdef4c16b4c,
    dnnl_gOIdhw16i16o4i = dnnl_aBCdef16c16b4c,
    dnnl_gOIdhw2i8o4i = dnnl_aBCdef2c8b4c,
    dnnl_gOIdhw16i16o2i = dnnl_aBCdef16c16b2c,
    dnnl_gOIdhw16o16i = dnnl_aBCdef16b16c,
    dnnl_gOIdhw16o16i2o = dnnl_aBCdef16b16c2b,
    dnnl_gOidhw16o = dnnl_aBcdef16b,
    dnnl_gOIdhw4i4o = dnnl_aBCdef4c4b,
    dnnl_gOIdhw4o4i = dnnl_aBCdef4b4c,
    dnnl_gOidhw4o = dnnl_aBcdef4b,
    dnnl_gOIdhw8i16o2i = dnnl_aBCdef8c16b2c,
    dnnl_gOIdhw8i8o = dnnl_aBCdef8c8b,
    dnnl_gOIdhw8o16i2o = dnnl_aBCdef8b16c2b,
    dnnl_gIOdhw8o16i2o = dnnl_aCBdef8b16c2b,
    dnnl_gOIdhw8o8i = dnnl_aBCdef8b8c,
    dnnl_gOIdhw8o4i = dnnl_aBCdef8b4c,
    dnnl_Goidhw16g = dnnl_Abcdef16a,
    dnnl_Goidhw32g = dnnl_Abcdef32a,
    dnnl_gOIdhw2i4o2i = dnnl_aBCdef2c4b2c,
    dnnl_gOIdhw4i8o2i = dnnl_aBCdef4c8b2c,
    dnnl_gOIdhw2o4i2o = dnnl_aBCdef2b4c2b,
    dnnl_gOIdhw4o8i2o = dnnl_aBCdef4b8c2b,
    dnnl_goIdhw4i = dnnl_abCdef4c,
    dnnl_goIdhw32i = dnnl_abCdef32c,

    // weights, 3D
    dnnl_Owi24o = dnnl_Acb24a,
    dnnl_OwI24o2i = dnnl_AcB24a2b,
    dnnl_OwI24o4i = dnnl_AcB24a4b,
    dnnl_Owi32o = dnnl_Acb32a,
    dnnl_OwI32o2i = dnnl_AcB32a2b,
    dnnl_OwI32o4i = dnnl_AcB32a4b,
    dnnl_Owi48o = dnnl_Acb48a,
    dnnl_OwI48o2i = dnnl_AcB48a2b,
    dnnl_OwI48o4i = dnnl_AcB48a4b,
    dnnl_Owi64o = dnnl_Acb64a,
    dnnl_OwI64o2i = dnnl_AcB64a2b,
    dnnl_OwI64o4i = dnnl_AcB64a4b,
    dnnl_Iwo32i = dnnl_Bca32b,
    dnnl_IwO32i2o = dnnl_BcA32b2a,
    dnnl_IwO32i4o = dnnl_BcA32b4a,
    dnnl_Iwo48i = dnnl_Bca48b,
    dnnl_IwO48i2o = dnnl_BcA48b2a,
    dnnl_IwO48i4o = dnnl_BcA48b4a,
    dnnl_Iwo64i = dnnl_Bca64b,
    dnnl_IwO64i2o = dnnl_BcA64b2a,
    dnnl_IwO64i4o = dnnl_BcA64b4a,
    dnnl_wIo2i = dnnl_cBa2b,
    dnnl_wIo4i = dnnl_cBa4b,
    dnnl_gOwi24o = dnnl_aBdc24b,
    dnnl_gOwI24o2i = dnnl_aBdC24b2c,
    dnnl_gOwI24o4i = dnnl_aBdC24b4c,
    dnnl_gOwi32o = dnnl_aBdc32b,
    dnnl_gOwI32o2i = dnnl_aBdC32b2c,
    dnnl_gOwI32o4i = dnnl_aBdC32b4c,
    dnnl_gOwi48o = dnnl_aBdc48b,
    dnnl_gOwI48o2i = dnnl_aBdC48b2c,
    dnnl_gOwI48o4i = dnnl_aBdC48b4c,
    dnnl_gOwi64o = dnnl_aBdc64b,
    dnnl_gOwI64o2i = dnnl_aBdC64b2c,
    dnnl_gOwI64o4i = dnnl_aBdC64b4c,
    dnnl_gIwo32i = dnnl_aCdb32c,
    dnnl_gIwO32i2o = dnnl_aCdB32c2b,
    dnnl_gIwO32i4o = dnnl_aCdB32c4b,
    dnnl_gIwo48i = dnnl_aCdb48c,
    dnnl_gIwO48i2o = dnnl_aCdB48c2b,
    dnnl_gIwO48i4o = dnnl_aCdB48c4b,
    dnnl_gIwo64i = dnnl_aCdb64c,
    dnnl_gIwO64i2o = dnnl_aCdB64c2b,
    dnnl_gIwO64i4o = dnnl_aCdB64c4b,
    dnnl_gwio = dnnl_adcb,
    dnnl_gwIo2i = dnnl_adCb2c,
    dnnl_gwIo4i = dnnl_adCb4c,
    // weights, 4D
    dnnl_OhwI24o = dnnl_Acdb24a,
    dnnl_OhwI24o2i = dnnl_AcdB24a2b,
    dnnl_OhwI24o4i = dnnl_AcdB24a4b,
    dnnl_OhwI32o = dnnl_Acdb32a,
    dnnl_OhwI32o2i = dnnl_AcdB32a2b,
    dnnl_OhwI32o4i = dnnl_AcdB32a4b,
    dnnl_Ohwi48o = dnnl_Acdb48a,
    dnnl_OhwI48o2i = dnnl_AcdB48a2b,
    dnnl_OhwI48o4i = dnnl_AcdB48a4b,
    dnnl_Ohwi64o = dnnl_Acdb64a,
    dnnl_OhwI64o2i = dnnl_AcdB64a2b,
    dnnl_OhwI64o4i = dnnl_AcdB64a4b,
    dnnl_Ihwo32i = dnnl_Bcda32b,
    dnnl_IhwO32i2o = dnnl_BcdA32b2a,
    dnnl_IhwO32i4o = dnnl_BcdA32b4a,
    dnnl_Ihwo48i = dnnl_Bcda48b,
    dnnl_IhwO48i2o = dnnl_BcdA48b2a,
    dnnl_IhwO48i4o = dnnl_BcdA48b4a,
    dnnl_Ihwo64i = dnnl_Bcda64b,
    dnnl_IhwO64i2o = dnnl_BcdA64b2a,
    dnnl_IhwO64i4o = dnnl_BcdA64b4a,
    dnnl_hwIo2i = dnnl_cdBa2b,
    dnnl_hwIo4i = dnnl_cdBa4b,
    dnnl_gOhwI24o = dnnl_aBdec24b,
    dnnl_gOhwI32o = dnnl_aBdec32b,
    dnnl_gOhwI32o2i = dnnl_aBdeC32b2c,
    dnnl_gOhwI32o4i = dnnl_aBdeC32b4c,
    dnnl_gOhwi48o = dnnl_aBdec48b,
    dnnl_gOhwI48o2i = dnnl_aBdeC48b2c,
    dnnl_gOhwI48o4i = dnnl_aBdeC48b4c,
    dnnl_gOhwi64o = dnnl_aBdec64b,
    dnnl_gOhwI64o2i = dnnl_aBdeC64b2c,
    dnnl_gOhwI64o4i = dnnl_aBdeC64b4c,
    dnnl_gIhwo32i = dnnl_aCdeb32c,
    dnnl_gIhwO32i2o = dnnl_aCdeB32c2b,
    dnnl_gIhwO32i4o = dnnl_aCdeB32c4b,
    dnnl_gIhwo48i = dnnl_aCdeb48c,
    dnnl_gIhwO48i2o = dnnl_aCdeB48c2b,
    dnnl_gIhwO48i4o = dnnl_aCdeB48c4b,
    dnnl_gIhwo64i = dnnl_aCdeb64c,
    dnnl_gIhwO64i2o = dnnl_aCdeB64c2b,
    dnnl_gIhwO64i4o = dnnl_aCdeB64c4b,
    dnnl_ghwio = dnnl_adecb,
    dnnl_ghwIo2i = dnnl_adeCb2c,
    dnnl_ghwIo4i = dnnl_adeCb4c,
    // weights, 5D
    dnnl_Odhwi24o = dnnl_Acdeb24a,
    dnnl_OdhwI24o2i = dnnl_AcdeB24a2b,
    dnnl_OdhwI24o4i = dnnl_AcdeB24a4b,
    dnnl_Odhwi32o = dnnl_Acdeb32a,
    dnnl_OdhwI32o2i = dnnl_AcdeB32a2b,
    dnnl_OdhwI32o4i = dnnl_AcdeB32a4b,
    dnnl_Odhwi48o = dnnl_Acdeb48a,
    dnnl_OdhwI48o2i = dnnl_AcdeB48a2b,
    dnnl_OdhwI48o4i = dnnl_AcdeB48a4b,
    dnnl_Odhwi64o = dnnl_Acdeb64a,
    dnnl_OdhwI64o2i = dnnl_AcdeB64a2b,
    dnnl_OdhwI64o4i = dnnl_AcdeB64a4b,
    dnnl_Idhwo32i = dnnl_Bcdea32b,
    dnnl_IdhwO32i2o = dnnl_BcdeA32b2a,
    dnnl_IdhwO32i4o = dnnl_BcdeA32b4a,
    dnnl_Idhwo48i = dnnl_Bcdea48b,
    dnnl_IdhwO48i2o = dnnl_BcdeA48b2a,
    dnnl_IdhwO48i4o = dnnl_BcdeA48b4a,
    dnnl_Idhwo64i = dnnl_Bcdea64b,
    dnnl_IdhwO64i2o = dnnl_BcdeA64b2a,
    dnnl_IdhwO64i4o = dnnl_BcdeA64b4a,
    dnnl_dhwIo2i = dnnl_cdeBa2b,
    dnnl_dhwIo4i = dnnl_cdeBa4b,
    dnnl_gOdhwi24o = dnnl_aBdefc24b,
    dnnl_gOdhwI24o2i = dnnl_aBdefC24b2c,
    dnnl_gOdhwI24o4i = dnnl_aBdefC24b4c,
    dnnl_gOdhwi32o = dnnl_aBdefc32b,
    dnnl_gOdhwI32o2i = dnnl_aBdefC32b2c,
    dnnl_gOdhwI32o4i = dnnl_aBdefC32b4c,
    dnnl_gOdhwi48o = dnnl_aBdefc48b,
    dnnl_gOdhwI48o2i = dnnl_aBdefC48b2c,
    dnnl_gOdhwI48o4i = dnnl_aBdefC48b4c,
    dnnl_gOdhwi64o = dnnl_aBdefc64b,
    dnnl_gOdhwI64o2i = dnnl_aBdefC64b2c,
    dnnl_gOdhwI64o4i = dnnl_aBdefC64b4c,
    dnnl_gIdhwo32i = dnnl_aCdefb32c,
    dnnl_gIdhwO32i2o = dnnl_aCdefB32c2b,
    dnnl_gIdhwO32i4o = dnnl_aCdefB32c4b,
    dnnl_gIdhwo48i = dnnl_aCdefb48c,
    dnnl_gIdhwO48i2o = dnnl_aCdefB48c2b,
    dnnl_gIdhwO48i4o = dnnl_aCdefB48c4b,
    dnnl_gIdhwo64i = dnnl_aCdefb64c,
    dnnl_gIdhwO64i2o = dnnl_aCdefB64c2b,
    dnnl_gIdhwO64i4o = dnnl_aCdefB64c4b,
    dnnl_gdhwio = dnnl_adefcb,
    dnnl_gdhwIo2i = dnnl_adefCb2c,
    dnnl_gdhwIo4i = dnnl_adefCb4c,
    dnnl_OI16i32o4i = dnnl_AB16b32a4b,
    dnnl_OI16i48o4i = dnnl_AB16b48a4b,
    dnnl_OI16i64o4i = dnnl_AB16b64a4b,
    dnnl_OI16i16o2i = dnnl_AB16b16a2b,
    dnnl_OI16i32o2i = dnnl_AB16b32a2b,
    dnnl_OI16i48o2i = dnnl_AB16b48a2b,
    dnnl_OI16i64o2i = dnnl_AB16b64a2b,
    dnnl_OIw16i32o4i = dnnl_ABc16b32a4b,
    dnnl_OIw16i48o4i = dnnl_ABc16b48a4b,
    dnnl_OIw16i64o4i = dnnl_ABc16b64a4b,
    dnnl_OIw16i32o2i = dnnl_ABc16b32a2b,
    dnnl_OIw16i48o2i = dnnl_ABc16b48a2b,
    dnnl_OIw16i64o2i = dnnl_ABc16b64a2b,
    dnnl_OIhw16i32o4i = dnnl_ABcd16b32a4b,
    dnnl_OIhw16i48o4i = dnnl_ABcd16b48a4b,
    dnnl_OIhw16i64o4i = dnnl_ABcd16b64a4b,
    dnnl_OIhw16i32o2i = dnnl_ABcd16b32a2b,
    dnnl_OIhw16i48o2i = dnnl_ABcd16b48a2b,
    dnnl_OIhw16i64o2i = dnnl_ABcd16b64a2b,
    dnnl_OIdhw16i32o4i = dnnl_ABcde16b32a4b,
    dnnl_OIdhw16i48o4i = dnnl_ABcde16b48a4b,
    dnnl_OIdhw16i64o4i = dnnl_ABcde16b64a4b,
    dnnl_OIdhw16i32o2i = dnnl_ABcde16b32a2b,
    dnnl_OIdhw16i48o2i = dnnl_ABcde16b48a2b,
    dnnl_OIdhw16i64o2i = dnnl_ABcde16b64a2b,
    dnnl_OwI16i16o2i = dnnl_AcB16b16a2b,
    dnnl_OwI16i16o4i = dnnl_AcB16b16a4b,
    dnnl_OhwI16i16o2i = dnnl_AcdB16b16a2b,
    dnnl_OhwI16i16o4i = dnnl_AcdB16b16a4b,
    dnnl_OdhwI16i16o2i = dnnl_AcdeB16b16a2b,
    dnnl_OdhwI16i16o4i = dnnl_AcdeB16b16a4b,
    dnnl_IwO16o16i2o = dnnl_BcA16a16b2a,
    dnnl_IwO16o16i4o = dnnl_BcA16a16b4a,
    dnnl_IhwO16o16i2o = dnnl_BcdA16a16b2a,
    dnnl_IhwO16o16i4o = dnnl_BcdA16a16b4a,
    dnnl_IdhwO16o16i2o = dnnl_BcdeA16a16b2a,
    dnnl_IdhwO16o16i4o = dnnl_BcdeA16a16b4a,
    dnnl_gOwI16i16o2i = dnnl_aBdC16c16b2c,
    dnnl_gOwI16i16o4i = dnnl_aBdC16c16b4c,
    dnnl_gOhwI16i16o2i = dnnl_aBdeC16c16b2c,
    dnnl_gOhwI16i16o4i = dnnl_aBdeC16c16b4c,
    dnnl_gOdhwI16i16o2i = dnnl_aBdefC16c16b2c,
    dnnl_gOdhwI16i16o4i = dnnl_aBdefC16c16b4c,
    dnnl_gIwO16o16i2o = dnnl_aCdB16b16c2b,
    dnnl_gIwO16o16i4o = dnnl_aCdB16b16c4b,
    dnnl_gIhwO16o16i2o = dnnl_aCdeB16b16c2b,
    dnnl_gIhwO16o16i4o = dnnl_aCdeB16b16c4b,
    dnnl_gIdhwO16o16i2o = dnnl_aCdefB16b16c2b,
    dnnl_gIdhwO16o16i4o = dnnl_aCdefB16b16c4b,
    dnnl_OwI16i32o2i = dnnl_AcB16b32a2b,
    dnnl_OwI16i32o4i = dnnl_AcB16b32a4b,
    dnnl_OwI16i48o2i = dnnl_AcB16b48a2b,
    dnnl_OwI16i48o4i = dnnl_AcB16b48a4b,
    dnnl_OwI16i64o2i = dnnl_AcB16b64a2b,
    dnnl_OwI16i64o4i = dnnl_AcB16b64a4b,
    dnnl_IwO16o32i2o = dnnl_BcA16a32b2a,
    dnnl_IwO16o32i4o = dnnl_BcA16a32b4a,
    dnnl_IwO16o48i2o = dnnl_BcA16a48b2a,
    dnnl_IwO16o48i4o = dnnl_BcA16a48b4a,
    dnnl_IwO16o64i2o = dnnl_BcA16a64b2a,
    dnnl_IwO16o64i4o = dnnl_BcA16a64b4a,
    dnnl_gOwI16i32o2i = dnnl_aBdC16c32b2c,
    dnnl_gOwI16i32o4i = dnnl_aBdC16c32b4c,
    dnnl_gOwI16i48o2i = dnnl_aBdC16c48b2c,
    dnnl_gOwI16i48o4i = dnnl_aBdC16c48b4c,
    dnnl_gOwI16i64o2i = dnnl_aBdC16c64b2c,
    dnnl_gOwI16i64o4i = dnnl_aBdC16c64b4c,
    dnnl_gIwO16o32i2o = dnnl_aCdB16b32c2b,
    dnnl_gIwO16o32i4o = dnnl_aCdB16b32c4b,
    dnnl_gIwO16o48i2o = dnnl_aCdB16b48c2b,
    dnnl_gIwO16o48i4o = dnnl_aCdB16b48c4b,
    dnnl_gIwO16o64i2o = dnnl_aCdB16b64c2b,
    dnnl_gIwO16o64i4o = dnnl_aCdB16b64c4b,
    dnnl_OhwI16i32o2i = dnnl_AcdB16b32a2b,
    dnnl_OhwI16i32o4i = dnnl_AcdB16b32a4b,
    dnnl_OhwI16i48o2i = dnnl_AcdB16b48a2b,
    dnnl_OhwI16i48o4i = dnnl_AcdB16b48a4b,
    dnnl_OhwI16i64o2i = dnnl_AcdB16b64a2b,
    dnnl_OhwI16i64o4i = dnnl_AcdB16b64a4b,
    dnnl_IhwO16o32i2o = dnnl_BcdA16a32b2a,
    dnnl_IhwO16o32i4o = dnnl_BcdA16a32b4a,
    dnnl_IhwO16o48i2o = dnnl_BcdA16a48b2a,
    dnnl_IhwO16o48i4o = dnnl_BcdA16a48b4a,
    dnnl_IhwO16o64i2o = dnnl_BcdA16a64b2a,
    dnnl_IhwO16o64i4o = dnnl_BcdA16a64b4a,
    dnnl_gOhwI16i32o2i = dnnl_aBdeC16c32b2c,
    dnnl_gOhwI16i32o4i = dnnl_aBdeC16c32b4c,
    dnnl_gOhwI16i48o2i = dnnl_aBdeC16c48b2c,
    dnnl_gOhwI16i48o4i = dnnl_aBdeC16c48b4c,
    dnnl_gOhwI16i64o2i = dnnl_aBdeC16c64b2c,
    dnnl_gOhwI16i64o4i = dnnl_aBdeC16c64b4c,
    dnnl_gIhwO16o32i2o = dnnl_aCdeB16b32c2b,
    dnnl_gIhwO16o32i4o = dnnl_aCdeB16b32c4b,
    dnnl_gIhwO16o48i2o = dnnl_aCdeB16b48c2b,
    dnnl_gIhwO16o48i4o = dnnl_aCdeB16b48c4b,
    dnnl_gIhwO16o64i2o = dnnl_aCdeB16b64c2b,
    dnnl_gIhwO16o64i4o = dnnl_aCdeB16b64c4b,
    dnnl_OdhwI16i32o2i = dnnl_AcdeB16b32a2b,
    dnnl_OdhwI16i32o4i = dnnl_AcdeB16b32a4b,
    dnnl_OdhwI16i48o2i = dnnl_AcdeB16b48a2b,
    dnnl_OdhwI16i48o4i = dnnl_AcdeB16b48a4b,
    dnnl_OdhwI16i64o2i = dnnl_AcdeB16b64a2b,
    dnnl_OdhwI16i64o4i = dnnl_AcdeB16b64a4b,
    dnnl_IdhwO16o32i2o = dnnl_BcdeA16a32b2a,
    dnnl_IdhwO16o32i4o = dnnl_BcdeA16a32b4a,
    dnnl_IdhwO16o48i2o = dnnl_BcdeA16a48b2a,
    dnnl_IdhwO16o48i4o = dnnl_BcdeA16a48b4a,
    dnnl_IdhwO16o64i2o = dnnl_BcdeA16a64b2a,
    dnnl_IdhwO16o64i4o = dnnl_BcdeA16a64b4a,
    dnnl_gOdhwI16i32o2i = dnnl_aBdefC16c32b2c,
    dnnl_gOdhwI16i32o4i = dnnl_aBdefC16c32b4c,
    dnnl_gOdhwI16i48o2i = dnnl_aBdefC16c48b2c,
    dnnl_gOdhwI16i48o4i = dnnl_aBdefC16c48b4c,
    dnnl_gOdhwI16i64o2i = dnnl_aBdefC16c64b2c,
    dnnl_gOdhwI16i64o4i = dnnl_aBdefC16c64b4c,
    dnnl_gIdhwO16o32i2o = dnnl_aCdefB16b32c2b,
    dnnl_gIdhwO16o32i4o = dnnl_aCdefB16b32c4b,
    dnnl_gIdhwO16o48i2o = dnnl_aCdefB16b48c2b,
    dnnl_gIdhwO16o48i4o = dnnl_aCdefB16b48c4b,
    dnnl_gIdhwO16o64i2o = dnnl_aCdefB16b64c2b,
    dnnl_gIdhwO16o64i4o = dnnl_aCdefB16b64c4b,
    dnnl_hwioG16g = dnnl_decbA16a,
    dnnl_hwioG8g = dnnl_decbA8a,
    dnnl_dhwioG16g = dnnl_defcbA16a,
    dnnl_dhwioG8g = dnnl_defcbA8a,
    dnnl_NCdhw40n16c = dnnl_ABcde40a16b,
    dnnl_NCw40n16c = dnnl_ABc40a16b,
    dnnl_NChw40n16c = dnnl_ABcd40a16b,
    dnnl_NCw40n32c = dnnl_ABc40a32b,
    dnnl_NChw40n32c = dnnl_ABcd40a32b,
    dnnl_NCdhw40n32c = dnnl_ABcde40a32b,
    dnnl_OIdhw4o8i8o2i = dnnl_ABcde4a8b8a2b,
    dnnl_OIhw4o8i8o2i = dnnl_ABcd4a8b8a2b,
    dnnl_OIw4o8i8o2i = dnnl_ABc4a8b8a2b,
    dnnl_gOIdhw4o8i8o2i = dnnl_aBCdef4b8c8b2c,
    dnnl_gOIhw4o8i8o2i = dnnl_aBCde4b8c8b2c,
    dnnl_gOIw4o8i8o2i = dnnl_aBCd4b8c8b2c,
    dnnl_IOdhw4i8o8i2o = dnnl_BAcde4b8a8b2a,
    dnnl_IOhw4i8o8i2o = dnnl_BAcd4b8a8b2a,
    dnnl_IOw4i8o8i2o = dnnl_BAc4b8a8b2a,
    dnnl_gIOdhw4i8o8i2o = dnnl_aCBdef4c8b8c2b,
    dnnl_gIOhw4i8o8i2o = dnnl_aCBde4c8b8c2b,
    dnnl_gIOw4i8o8i2o = dnnl_aCBd4c8b8c2b,
    dnnl_NCw2c32n8c = dnnl_ABc2b32a8b,
    dnnl_NChw2c32n8c = dnnl_ABcd2b32a8b,
    dnnl_NCdhw2c32n8c = dnnl_ABcde2b32a8b,
    dnnl_OIw2i8o16i4o = dnnl_ABc2b8a16b4a,
    dnnl_OIhw2i8o16i4o = dnnl_ABcd2b8a16b4a,
    dnnl_OIdhw2i8o16i4o = dnnl_ABcde2b8a16b4a,
    dnnl_OIw2o8i16o4i = dnnl_ABc2a8b16a4b,
    dnnl_OIw2o8i16o2i = dnnl_ABc2a8b16a2b,
    dnnl_IOw2i8o16i4o = dnnl_BAc2b8a16b4a,
    dnnl_IOw2i8o16i2o = dnnl_BAc2b8a16b2a,
    dnnl_OIhw2o8i16o4i = dnnl_ABcd2a8b16a4b,
    dnnl_OIhw2o8i16o2i = dnnl_ABcd2a8b16a2b,
    dnnl_IOhw2i8o16i4o = dnnl_BAcd2b8a16b4a,
    dnnl_IOhw2i8o16i2o = dnnl_BAcd2b8a16b2a,
    dnnl_OIdhw2o8i16o4i = dnnl_ABcde2a8b16a4b,
    dnnl_OIdhw2o8i16o2i = dnnl_ABcde2a8b16a2b,
    dnnl_IOdhw2i8o16i4o = dnnl_BAcde2b8a16b4a,
    dnnl_IOdhw2i8o16i2o = dnnl_BAcde2b8a16b2a,
    dnnl_gOIw2o8i16o2i = dnnl_aBCd2b8c16b2c,
    dnnl_gIOw2i8o16i2o = dnnl_aCBd2c8b16c2b,
    dnnl_gIOhw2i8o16i2o = dnnl_aBCde2c8b16c2b,
    dnnl_gIOdhw2i8o16i2o = dnnl_aBCdef2c8b16c2b,
    dnnl_gOIhw2o8i16o2i = dnnl_aBCde2b8c16b2c,
    dnnl_gOIdhw2o8i16o2i = dnnl_aBCdef2b8c16b2c,
    dnnl_gOIw2o8i16o4i = dnnl_aBCd2b8c16b4c,
    dnnl_gOIhw2o8i16o4i = dnnl_aBCde2b8c16b4c;

/** \} dnnl_api_memory
 <p>
 *  \addtogroup dnnl_api_primitives
 *  \{
 *  \addtogroup dnnl_api_primitives_common
 *  \{
 <p>
 *  Kinds of propagation. */
/** enum dnnl_prop_kind_t */
public static final int
    // TODO: suggest renames
    /** Undefined propagation type. */
    dnnl_prop_kind_undef = 0,
    /** Forward data propagation (training mode). In this mode primitives
     *  perform computations necessary for subsequent backward propagation. */
    dnnl_forward_training = 64,
    /** Forward data propagation (inference mode). In this mode primitives
     *  perform only computations that are necessary for inference and omit
     *  computations that are necessary only for backward propagation. */
    dnnl_forward_inference = 96,
    /** Forward data propagation (alias for \c dnnl_forward_training). */
    dnnl_forward = dnnl_forward_training,
    /** Backward propagation (with respect to all parameters). */
    dnnl_backward = 128,
    /** Backward data propagation. */
    dnnl_backward_data = 160,
    /** Backward weights propagation. */
    dnnl_backward_weights = 192,
    /** Backward bias propagation. */
    dnnl_backward_bias = 193;

/** Kinds of primitives. Used to implement a way to extend the library with new
 *  primitives without changing the ABI. */
/** enum dnnl_primitive_kind_t */
public static final int
    /** Undefined primitive */
    dnnl_undefined_primitive = 0,
    /** A reorder primitive. */
    dnnl_reorder = 1,
    /** A shuffle primitive. */
    dnnl_shuffle = 2,
    /** A (out-of-place) concat primitive. */
    dnnl_concat = 3,
    /** A sum primitive. */
    dnnl_sum = 4,
    /** A convolution primitive. */
    dnnl_convolution = 5,
    /** A deconvolution primitive. */
    dnnl_deconvolution = 6,
    /** An element-wise primitive. */
    dnnl_eltwise = 7,
    /** An LRN primitive. */
    dnnl_lrn = 8,
    /** A batch normalization primitive. */
    dnnl_batch_normalization = 9,
    /** An inner product primitive. */
    dnnl_inner_product = 10,
    /** A rnn primitive. */
    dnnl_rnn = 11,
    /** A matrix multiplication primitive (internal). */
    dnnl_gemm = 12,
    /** A binary primitive. */
    dnnl_binary = 13,
    /** A matrix multiplication primitive. */
    dnnl_matmul = 14,
    /** A resampling primitive. */
    dnnl_resampling = 15,
    /** A pooling primitive. */
    dnnl_pooling = 16,
    /** A reduction primitive. */
    dnnl_reduction = 17,
    /** A PReLU primitive. */
    dnnl_prelu = 18,
    /** A softmax primitive. */
    dnnl_softmax = 19,
    /** A layer normalization primitive. */
    dnnl_layer_normalization = 20,
    /** A group normalization primitive. */
    dnnl_group_normalization = 21,

    /** Parameter to allow internal only primitives without undefined behavior.
     *  This parameter is chosen to be valid for so long as sizeof(int) >= 2. */
    dnnl_primitive_kind_max = 0x7fff;

/** Kinds of algorithms. */
/** enum dnnl_alg_kind_t */
public static final int
    dnnl_alg_kind_undef = 0,
    /** Direct convolution */
    dnnl_convolution_direct = 0x1,
    /** Winograd convolution */
    dnnl_convolution_winograd = 0x2,
    /** Convolution algorithm(either direct or Winograd) is chosen just in time */
    dnnl_convolution_auto = 0x3,
    /** Direct deconvolution */
    dnnl_deconvolution_direct = 0xa,
    /** Winograd deconvolution */
    dnnl_deconvolution_winograd = 0xb,
    /** Eltwise: ReLU */
    dnnl_eltwise_relu = 0x20,
    /** Eltwise: hyperbolic tangent non-linearity (tanh) */
    dnnl_eltwise_tanh = 0x20 + 1,
    /** Eltwise: exponential linear unit (elu) */
    dnnl_eltwise_elu = 0x20 + 2,
    /** Eltwise: square */
    dnnl_eltwise_square = 0x20 + 3,
    /** Eltwise: abs */
    dnnl_eltwise_abs = 0x20 + 4,
    /** Eltwise: square root */
    dnnl_eltwise_sqrt = 0x20 + 5,
    /** Eltwise: linear */
    dnnl_eltwise_linear = 0x20 + 6,
    /** Eltwise: soft_relu */
    dnnl_eltwise_soft_relu = 0x20 + 7,
    /** Eltwise: hardsigmoid */
    dnnl_eltwise_hardsigmoid = 0x20 + 8,
    /** Eltwise: logistic */
    dnnl_eltwise_logistic = 0x20 + 9,
    /** Eltwise: exponent */
    
///
    dnnl_eltwise_exp = 0x20 + 10,
    /** Eltwise: gelu
     * 
     *  \note Tanh approximation formula is used to approximate
     *  the cumulative distribution function of a Gaussian here */
    dnnl_eltwise_gelu_tanh = 0x20 + 11,
    /** Eltwise: swish */
    dnnl_eltwise_swish = 0x20 + 12,
    /** Eltwise: natural logarithm */
    dnnl_eltwise_log = 0x20 + 13,
    /** Eltwise: clip */
    dnnl_eltwise_clip = 0x20 + 14,
    /** Eltwise: clip version 2 */
    dnnl_eltwise_clip_v2 = 0x20 + 15,
    /** Eltwise: pow */
    dnnl_eltwise_pow = 0x20 + 16,
    /** Eltwise: erf-based gelu */
    dnnl_eltwise_gelu_erf = 0x20 + 17,
    /** Eltwise: round */
    dnnl_eltwise_round = 0x20 + 18,
    /** Eltwise: mish */
    dnnl_eltwise_mish = 0x20 + 19,
    /** Eltwise: hardswish */
    dnnl_eltwise_hardswish = 0x20 + 20,
    /** Eltwise: ReLU (dst for backward) */
    dnnl_eltwise_relu_use_dst_for_bwd = 0x100,
    /** Eltwise: hyperbolic tangent non-linearity (tanh) (dst for backward) */
    dnnl_eltwise_tanh_use_dst_for_bwd = 0x100 + 1,
    /** Eltwise: exponential linear unit (elu) (dst for backward) */
    dnnl_eltwise_elu_use_dst_for_bwd = 0x100 + 2,
    /** Eltwise: square root (dst for backward) */
    dnnl_eltwise_sqrt_use_dst_for_bwd = 0x100 + 3,
    /** Eltwise: logistic (dst for backward) */
    dnnl_eltwise_logistic_use_dst_for_bwd = 0x100 + 4,
    /** Eltwise: exp (dst for backward) */
    dnnl_eltwise_exp_use_dst_for_bwd = 0x100 + 5,
    /** Eltwise: clip version 2 (dst for backward) */
    dnnl_eltwise_clip_v2_use_dst_for_bwd = 0x100 + 6,
    /** Max pooling */
    dnnl_pooling_max = 0x1ff,
    /** Average pooling include padding */
    dnnl_pooling_avg_include_padding = 0x2ff,
    /** Average pooling exclude padding */
    dnnl_pooling_avg_exclude_padding = 0x3ff,
    /** Local response normalization (LRN) across multiple channels */
    dnnl_lrn_across_channels = 0xaff,
    /** LRN within a single channel */
    dnnl_lrn_within_channel = 0xbff,
    /** RNN cell */
    dnnl_vanilla_rnn = 0x1fff,
    /** LSTM cell */
    dnnl_vanilla_lstm = 0x2fff,
    /** GRU cell */
    
///
    dnnl_vanilla_gru = 0x3fff,
    /** GRU cell with linear before reset
     * 
     *  Modification of original GRU cell. Differs from #dnnl_vanilla_gru
     *  in how the new memory gate is calculated:
     *  <pre>{@code \[ c_t = tanh(W_c*x_t + b_{c_x} + r_t*(U_c*h_{t-1}+b_{c_h})) \]}</pre>
     *  Primitive expects 4 biases on input:
     *  {@code [b_{u}, b_{r}, b_{c_x}, b_{c_h}]} */
    dnnl_lbr_gru = 0x4fff,
    /** AUGRU cell */
    dnnl_vanilla_augru = 0x5fff,
    /** AUGRU cell with linear before reset */
    dnnl_lbr_augru = 0x6fff,
    /** Binary add */
    dnnl_binary_add = 0x1fff0,
    /** Binary mul */
    dnnl_binary_mul = 0x1fff1,
    /** Binary max */
    dnnl_binary_max = 0x1fff2,
    /** Binary min */
    dnnl_binary_min = 0x1fff3,
    /** Binary div */
    dnnl_binary_div = 0x1fff4,
    /** Binary sub */
    dnnl_binary_sub = 0x1fff5,
    /** Binary greater or equal */
    dnnl_binary_ge = 0x1fff6,
    /** Binary greater than */
    dnnl_binary_gt = 0x1fff7,
    /** Binary less or equal */
    dnnl_binary_le = 0x1fff8,
    /** Binary less than */
    dnnl_binary_lt = 0x1fff9,
    /** Binary equal */
    dnnl_binary_eq = 0x1fffa,
    /** Binary not equal */
    dnnl_binary_ne = 0x1fffb,
    /** Nearest Neighbor Resampling Method */
    dnnl_resampling_nearest = 0x2fff0,
    /** Linear Resampling Method */
    dnnl_resampling_linear = 0x2fff1,
    /** Reduction using max */
    dnnl_reduction_max = 0x2fff1 + 1,
    /** Reduction using min */
    dnnl_reduction_min = 0x2fff1 + 2,
    /** Reduction using sum */
    dnnl_reduction_sum = 0x2fff1 + 3,
    /** Reduction using mul */
    dnnl_reduction_mul = 0x2fff1 + 4,
    /** Reduction using mean */
    dnnl_reduction_mean = 0x2fff1 + 5,
    /** Reduction using lp norm */
    dnnl_reduction_norm_lp_max = 0x2fff1 + 6,
    /** Reduction using lp norm */
    dnnl_reduction_norm_lp_sum = 0x2fff1 + 7,
    /** Reduction using lp norm without final pth-root */
    dnnl_reduction_norm_lp_power_p_max = 0x2fff1 + 8,
    /** Reduction using lp norm without final pth-root */
    dnnl_reduction_norm_lp_power_p_sum = 0x2fff1 + 9,
    /** Softmax */
    dnnl_softmax_accurate = 0x30000,
    /** Logsoftmax */
    dnnl_softmax_log = 0x30000 + 1;

/** Flags for normalization primitives. */
/** enum dnnl_normalization_flags_t */
public static final int
    /** Use no normalization flags
     * 
     *  If specified
     *   - on forward training propagation mean and variance are computed and
     *     stored as output
     *   - on backward propagation compute full derivative wrt data
     *   - on backward propagation prop_kind == #dnnl_backward_data has the same
     *     behavior as prop_kind == #dnnl_backward */
    
///
///
    dnnl_normalization_flags_none = 0x0,

    /** Use global statistics
     * 
     *  If specified
     *   - on forward propagation use mean and variance provided by user (input)
     *   - on backward propagation reduces the amount of computations, since
     *     mean and variance are considered as constants
     * 
     *   If not specified:
     *    - on forward propagation mean and variance are computed and stored as
     *      output
     *    - on backward propagation compute full derivative wrt data */
    
///
    dnnl_use_global_stats = 0x1,

    /** Use scale parameter
     * 
     *  If specified:
     *   - on forward propagation use scale for the normalization results
     *   - on backward propagation (for prop_kind == #dnnl_backward) compute
     *     diff wrt scale (hence one extra output used) */
    
///
    dnnl_use_scale = 0x2,

    /** Use shift parameter
     * 
     *  If specified:
     *   - on forward propagation use shift (aka bias) for the normalization
     *     results
     *   - on backward propagation (for prop_kind == #dnnl_backward) compute
     *     diff wrt shift (hence one extra output used) */
    
///
///
    dnnl_use_shift = 0x4,

    /** Fuse with ReLU
     * 
     *  The flag implies negative slope being 0. On training this is the only
     *  configuration supported. For inference, to use non-zero negative slope
     *  consider using \ref dev_guide_attributes_post_ops.
     * 
     *  If specified:
     *   - on inference this option behaves the same as if the primitive were
     *     fused with ReLU using post ops API with zero negative slope.
     *   - on training primitive requires workspace (required to be able to
     *     perform backward pass) */
    
///
///
    dnnl_fuse_norm_relu = 0x8,

    /** Fuse with Add and then fuse with ReLU
     * 
     *  If specified:
     * 
     *   - on forward propagation apply element-wise binary Add operation to
     *     to the normalization results with an additional input tensor and then
     *     apply ReLU with negative slope being 0.
     *   - on training primitive requires workspace (required to be able to
     *     perform backward pass).
     *   - on backward propagation save the result of backward ReLU operation
     *     with input tensor and workspace from forward pass to extra output
     *     tensor and then perform backward normalization. */
    dnnl_fuse_norm_add_relu = 0x10;

/** \} dnnl_api_primitives_common
 *  \} dnnl_api_primitives
 <p>
 *  \addtogroup dnnl_api_memory
 *  \{
 <p>
 *  A wildcard value for dimensions that are unknown at a primitive creation
 *  time. */
public static native @MemberGetter long DNNL_RUNTIME_DIM_VAL();
public static final long DNNL_RUNTIME_DIM_VAL = DNNL_RUNTIME_DIM_VAL();

/** A {@code size_t} counterpart of the DNNL_RUNTIME_DIM_VAL.
 *  For instance, this value is returned by dnnl_memory_desc_get_size() if
 *  either of the dimensions or strides equal to #DNNL_RUNTIME_DIM_VAL. */
public static native @MemberGetter long DNNL_RUNTIME_SIZE_VAL();
public static final long DNNL_RUNTIME_SIZE_VAL = DNNL_RUNTIME_SIZE_VAL();

/** \cond DO_NOT_DOCUMENT_THIS
 *  Hex representation for a **special** quiet NAN (!= NAN from math.h) */
    
    
/** \endcond
 <p>
 *  A wildcard value for floating point values that are unknown at a primitive
 *  creation time. */
public static native @MemberGetter double DNNL_RUNTIME_F32_VAL();
public static final double DNNL_RUNTIME_F32_VAL = DNNL_RUNTIME_F32_VAL();

/** \cond DO_NOT_DOCUMENT_THIS */
@MemberGetter public static native int DNNL_RUNTIME_S32_VAL_REP();
public static final int DNNL_RUNTIME_S32_VAL_REP = DNNL_RUNTIME_S32_VAL_REP();
/** \endcond
 <p>
 *  A wildcard value for int32_t values that are unknown at a primitive creation
 *  time. */
public static final int DNNL_RUNTIME_S32_VAL = DNNL_RUNTIME_S32_VAL_REP;
// Targeting ../dnnl_memory_desc.java



/** A memory descriptor handle. */

/** A memory descriptor handle. */
// Targeting ../dnnl_memory.java



/** A memory handle. */

/** A constant memory handle. */

/** Special pointer value that indicates that a memory object should not have
 *  an underlying buffer. */
// #define DNNL_MEMORY_NONE (NULL)

/** Special pointer value that indicates that the library needs to allocate an
 *  underlying buffer for a memory object. */
// #define DNNL_MEMORY_ALLOCATE ((void *)(size_t)-1)

/** \} dnnl_api_memory
 <p>
 *  \addtogroup dnnl_api_primitives
 *  \{
 <p>
 *  \addtogroup dnnl_api_rnn
 *  \{
 <p>
 *  Flags for RNN cell. */
/** enum dnnl_rnn_flags_t */
public static final int
    /** Undefined RNN flags */
    dnnl_rnn_flags_undef = 0x0,
    /** Do not add weights gradient to existing diff_weights memory */
    dnnl_rnn_flags_diff_weights_overwrite = 0x1;

/** A direction of RNN primitive execution. */
/** enum dnnl_rnn_direction_t */
public static final int
    /** Undefined RNN direction. */
    dnnl_rnn_direction_undef = 0,
    /** Unidirectional execution of RNN primitive from left to right. */
    dnnl_unidirectional_left2right = 1,
    /** Unidirectional execution of RNN primitive from right to left. */
    dnnl_unidirectional_right2left = 2,
    /** Bidirectional execution of RNN primitive with concatenation of the
     *  results. */
    dnnl_bidirectional_concat = 3,
    /** Bidirectional execution of RNN primitive with summation of the
     *  results. */
    dnnl_bidirectional_sum = 4;
// Targeting ../dnnl_primitive_desc.java



/** \brief A primitive descriptor handle. */

/** \brief A constant primitive descriptor handle. */

/** \} dnnl_api_primitives_common
 <p>
 *  \addtogroup dnnl_api_attributes
 *  \{
 <p>
 *  Scratchpad mode */
/** enum dnnl_scratchpad_mode_t */
public static final int
    /** The library manages the scratchpad allocation according to the policy
     *  specified by the {@code DNNL_ENABLE_CONCURRENT_EXEC}
     *  [build option](\ref dev_guide_build_options) (default).
     * 
     *  When {@code DNNL_ENABLE_CONCURRENT_EXEC=OFF} (default), the library
     *  scratchpad is common to all primitives to reduce the memory footprint.
     *  This configuration comes with limited thread-safety properties, namely
     *  primitives can be created and executed in parallel but cannot migrate
     *  between threads (in other words, each primitive should be executed in
     *  the same thread it was created in).
     * 
     *  When {@code DNNL_ENABLE_CONCURRENT_EXEC=ON}, the library scratchpad is
     *  private to each primitive. The memory footprint is larger than when
     *  using {@code DNNL_ENABLE_CONCURRENT_EXEC=OFF} but different primitives can be
     *  created and run concurrently (the same primitive cannot be run
     *  concurrently from two different threads though). */
    dnnl_scratchpad_mode_library = 0,
    /** The user manages the scratchpad allocation by querying and providing
     *  the scratchpad memory to primitives. This mode is thread-safe as long
     *  as the scratchpad buffers are not used concurrently by two primitive
     *  executions. */
    dnnl_scratchpad_mode_user = 1;
// Targeting ../dnnl_primitive_attr.java



/** \brief A primitive descriptor attributes handle that controls primitive
 *  behavior. */

/** \brief A constant primitive descriptor attributes handle. */
// Targeting ../dnnl_post_ops.java



/** \brief A post operation chain handle. */

/** \brief A constant post operation chain handle. */
// Targeting ../dnnl_primitive.java


/** A primitive handle. */
/** A constant primitive handle. */

/** Source argument #0. */
public static final int DNNL_ARG_SRC_0 = 1;
/** A special mnemonic for source argument for primitives that have a
 *  single source. An alias for #DNNL_ARG_SRC_0. */
public static final int DNNL_ARG_SRC = DNNL_ARG_SRC_0;
/** A special mnemonic for RNN input vector. An alias for
 *  #DNNL_ARG_SRC_0. */
public static final int DNNL_ARG_SRC_LAYER = DNNL_ARG_SRC_0;
/** A special mnemonic for reorder source argument. An alias for
 *  #DNNL_ARG_SRC_0. */
public static final int DNNL_ARG_FROM = DNNL_ARG_SRC_0;

/** Source argument #1. */
public static final int DNNL_ARG_SRC_1 = 2;
/** A special mnemonic for RNN input recurrent hidden state vector. An alias
 *  for #DNNL_ARG_SRC_1. */
public static final int DNNL_ARG_SRC_ITER = DNNL_ARG_SRC_1;

/** Source argument #2. */
public static final int DNNL_ARG_SRC_2 = 3;
/** A special mnemonic for RNN input recurrent cell state vector. An alias for
 *  #DNNL_ARG_SRC_2. */
public static final int DNNL_ARG_SRC_ITER_C = DNNL_ARG_SRC_2;

/** Source argument #3. */
public static final int DNNL_ARG_SRC_3 = 4;
/** A special mnemonic for RNN input recurrent cell attention vector. An alias for
 *  #DNNL_ARG_SRC_3. */
public static final int DNNL_ARG_AUGRU_ATTENTION = DNNL_ARG_SRC_3;

/** Destination argument #0. */
public static final int DNNL_ARG_DST_0 = 17;
/** A special mnemonic for destination argument for primitives that have a
 *  single destination. An alias for #DNNL_ARG_DST_0. */
public static final int DNNL_ARG_DST = DNNL_ARG_DST_0;
/** A special mnemonic for reorder destination argument. An alias for
 *  #DNNL_ARG_DST_0. */
public static final int DNNL_ARG_TO = DNNL_ARG_DST_0;
/** A special mnemonic for RNN output vector. An alias for #DNNL_ARG_DST_0. */
public static final int DNNL_ARG_DST_LAYER = DNNL_ARG_DST_0;

/** Destination argument #1. */
public static final int DNNL_ARG_DST_1 = 18;
/** A special mnemonic for RNN input recurrent hidden state vector. An
 *  alias for #DNNL_ARG_DST_1. */
public static final int DNNL_ARG_DST_ITER = DNNL_ARG_DST_1;

/** Destination argument #2. */
public static final int DNNL_ARG_DST_2 = 19;
/** A special mnemonic for LSTM output recurrent cell state vector. An
 *  alias for #DNNL_ARG_DST_2. */
public static final int DNNL_ARG_DST_ITER_C = DNNL_ARG_DST_2;

/** Weights argument #0. */
public static final int DNNL_ARG_WEIGHTS_0 = 33;
/** A special mnemonic for primitives that have a single weights
 *  argument. Alias for #DNNL_ARG_WEIGHTS_0. */
public static final int DNNL_ARG_WEIGHTS = DNNL_ARG_WEIGHTS_0;
/** A special mnemonic for RNN weights applied to the layer input. An
 *  alias for #DNNL_ARG_WEIGHTS_0. */
public static final int DNNL_ARG_WEIGHTS_LAYER = DNNL_ARG_WEIGHTS_0;

/** Weights argument #1. */
public static final int DNNL_ARG_WEIGHTS_1 = 34;
/** A special mnemonic for RNN weights applied to the recurrent input.
 *  An alias for #DNNL_ARG_WEIGHTS_1. */
public static final int DNNL_ARG_WEIGHTS_ITER = DNNL_ARG_WEIGHTS_1;

/** Weights argument #2. */
public static final int DNNL_ARG_WEIGHTS_2 = 35;
/** A special mnemonic for RNN weights applied to the peephole weights.
 *  An alias for #DNNL_ARG_WEIGHTS_2. */
public static final int DNNL_ARG_WEIGHTS_PEEPHOLE = DNNL_ARG_WEIGHTS_2;

/** Weights argument #3. */
public static final int DNNL_ARG_WEIGHTS_3 = 36;
/** A special mnemonic for RNN weights applied to the projection weights.
 *  An alias for #DNNL_ARG_WEIGHTS_3. */
public static final int DNNL_ARG_WEIGHTS_PROJECTION = DNNL_ARG_WEIGHTS_3;

/** Bias tensor argument. */
public static final int DNNL_ARG_BIAS = 41;

/** Mean values tensor argument. */
public static final int DNNL_ARG_MEAN = 49;
/** Variance values tensor argument. */
public static final int DNNL_ARG_VARIANCE = 50;

/** A special mnemonic for scale argument of normalization primitives. */
public static final int DNNL_ARG_SCALE = 51;
/** A special mnemonic for shift argument of normalization primitives. */
public static final int DNNL_ARG_SHIFT = 52;

/** Workspace tensor argument. Workspace is used to pass information
 *  from forward propagation to backward propagation computations. */
public static final int DNNL_ARG_WORKSPACE = 64;
/** Scratchpad (temporary storage) tensor argument. */
public static final int DNNL_ARG_SCRATCHPAD = 80;

/** Gradient (diff) of the source argument #0. */
public static final int DNNL_ARG_DIFF_SRC_0 = 129;
/** A special mnemonic for primitives that have a single diff source argument.
 *  An alias for #DNNL_ARG_DIFF_SRC_0. */
public static final int DNNL_ARG_DIFF_SRC = DNNL_ARG_DIFF_SRC_0;
/** A special mnemonic for gradient (diff) of RNN input vector. An alias for
 *  #DNNL_ARG_DIFF_SRC_0. */
public static final int DNNL_ARG_DIFF_SRC_LAYER = DNNL_ARG_DIFF_SRC_0;

/** Gradient (diff) of the source argument #1. */
public static final int DNNL_ARG_DIFF_SRC_1 = 130;
/** A special mnemonic for gradient (diff) of RNN input recurrent hidden state
 *  vector. An alias for #DNNL_ARG_DIFF_SRC_1. */
public static final int DNNL_ARG_DIFF_SRC_ITER = DNNL_ARG_DIFF_SRC_1;

/** Gradient (diff) of the source argument #2. */
public static final int DNNL_ARG_DIFF_SRC_2 = 131;
/** A special mnemonic for gradient (diff) of RNN input recurrent cell state
 *  vector. An alias for #DNNL_ARG_DIFF_SRC_1. */
public static final int DNNL_ARG_DIFF_SRC_ITER_C = DNNL_ARG_DIFF_SRC_2;

/** Gradient (diff) of the source argument #3. */
public static final int DNNL_ARG_DIFF_SRC_3 = 132;
/** A special mnemonic for gradient (diff) of RNN input recurrent cell attention
 *  vector. An alias for #DNNL_ARG_DIFF_SRC_3. */
public static final int DNNL_ARG_DIFF_AUGRU_ATTENTION = DNNL_ARG_DIFF_SRC_3;

/** Gradient (diff) of the destination argument #0. */
public static final int DNNL_ARG_DIFF_DST_0 = 145;
/** A special mnemonic for primitives that have a single diff destination
 *  argument. An alias for #DNNL_ARG_DIFF_DST_0. */
public static final int DNNL_ARG_DIFF_DST = DNNL_ARG_DIFF_DST_0;
/** A special mnemonic for gradient (diff) of RNN output vector. An alias for
 *  #DNNL_ARG_DIFF_DST_0. */
public static final int DNNL_ARG_DIFF_DST_LAYER = DNNL_ARG_DIFF_DST_0;

/** Gradient (diff) of the destination argument #1. */
public static final int DNNL_ARG_DIFF_DST_1 = 146;
/** A special mnemonic for gradient (diff) of RNN input recurrent hidden state
 *  vector. An alias for #DNNL_ARG_DIFF_DST_1. */
public static final int DNNL_ARG_DIFF_DST_ITER = DNNL_ARG_DIFF_DST_1;

/** Gradient (diff) of the destination argument #2. */
public static final int DNNL_ARG_DIFF_DST_2 = 147;
/** A special mnemonic for gradient (diff) of RNN input recurrent cell state
 *  vector. An alias for #DNNL_ARG_DIFF_DST_2. */
public static final int DNNL_ARG_DIFF_DST_ITER_C = DNNL_ARG_DIFF_DST_2;

/** Gradient (diff) of the weights argument #0. */
public static final int DNNL_ARG_DIFF_WEIGHTS_0 = 161;
/** A special mnemonic for primitives that have a single diff weights
 *  argument. Alias for #DNNL_ARG_DIFF_WEIGHTS_0. */
public static final int DNNL_ARG_DIFF_WEIGHTS = DNNL_ARG_DIFF_WEIGHTS_0;
/** A special mnemonic for diff of RNN weights applied to the layer input. An
 *  alias for #DNNL_ARG_DIFF_WEIGHTS_0. */
public static final int DNNL_ARG_DIFF_WEIGHTS_LAYER = DNNL_ARG_DIFF_WEIGHTS_0;

/** Gradient (diff) of the weights argument #1. */
public static final int DNNL_ARG_DIFF_WEIGHTS_1 = 162;
/** A special mnemonic for diff of RNN weights applied to the recurrent input.
 *  An alias for #DNNL_ARG_DIFF_WEIGHTS_1. */
public static final int DNNL_ARG_DIFF_WEIGHTS_ITER = DNNL_ARG_DIFF_WEIGHTS_1;

/** Gradient (diff) of the weights argument #2. */
public static final int DNNL_ARG_DIFF_WEIGHTS_2 = 163;
/** A special mnemonic for diff of RNN weights applied to the peephole weights.
 *  An alias for #DNNL_ARG_DIFF_WEIGHTS_2. */
public static final int DNNL_ARG_DIFF_WEIGHTS_PEEPHOLE = DNNL_ARG_DIFF_WEIGHTS_2;

/** Gradient (diff) of the weights argument #3. */
public static final int DNNL_ARG_DIFF_WEIGHTS_3 = 164;
/** A special mnemonic for diff of RNN weights applied to the projection
 *  weights. An alias for #DNNL_ARG_DIFF_WEIGHTS_3. */
public static final int DNNL_ARG_DIFF_WEIGHTS_PROJECTION = DNNL_ARG_DIFF_WEIGHTS_3;

/** Gradient (diff) of the bias tensor argument. */
public static final int DNNL_ARG_DIFF_BIAS = 169;

/** A special mnemonic for scale argument of normalization primitives. */
public static final int DNNL_ARG_DIFF_SCALE = 255;
/** A special mnemonic for shift argument of normalization primitives. */
public static final int DNNL_ARG_DIFF_SHIFT = 256;

/** Output scaling factors provided at execution time. */
public static final int DNNL_ARG_ATTR_OUTPUT_SCALES = 513;

/** Starting index for source arguments for primitives that take a variable
 *  number of source arguments. */
public static final int DNNL_ARG_MULTIPLE_SRC = 1024;
/** Starting index for destination arguments for primitives that produce a
 *  variable number of destination arguments. */
public static final int DNNL_ARG_MULTIPLE_DST = 2048;

/** Scaling factors provided at execution time. */
public static final int DNNL_ARG_ATTR_SCALES = 4096;

/** Zero points provided at execution time. */
public static final int DNNL_ARG_ATTR_ZERO_POINTS = 8192;

/** Arguments for fused depthwise convolution.
 *  See \ref dev_guide_attributes_post_ops_depthwise_fusion */
public static final int DNNL_ARG_ATTR_POST_OP_DW = 16384;

/** Starting point for a binary post operation. */
public static final int DNNL_ARG_ATTR_MULTIPLE_POST_OP_BASE = 32768;

/** Arguments for a binary post operation. Up to 32 arguments are supported.
 *  See \ref dev_guide_attributes_post_ops_binary_fusion */
// #define DNNL_ARG_ATTR_MULTIPLE_POST_OP(idx)
//     (DNNL_ARG_ATTR_MULTIPLE_POST_OP_BASE * ((idx) + 1))
// Targeting ../dnnl_exec_arg_t.java



/** \} dnnl_api_primitives_common
 <p>
 *  \addtogroup dnnl_api_primitives_common
 *  \{
 <p>
 *  Primitive descriptor query specification
 * 
 *  For generic function dnnl_primitive_desc_query(), the type of result must
 *  agree with the queried argument. The correspondence table:
 * 
 *  Query kind                      | Type of query result
 *  --------------------------------|-----------------------------
 *  dnnl_query_*_engine             | #dnnl_engine_t *
 *  #dnnl_query_primitive_kind      | #dnnl_primitive_kind_t *
 *  dnnl_query_*_s32                | int *
 *  dnnl_query_*_s64                | #dnnl_dim_t * (same as int64_t *)
 *  dnnl_query_*_f32                | float *
 *  dnnl_query_*_f64                | double *
 *  dnnl_query_*_str                | const char **
 *  dnnl_query_*_md                 | #const_dnnl_memory_desc_t *
 *  dnnl_query_*_pd                 | #const_dnnl_primitive_desc_t *
 *  dnnl_query_cache_blob_id        | const uint8_t **
 *  dnnl_query_strides              | const #dnnl_dims_t **
 *  dnnl_query_dilations            | const #dnnl_dims_t **
 *  dnnl_query_padding_l            | const #dnnl_dims_t **
 *  dnnl_query_padding_r            | const #dnnl_dims_t **
 *  dnnl_query_flags                | unsigned *
 *  dnnl_query_alg_kind             | #dnnl_alg_kind_t *
 *  dnnl_query_factors              | const float **
 *  dnnl_query_cell_kind            | #dnnl_alg_kind_t *
 *  dnnl_query_direction            | #dnnl_rnn_direction_t *
 *  dnnl_query_activation_kind      | #dnnl_alg_kind_t *
 *  dnnl_query_kernel               | const #dnnl_dims_t **
 *  dnnl_query_dims                 | const #dnnl_dims_t **
 *  dnnl_query_data_type            | #dnnl_data_type_t *
 *  dnnl_query_padded_dims          | const #dnnl_dims_t **
 *  dnnl_query_padded_offsets       | const #dnnl_dims_t **
 *  dnnl_query_format_kind          | #dnnl_format_kind_t *
 *  dnnl_query_inner_blks           | const #dnnl_dims_t **
 *  dnnl_query_inner_idxs           | const #dnnl_dims_t **
 *  dnnl_query_sparse_encoding      | #dnnl_sparse_encoding_t *
 * 
 *  \note
 *      Rule of thumb: all opaque types and structures are returned by
 *      reference. All numbers are returned by value.
 * 
 *  \warning
 *      All returned references point to constant objects and are valid only
 *      during the lifetime of the queried primitive descriptor. Returned objects
 *      must not be destroyed by the user. If you need to keep the object longer
 *      than the lifetime of the queried primitive descriptor, use
 *      dnnl_primitive_desc_clone() to make a copy. */
/** enum dnnl_query_t */
public static final int
    /** no query */
    dnnl_query_undef = 0,

    /** execution engine */
    dnnl_query_engine = 1,
    /** primitive kind */
    dnnl_query_primitive_kind = 2,

    /** number of inputs expected */
    dnnl_query_num_of_inputs_s32 = 3,
    /** number of outputs expected */
    dnnl_query_num_of_outputs_s32 = 4,

    /** runtime estimation (seconds) */
    dnnl_query_time_estimate_f64 = 5,
    /** memory consumption -- extra */
    dnnl_query_memory_consumption_s64 = 6,
    /**  (scratch) memory, additional to
     *   all inputs and outputs memory
     *   (bytes) */

    /** scratchpad engine -- engine to be used */
    dnnl_query_scratchpad_engine = 7,
    /**  for creating scratchpad memory */

    /** implementation name */
    dnnl_query_impl_info_str = 8,

    /** source engine */
    dnnl_query_reorder_src_engine = 9,
    /** destination engine */
    dnnl_query_reorder_dst_engine = 10,

    /** propagation kind */
    dnnl_query_prop_kind = 11,

    /** size of cache blob ID in bytes */
    dnnl_query_cache_blob_id_size_s64 = 12,
    /** cache blob  ID (pointer to array) */
    dnnl_query_cache_blob_id = 13,

    /** strides */
    dnnl_query_strides = 14,
    /** dilations */
    dnnl_query_dilations = 15,
    /** left padding */
    dnnl_query_padding_l = 16,
    /** right padding */
    dnnl_query_padding_r = 17,
    /** epsilon */
    dnnl_query_epsilon_f32 = 18,
    /** flags */
    dnnl_query_flags = 19,
    /** algorithm kind */
    dnnl_query_alg_kind = 20,
    /** alpha */
    dnnl_query_alpha_f32 = 21,
    /** beta */
    dnnl_query_beta_f32 = 22,
    /** axis */
    dnnl_query_axis_s32 = 23,
    /** LRN parameter local size */
    dnnl_query_local_size_s64 = 24,
    /** LRN parameter K */
    dnnl_query_k_f32 = 25,
    /** Reduction parameter P */
    dnnl_query_p_f32 = 26,
    /** Resampling parameter factors */
    dnnl_query_factors = 27,
    /** RNN parameter cell kind */
    dnnl_query_cell_kind = 28,
    /** RNN parameter direction */
    dnnl_query_direction = 29,
    /** RNN parameter activation kind */
    dnnl_query_activation_kind = 30,
    /** Pooling parameter kernel */
    dnnl_query_kernel = 31,
    /** Shuffle parameter group size */
    dnnl_query_group_size_s64 = 32,

    // memory descriptor section
    /** stub */
    dnnl_query_some_md = 128,
    /** source memory desc */
    dnnl_query_src_md = 129,
    /** source gradient memory desc */
    dnnl_query_diff_src_md = 130,
    /** weights memory descriptor desc */
    dnnl_query_weights_md = 131,
    /** weights grad. memory desc */
    dnnl_query_diff_weights_md = 132,
    /** destination memory desc */
    dnnl_query_dst_md = 133,
    /** destination grad. memory desc */
    dnnl_query_diff_dst_md = 134,
    /** workspace memory desc */
    dnnl_query_workspace_md = 135,
    /** scratchpad memory desc */
    dnnl_query_scratchpad_md = 136,
    /** memory desc of an execute argument */
    dnnl_query_exec_arg_md = 255,

    /** number of dimensions */
    dnnl_query_ndims_s32 = 256,
    /** vector of dimensions */
    dnnl_query_dims = 257,
    /** data type */
    dnnl_query_data_type = 258,
    /** submemory offset */
    dnnl_query_submemory_offset_s64 = 259,
    /** vector of padded dimensions */
    dnnl_query_padded_dims = 260,
    /** vector of padded offsets */
    dnnl_query_padded_offsets = 261,
    /** format kind */
    dnnl_query_format_kind = 262,
    /** number of innermost blocks */
    dnnl_query_inner_nblks_s32 = 263,
    /** vector of sizes of the innermost blocks */
    dnnl_query_inner_blks = 264,
    /** vector of logical indices of the blocks */
    dnnl_query_inner_idxs = 265,
// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif
    // Max value to prevent UB for internal use only dnnl_query_t
    dnnl_query_max = 0x7fff;

/** \} dnnl_api_primitives_common
 <p>
 *  \} dnnl_api_primitives
 <p>
 *  \addtogroup dnnl_api_service
 *  \{
 <p>
 *  Disable profiling completely */
public static final int DNNL_JIT_PROFILE_NONE = 0;

/** Enable VTune Profiler integration */
public static final int DNNL_JIT_PROFILE_VTUNE = 1;

/** Enable Linux perf integration via perfmap files */
public static final int DNNL_JIT_PROFILE_LINUX_PERFMAP = 2;

/** Enable Linux perf integration via jitdump files */
public static final int DNNL_JIT_PROFILE_LINUX_JITDUMP = 4;

/** Instruct Linux perf integration via jitdump files to use TSC. \ref
 *  DNNL_JIT_PROFILE_LINUX_JITDUMP must be set too for this to take effect. */
public static final int DNNL_JIT_PROFILE_LINUX_JITDUMP_USE_TSC = 8;

/** Enable Linux perf integration (both jitdump and perfmap) */
public static final int DNNL_JIT_PROFILE_LINUX_PERF = 
    (DNNL_JIT_PROFILE_LINUX_JITDUMP | DNNL_JIT_PROFILE_LINUX_PERFMAP);

/** CPU instruction set flags */
/** enum dnnl_cpu_isa_t */
public static final int
    /** Library choice of ISA (excepting those listed as initial support) */
    dnnl_cpu_isa_default = 0x0,

    /** Intel Streaming SIMD Extensions 4.1 (Intel SSE4.1) */
    dnnl_cpu_isa_sse41 = 0x1,

    /** Intel Advanced Vector Extensions (Intel AVX) */
    dnnl_cpu_isa_avx = 0x3,

    /** Intel Advanced Vector Extensions 2 (Intel AVX2) */
    dnnl_cpu_isa_avx2 = 0x7,

    /** Intel AVX2 and Intel Deep Learning Boost (Intel DL Boost) support */
    dnnl_cpu_isa_avx2_vnni = 0xf,

    /** Intel AVX2 and Intel Deep Learning Boost (Intel DL Boost)
     *  with 8-bit integer, float16 and bfloat16 support
     *  (preview support) */
    dnnl_cpu_isa_avx2_vnni_2 = 0x1f,

    /** Intel AVX-512 subset for Intel Xeon Scalable processor family
     *  and Intel Core processor family. */
    dnnl_cpu_isa_avx512_core = 0x27,

    /** Intel AVX-512 and Intel Deep Learning Boost (Intel DL Boost) support
     *  for Intel Xeon Scalable processor family
     *  and Intel Core processor family. */
    dnnl_cpu_isa_avx512_core_vnni = 0x67,

    /** Intel AVX-512, Intel DL Boost and bfloat16 support
     *  for Intel Xeon Scalable processor family
     *  and Intel Core processor family. */
    dnnl_cpu_isa_avx512_core_bf16 = 0xe7,

    /** Intel AVX-512 with float16, Intel DL Boost and bfloat16 support
     *  for Intel Xeon Scalable processor family
     *  and Intel Core processor family. */
    dnnl_cpu_isa_avx512_core_fp16 = 0x1ef,

    /** Intel AVX-512 with float16, Intel DL Boost and bfloat16 support and
     *  Intel AMX with 8-bit integer and bfloat16 support */
    dnnl_cpu_isa_avx512_core_amx = 0xfef,

    /** Intel AVX-512 with float16, Intel DL Boost and bfloat16 support and
     *  Intel AMX with 8-bit integer, bfloat16 and float16 support
     *  (preview support) */
    dnnl_cpu_isa_avx512_core_amx_fp16 = 0x1fef;

/** CPU ISA hints flags */
/** enum dnnl_cpu_isa_hints_t */
public static final int
    /** No hints (use default features) */
    dnnl_cpu_isa_no_hints = 0x0,

    /** Prefer to exclusively use Ymm registers for computations */
    dnnl_cpu_isa_prefer_ymm = 0x1;

/** \} dnnl_api_service
 <p>
 *  \} dnnl_api */

// #ifdef __cplusplus
// #endif

// #endif /* ONEAPI_DNNL_TYPES_H */


// Parsed from oneapi/dnnl/dnnl_common.h

/*******************************************************************************
* Copyright 2022-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C common API */

// #ifndef ONEAPI_DNNL_DNNL_COMMON_H
// #define ONEAPI_DNNL_DNNL_COMMON_H

// #include "oneapi/dnnl/dnnl_common_types.h"
// #include "oneapi/dnnl/dnnl_config.h"
// #include "oneapi/dnnl/dnnl_version.h"

// #ifdef __cplusplus
// #endif

/** \addtogroup dnnl_api  oneDNN API
 *  \{
 <p>
 *  \addtogroup dnnl_api_common Common API
 *  \{
 <p>
 *  \addtogroup dnnl_api_engine Engine
 *  \{
 <p>
 *  Returns the number of engines of a particular kind.
 * 
 *  @param kind Kind of engines to count.
 *  @return Count of the engines. */

///
public static native @Cast("size_t") long dnnl_engine_get_count(@Cast("dnnl_engine_kind_t") int kind);

/** Creates an engine.
 * 
 *  @param engine Output engine.
 *  @param kind Engine kind.
 *  @param index Engine index that should be between 0 and the count of
 *      engines of the requested kind.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_engine_create(
        @ByPtrPtr dnnl_engine engine, @Cast("dnnl_engine_kind_t") int kind, @Cast("size_t") long index);
public static native @Cast("dnnl_status_t") int dnnl_engine_create(
        @Cast("dnnl_engine_t*") PointerPointer engine, @Cast("dnnl_engine_kind_t") int kind, @Cast("size_t") long index);

/** Returns the kind of an engine.
 * 
 *  @param engine Engine to query.
 *  @param kind Output engine kind.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") IntPointer kind);
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") IntBuffer kind);
public static native @Cast("dnnl_status_t") int dnnl_engine_get_kind(
        dnnl_engine engine, @Cast("dnnl_engine_kind_t*") int[] kind);

/** Destroys an engine.
 * 
 *  @param engine Engine to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_engine_destroy(dnnl_engine engine);

/** \} dnnl_api_engine
 <p>
 *  \addtogroup dnnl_api_stream Stream
 *  \{
 <p>
 *  Creates an execution stream.
 * 
 *  @param stream Output execution stream.
 *  @param engine Engine to create the execution stream on.
 *  @param flags Stream behavior flags (@see dnnl_stream_flags_t).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_stream_create(
        @ByPtrPtr dnnl_stream stream, dnnl_engine engine, @Cast("unsigned") int flags);
public static native @Cast("dnnl_status_t") int dnnl_stream_create(
        @Cast("dnnl_stream_t*") PointerPointer stream, dnnl_engine engine, @Cast("unsigned") int flags);

/** Returns the engine of a stream object.
 * 
 *  @param stream Stream object.
 *  @param engine Output engine on which the stream is created.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_stream_get_engine(
        @Const dnnl_stream stream, @ByPtrPtr dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_stream_get_engine(
        @Const dnnl_stream stream, @Cast("dnnl_engine_t*") PointerPointer engine);

/** Waits for all primitives in the execution stream to finish computations.
 * 
 *  @param stream Execution stream.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_stream_wait(dnnl_stream stream);

/** Destroys an execution stream.
 * 
 *  @param stream Execution stream to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_stream_destroy(dnnl_stream stream);

/** \} dnnl_api_stream
 <p>
 *  \addtogroup dnnl_api_fpmath_mode Floating-point Math Mode
 *  \{
 <p>
 *  Returns the floating-point math mode that will be used by default
 *  for all subsequently created primitives.
 * 
 *  @param mode Output FP math mode.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_get_default_fpmath_mode(@Cast("dnnl_fpmath_mode_t*") IntPointer mode);
public static native @Cast("dnnl_status_t") int dnnl_get_default_fpmath_mode(@Cast("dnnl_fpmath_mode_t*") IntBuffer mode);
public static native @Cast("dnnl_status_t") int dnnl_get_default_fpmath_mode(@Cast("dnnl_fpmath_mode_t*") int[] mode);

/** Sets the floating-point math mode that will be used by default
 *  for all subsequently created primitives.
 * 
 *  @param mode FP math mode. The possible values are:
 *      #dnnl_fpmath_mode_strict,
 *      #dnnl_fpmath_mode_bf16,
 *      #dnnl_fpmath_mode_f16,
 *      #dnnl_fpmath_mode_tf32,
 *      #dnnl_fpmath_mode_any.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_set_default_fpmath_mode(@Cast("dnnl_fpmath_mode_t") int mode);

/** \} dnnl_api_fpmath_mode
 <p>
 *  \addtogroup dnnl_api_service
 *  \{
 <p>
 *  Configures verbose output to stdout.
 * 
 *  \note
 *      Enabling verbose output affects performance.
 *      This setting overrides the ONEDNN_VERBOSE environment variable.
 * 
 *  @param level Verbosity level:
 *   - 0: no verbose output (default),
 *   - 1: primitive and graph information at execution,
 *   - 2: primitive and graph information at creation/compilation and execution.
 *  @return #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the
 *      \p level value is invalid, and #dnnl_success/#dnnl::status::success on
 *      success. */
public static native @Cast("dnnl_status_t") int dnnl_set_verbose(int level);

/** Returns library version information.
 *  @return Pointer to a constant structure containing
 *   - major: major version number,
 *   - minor: minor version number,
 *   - patch: patch release number,
 *   - hash: git commit hash. */
public static native @Const dnnl_version_t dnnl_version();

/** \} dnnl_api_service
 <p>
 *  \} dnnl_api_common
 <p>
 *  \} dnnl_api */

// #ifdef __cplusplus
// #endif

// #endif /* ONEAPI_DNNL_DNNL_COMMON_H */


// Parsed from oneapi/dnnl/dnnl_config.h

/*******************************************************************************
* Copyright 2019-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

// #ifndef ONEAPI_DNNL_DNNL_CONFIG_H
// #define ONEAPI_DNNL_DNNL_CONFIG_H

/** \cond DO_NOT_DOCUMENT_THIS */

// All symbols shall be internal unless marked as DNNL_API
// #if defined _WIN32 || defined __CYGWIN__
// #define DNNL_HELPER_DLL_IMPORT __declspec(dllimport)
// #define DNNL_HELPER_DLL_EXPORT __declspec(dllexport)
// #else
// #if __GNUC__ >= 4
// #define DNNL_HELPER_DLL_IMPORT __attribute__((visibility("default")))
// #define DNNL_HELPER_DLL_EXPORT __attribute__((visibility("default")))
// #else
// #define DNNL_HELPER_DLL_IMPORT
// #define DNNL_HELPER_DLL_EXPORT
// #endif
// #endif

// #ifdef DNNL_DLL
// #ifdef DNNL_DLL_EXPORTS
// #define DNNL_API DNNL_HELPER_DLL_EXPORT
// #else
// #define DNNL_API DNNL_HELPER_DLL_IMPORT
// #endif
// #else
// #define DNNL_API
// #endif

// #if defined(__GNUC__)
// #define DNNL_DEPRECATED __attribute__((deprecated))
// #elif defined(_MSC_VER)
// #define DNNL_DEPRECATED __declspec(deprecated)
// #else
// #define DNNL_DEPRECATED
// #endif

/** \endcond */

// clang-format off

/** \addtogroup dnnl_api_service
/** \{
<p>
/** No runtime (disabled) */
public static final int DNNL_RUNTIME_NONE = 0;

/** Sequential runtime (CPU only) */
public static final int DNNL_RUNTIME_SEQ = 1;

/** OpenMP runtime (CPU only) */
public static final int DNNL_RUNTIME_OMP = 2;

/** TBB runtime (CPU only) */
public static final int DNNL_RUNTIME_TBB = 4;

/** Threadpool runtime (CPU only) */
public static final int DNNL_RUNTIME_THREADPOOL = 8;

/** OpenCL runtime */
public static final int DNNL_RUNTIME_OCL = 256;

/** SYCL runtime */
public static final int DNNL_RUNTIME_SYCL = 512;

/** DPC++ runtime */
public static final int DNNL_RUNTIME_DPCPP = DNNL_RUNTIME_SYCL;

/** \} dnnl_api_service */

// oneDNN CPU threading runtime
public static final int DNNL_CPU_THREADING_RUNTIME = DNNL_RUNTIME_OMP;

// oneDNN CPU engine runtime
public static final int DNNL_CPU_RUNTIME = DNNL_RUNTIME_OMP;

// oneDNN GPU engine runtime
public static final int DNNL_GPU_RUNTIME = DNNL_RUNTIME_OCL;

// clang-format on

// #if defined(DNNL_CPU_RUNTIME) && defined(DNNL_GPU_RUNTIME)
// #if (DNNL_CPU_RUNTIME == DNNL_RUNTIME_OCL)
// #error "Unexpected DNNL_CPU_RUNTIME"
// #endif
// #if (DNNL_GPU_RUNTIME != DNNL_RUNTIME_NONE)
//         && (DNNL_GPU_RUNTIME != DNNL_RUNTIME_OCL)
//         && (DNNL_GPU_RUNTIME != DNNL_RUNTIME_SYCL)
// #error "Unexpected DNNL_GPU_RUNTIME"
// #endif
// #if (DNNL_CPU_RUNTIME == DNNL_RUNTIME_NONE
//         && DNNL_GPU_RUNTIME == DNNL_RUNTIME_NONE)
// #error "At least one runtime must be specified"
// #endif
// #else
// #error "BOTH DNNL_CPU_RUNTIME and DNNL_GPU_RUNTIME must be defined"
// #endif

// For SYCL CPU, a primitive may be created and executed in different threads
// hence the global scratchpad does not work. This enables concurrent execution
// when CPU runtime is SYCL to avoid the issue.
// #if DNNL_CPU_RUNTIME == DNNL_RUNTIME_SYCL
// #ifndef DNNL_ENABLE_CONCURRENT_EXEC
// #define DNNL_ENABLE_CONCURRENT_EXEC
// #endif
// #endif

// When defined, primitive cache stores runtime objects.
/* #undef DNNL_USE_RT_OBJECTS_IN_PRIMITIVE_CACHE */

// When defined, DPCPP is supported.
/* #undef DNNL_WITH_SYCL */

// When defined, Level Zero is supported.
/* #undef DNNL_WITH_LEVEL_ZERO */

// When defined, SYCL CUDA backend is used.
/* #undef DNNL_SYCL_CUDA */

// When defined, SYCL HIP backend is used.
/* #undef DNNL_SYCL_HIP */

// When defined, stack checker is enabled.
/* #undef DNNL_ENABLE_STACK_CHECKER */

// When defined, experimental features are enabled.
/* #undef DNNL_EXPERIMENTAL */

// When defined, experimental functionality for sparse domain is enabled.
/* #undef DNNL_EXPERIMENTAL_SPARSE */

// When defined, graph component is enabled.
/* #undef ONEDNN_BUILD_GRAPH */

// When defined, experimental profiling capabilities are enabled.
/* #undef DNNL_EXPERIMENTAL_PROFILING */

// List of configurating build controls
// Workload controls
public static final int BUILD_TRAINING = 1;
public static final int BUILD_INFERENCE = 0;
// Primitive controls
public static final int BUILD_PRIMITIVE_ALL = 1;
public static final int BUILD_BATCH_NORMALIZATION = 0;
public static final int BUILD_BINARY = 0;
public static final int BUILD_CONCAT = 0;
public static final int BUILD_CONVOLUTION = 0;
public static final int BUILD_DECONVOLUTION = 0;
public static final int BUILD_ELTWISE = 0;
public static final int BUILD_GROUP_NORMALIZATION = 0;
public static final int BUILD_INNER_PRODUCT = 0;
public static final int BUILD_LAYER_NORMALIZATION = 0;
public static final int BUILD_LRN = 0;
public static final int BUILD_MATMUL = 0;
public static final int BUILD_POOLING = 0;
public static final int BUILD_PRELU = 0;
public static final int BUILD_REDUCTION = 0;
public static final int BUILD_REORDER = 0;
public static final int BUILD_RESAMPLING = 0;
public static final int BUILD_RNN = 0;
public static final int BUILD_SHUFFLE = 0;
public static final int BUILD_SOFTMAX = 0;
public static final int BUILD_SUM = 0;
// Primitives CPU ISA controls
public static final int BUILD_PRIMITIVE_CPU_ISA_ALL = 1;
public static final int BUILD_SSE41 = 0;
public static final int BUILD_AVX2 = 0;
public static final int BUILD_AVX512 = 0;
public static final int BUILD_AMX = 0;
// Primitives GPU ISA controls
public static final int BUILD_PRIMITIVE_GPU_ISA_ALL = 1;
public static final int BUILD_GEN9 = 0;
public static final int BUILD_GEN11 = 0;
public static final int BUILD_XELP = 0;
public static final int BUILD_XEHP = 0;
public static final int BUILD_XEHPG = 0;
public static final int BUILD_XEHPC = 0;
// GeMM kernels ISA controls
public static final int BUILD_GEMM_KERNELS_ALL = 1;
public static final int BUILD_GEMM_KERNELS_NONE = 0;
public static final int BUILD_GEMM_SSE41 = 0;
public static final int BUILD_GEMM_AVX2 = 0;
public static final int BUILD_GEMM_AVX512 = 0;
// #endif


// Parsed from oneapi/dnnl/dnnl_version.h

/*******************************************************************************
* Copyright 2019-2020 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

// #ifndef ONEAPI_DNNL_DNNL_VERSION_H
// #define ONEAPI_DNNL_DNNL_VERSION_H

// clang-format off

/** Major version */
public static final int DNNL_VERSION_MAJOR = 3;

/** Minor version */
public static final int DNNL_VERSION_MINOR = 3;

/** Patch version */
public static final int DNNL_VERSION_PATCH = 4;

/** Git commit hash */
public static native @MemberGetter String DNNL_VERSION_HASH();
public static final String DNNL_VERSION_HASH = DNNL_VERSION_HASH();

// clang-format on

// #endif


// Parsed from oneapi/dnnl/dnnl.h

/*******************************************************************************
* Copyright 2016-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C API */

// #ifndef ONEAPI_DNNL_DNNL_H
// #define ONEAPI_DNNL_DNNL_H

// #include "oneapi/dnnl/dnnl_common.h"
// #include "oneapi/dnnl/dnnl_config.h"
// #include "oneapi/dnnl/dnnl_types.h"
// #include "oneapi/dnnl/dnnl_version.h"

// #ifdef __cplusplus
// #endif

/** \addtogroup dnnl_api
 *  \{
 <p>
 *  \addtogroup dnnl_api_primitives
 *  \{
 <p>
 *  \addtogroup dnnl_api_primitives_common
 *  \{
 <p>
 *  Changes the primitive descriptor to point to the next available
 *  implementation.
 * 
 *  @param primitive_desc A primitive descriptor to change.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 *  @return #dnnl_last_impl_reached if no more implementations available,
 *  in which case the primitive descriptor itself is kept unchanged. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_next_impl(
        dnnl_primitive_desc primitive_desc);

/** Clones a primitive descriptor. The resulting primitive descriptor must be
 *  destroyed separately.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param existing_primitive_desc Primitive descriptor to clone.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_clone(
        @ByPtrPtr dnnl_primitive_desc primitive_desc,
        @Const dnnl_primitive_desc existing_primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_clone(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc,
        @Const dnnl_primitive_desc existing_primitive_desc);

/** Returns a constant reference to the attributes of a primitive descriptor.
 * 
 *  \warning
 *      It is an error to destroy the resulting \p attr.
 * 
 *  \warning
 *      The lifetime of an \p attr is the same as that of a \p
 *      primitive_desc, so it is an error to use the \p attr once the \p
 *      primitive_desc has been destroyed.
 * 
 *  @param primitive_desc Primitive descriptor.
 *  @param attr Output primitive attributes.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_get_attr(
        @Const dnnl_primitive_desc primitive_desc,
        @Const @ByPtrPtr dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_get_attr(
        @Const dnnl_primitive_desc primitive_desc,
        @Cast("const_dnnl_primitive_attr_t*") PointerPointer attr);

/** Destroys a primitive descriptor.
 * 
 *  @param primitive_desc Primitive descriptor to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_destroy(
        dnnl_primitive_desc primitive_desc);

/** Queries a primitive descriptor for various pieces of information.
 * 
 *  The most common use case is to query a primitive descriptor, created with
 *  source, weights, and destination memory descriptors with format tags set
 *  to #dnnl_format_tag_any, for the corresponding memory descriptors (in this
 *  case the \p what is set to #dnnl_query_src_md, #dnnl_query_weights_md, and
 *  #dnnl_query_dst_md respectively) so that it is possible to create memory
 *  objects and reorder primitives if necessary.
 * 
 *  Another typical use case is to query a primitive descriptor for workspace
 *  memory descriptor (with \p what set to #dnnl_query_workspace_md). If this
 *  query returns #dnnl_not_required status, then workspace memory is not
 *  required.
 * 
 *  \note
 *      When querying for a memory descriptor for a scratchpad, a workspace,
 *      or an optional parameter, the query will return a pointer to a zero
 *      memory descriptor if the parameter is not needed.
 * 
 *  A few other use cases:
 *   - query a primitive descriptor for the implementation information string
 *     (#dnnl_query_impl_info_str)
 *   - query a primitive descriptor for the number of inputs and outputs
 *     (#dnnl_query_num_of_inputs_s32 and #dnnl_query_num_of_outputs_s32
 *     respectively)
 * 
 *  @see dnnl_query_t for more options
 * 
 *  @param primitive_desc Primitive descriptor.
 *  @param what Parameter to query.
 *  @param index Index of the parameter to query for.
 *  @param result Output result. The type depends on the query. For example,
 *      it must be a \c dnnl_memory_desc_t* if querying for a memory
 *      descriptor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_desc_query(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index, Pointer result);

/** Queries primitive descriptor for a memory descriptor.
 * 
 *  \note
 *      This function is a convenience version of
 *      #dnnl_primitive_desc_query().
 * 
 *  @param primitive_desc Primitive descriptor.
 *  @param what Kind of memory descriptor parameter to query for.
 *  @param index Index of the parameter to query.
 *  @return A pointer to the requested memory descriptor.
 *  @return A pointer to a zero memory descriptor if the parameter is not
 *           needed.
 *  @return NULL in case of any error.
 *  */

///
///
public static native @Const dnnl_memory_desc dnnl_primitive_desc_query_md(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index);

/** Queries primitive descriptor for a signed 32bit int.
 * 
 *  \note
 *      This function is a convenience version of
 *      #dnnl_primitive_desc_query().
 * 
 *  @param primitive_desc Primitive descriptor.
 *  @param what Kind of the value to query for.
 *  @param index Index of the parameter to query.
 *  @return The requested value.
 *  @return 0 in case of any error (in particular if the queried entity is
 *      not of type int32_t). Note that 0 may also be the actual returned
 *      value. */

///
public static native int dnnl_primitive_desc_query_s32(
        @Const dnnl_primitive_desc primitive_desc, @Cast("dnnl_query_t") int what,
        int index);

/** Creates a primitive.
 * 
 *  @param primitive Output primitive.
 *  @param primitive_desc Primitive descriptor used to create the primitive.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_create(@ByPtrPtr dnnl_primitive primitive,
        @Const dnnl_primitive_desc primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create(@Cast("dnnl_primitive_t*") PointerPointer primitive,
        @Const dnnl_primitive_desc primitive_desc);

/** Creates a primitive from a cache blob.
 * 
 *  @param primitive Output primitive.
 *  @param primitive_desc Primitive descriptor used to create the primitive.
 *  @param size Size of the cache blob in bytes.
 *  @param cache_blob Cache blob of size \p size.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @ByPtrPtr dnnl_primitive primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") BytePointer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @Cast("dnnl_primitive_t*") PointerPointer primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") ByteBuffer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @ByPtrPtr dnnl_primitive primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") byte[] cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @Cast("dnnl_primitive_t*") PointerPointer primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") BytePointer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @ByPtrPtr dnnl_primitive primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") ByteBuffer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_create_from_cache_blob(
        @Cast("dnnl_primitive_t*") PointerPointer primitive, @Const dnnl_primitive_desc primitive_desc,
        @Cast("size_t") long size, @Cast("const uint8_t*") byte[] cache_blob);

/** Executes a primitive.
 * 
 *  @param primitive Primitive to execute.
 *  @param stream Stream to use.
 *  @param nargs Number of arguments.
 *  @param args Array of arguments. Each argument is an
 *      <index, #dnnl_memory_t> pair. The index is one of the {@code DNNL_ARG_*}
 *      values such as {@code DNNL_ARG_SRC}. Unless runtime shapes are used (see
 *      #DNNL_RUNTIME_DIM_VAL), the memory object must have the same memory
 *      descriptor as that returned by
 *      #dnnl_primitive_desc_query_md(#dnnl_query_exec_arg_md, index).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 <p>
 *  \note If any argument in \p args is padded (padded_dims >
 *  dims), the primitive execution will assume properly zero-padded
 *  input arguments, and produce zero-padded output arguments. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_execute(@Const dnnl_primitive primitive,
        dnnl_stream stream, int nargs, @Const dnnl_exec_arg_t args);

/** Retrieves a constant reference to the primitive descriptor of a given
 *  primitive.
 * 
 *  \warning
 *      It is an error to destroy the returned object. It is owned by the
 *      primitive. The \c const qualifier of the returned object prevents
 *      such attempts.
 * 
 *  @param primitive Primitive to query for the primitive descriptor.
 *  @param primitive_desc Output primitive descriptor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_primitive_desc(
        @Const dnnl_primitive primitive,
        @Const @ByPtrPtr dnnl_primitive_desc primitive_desc);
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_primitive_desc(
        @Const dnnl_primitive primitive,
        @Cast("const_dnnl_primitive_desc_t*") PointerPointer primitive_desc);

/** Retrieves a cache blob associated with the given primitive.
 * 
 *  @param primitive Primitive to query for the cache blob.
 *  @param size Size of the cache blob in bytes.
 *  @param cache_blob Cache blob of size \p size. If the \p cache_blob is
 *      nullptr then the size of the cache blob is returned in \p size.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 * 
 *  \note The cache blob can be empty. It's the user's responsibility to check
 *      whether it's empty prior to passing it to
 *      #dnnl_primitive_create_from_cache_blob(). */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_cache_blob(
        @Const dnnl_primitive primitive, @Cast("size_t*") SizeTPointer size, @Cast("uint8_t*") BytePointer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_cache_blob(
        @Const dnnl_primitive primitive, @Cast("size_t*") SizeTPointer size, @Cast("uint8_t*") ByteBuffer cache_blob);
public static native @Cast("dnnl_status_t") int dnnl_primitive_get_cache_blob(
        @Const dnnl_primitive primitive, @Cast("size_t*") SizeTPointer size, @Cast("uint8_t*") byte[] cache_blob);

/** Destroys a primitive.
 * 
 *  @param primitive The primitive to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_destroy(dnnl_primitive primitive);

/** \} dnnl_api_primitives_common
 <p>
 *  \addtogroup dnnl_api_attributes
 *  \{
 <p>
 *  Creates an empty (default) primitive attributes with all the parameters
 *  set to their default values.
 * 
 *  Empty attributes are implied whenever the respective argument is NULL.
 * 
 *  @param attr Output primitive attributes.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_create(@ByPtrPtr dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_create(@Cast("dnnl_primitive_attr_t*") PointerPointer attr);

/** Clones primitive attributes.
 * 
 *  @param attr Output primitive attributes.
 *  @param existing_attr Primitive attributes to clone.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_clone(
        @ByPtrPtr dnnl_primitive_attr attr, @Const dnnl_primitive_attr existing_attr);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_clone(
        @Cast("dnnl_primitive_attr_t*") PointerPointer attr, @Const dnnl_primitive_attr existing_attr);

/** Destroys primitive attributes.
 * 
 *  @param attr Primitive attributes to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_destroy(dnnl_primitive_attr attr);

/** Returns the floating-point math mode primitive attribute.
 * 
 *  @param attr Primitive attributes.
 *  @param mode Output FP math mode.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_fpmath_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_fpmath_mode_t*") IntPointer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_fpmath_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_fpmath_mode_t*") IntBuffer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_fpmath_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_fpmath_mode_t*") int[] mode);

/** Sets the floating-point math mode primitive attributes.
 * 
 *  @param attr Primitive attributes.
 *  @param mode FP math mode. The possible values are:
 *      #dnnl_fpmath_mode_strict (default),
 *      #dnnl_fpmath_mode_bf16,
 *      #dnnl_fpmath_mode_f16,
 *      #dnnl_fpmath_mode_tf32,
 *      #dnnl_fpmath_mode_any.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_fpmath_mode(
        dnnl_primitive_attr attr, @Cast("dnnl_fpmath_mode_t") int mode);

/** Returns the primitive attributes scratchpad mode.
 * 
 *  @param attr Primitive attributes.
 *  @param mode Output scratchpad mode.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") IntPointer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") IntBuffer mode);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_scratchpad_mode(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t*") int[] mode);

/** Sets primitive attributes scratchpad mode.
 * 
 *  @param attr Primitive attributes.
 *  @param mode Scratchpad mode. The possible values are:
 *      #dnnl_scratchpad_mode_library (default) and
 *      #dnnl_scratchpad_mode_user.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_scratchpad_mode(
        dnnl_primitive_attr attr, @Cast("dnnl_scratchpad_mode_t") int mode);

/** Sets primitive attributes scaling factors for primitive operations for a
 *  given memory argument. The scaling factors must be passed at execution time
 *  as an argument with index #DNNL_ARG_ATTR_SCALES | arg.
 * 
 *  @see dnnl_primitive_attr_set_scales_mask
 * 
 * 
 *  @param attr Primitive attributes.
 *  @param arg Parameter argument index as passed to the
 *      dnnl_primitive_execute() call.
 *  @param mask Scaling factors correspondence mask that defines the
 *      correspondence between the tensor dimensions and the \p scales array.
 *      The set i-th bit indicates that a dedicated scaling factor is used for
 *      each index along that dimension. Set the mask to 0 to use a common
 *      scaling factor for the whole output tensor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_scales_mask(
        dnnl_primitive_attr attr, int arg, int mask);

/** Sets primitive attributes zero points for primitive operations for a given
 *  memory argument. The zero points must be passed at execution time
 *  as an argument with index #DNNL_ARG_ATTR_ZERO_POINTS | arg.
 * 
 *  @see dnnl_primitive_attr_set_zero_points_mask
 * 
 * 
 *  @param attr Primitive attributes.
 *  @param arg Parameter argument index as passed to the
 *      dnnl_primitive_execute() call.
 *  @param mask Zero point correspondence mask that defines the
 *      correspondence between the tensor dimensions and the \p
 *      zero_points array. The set i-th bit indicates that a dedicated
 *      zero point is used for each index along that dimension. Set the
 *      mask to 0 to use a common zero point for the whole output tensor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_zero_points_mask(
        dnnl_primitive_attr attr, int arg, int mask);

/** Returns primitive attributes post-ops.
 * 
 *  \warning
 *      The output \p post_ops points to the internal \p attr field, so it is
 *      an error to modify or destroy them. The lifetime of \p post_ops is
 *      the same as that of the \p attr it belongs to, so it is an error to
 *      use \p post_ops after \p attr has been destroyed.
 * 
 *  @param attr Primitive attributes.
 *  @param post_ops Output post-ops.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_post_ops(
        @Const dnnl_primitive_attr attr, @Const @ByPtrPtr dnnl_post_ops post_ops);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_post_ops(
        @Const dnnl_primitive_attr attr, @Cast("const_dnnl_post_ops_t*") PointerPointer post_ops);

/** Sets primitive attributes post-ops.
 * 
 *  \note
 *      There is no way to check whether the post-ops would be supported by
 *      the target primitive. Any error will be reported by the
 *      dnnl_<primitive name>_[propagation kind]_primitive_desc_create() function call.
 * 
 *  @param attr Primitive attributes.
 *  @param post_ops Post-ops to set.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_post_ops(
        dnnl_primitive_attr attr, @Const dnnl_post_ops post_ops);

/** Creates empty post-ops sequence.
 * 
 *  @param post_ops Output post-ops.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_create(@ByPtrPtr dnnl_post_ops post_ops);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_create(@Cast("dnnl_post_ops_t*") PointerPointer post_ops);

/** Clones post-ops primitive attribute.
 * 
 *  @param post_ops Output post-ops primitive attribute.
 *  @param existing_post_ops Post-ops primitive attribute to clone.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_clone(
        @ByPtrPtr dnnl_post_ops post_ops, @Const dnnl_post_ops existing_post_ops);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_clone(
        @Cast("dnnl_post_ops_t*") PointerPointer post_ops, @Const dnnl_post_ops existing_post_ops);

/** Destroys post-ops.
 * 
 *  @param post_ops Post-ops to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_destroy(dnnl_post_ops post_ops);

/** Returns the length of post-ops.
 * 
 *  @param post_ops Post-ops.
 *  @return The number of post-ops entries. */

///
public static native int dnnl_post_ops_len(@Const dnnl_post_ops post_ops);

/** Returns the kind of a post-op entry.
 * 
 *  @param post_ops Post-ops.
 *  @param index Post-op entry index.
 *  @return The kind of the post-op with the specified index.
 *  @return #dnnl_undefined_primitive if there is no post-op at the specified
 *      index. */

///
///
///
///
///
///
///
public static native @Cast("dnnl_primitive_kind_t") int dnnl_post_ops_get_kind(
        @Const dnnl_post_ops post_ops, int index);

/** Appends an accumulation v3 (sum) to post-ops. Prior to accumulating the
 *  result, a zero point is subtracted from the previous value and is
 *  multiplied by the scale.
 * 
 *  The kind of this post-op is #dnnl_sum.
 * 
 *  This feature may improve performance for cases like dequantize the
 *  asymmetrically quantized sum's src1 tensor to f32 domain before performing
 *  the sum operation by subtracting the \p zero_point before the scaling.
 * 
 *  In the simplest case where accumulation is the only post-op, the
 *  computations will be:
 * 
 *      dst[:] <- scale * (dst[:] - zero_point) + op(...)
 *                                              // instead of dst[:] <- op(...)
 * 
 *  If \p data_type is specified, original dst tensor will be reinterpreted
 *  as a tensor with provided data type. Since it is reinterpretation,
 *  data_type and dst data type should have the same size.
 *  As a result, computations will be:
 * 
 *      dst[:] <- scale * (as_data_type(dst[:]) - zero_point) + op(...)
 *                                         // instead of dst[:] <- op(...)
 *  \note
 *      This post-op executes in-place and does not change the
 *      destination layout.
 * 
 *  @param post_ops Post-ops.
 *  @param scale Accumulation scaling factor.
 *  @param zero_point Single scalar int32_t value of zero point.
 *  @param data_type Accumulation data_type.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_sum(dnnl_post_ops post_ops,
        float scale, int zero_point, @Cast("dnnl_data_type_t") int data_type);

/** Returns the parameters of an accumulation (sum) post-op with
 *  zero point and data type parameter.
 * 
 *  @param post_ops Post-ops.
 *  @param index Index of the sum post-op.
 *  @param scale Output accumulation scaling factor.
 *  @param zero_point Zero point.
 *  @param data_type Data type for accumulation.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, FloatPointer scale,
        IntPointer zero_point, @Cast("dnnl_data_type_t*") IntPointer data_type);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, FloatBuffer scale,
        IntBuffer zero_point, @Cast("dnnl_data_type_t*") IntBuffer data_type);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_sum(
        @Const dnnl_post_ops post_ops, int index, float[] scale,
        int[] zero_point, @Cast("dnnl_data_type_t*") int[] data_type);

/** Appends an elementwise post-op.
 * 
 *  The kind of this post operation is #dnnl_eltwise.
 * 
 *  In the simplest case when the elementwise is the only post operation, the
 *  computations would be:
 * 
 *      dst[:] <- eltwise_op (op(...)) // instead of dst[:] <- op(...)
 * 
 *  where eltwise_op is configured with the given parameters.
 * 
 *  @param post_ops Post-ops.
 *  @param alg_kind Elementwise algorithm for the post-op.
 *  @param alpha Alpha parameter for the elementwise algorithm.
 *  @param beta Beta parameter for the elementwise algorithm.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_eltwise(dnnl_post_ops post_ops,
        @Cast("dnnl_alg_kind_t") int alg_kind, float alpha, float beta);

/** Returns the parameters of an elementwise post-op.
 * 
 *  @param post_ops Post-ops.
 *  @param index Index of the elementwise post-op.
 *  @param alg_kind Output elementwise algorithm kind.
 *  @param alpha Output alpha parameter for the elementwise algorithm.
 *  @param beta Output beta parameter for the elementwise algorithm.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 *  @return #dnnl_invalid_arguments if \p index does not refer to an
 *      elementwise post-op. */

///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntPointer alg_kind,
        FloatPointer alpha, FloatPointer beta);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntBuffer alg_kind,
        FloatBuffer alpha, FloatBuffer beta);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_eltwise(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") int[] alg_kind,
        float[] alpha, float[] beta);

/** Appends a depthwise post-op convolution.
 * 
 *  This post-op can only be fused with a 2D 1x1 convolution (convolution with
 *  weights spatial dimensions equal to 1 i.e., kh=kw=1).
 * 
 *  The kind of this post-op is #dnnl_convolution.
 * 
 *  The number of outputs for primitive with fusion is one. The output spatial
 *  size can be derived as below:
 * 
 *  output_height = ceil(output_height_1x1_convolution, stride)
 *  output_width = ceil(output_width_1x1_convolution, stride)
 * 
 *  See \ref dev_guide_attributes_post_ops_depthwise and
 *  \ref dev_guide_attributes_post_ops_depthwise_fusion for more info.
 * 
 *  @param post_ops Post-ops.
 *  @param weights_data_type Weights data type of depthwise post-op
 *  @param bias_data_type Bias data type of depthwise post-op
 *  @param dst_data_type Output data type of depthwise post-op
 *  @param kernel_size Size of kernel of depthwise post-op
 *  @param stride_size Size of stride of depthwise post-op
 *  @param padding_l_size Size of left and top paddings of depthwise post-op
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_dw(dnnl_post_ops post_ops,
        @Cast("dnnl_data_type_t") int weights_data_type, @Cast("dnnl_data_type_t") int bias_data_type,
        @Cast("dnnl_data_type_t") int dst_data_type, @Cast("dnnl_dim_t") long kernel_size,
        @Cast("dnnl_dim_t") long stride_size, @Cast("dnnl_dim_t") long padding_l_size);

/** Returns the parameters of an depthwise post-op.
 * 
 *  @param post_ops Post-ops.
 *  @param index Index of the elementwise post-op.
 *  @param weights_data_type Weights data type of depthwise post-op
 *  @param bias_data_type Bias data type of depthwise post-op
 *  @param dst_data_type Output data type of depthwise post-op
 *  @param kernel_size Size of kernel of depthwise post-op
 *  @param stride_size Size of stride of depthwise post-op
 *  @param padding_l_size Size of left and top paddings of depthwise post-op
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_dw(
        @Const dnnl_post_ops post_ops, int index,
        @Cast("dnnl_data_type_t*") IntPointer weights_data_type, @Cast("dnnl_data_type_t*") IntPointer bias_data_type,
        @Cast("dnnl_data_type_t*") IntPointer dst_data_type, @Cast("dnnl_dim_t*") LongPointer kernel_size,
        @Cast("dnnl_dim_t*") LongPointer stride_size, @Cast("dnnl_dim_t*") LongPointer padding_l_size);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_dw(
        @Const dnnl_post_ops post_ops, int index,
        @Cast("dnnl_data_type_t*") IntBuffer weights_data_type, @Cast("dnnl_data_type_t*") IntBuffer bias_data_type,
        @Cast("dnnl_data_type_t*") IntBuffer dst_data_type, @Cast("dnnl_dim_t*") LongBuffer kernel_size,
        @Cast("dnnl_dim_t*") LongBuffer stride_size, @Cast("dnnl_dim_t*") LongBuffer padding_l_size);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_dw(
        @Const dnnl_post_ops post_ops, int index,
        @Cast("dnnl_data_type_t*") int[] weights_data_type, @Cast("dnnl_data_type_t*") int[] bias_data_type,
        @Cast("dnnl_data_type_t*") int[] dst_data_type, @Cast("dnnl_dim_t*") long[] kernel_size,
        @Cast("dnnl_dim_t*") long[] stride_size, @Cast("dnnl_dim_t*") long[] padding_l_size);

/** Appends a binary post-op.
 * 
 *  The kind of this post operation is #dnnl_binary.
 * 
 *  In the simplest case when the binary is the only post operation, the
 *  computations would be:
 * 
 *      dst[:] <- binary_op (dst[:], another_input[:])
 * 
 *  where binary_op is configured with the given parameters. binary_op supports
 *  broadcast semantics for a second operand.
 * 
 *  @param post_ops Post-ops.
 *  @param alg_kind Binary algorithm for the post-op.
 *  @param src1_desc Memory descriptor of a second operand.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_binary(dnnl_post_ops post_ops,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src1_desc);

/** Returns the parameters of a binary post-op.
 * 
 *  @param post_ops Post-ops.
 *  @param index Index of the binary post-op.
 *  @param alg_kind Output binary algorithm kind.
 *  @param src1_desc Output memory descriptor of a second operand.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 *  @return #dnnl_invalid_arguments if \p index does not refer to a binary
 *      post-op. */

///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntPointer alg_kind,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src1_desc);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntBuffer alg_kind,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src1_desc);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") int[] alg_kind,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src1_desc);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntPointer alg_kind,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src1_desc);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") IntBuffer alg_kind,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src1_desc);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_binary(
        @Const dnnl_post_ops post_ops, int index, @Cast("dnnl_alg_kind_t*") int[] alg_kind,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src1_desc);

/** Appends a prelu forward post-op.
 * 
 *  The kind of this post-op is #dnnl::primitive::kind::prelu.
 * 
 *  The post-op can be defined as:
 * 
 *       dst[:] <- prelu(dst[:], weights[:])
 *       prelu:
 *       dst[:] <- dst[:] if dst[:] > 0
 *       dst[:] <- dst[:] * weights[:] if dst[:] <= 0
 * 
 * 
 *  \note
 *      The order of dimensions does not depend on how elements are laid
 *      out in memory. For example:
 *      - for a 2D CNN activations tensor the order is always (n, c)
 *      - for a 4D CNN activations tensor the order is always (n, c, h, w)
 *      - for a 5D CNN weights tensor the order is always
 *         (g, oc, ic, kh, kw)
 * 
 *     Prelu weights tensor is passed in runtime execution phase. Prelu
 *     weights tensor data type is implicitly assumed as f32 using plain
 *     layout (a, ab, acb, acdb, acdeb)
 * 
 *  @param post_ops Post-ops.
 *  @param mask Defines the correspondence between the output tensor
 *      dimensions and the prelu weights tensor. The set i-th bit indicates
 *      that a dedicated weights value is used for each index along that
 *      dimension. Set the mask to 0 to use a common weights value
 *      for the whole output tensor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_append_prelu(
        dnnl_post_ops post_ops, int mask);

/** Returns the parameters of a prelu post-op.
 * 
 *  @param post_ops Post-ops.
 *  @param index Index of the prelu post-op.
 *  @param mask Mask of the prelu post-op.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_prelu(
        @Const dnnl_post_ops post_ops, int index, IntPointer mask);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_prelu(
        @Const dnnl_post_ops post_ops, int index, IntBuffer mask);
public static native @Cast("dnnl_status_t") int dnnl_post_ops_get_params_prelu(
        @Const dnnl_post_ops post_ops, int index, int[] mask);

/** \} dnnl_api_attributes
 <p>
 *  \} dnnl_api_primitives
 <p>
 *  \addtogroup dnnl_api_memory
 *  \{
 <p>
 *  Destroys a memory descriptor.
 * 
 *  @param memory_desc Memory descriptor to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_destroy(dnnl_memory_desc memory_desc);

/** Clones a memory descriptor. The resulting memory descriptor must be
 *  destroyed separately.
 * 
 *  @param memory_desc Output memory descriptor.
 *  @param existing_memory_desc Memory descriptor to clone.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_clone(@ByPtrPtr dnnl_memory_desc memory_desc,
        @Const dnnl_memory_desc existing_memory_desc);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_clone(@Cast("dnnl_memory_desc_t*") PointerPointer memory_desc,
        @Const dnnl_memory_desc existing_memory_desc);

/** Creates a memory descriptor using dimensions and strides.
 * 
 *  \note
 *      As always, the logical order of dimensions corresponds to the {@code abc...}
 *      format tag, and the physical meaning of the dimensions depends on both
 *      the primitive that consumes the memory and the context of that
 *      consumption.
 * 
 *  @param memory_desc Output memory descriptor.
 *  @param ndims Number of dimensions
 *  @param dims Array of dimensions.
 *  @param data_type Elements data type.
 *  @param strides Strides in each dimension.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongPointer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongBuffer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") long[] strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongPointer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") LongBuffer strides);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_strides(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("const int64_t*") long[] strides);

/** Creates a memory descriptor using dimensions and memory format tag.
 * 
 *  \note
 *      As always, the logical order of dimensions corresponds to the {@code abc...}
 *      format tag, and the physical meaning of the dimensions depends on both
 *      the primitive that consumes the memory and the context of that
 *      consumption.
 * 
 *  @param memory_desc Output memory descriptor.
 *  @param ndims Number of dimensions
 *  @param dims Array of dimensions.
 *  @param data_type Elements data type.
 *  @param tag Memory format tag. Can be #dnnl_format_tag_any which would
 *      allow a primitive to chose the final memory format. In this case the
 *      format_kind field of the memory descriptor would be set to
 *      #dnnl_format_kind_any.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") LongPointer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @ByPtrPtr dnnl_memory_desc memory_desc, int ndims, @Cast("const int64_t*") LongBuffer dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_with_tag(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc, int ndims, @Cast("const int64_t*") long[] dims,
        @Cast("dnnl_data_type_t") int data_type, @Cast("dnnl_format_tag_t") int tag);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Creates a memory descriptor for a region inside an area
 *  described by an existing memory descriptor.
 * 
 *  \warning
 *      Some combinations of physical memory layout and/or offsets or dims may
 *      result in a failure to create a submemory. */
//
/** @param memory_desc Output memory descriptor.
/** @param parent_memory_desc An existing memory descriptor.
/** @param dims Sizes of the region.
/** @param offsets Offsets to the region from the encompassing
/**     memory object in each dimension
/** @return #dnnl_success on success and a status describing the error
/**     otherwise. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @ByPtrPtr dnnl_memory_desc memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") LongPointer dims,
        @Cast("const int64_t*") LongPointer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") LongBuffer dims,
        @Cast("const int64_t*") LongBuffer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @ByPtrPtr dnnl_memory_desc memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") long[] dims,
        @Cast("const int64_t*") long[] offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") LongPointer dims,
        @Cast("const int64_t*") LongPointer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @ByPtrPtr dnnl_memory_desc memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") LongBuffer dims,
        @Cast("const int64_t*") LongBuffer offsets);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_create_submemory(
        @Cast("dnnl_memory_desc_t*") PointerPointer memory_desc,
        @Const dnnl_memory_desc parent_memory_desc, @Cast("const int64_t*") long[] dims,
        @Cast("const int64_t*") long[] offsets);

/** Creates a memory descriptor by reshaping an existing one. The new
 *  memory descriptor inherits the data type. This operation is valid only for
 *  memory descriptors that have format_kind #dnnl_blocked or
 *  #dnnl_format_kind_any.
 * 
 *  The resulting memory descriptor must be destroyed separately.
 * 
 *  The operation ensures the transformation of the physical memory format
 *  corresponds to the transformation of the logical dimensions. If such
 *  transformation is impossible, the function returns #dnnl_invalid_arguments.
 * 
 *  The reshape operation can be described as a combination of the following
 *  basic operations:
 *  1. Add a dimension of size {@code 1}. This is always possible.
 *  2. Remove a dimension of size {@code 1}. This is possible only if the dimension
 *     has no padding (i.e. {@code padded_dims[dim] == dims[dim] && dims[dim] == 1}).
 *  3. Split a dimension into multiple ones. This is possible only if the size
 *     of the dimension is exactly equal to the product of the split ones and
 *     the dimension does not have padding (i.e.
 *     {@code padded_dims[dim] = dims[dim]}).
 *  4. Joining multiple consecutive dimensions into a single one. As in the
 *     cases above, this requires that the dimensions do not have padding and
 *     that the memory format is such that in physical memory these dimensions
 *     are dense and have the same order as their logical counterparts. This
 *     also assumes that these dimensions are not blocked.
 *     - Here, dense means:
 *       {@code stride for dim[i] == (stride for dim[i + 1]) * dim[i + 1]};
 *     - And same order means:
 *       {@code i < j} if and only if {@code stride for dim[j] <= stride for dim[i]}.
 * 
 *  \warning
 *      Some combinations of physical memory layout and/or offsets or
 *      dimensions may result in a failure to make a reshape.
 * 
 *  @param out_memory_desc Output memory descriptor.
 *  @param in_memory_desc An existing memory descriptor. Must have format_kind
 *      set to #dnnl_blocked or #dnnl_format_kind_any.
 *  @param ndims Number of dimensions for the output memory descriptor.
 *  @param dims Dimensions for the output memory descriptor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") LongPointer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") LongBuffer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") long[] dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") LongPointer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") LongBuffer dims);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_reshape(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, int ndims,
        @Cast("const int64_t*") long[] dims);

/** Creates a memory descriptor by permuting axes in an existing one.
 * 
 *  The physical memory layout representation is adjusted accordingly to
 *  maintain the consistency between the logical and physical parts of the
 *  memory descriptor.
 * 
 *  The resulting memory descriptor must be destroyed separately.
 * 
 *  The new memory descriptor inherits the data type. This operation is valid
 *  only for memory descriptors that have format_kind set to #dnnl_blocked or
 *  #dnnl_format_kind_any.
 * 
 *  The logical axes will be permuted in the following manner:
 *  <pre>{@code
 *  for (i: 0 .. in_memory_desc->ndims)
 *      out_memory_desc->dims[permutation[i]] = in_memory_desc->dims[i];
 *  }</pre>
 * 
 *  Example:
 *  <pre>{@code
 *      dnnl_memory_desc_t in_md, out_md, expect_out_md;
 * 
 *      const int permutation[] = {1, 0}; // swap the first and the second axes
 * 
 *      dnnl_dims_t in_dims = {2, 3}, out_dims = {3, 2};
 *      dnnl_format_tag_t in_tag = dnnl_ab, out_tag = dnnl_ba;
 * 
 *      dnnl_memory_desc_create_with_tag(
 *              &in_md, 2, in_dims, data_type, in_tag);
 *      dnnl_memory_desc_create_with_tag(
 *              &expect_out_md, 2, out_dims, data_type, out_tag);
 * 
 *      dnnl_memory_desc_permute_axes(&out_md, in_md, permutation);
 *      assert(dnnl_memory_desc_equal(out_md, expect_out_md));
 * 
 *      dnnl_memory_desc_destroy(in_md);
 *      dnnl_memory_desc_destroy(out_md);
 *      dnnl_memory_desc_destroy(expect_out_md);
 *  }</pre>
 * 
 *  @param out_memory_desc Output memory descriptor.
 *  @param in_memory_desc An existing memory descriptor. Must have format_kind
 *      set to #dnnl_blocked or #dnnl_format_kind_any.
 *  @param permutation Axes permutation (of size {@code in_memory_desc->ndims}).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const IntPointer permutation);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const IntBuffer permutation);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const int[] permutation);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const IntPointer permutation);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @ByPtrPtr dnnl_memory_desc out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const IntBuffer permutation);
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_permute_axes(
        @Cast("dnnl_memory_desc_t*") PointerPointer out_memory_desc,
        @Const dnnl_memory_desc in_memory_desc, @Const int[] permutation);

/** Queries a memory descriptor for various pieces of information.
 * 
 *  The following information can be queried:
 *   - Number of dimensions (#dnnl_query_ndims_s32)
 *   - Dimensions (#dnnl_query_dims) in the following order:
 *     - CNN data tensors: mini-batch, channel, spatial
 *       (<code>{N, C, [[D,] H,] W}</code>)
 *     - CNN weight tensors: group (optional), output channel, input channel,
 *       spatial (<code>{[G,] O, I, [[D,] H,] W}</code>)
 *     - RNN data tensors: time, mini-batch, channels (<code>{T, N, C}</code>)
 *       or layers, directions, states, mini-batch, channels
 *       (<code>{L, D, S, N, C}</code>)
 *     - RNN weight tensor: layers, directions, input channel, gates, output
 *       channels (<code>{L, D, I, G, O}</code>)
 *   - Data type of the tensor elements (#dnnl_query_data_type)
 *   - Padded dimensions (#dnnl_query_padded_dims) - size of the data including
 *     padding in each dimension
 *   - Padded offsets (#dnnl_query_padded_offsets) - per-dimension offset from
 *     the padding to actual data, the top-level tensor with offsets applied
 *     must lie within the padding area.
 *   - Submemory offset (#dnnl_query_submemory_offset_s64) - offset from memory
 *     origin to the current block, non-zero only in a description of a memory
 *     sub-block.
 *   - Format kind (#dnnl_query_format_kind) - memory format kind
 * 
 *  \note
 *     The order of dimensions does not depend on the memory format, so
 *     whether the data is laid out in #dnnl_nchw or #dnnl_nhwc
 *     the dims for 4D CN data tensor would be <code>{N, C, H, W}</code>.
 * 
 *  The following queries are applicable only to format kind #dnnl_blocked.
 *   - Strides (#dnnl_query_strides) between the outermost blocks or in case
 *     of plain (non-blocked) formats the strides between dimensions
 *   - Number of innermost blocks (#dnnl_query_inner_nblks_s32), e.g.
 *     {@code {4, 16, 4}} in case of {@code OIhw_4i16o4i}
 *   - Size of the innermost blocks (#dnnl_query_inner_blks), e.g. 3 in case
 *     of {@code OIhw_4i16o4i_}
 *   - Logical indices of the blocks (#dnnl_query_inner_idxs), e.g. {@code {1, 0, 1}}
 *     in case of {@code 4i16o4i}, because {@code i} is the 1st dim and {@code o} is the 0st dim
 * 
 *  @param memory_desc Memory descriptor.
 *  @param what Parameter to query.
 *  @param result Output result. The type depends on the query. For example,
 *      it must be a \c dnnl_dims_t** if querying for a strides.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_desc_query(
        @Const dnnl_memory_desc memory_desc, @Cast("dnnl_query_t") int what, Pointer result);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Compares two memory descriptors.
 * 
 *  Use this function to identify whether a reorder is required between the
 *  two memories
 * 
 *  @param lhs Left-hand side of the comparison.
 *  @param rhs Right-hand side of the comparison.
 *  @return 1 if the descriptors are the same.
 *  @return 0 if the descriptors are different. */

///
public static native int dnnl_memory_desc_equal(
        @Const dnnl_memory_desc lhs, @Const dnnl_memory_desc rhs);

/** Returns the size of a memory descriptor.
 * 
 *  @param memory_desc Memory descriptor.
 *  @return The number of bytes required for memory described by a memory
 *      descriptor. */

///
public static native @Cast("size_t") long dnnl_memory_desc_get_size(@Const dnnl_memory_desc memory_desc);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Returns the size of data type.
 * 
 *  @param data_type Data type.
 *  @return The number of bytes occupied by data type. */

///
///
///
public static native @Cast("size_t") long dnnl_data_type_size(@Cast("dnnl_data_type_t") int data_type);

/** Creates a memory object.
 * 
 *  Unless \p handle is equal to DNNL_MEMORY_NONE, the constructed memory
 *  object will have the underlying buffer set. In this case, the buffer will
 *  be initialized as if dnnl_memory_set_data_handle() had been called.
 * 
 *  @see dnnl_memory_set_data_handle()
 * 
 *  @param memory Output memory object.
 *  @param memory_desc Memory descriptor.
 *  @param engine Engine to use.
 *  @param handle Handle of the memory buffer to use as an underlying storage.
 *      - A pointer to the user-allocated buffer. In this case the library
 *        doesn't own the buffer.
 *      - The DNNL_MEMORY_ALLOCATE special value. Instructs the library to
 *        allocate the buffer for the memory object. In this case the library
 *        owns the buffer.
 *      - DNNL_MEMORY_NONE to create dnnl_memory without an underlying buffer.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_create(@ByPtrPtr dnnl_memory memory,
        @Const dnnl_memory_desc memory_desc, dnnl_engine engine,
        Pointer handle);
public static native @Cast("dnnl_status_t") int dnnl_memory_create(@Cast("dnnl_memory_t*") PointerPointer memory,
        @Const dnnl_memory_desc memory_desc, dnnl_engine engine,
        Pointer handle);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Returns the memory descriptor for a memory object.
 * 
 *  @param memory Memory object.
 *  @param memory_desc Output memory descriptor (a copy).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_get_memory_desc(
        @Const dnnl_memory memory, @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc memory_desc);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_memory_desc(
        @Const dnnl_memory memory, @Cast("const_dnnl_memory_desc_t*") PointerPointer memory_desc);

/** Returns the engine of a memory object.
 * 
 *  @param memory Memory object.
 *  @param engine Output engine on which the memory is located.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_get_engine(
        @Const dnnl_memory memory, @ByPtrPtr dnnl_engine engine);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_engine(
        @Const dnnl_memory memory, @Cast("dnnl_engine_t*") PointerPointer engine);

/** Maps a memory object and returns a host-side pointer to a memory buffer
 *  with a copy of its contents.
 * 
 *  Mapping enables explicit direct access to memory contents for the engines
 *  that do not support it implicitly.
 * 
 *  Mapping is an exclusive operation - a memory object cannot be used in
 *  other operations until this memory object is unmapped.
 * 
 *  \note
 *      Any primitives working with \p memory should be completed before
 *      the memory is mapped. Use dnnl_stream_wait to synchronize the
 *      corresponding execution stream.
 * 
 *  \note
 *      The dnnl_memory_map_data() and dnnl_memory_unmap_data() functions are
 *      mainly provided for debug and testing purposes, and their performance
 *      may be suboptimal.
 * 
 *  @param memory Memory object.
 *  @param mapped_ptr Output pointer to the mapped buffer.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_memory_map_data(
        @Const dnnl_memory memory, @Cast("void**") PointerPointer mapped_ptr);
public static native @Cast("dnnl_status_t") int dnnl_memory_map_data(
        @Const dnnl_memory memory, @Cast("void**") @ByPtrPtr Pointer mapped_ptr);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Unmaps a memory object and writes back any changes made to the previously
 *  mapped memory buffer. The pointer to the mapped buffer must be obtained
 *  via the dnnl_memory_map_data() call.
 * 
 *  \note
 *      The dnnl_memory_map_data() and dnnl_memory_unmap_data() functions are
 *      mainly provided for debug and testing purposes, and their performance
 *      may be suboptimal.
 * 
 *  @param memory Memory object.
 *  @param mapped_ptr Pointer to the mapped buffer that must have been
 *      obtained using the dnnl_memory_map_data() function.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_unmap_data(
        @Const dnnl_memory memory, Pointer mapped_ptr);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Returns memory object's data handle.
 * 
 *  @param memory Memory object.
 *  @param handle Output data handle. For the CPU engine, the data handle is a
 *      pointer to the actual data. For OpenCL it is a cl_mem.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_get_data_handle(
        @Const dnnl_memory memory, @Cast("void**") PointerPointer handle);
public static native @Cast("dnnl_status_t") int dnnl_memory_get_data_handle(
        @Const dnnl_memory memory, @Cast("void**") @ByPtrPtr Pointer handle);

/** Sets the underlying memory buffer.
 * 
 *  @param memory Memory object.
 *  @param handle Data handle. For the CPU engine or when USM is used, the
 *      memory buffer is a pointer to the actual data. For OpenCL it is a
 *      {@code cl_mem}.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_set_data_handle(
        dnnl_memory memory, Pointer handle);

// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

/** Destroys a memory object.
 * 
 *  @param memory Memory object to destroy.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_memory_destroy(dnnl_memory memory);

/** \} dnnl_api_memory
 <p>
 *  \addtogroup dnnl_api_primitives
 *  \{
 <p>
 *  \addtogroup dnnl_api_reorder
 *  \{
 <p>
 *  Creates a primitive descriptor for a reorder primitive.
 * 
 *  @param reorder_primitive_desc Output primitive descriptor.
 *  @param src_desc Source memory descriptor.
 *  @param src_engine Engine on which the source memory object will be
 *      located.
 *  @param dst_desc Destination memory descriptor.
 *  @param dst_engine Engine on which the destination memory object
 *      will be located.
 *  @param attr Primitive attributes to use (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_reorder_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc reorder_primitive_desc,
        @Const dnnl_memory_desc src_desc, dnnl_engine src_engine,
        @Const dnnl_memory_desc dst_desc, dnnl_engine dst_engine,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_reorder_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer reorder_primitive_desc,
        @Const dnnl_memory_desc src_desc, dnnl_engine src_engine,
        @Const dnnl_memory_desc dst_desc, dnnl_engine dst_engine,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_reorder
 <p>
 *  \addtogroup dnnl_api_concat
 *  \{
 <p>
 *  Creates a primitive descriptor for an out-of-place concatenation
 *  primitive.
 * 
 *  @param concat_primitive_desc Output primitive descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param n Number of source parameters.
 *  @param concat_dimension Source tensors will be concatenated over
 *      dimension with this index. Note that order of dimensions does
 *      not depend on memory format.
 *  @param src_descs Array of source memory descriptors with \p n elements.
 *  @param attr Primitive attributes to use (can be NULL).
 *  @param engine Engine to use.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_concat_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc concat_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, int concat_dimension,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_concat_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer concat_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, int concat_dimension,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src_descs,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_concat
 <p>
 *  \addtogroup dnnl_api_sum
 *  \{
 <p>
 *  Creates a primitive descriptor for an (out-of-place) sum primitive.
 * 
 *  @param sum_primitive_desc Output primitive descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param n Number of source parameters.
 *  @param scales Vector of scales to multiply data in each source
 *      memory by.
 *  @param src_descs Array of source memory descriptors having \p n elements.
 *  @param attr Primitive attributes to use (can be NULL).
 *  @param engine Engine to use.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const FloatPointer scales,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const FloatBuffer scales,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const float[] scales,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const FloatPointer scales,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const FloatBuffer scales,
        @Cast("const_dnnl_memory_desc_t*") @ByPtrPtr dnnl_memory_desc src_descs,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_sum_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer sum_primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc dst_desc, int n, @Const float[] scales,
        @Cast("const_dnnl_memory_desc_t*") PointerPointer src_descs,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_sum
 <p>
 *  \addtogroup dnnl_api_binary
 *  \{
 <p>
 *  Creates a primitive descriptor for a binary primitive.
 * 
 *  \note
 *      Memory descriptors \p src1_desc and \p dst_desc are alloweded to be
 *      initialized with #dnnl_format_tag_any or with format_kind set to
 *      #dnnl_format_kind_any.
 * 
 *  \note
 *      Both memory descriptors must have the same number of dimensions.
 *      Element broadcasting is supported for memory descriptor \p src1_desc
 *      and are applied to \p src1_desc dimensions that have size equal to 1.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Algorithm kind. Valid values are #dnnl_binary_add,
 *      #dnnl_binary_mul, #dnnl_binary_max, #dnnl_binary_min, #dnnl_binary_div,
 *      #dnnl_binary_sub, #dnnl_binary_ge, #dnnl_binary_gt, #dnnl_binary_le,
 *      #dnnl_binary_lt, #dnnl_binary_eq and #dnnl_binary_ne.
 *  @param src0_desc Source 0 memory descriptor.
 *  @param src1_desc Source 1 memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_binary_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src0_desc,
        @Const dnnl_memory_desc src1_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_binary_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src0_desc,
        @Const dnnl_memory_desc src1_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_binary
 <p>
 *  \addtogroup dnnl_api_convolution
 *  \{
 <p>
 *  Creates a primitive descriptor for a convolution forward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind Convolution algorithm. Possible values are
 *      #dnnl_convolution_direct, #dnnl_convolution_winograd,
 *      #dnnl_convolution_auto.
 *  @param src_desc Source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param bias_desc Bias memory descriptor. Passing NULL, a zero memory
 *      descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param dst_desc Destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a convolution backward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Convolution algorithm. Possible values are
 *      #dnnl_convolution_direct, #dnnl_convolution_winograd,
 *      #dnnl_convolution_auto.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a convolution weights gradient primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Convolution algorithm. Possible values are
 *      #dnnl_convolution_direct, #dnnl_convolution_winograd,
 *      #dnnl_convolution_auto.
 *  @param src_desc Source memory descriptor.
 *  @param diff_weights_desc Diff weights memory descriptor.
 *  @param diff_bias_desc Diff bias memory descriptor. Passing NULL, a zero
 *      memory descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_convolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_convolution
 <p>
 *  \addtogroup dnnl_api_deconvolution
 *  \{
 <p>
 *  Creates a primitive descriptor for a deconvolution forward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind Deconvolution algorithm. Possible values are
 *      #dnnl_deconvolution_direct, #dnnl_deconvolution_winograd.
 *  @param src_desc Source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param bias_desc Bias memory descriptor. Passing NULL, a zero memory
 *      descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param dst_desc Destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer dilates,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer dilates,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] dilates,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a deconvolution backward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Deconvolution algorithm. Possible values are
 *      #dnnl_deconvolution_direct, #dnnl_deconvolution_winograd.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a deconvolution weights gradient
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  Arrays \p strides, \p dilates, \p padding_l, and \p padding_r contain
 *  values for spatial dimensions only and hence must have the same number of
 *  elements as there are spatial dimensions. The order of values is the same
 *  as in the tensor: depth (for 3D tensors), height (for 3D and 2D tensors),
 *  and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Deconvolution algorithm. Possible values are
 *      #dnnl_deconvolution_direct, #dnnl_deconvolution_winograd.
 *  @param src_desc Source memory descriptor.
 *  @param diff_weights_desc Diff weights memory descriptor.
 *  @param diff_bias_desc Diff bias memory descriptor. Passing NULL, a zero
 *      memory descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param dilates Array of dilations for spatial dimension. A zero value
 *      means no dilation in the corresponding dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer dilates, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer dilates, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_deconvolution_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] dilates, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_deconvolution
 <p>
 *  \addtogroup dnnl_api_shuffle
 *  \{
 <p>
 *  Creates a primitive descriptor for a shuffle forward propagation primitive
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param axis The axis along which the data is shuffled.
 *  @param group_size Shuffle group size.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_shuffle_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, int axis, @Cast("dnnl_dim_t") long group_size,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_shuffle_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, int axis, @Cast("dnnl_dim_t") long group_size,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a shuffle backward propagation primitive
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param axis The axis along which the data is shuffled.
 *  @param group_size Shuffle group size.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_shuffle_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, int axis, @Cast("dnnl_dim_t") long group_size,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_shuffle_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, int axis, @Cast("dnnl_dim_t") long group_size,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_shuffle
 <p>
 *  \addtogroup dnnl_api_eltwise
 *  \{
 <p>
 *  Creates a primitive descriptor for an eltwise forward propagation primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind Elementwise algorithm kind.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param alpha The alpha parameter for the elementwise operation. Specific
 *      meaning depends on the algorithm.
 *  @param beta The beta parameter for the elementwise operation. Specific
 *      meaning depends on the algorithm.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_eltwise_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        float alpha, float beta, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_eltwise_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        float alpha, float beta, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an eltwise backward propagation
 *      primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Elementwise algorithm kind.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param data_desc Destination memory descriptor if one of the
 *      "use_dst_for_bwd" algorithms are used (such as
 *      #dnnl_eltwise_relu_use_dst_for_bwd), source memory descriptor otherwise.
 *  @param alpha The alpha parameter for the elementwise operation. Specific
 *      meaning depends on the algorithm.
 *  @param beta The beta parameter for the elementwise operation. Specific
 *      meaning depends on the algorithm.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_eltwise_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc data_desc, float alpha, float beta,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_eltwise_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc data_desc, float alpha, float beta,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_eltwise
 <p>
 *  \addtogroup dnnl_api_softmax
 *  \{
 <p>
 *  Creates a primitive descriptor for a softmax forward propagation primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind Softmax algorithm kind: either #dnnl_softmax_accurate, or
 *      #dnnl_softmax_log.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param softmax_axis Axis over which softmax is computed.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_softmax_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        int softmax_axis, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_softmax_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        int softmax_axis, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a softmax backward propagation primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Softmax algorithm kind: either #dnnl_softmax_accurate, or
 *      #dnnl_softmax_log.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param softmax_axis Axis over which softmax is computed.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_softmax_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc dst_desc, int softmax_axis,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_softmax_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc dst_desc, int softmax_axis,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_softmax
 <p>
 *  \addtogroup dnnl_api_pooling
 *  \{
 <p>
 *  Creates a primitive descriptor for a pooling forward propagation
 *      primitive.
 * 
 *  Arrays \p strides, \p kernel, \p dilation, \p padding_l and \p padding_r
 *  contain values for spatial dimensions only and hence must have the same
 *  number of elements as there are spatial dimensions. The order of values
 *  is the same as in the tensor: depth (for 3D tensors),
 *  height (for 3D and 2D tensors), and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind Pooling algorithm kind: either #dnnl_pooling_max,
 *      #dnnl_pooling_avg_include_padding, or #dnnl_pooling_avg_exclude_padding.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param kernel Array of kernel spatial dimensions.
 *  @param dilation Array of dilations for spatial dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer kernel,
        @Cast("const int64_t*") LongPointer dilation, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer kernel,
        @Cast("const int64_t*") LongBuffer dilation, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] kernel,
        @Cast("const int64_t*") long[] dilation, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongPointer strides, @Cast("const int64_t*") LongPointer kernel,
        @Cast("const int64_t*") LongPointer dilation, @Cast("const int64_t*") LongPointer padding_l,
        @Cast("const int64_t*") LongPointer padding_r, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") LongBuffer strides, @Cast("const int64_t*") LongBuffer kernel,
        @Cast("const int64_t*") LongBuffer dilation, @Cast("const int64_t*") LongBuffer padding_l,
        @Cast("const int64_t*") LongBuffer padding_r, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("const int64_t*") long[] strides, @Cast("const int64_t*") long[] kernel,
        @Cast("const int64_t*") long[] dilation, @Cast("const int64_t*") long[] padding_l,
        @Cast("const int64_t*") long[] padding_r, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a pooling backward propagation
 *      primitive.
 * 
 *  Arrays \p strides, \p kernel, \p dilation, \p padding_l and \p padding_r
 *  contain values for spatial dimensions only and hence must have the same
 *  number of elements as there are spatial dimensions. The order of values
 *  is the same as in the tensor: depth (for 3D tensors),
 *  height (for 3D and 2D tensors), and width.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind Pooling algorithm kind: either #dnnl_pooling_max,
 *      #dnnl_pooling_avg_include_padding, or #dnnl_pooling_avg_exclude_padding.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param strides Array of strides for spatial dimension.
 *  @param kernel Array of kernel spatial dimensions.
 *  @param dilation Array of dilations for spatial dimension.
 *  @param padding_l Array of padding values for low indices for each spatial
 *      dimension {@code ([[front,] top,] left)}.
 *  @param padding_r Array of padding values for high indices for each spatial
 *      dimension {@code ([[back,] bottom,] right)}. Can be NULL in which case
 *      padding is considered to be symmetrical.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer kernel, @Cast("const int64_t*") LongPointer dilation,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer kernel, @Cast("const int64_t*") LongBuffer dilation,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] kernel, @Cast("const int64_t*") long[] dilation,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongPointer strides,
        @Cast("const int64_t*") LongPointer kernel, @Cast("const int64_t*") LongPointer dilation,
        @Cast("const int64_t*") LongPointer padding_l, @Cast("const int64_t*") LongPointer padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") LongBuffer strides,
        @Cast("const int64_t*") LongBuffer kernel, @Cast("const int64_t*") LongBuffer dilation,
        @Cast("const int64_t*") LongBuffer padding_l, @Cast("const int64_t*") LongBuffer padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_pooling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc, @Cast("const int64_t*") long[] strides,
        @Cast("const int64_t*") long[] kernel, @Cast("const int64_t*") long[] dilation,
        @Cast("const int64_t*") long[] padding_l, @Cast("const int64_t*") long[] padding_r,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_pooling
 <p>
 *  \addtogroup dnnl_api_prelu
 *  \{
 <p>
 *  Creates a primitive descriptor for a PReLU (leaky ReLU with trainable
 *      alpha parameter) forward propagation primitive.
 * 
 *  \note
 *      weights descriptor is allowed to be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param weights_desc Alpha parameters memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_prelu_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_prelu_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a PReLU (leaky ReLU with trainable
 *      alpha parameter) backward propagation primitive.
 * 
 *  \note
 *      weights descriptor and diff_weights descriptor are allowed
 *      to be initialized with #dnnl_format_tag_any or with format_kind
 *      set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param src_desc Source memory descriptor.
 *  @param weights_desc Alpha parameters memory descriptor.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_weights_desc Diff alpha parameters memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_prelu_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_prelu_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_prelu
 <p>
 *  \addtogroup dnnl_api_lrn
 *  \{
 <p>
 *  Creates a primitive descriptor for an LRN forward propagation primitive.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind LRN algorithm kind: either #dnnl_lrn_across_channels or
 *      #dnnl_lrn_within_channel.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param local_size Regularization local size.
 *  @param alpha The alpha regularization parameter.
 *  @param beta The beta regularization parameter.
 *  @param k The k regularization parameter.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_lrn_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("dnnl_dim_t") long local_size, float alpha, float beta, float k,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lrn_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc dst_desc,
        @Cast("dnnl_dim_t") long local_size, float alpha, float beta, float k,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an LRN backward propagation primitive.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind LRN algorithm kind: either #dnnl_lrn_across_channels or
 *      #dnnl_lrn_within_channel.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param src_desc Source memory descriptor.
 *  @param local_size Regularization local size.
 *  @param alpha The alpha regularization parameter.
 *  @param beta The beta regularization parameter.
 *  @param k The k regularization parameter.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_lrn_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Cast("dnnl_dim_t") long local_size, float alpha,
        float beta, float k, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lrn_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Cast("dnnl_dim_t") long local_size, float alpha,
        float beta, float k, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_lrn
 <p>
 *  \addtogroup dnnl_api_batch_normalization
 *  \{
 <p>
 *  Creates a primitive descriptor for a batch normalization forward propagation
 *      primitive.
 * 
 *  \note
 *      In-place operation is supported: the dst can refer to the same memory
 *      as the src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param epsilon Batch normalization epsilon parameter.
 *  @param flags Batch normalization flags (\ref dnnl_normalization_flags_t).
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, float epsilon, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, float epsilon, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a batch normalization backward
 *      propagation primitive.
 * 
 *  \note
 *      In-place operation is supported: the diff_dst can refer to the same
 *      memory as the diff_src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_backward_data and #dnnl_backward (diffs for all parameters are
 *      computed in this case).
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param src_desc Source memory descriptor.
 *  @param epsilon Batch normalization epsilon parameter.
 *  @param flags Batch normalization flags (\ref dnnl_normalization_flags_t).
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, float epsilon, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_batch_normalization_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, float epsilon, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_batch_normalization
 <p>
 *  \addtogroup dnnl_api_group_normalization
 *  \{
 <p>
 *  Creates a primitive descriptor for a group normalization forward propagation
 *      primitive.
 * 
 *  \note
 *      In-place operation is supported: the dst can refer to the same memory
 *      as the src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param groups Group normalization groups parameter.
 *  @param epsilon Group normalization epsilon parameter.
 *  @param flags Group normalization flags (\ref dnnl_normalization_flags_t).
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_group_normalization_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Cast("dnnl_dim_t") long groups, float epsilon,
        @Cast("unsigned") int flags, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_group_normalization_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Cast("dnnl_dim_t") long groups, float epsilon,
        @Cast("unsigned") int flags, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a group normalization backward
 *      propagation primitive.
 * 
 *  \note
 *      In-place operation is supported: the diff_dst can refer to the same
 *      memory as the diff_src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_backward_data and #dnnl_backward (diffs for all parameters are
 *      computed in this case).
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param src_desc Source memory descriptor.
 *  @param groups Group normalization groups parameter.
 *  @param epsilon Group normalization epsilon parameter.
 *  @param flags Group normalization flags (\ref dnnl_normalization_flags_t).
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_group_normalization_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Cast("dnnl_dim_t") long groups, float epsilon,
        @Cast("unsigned") int flags, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_group_normalization_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Cast("dnnl_dim_t") long groups, float epsilon,
        @Cast("unsigned") int flags, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_group_normalization
 <p>
 *  \addtogroup dnnl_api_layer_normalization
 *  \{
 <p>
 *  Creates a primitive descriptor for a layer normalization forward propagation
 *      primitive.
 * 
 *  \note
 *      In-place operation is supported: the dst can refer to the same memory
 *      as the src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param stat_desc Memory descriptor for mean and variance. If this
 *      parameter is NULL, a zero memory descriptor, or a memory descriptor
 *      with format_kind set to #dnnl_format_kind_undef, then the memory
 *      descriptor for stats is derived from \p src_desc by removing the last
 *      dimension.
 *  @param epsilon Layer normalization epsilon parameter.
 *  @param flags Layer normalization flags (\ref dnnl_normalization_flags_t).
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_memory_desc stat_desc,
        float epsilon, @Cast("unsigned") int flags, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_memory_desc stat_desc,
        float epsilon, @Cast("unsigned") int flags, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a layer normalization backward
 *      propagation primitive.
 * 
 *  \note
 *      In-place operation is supported: the diff_dst can refer to the same
 *      memory as the diff_src.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_backward_data and #dnnl_backward (diffs for all parameters are
 *      computed in this case).
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param src_desc Source memory descriptor.
 *  @param stat_desc Memory descriptor for mean and variance. If this
 *      parameter is NULL, a zero memory descriptor, or a memory descriptor
 *      with format_kind set to #dnnl_format_kind_undef, then the memory
 *      descriptor for stats is derived from \p src_desc by removing the last
 *      dimension.
 *  @param epsilon Layer normalization epsilon parameter.
 *  @param flags Layer normalization flags (\ref dnnl_normalization_flags_t).
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc stat_desc,
        float epsilon, @Cast("unsigned") int flags, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_layer_normalization_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_memory_desc src_desc, @Const dnnl_memory_desc stat_desc,
        float epsilon, @Cast("unsigned") int flags, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_layer_normalization
 <p>
 *  \addtogroup dnnl_api_inner_product
 *  \{
 <p>
 *  Creates a primitive descriptor for an inner product forward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param src_desc Source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param bias_desc Bias memory descriptor. Passing NULL, a zero memory
 *      descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param dst_desc Destination memory descriptor.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_inner_product_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an inner product backward propagation
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param weights_desc Weights memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_data_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_data_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an inner product  weights gradient
 *      primitive.
 * 
 *  \note
 *      Memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive_descriptor.
 *  @param engine Engine to use.
 *  @param src_desc Source memory descriptor.
 *  @param diff_weights_desc Diff weights memory descriptor.
 *  @param diff_bias_desc Diff bias memory descriptor. Passing NULL, a zero
 *      memory descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_weights_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_inner_product_backward_weights_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc diff_weights_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_inner_product
 <p>
 *  \addtogroup dnnl_api_attributes
 *  \{
 <p>
 *  Set quantization scale and shift parameters for RNN data tensors.
 * 
 *  For performance reasons, the low-precision configuration of the RNN
 *  primitives expects input activations to have the unsigned 8-bit integer
 *  data type. The scale and shift parameters are used to quantize
 *  floating-point data to unsigned integer and must be passed to the RNN
 *  primitive using attributes.
 * 
 *  The quantization formula is {@code scale * data + shift}.
 * 
 *  \note
 *      Quantization scale and shift are common for src_layer, src_iter,
 *      dst_iter, and dst_layer.
 * 
 *  Example usage:
 *  <pre>{@code
 *      // RNN parameters
 *      int l = 2, t = 2, mb = 32, sic = 32, slc = 32, dic = 32, dlc = 32;
 *      // Activations quantization parameters
 *      float scale = 63.f, shift = 64.f;
 * 
 *      dnnl_primitive_attr_t rnn_attr;
 *      // Create default attributes
 *      dnnl_primitive_attr_create(&rnn_attr);
 * 
 *      // Set scale and shift for int8 quantization of activation
 *      dnnl_primitive_attr_set_rnn_data_qparams(rnn_attr, scale, shift);
 * 
 *      // Create an RNN primitive descriptor.
 *      dnnl_primitive_desc_t rnn_pd;
 *      dnnl_vanilla_rnn_forward_primitive_desc_create(&rnn_pd,
 *              engine, /* arguments * /, attr);
 *  }</pre>
 * 
 *  @param attr Primitive attributes.
 *  @param scale The value to scale the data by.
 *  @param shift The value to shift the data by.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_data_qparams(
        dnnl_primitive_attr attr, float scale, float shift);

/** Returns the quantization scale and shift parameters for RNN data tensors.
 * 
 *  \note
 *      Quantization scale and shift are common for src_layer, src_iter,
 *      dst_iter, and dst_layer.
 * 
 *  @param attr Primitive attributes.
 *  @param scale The value to scale the data by.
 *  @param shift The value to shift the data by.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_data_qparams(
        @Const dnnl_primitive_attr attr, FloatPointer scale, FloatPointer shift);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_data_qparams(
        @Const dnnl_primitive_attr attr, FloatBuffer scale, FloatBuffer shift);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_data_qparams(
        @Const dnnl_primitive_attr attr, float[] scale, float[] shift);

/** Sets quantization scaling factors for RNN weights tensors. The
 *  low-precision configuration of the RNN primitives expects input weights to
 *  use the signed 8-bit integer data type. The scaling factors are used to
 *  quantize floating-point data to signed integer and must be passed to RNN
 *  primitives using attributes.
 * 
 *  \note
 *      The dimension order is always native and does not depend on the actual
 *      layout used. For example, five-dimensional weights always have (l, d,
 *      i, g, o) logical dimension ordering.
 * 
 *  \note
 *      Quantization scales are common for weights_layer and weights_iteration
 * 
 *  @param attr Primitive attributes.
 *  @param count Number of elements in the \p scales array.
 *  @param mask Scaling factors correspondence mask that defines the
 *      correspondence between the output tensor dimensions and the \p
 *      scales vector. The set i-th bit indicates that a dedicated scaling
 *      factor should be used for each index along that dimension. Set the
 *      mask to 0 to use a common scaling factor for the whole output
 *      tensor.
 *  @param scales Array of output scaling factors that must contain \p count
 *      values and the following equality must hold:
 *      <pre>{@code \[count = \prod\limits_{d \in mask} weights.dims[d].\]}</pre>
 *      Violations can only be detected when the attributes are used to create
 *      a primitive descriptor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const float[] scales);

/** Returns the quantization scaling factors for RNN weights tensors.
 * 
 *  @param attr Primitive attributes.
 *  @param count Number of elements in the \p scales array.
 *  @param mask Scaling factors correspondence mask that defines the
 *      correspondence between the output tensor dimensions and the \p
 *      scales vector. The set i-th bit indicates that a dedicated scaling
 *      factor should be used for each index along that dimension. Set the
 *      mask to 0 to use a common scaling factor for the whole output
 *      tensor.
 *  @param scales Array of output scaling factors that contain \p count
 *      values and the following equality must hold:
 *      <pre>{@code \[count = \prod\limits_{d \in mask} weights.dims[d].\]}</pre>
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Cast("const float**") PointerPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Const @ByPtrPtr FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongBuffer count, IntBuffer mask,
        @Const @ByPtrPtr FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") long[] count, int[] mask,
        @Const @ByPtrPtr float[] scales);

/** Sets quantization scaling factors for RNN projection weights tensors. The
 *  low-precision configuration of the RNN primitives expects input weights to
 *  use the signed 8-bit integer data type. The scaling factors are used to
 *  quantize floating-point data to signed integer and must be passed to RNN
 *  primitives using attributes.
 * 
 *  \note
 *      The dimension order is always native and does not depend on the actual
 *      layout used. For example, five-dimensional weights always have (l, d,
 *      i, g, o) logical dimension ordering.
 * 
 *  @param attr Primitive attributes.
 *  @param count Number of elements in the \p scales array.
 *  @param mask Scaling factors correspondence mask that defines the
 *      correspondence between the output tensor dimensions and the \p
 *      scales vector. The set i-th bit indicates that a dedicated scaling
 *      factor should be used for each index along that dimension. Set the
 *      mask to 0 to use a common scaling factor for the whole output
 *      tensor.
 *  @param scales Array of output scaling factors that must contain \p count
 *      values and the following equality must hold:
 *      <pre>{@code \[count = \prod\limits_{d \in mask} weights.dims[d].\]}</pre>
 *      Violations can only be detected when the attributes are used to create
 *      a primitive descriptor.
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_projection_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_projection_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_set_rnn_weights_projection_qparams(
        dnnl_primitive_attr attr, @Cast("dnnl_dim_t") long count, int mask,
        @Const float[] scales);

/** Returns the quantization scaling factors for RNN projection weights tensors.
 * 
 *  @param attr Primitive attributes.
 *  @param count Number of elements in the \p scales array.
 *  @param mask Scaling factors correspondence mask that defines the
 *      correspondence between the output tensor dimensions and the \p
 *      scales vector. The set i-th bit indicates that a dedicated scaling
 *      factor should be used for each index along that dimension. Set the
 *      mask to 0 to use a common scaling factor for the whole output
 *      tensor.
 *  @param scales Array of output scaling factors that contain \p count
 *      values and the following equality must hold:
 *      <pre>{@code \[count = \prod\limits_{d \in mask} weights.dims[d].\]}</pre>
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_projection_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Cast("const float**") PointerPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_projection_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongPointer count, IntPointer mask,
        @Const @ByPtrPtr FloatPointer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_projection_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") LongBuffer count, IntBuffer mask,
        @Const @ByPtrPtr FloatBuffer scales);
public static native @Cast("dnnl_status_t") int dnnl_primitive_attr_get_rnn_weights_projection_qparams(
        @Const dnnl_primitive_attr attr, @Cast("dnnl_dim_t*") long[] count, int[] mask,
        @Const @ByPtrPtr float[] scales);

/** \} dnnl_api_attributes
 <p>
 *  \addtogroup dnnl_api_rnn
 *  \{
 <p>
 *  Creates a primitive descriptor for vanilla RNN forward propagation
 *      primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc.
 * 
 *  This would then indicate that the RNN forward propagation primitive should
 *  not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param activation Activation kind. Possible values are #dnnl_eltwise_relu,
 *      #dnnl_eltwise_tanh or #dnnl_eltwise_logistic.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param flags Unused.
 *  @param alpha Negative slope if activation is #dnnl_eltwise_relu.
 *  @param beta Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("const dnnl_alg_kind_t") int activation,
        @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags, float alpha,
        float beta, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("const dnnl_alg_kind_t") int activation,
        @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags, float alpha,
        float beta, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for vanilla RNN backward propagation
 *      primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p diff_src_iter_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p diff_dst_iter_desc.
 * 
 *  This would then indicate that the RNN backward propagation primitive should
 *  not use the respective data and should use zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param activation Activation kind. Possible values are #dnnl_eltwise_relu,
 *      #dnnl_eltwise_tanh or #dnnl_eltwise_logistic.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param flags Unused.
 *  @param alpha Negative slope if activation is #dnnl_eltwise_relu.
 *  @param beta Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("const dnnl_alg_kind_t") int activation,
        @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        float alpha, float beta, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_vanilla_rnn_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("const dnnl_alg_kind_t") int activation,
        @Cast("const dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        float alpha, float beta, @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an LSTM forward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p src_iter_c_desc,
 *  - \p weights_peephole_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc together with \p dst_iter_c_desc.
 * 
 *  This would then indicate that the LSTM forward propagation primitive should
 *  not use them and should default to zero values instead.
 * 
 *  The \p weights_projection_desc could either be \c NULL or point to a zero
 *  memory descriptor. This would then indicate that the LSTM doesn't have
 *  recurrent projection layer.
 * 
 *  \note
 *      All memory descriptors can be initialized with #dnnl_format_tag_any or
 *      with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param src_iter_c_desc Memory descriptor for the input recurrent cell
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param weights_peephole_desc Memory descriptor for the weights applied to
 *      the cell states (according to the Peephole LSTM formula).
 *  @param weights_projection_desc Memory descriptor for the weights applied to
 *      the hidden states to get the recurrent projection (according to the
 *      Projection LSTM formula).
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param dst_iter_c_desc Memory descriptor for the output recurrent cell
 *      state vector.
 *  @param flags Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lstm_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc src_iter_c_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc weights_peephole_desc,
        @Const dnnl_memory_desc weights_projection_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc dst_iter_c_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lstm_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc src_iter_c_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc weights_peephole_desc,
        @Const dnnl_memory_desc weights_projection_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc dst_iter_c_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for an LSTM backward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p src_iter_c_desc, \p diff_src_iter_desc,
 *    and \p diff_src_iter_c_desc,
 *  - \p weights_peephole_desc together with \p diff_weights_peephole_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p dst_iter_c_desc, \p diff_dst_iter_desc,
 *    and \p diff_dst_iter_c_desc.
 * 
 *  This would then indicate that the LSTM backward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  The \p weights_projection_desc together with \p
 *  diff_weights_projection_desc could either be \c NULL or point to a zero
 *  memory descriptor. This would then indicate that the LSTM doesn't have
 *  recurrent projection layer.
 * 
 *  \note
 *      All memory descriptors can be initialized with #dnnl_format_tag_any or
 *      with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param src_iter_c_desc Memory descriptor for the input recurrent cell
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param weights_peephole_desc Memory descriptor for the weights applied to
 *      the cell states (according to the Peephole LSTM formula).
 *  @param weights_projection_desc Memory descriptor for the weights applied to
 *      the hidden states to get the recurrent projection (according to the
 *      Projection LSTM formula).
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param dst_iter_c_desc Memory descriptor for the output recurrent cell
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_src_iter_c_desc Memory descriptor for the diff of input
 *  recurrent cell state vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_weights_peephole_desc Memory descriptor for the diff of weights
 *      applied to the cell states (according to the Peephole LSTM formula).
 *  @param diff_weights_projection_desc Memory descriptor for the diff of
 *      weights applied to the hidden states to get the recurrent projection
 *      (according to the Projection LSTM formula).
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param diff_dst_iter_c_desc Memory descriptor for the diff of output
 *      recurrent cell state vector.
 *  @param flags Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lstm_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc src_iter_c_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc weights_peephole_desc,
        @Const dnnl_memory_desc weights_projection_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc dst_iter_c_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_src_iter_c_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_weights_peephole_desc,
        @Const dnnl_memory_desc diff_weights_projection_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc,
        @Const dnnl_memory_desc diff_dst_iter_c_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lstm_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc src_iter_c_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc weights_peephole_desc,
        @Const dnnl_memory_desc weights_projection_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc dst_iter_c_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_src_iter_c_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_weights_peephole_desc,
        @Const dnnl_memory_desc diff_weights_projection_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc,
        @Const dnnl_memory_desc diff_dst_iter_c_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for GRU forward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc.
 * 
 *  This would then indicate that the GRU forward propagation primitive should
 *  not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param flags Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_gru_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_gru_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for GRU backward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p diff_src_iter_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p diff_dst_iter_desc.
 * 
 *  This would then indicate that the GRU backward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param flags Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_gru_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_gru_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a descriptor for LBR GRU forward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc.
 * 
 *  This would then indicate that the LBR GRU forward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param flags Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for LBR GRU backward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p diff_src_iter_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p diff_dst_iter_desc.
 * 
 *  This would then indicate that the LBR GRU backward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param flags Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lbr_gru_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for AUGRU forward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc.
 * 
 *  This would then indicate that the AUGRU forward propagation primitive should
 *  not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param attention_desc Memory descriptor for the attention vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param flags Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_augru_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_augru_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for AUGRU backward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p diff_src_iter_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p diff_dst_iter_desc.
 * 
 *  This would then indicate that the AUGRU backward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param attention_desc Memory descriptor for the attention vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_attention_desc Memory descriptor for the diff of attention vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param flags Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
public static native @Cast("dnnl_status_t") int dnnl_augru_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_attention_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_augru_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_attention_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for LBR AUGRU forward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc,
 *  - \p bias_desc,
 *  - \p dst_iter_desc.
 * 
 *  This would then indicate that the LBR AUGRU forward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param attention_desc Memory descriptor for the attention vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param flags Unused.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_lbr_augru_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lbr_augru_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for LBR AUGRU backward propagation primitive.
 * 
 *  The following arguments may either be \c NULL or point to a zero memory
 *  descriptor:
 *  - \p src_iter_desc together with \p diff_src_iter_desc,
 *  - \p bias_desc together with \p diff_bias_desc,
 *  - \p dst_iter_desc together with \p diff_dst_iter_desc.
 * 
 *  This would then indicate that the LBR AUGRU backward propagation primitive
 *  should not use them and should default to zero values instead.
 * 
 *  \note
 *      All memory descriptors can be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Must be #dnnl_backward.
 *  @param direction RNN direction. See \ref dnnl_rnn_direction_t for more
 *      info.
 *  @param src_layer_desc Memory descriptor for the input vector.
 *  @param src_iter_desc Memory descriptor for the input recurrent hidden
 *      state vector.
 *  @param attention_desc Memory descriptor for the attention vector.
 *  @param weights_layer_desc Memory descriptor for the weights applied to the
 *      layer input.
 *  @param weights_iter_desc Memory descriptor for the weights applied to the
 *      recurrent input.
 *  @param bias_desc Bias memory descriptor.
 *  @param dst_layer_desc Memory descriptor for the output vector.
 *  @param dst_iter_desc Memory descriptor for the output recurrent hidden
 *      state vector.
 *  @param diff_src_layer_desc Memory descriptor for the diff of input vector.
 *  @param diff_src_iter_desc Memory descriptor for the diff of input recurrent
 *      hidden state vector.
 *  @param diff_attention_desc Memory descriptor for the diff of attention vector.
 *  @param diff_weights_layer_desc Memory descriptor for the diff of weights
 *      applied to the layer input.
 *  @param diff_weights_iter_desc Memory descriptor for the diff of weights
 *      applied to the recurrent input.
 *  @param diff_bias_desc Diff bias memory descriptor.
 *  @param diff_dst_layer_desc Memory descriptor for the diff of output
 *      vector.
 *  @param diff_dst_iter_desc Memory descriptor for the diff of output
 *      recurrent hidden state vector.
 *  @param flags Unused.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_lbr_augru_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_attention_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_lbr_augru_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_rnn_direction_t") int direction,
        @Const dnnl_memory_desc src_layer_desc,
        @Const dnnl_memory_desc src_iter_desc,
        @Const dnnl_memory_desc attention_desc,
        @Const dnnl_memory_desc weights_layer_desc,
        @Const dnnl_memory_desc weights_iter_desc,
        @Const dnnl_memory_desc bias_desc,
        @Const dnnl_memory_desc dst_layer_desc,
        @Const dnnl_memory_desc dst_iter_desc,
        @Const dnnl_memory_desc diff_src_layer_desc,
        @Const dnnl_memory_desc diff_src_iter_desc,
        @Const dnnl_memory_desc diff_attention_desc,
        @Const dnnl_memory_desc diff_weights_layer_desc,
        @Const dnnl_memory_desc diff_weights_iter_desc,
        @Const dnnl_memory_desc diff_bias_desc,
        @Const dnnl_memory_desc diff_dst_layer_desc,
        @Const dnnl_memory_desc diff_dst_iter_desc, @Cast("unsigned") int flags,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_rnn
 <p>
 *  \addtogroup dnnl_api_matmul
 *  \{
 <p>
 *  Creates a primitive descriptor for a matrix multiplication primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param src_desc Source memory descriptor (matrix A)
 *  @param weights_desc Weights memory descriptor (matrix B)
 *  @param bias_desc Bias memory descriptor. Passing NULL, a zero memory
 *      descriptor, or a memory descriptor with format_kind set to
 *      #dnnl_format_kind_undef disables the bias term.
 *  @param dst_desc Destination memory descriptor (matrix C).
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_matmul_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_matmul_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc weights_desc,
        @Const dnnl_memory_desc bias_desc, @Const dnnl_memory_desc dst_desc,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_matmul
 <p>
 *  \addtogroup dnnl_api_resampling Resampling
 *  \{
 <p>
 *  Creates a primitive descriptor for a resampling forward propagation
 *      primitive.
 * 
 *  \note
 *      Destination memory descriptor is allowed to be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param prop_kind Propagation kind. Possible values are
 *      #dnnl_forward_training and #dnnl_forward_inference.
 *  @param alg_kind resampling algorithm kind: either #dnnl_resampling_nearest,
 *      or #dnnl_resampling_linear.
 *  @param factors Array of scaling factors for spatial dimension.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const FloatPointer factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const FloatBuffer factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const float[] factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const FloatPointer factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const FloatBuffer factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_forward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_prop_kind_t") int prop_kind, @Cast("dnnl_alg_kind_t") int alg_kind,
        @Const float[] factors, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, @Const dnnl_primitive_attr attr);

/** Creates a primitive descriptor for a resampling backward propagation
 *      primitive.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind resamplinging algorithm kind: either
 *      #dnnl_resampling_nearest, or #dnnl_resampling_linear.
 *  @param diff_src_desc Diff source memory descriptor.
 *  @param diff_dst_desc Diff destination memory descriptor.
 *  @param factors Array of scaling factors for spatial dimension.
 *  @param hint_fwd_pd Primitive descriptor for a respective forward propagation
 *      primitive.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise.
 *  */

///
///
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const FloatPointer factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const FloatBuffer factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const float[] factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const FloatPointer factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const FloatBuffer factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_resampling_backward_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const float[] factors,
        @Const dnnl_memory_desc diff_src_desc,
        @Const dnnl_memory_desc diff_dst_desc,
        @Const dnnl_primitive_desc hint_fwd_pd,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_resampling
 <p>
 *  \addtogroup dnnl_api_reduction Reduction
 *  \{
 <p>
 *  Creates a primitive descriptor for a reduction primitive.
 * 
 *  \note
 *      Destination memory descriptor is allowed to be initialized with
 *      #dnnl_format_tag_any or with format_kind set to #dnnl_format_kind_any.
 * 
 *  @param primitive_desc Output primitive descriptor.
 *  @param engine Engine to use.
 *  @param alg_kind reduction algorithm kind. Possible values:
 *      #dnnl_reduction_max, #dnnl_reduction_min, #dnnl_reduction_sum,
 *      #dnnl_reduction_mul, #dnnl_reduction_mean, #dnnl_reduction_norm_lp_max,
 *      #dnnl_reduction_norm_lp_sum, #dnnl_reduction_norm_lp_power_p_max,
 *      #dnnl_reduction_norm_lp_power_p_sum.
 *  @param p Algorithm specific parameter.
 *  @param eps Algorithm specific parameter.
 *  @param src_desc Source memory descriptor.
 *  @param dst_desc Destination memory descriptor.
 *  @param attr Primitive attributes (can be NULL).
 *  @return #dnnl_success on success and a status describing the error
 *      otherwise. */

///
public static native @Cast("dnnl_status_t") int dnnl_reduction_primitive_desc_create(
        @ByPtrPtr dnnl_primitive_desc primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, float p, float eps,
        @Const dnnl_primitive_attr attr);
public static native @Cast("dnnl_status_t") int dnnl_reduction_primitive_desc_create(
        @Cast("dnnl_primitive_desc_t*") PointerPointer primitive_desc, dnnl_engine engine,
        @Cast("dnnl_alg_kind_t") int alg_kind, @Const dnnl_memory_desc src_desc,
        @Const dnnl_memory_desc dst_desc, float p, float eps,
        @Const dnnl_primitive_attr attr);

/** \} dnnl_api_reduction
 <p>
 *  \} dnnl_api_primitives
 <p>
 *  \addtogroup dnnl_api_primitive_cache
 *  \{
 <p>
 *  Returns the number of primitives that can be held in the primitive cache
 *  at the same time.
 * 
 *  @param capacity Primitive cache capacity to query. Concurrently
 *  accessing \p capacity is safe.
 *  @return #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the
 *      \p capacity value is invalid, and #dnnl_success/#dnnl::status::success on
 *      success. */

///
public static native @Cast("dnnl_status_t") int dnnl_get_primitive_cache_capacity(IntPointer _capacity);
public static native @Cast("dnnl_status_t") int dnnl_get_primitive_cache_capacity(IntBuffer _capacity);
public static native @Cast("dnnl_status_t") int dnnl_get_primitive_cache_capacity(int[] _capacity);

/** Sets a number of primitives that can be held in the primitive cache
 *  at a time.
 * 
 *  @param capacity Primitive cache capacity to set. If a new \p capacity is
 *  less than a number of primitives that the primitive cache already has
 *  then the excess entries will be evicted. Setting the \p capacity to 0
 *  clears the primitive cache and disables it. Concurrently modifying
 *  \p capacity is safe.
 *  @return #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the
 *      \p capacity value is invalid, and #dnnl_success/#dnnl::status::success on
 *      success. */

///
///
public static native @Cast("dnnl_status_t") int dnnl_set_primitive_cache_capacity(int _capacity);

/** \} dnnl_api_primitive_cache
 <p>
 *  \addtogroup dnnl_api_service
 *  \{
 <p>
 *  Configures dumping of JIT-generated code.
 * 
 *  \note
 *      This setting overrides the DNNL_JIT_DUMP environment variable.
 * 
 *  @param enable Flag value. Set to 0 to disable and set to 1 to enable.
 *  @return #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the
 *      \p flag value is invalid, and #dnnl_success/#dnnl::status::success on
 *      success. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_set_jit_dump(int enable);

/** Sets library profiling flags. The flags define which profilers are
 *  supported.
 * 
 *  \note
 *      This setting overrides DNNL_JIT_PROFILE environment variable.
 * 
 *  @see \ref dev_guide_profilers
 * 
 *  @param flags Profiling flags that can contain the following bits:
 *      - \ref DNNL_JIT_PROFILE_VTUNE -- integration with VTune Profiler
 *          (on by default)
 *      - \ref DNNL_JIT_PROFILE_LINUX_JITDUMP -- produce Linux-specific
 *          jit-pid.dump output (off by default). The location of the output
 *          is controlled via JITDUMPDIR environment variable or via
 *          dnnl_set_jit_profiling_jitdumpdir() function.
 *      - \ref DNNL_JIT_PROFILE_LINUX_PERFMAP -- produce Linux-specific
 *          perf-pid.map output (off by default). The output is always placed
 *          into /tmp.
 * 
 *      Passing \ref DNNL_JIT_PROFILE_NONE disables profiling completely.
 * 
 *  @return #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the
 *      \p flags value is invalid, and #dnnl_success/#dnnl::status::success on
 *      success. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_set_jit_profiling_flags(@Cast("unsigned") int flags);

/** Sets JIT dump output path. Only applicable to Linux and is only
 *  used when profiling flags have DNNL_JIT_PROFILE_LINUX_PERF bit set.
 * 
 *  After the first JIT kernel is generated, the jitdump output will be placed
 *  into temporary directory created using the mkdtemp template
 *  'dir/.debug/jit/dnnl.XXXXXX'.
 * 
 *  @see \ref dev_guide_profilers
 * 
 *  \note
 *      This setting overrides JITDUMPDIR environment variable.  If
 *      JITDUMPDIR is not set, and this function is never called, the path
 *      defaults to HOME. Passing NULL reverts the value to default.
 * 
 *  \note
 *      The directory is accessed only when the first JIT kernel is being
 *      created. JIT profiling will be disabled in case of any errors
 *      accessing or creating this directory.
 * 
 *  @param dir JIT dump output path.
 *  @return #dnnl_success/#dnnl::status::success if the
 *      output directory was set correctly and an error status otherwise.
 *  @return #dnnl_unimplemented/#dnnl::status::unimplemented on Windows. */

///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_set_jit_profiling_jitdumpdir(@Cast("const char*") BytePointer dir);
public static native @Cast("dnnl_status_t") int dnnl_set_jit_profiling_jitdumpdir(String dir);

/** Sets the maximal ISA the library can dispatch to on the CPU. See
 *  #dnnl_cpu_isa_t and #dnnl::cpu_isa for the list of the values accepted by
 *  the C and C++ API functions respectively.
 * 
 *  This function has effect only once, and returns an error on subsequent
 *  calls. It should also be invoked before any other oneDNN API call, otherwise
 *  it may return an error.
 * 
 *  This function overrides the DNNL_MAX_CPU_ISA environment variable. The
 *  environment variable can be set to the desired maximal ISA name in upper
 *  case and with dnnl_cpu_isa prefix removed. For example:
 *  {@code DNNL_MAX_CPU_ISA=AVX2}.
 * 
 *  \note
 *      The ISAs are only partially ordered:
 *          - SSE41 < AVX < AVX2 < AVX2_VNNI < AVX2_VNNI_2,
 *          - AVX2 < AVX512_CORE < AVX512_CORE_VNNI < AVX512_CORE_BF16
 *            < AVX512_CORE_FP16 < AVX512_CORE_AMX < AVX512_CORE_AMX_FP16,
 *          - AVX2_VNNI < AVX512_CORE_FP16.
 * 
 *  @see \ref dev_guide_cpu_dispatcher_control for more details
 * 
 *  @param isa Maximal ISA the library should dispatch to. Pass
 *      #dnnl_cpu_isa_default/#dnnl::cpu_isa::isa_default to remove ISA restrictions
 *      (except for ISAs with initial support in the library).
 *  @return #dnnl_success/#dnnl::status::success on success and a
 *      #dnnl_invalid_arguments/#dnnl::status::invalid_arguments if the \p isa
 *      parameter is invalid or the ISA cannot be changed at this time.
 *  @return #dnnl_unimplemented/#dnnl::status::unimplemented if the feature
 *      was disabled at build time (see \ref dev_guide_build_options for more
 *      details). */

///
///
public static native @Cast("dnnl_status_t") int dnnl_set_max_cpu_isa(@Cast("dnnl_cpu_isa_t") int isa);

/** Gets the maximal ISA the library can dispatch to on the CPU. See
 *  #dnnl_cpu_isa_t and #dnnl::cpu_isa for the list of the values returned by
 *  the C and C++ API functions respectively.
 * 
 *  @see \ref dev_guide_cpu_dispatcher_control for more details
 * 
 *  @return #dnnl_cpu_isa_t value reflecting the maximal ISA the library may
 *      dispatch to. */

///
///
///
public static native @Cast("dnnl_cpu_isa_t") int dnnl_get_effective_cpu_isa();

/** Sets the hints flag for the CPU ISA. See #dnnl_cpu_isa_hints_t and
 *  #dnnl::cpu_isa_hints for the list of the values accepted by the C and C++
 *  API functions respectively.
 * 
 *  This function has effect only once, and returns an error on subsequent
 *  calls. It should also be invoked before any other oneDNN API call, otherwise
 *  it may return an error.
 * 
 *  This function overrides the DNNL_CPU_ISA_HINTS environment variable.
 *  @see \ref dev_guide_cpu_isa_hints for more details
 * 
 *  @param isa_hints CPU ISA hints to be passed over to the implementation.
 *      Pass #dnnl_cpu_isa_no_hints/#dnnl::cpu_isa_hints::no_hints to use
 *      default features i.e. no hints.
 *  @return #dnnl_success/#dnnl::status::success on success and a
 *      #dnnl_runtime_error/#dnnl::status::runtime_error if the ISA hints cannot
 *      be specified at the current time.
 *  @return #dnnl_unimplemented/#dnnl::status::unimplemented if the feature
 *      was disabled at build time (see \ref dev_guide_build_options for more
 *      details). */

///
///
public static native @Cast("dnnl_status_t") int dnnl_set_cpu_isa_hints(@Cast("dnnl_cpu_isa_hints_t") int isa_hints);

/** Gets the ISA specific hints that library can follow. See
 *  #dnnl_cpu_isa_hints_t and #dnnl::cpu_isa_hints for the list of the values
 *   returned by the C and C++ API functions respectively.
 * 
 *  @see \ref dev_guide_cpu_isa_hints for more details
 * 
 *  @return #dnnl_cpu_isa_hints_t value reflecting the ISA specific hints the
 *  library can follow. */

///
///
///
///
///
///
public static native @Cast("dnnl_cpu_isa_hints_t") int dnnl_get_cpu_isa_hints();

/** \} dnnl_api_service */

// #ifdef DNNL_EXPERIMENTAL_PROFILING
// #endif

/** \addtogroup dnnl_api_blas
/** \{
<p>
/** Performs single-precision matrix-matrix multiply.
/**
/** The operation is defined as:
/**
/** {@code C := alpha * op( A ) * op( B ) + beta * C}
/**
/** where
/**  - {@code op( X ) = X} or {@code op( X ) = X**T},
/**  - {@code alpha} and {@code beta} are scalars, and
/**  - {@code A}, {@code B}, and {@code C} are matrices:
/**     - {@code op( A )} is an {@code MxK} matrix,
/**     - {@code op( B )} is an {@code KxN} matrix,
/**     - {@code C} is an {@code MxN} matrix.
/**
/** The matrices are assumed to be stored in row-major order (the elements in
/** each of the matrix rows are contiguous in memory).
/**
/** \note
/**     This API does not support XERBLA. Instead, unlike the standard BLAS
/**     functions, this one returns a dnnl_status_t value to allow error
/**     handling.
/**
/** @param transa Transposition flag for matrix A: 'N' or 'n' means A is not
/**     transposed, and 'T' or 't' means that A is transposed.
/** @param transb Transposition flag for matrix B: 'N' or 'n' means B is not
/**     transposed, and 'T' or 't' means that B is transposed.
/** @param M The M dimension.
/** @param N The N dimension.
/** @param K The K dimension.
/** @param alpha The alpha parameter that is used to scale the product of
/**     matrices A and B.
/** @param A A pointer to the A matrix data.
/** @param lda The leading dimension for the matrix A.
/** @param B A pointer to the B matrix data.
/** @param ldb The leading dimension for the matrix B.
/** @param beta The beta parameter that is used to scale the matrix C.
/** @param C A pointer to the C matrix data.
/** @param ldc The leading dimension for the matrix C.
/** @return #dnnl_success/#dnnl::status::success on success and a status
/**     describing the error otherwise. */

///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const FloatPointer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatPointer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatPointer C, @Cast("dnnl_dim_t") long ldc);
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const FloatBuffer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatBuffer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatBuffer C, @Cast("dnnl_dim_t") long ldc);
public static native @Cast("dnnl_status_t") int dnnl_sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const float[] A, @Cast("dnnl_dim_t") long lda,
        @Const float[] B, @Cast("dnnl_dim_t") long ldb, float beta, float[] C, @Cast("dnnl_dim_t") long ldc);

/** Performs integer matrix-matrix multiply on 8-bit unsigned matrix A, 8-bit
 *  signed matrix B, and 32-bit signed resulting matrix C.
 * 
 *  The operation is defined as:
 * 
 *  {@code C := alpha * (op(A) - A_offset) * (op(B) - B_offset) + beta * C + C_offset}
 * 
 *  where
 *   - {@code op( X ) = X} or {@code op( X ) = X**T},
 *   - {@code alpha} and {@code beta} are scalars, and
 *   - {@code A}, {@code B}, and {@code C} are matrices:
 *      - {@code op( A )} is an {@code MxK} matrix,
 *      - {@code op( B )} is an {@code KxN} matrix,
 *      - {@code C} is an {@code MxN} matrix.
 *   - {@code A_offset} is an {@code MxK} matrix with every element equal the {@code ao} value,
 *   - {@code B_offset} is an {@code KxN} matrix with every element equal the {@code bo} value,
 *   - {@code C_offset} is an {@code MxN} matrix which is defined by the {@code co} array of size {@code len}:
 *     - if {@code offsetc = F}: the {@code len} must be at least {@code 1},
 *     - if {@code offsetc = C}: the {@code len} must be at least {@code max(1, m)},
 *     - if {@code offsetc = R}: the {@code len} must be at least {@code max(1, n)},
 * 
 *  The matrices are assumed to be stored in row-major order (the elements in
 *  each of the matrix rows are contiguous in memory).
 * 
 *  \note
 *      This API does not support XERBLA. Instead, unlike the standard BLAS
 *      functions, this one returns a dnnl_status_t value to allow error
 *      handling.
 * 
 *  \warning
 *      On some architectures saturation may happen during intermediate
 *      computations, which would lead to unexpected results. For more
 *      details, refer to \ref dev_guide_int8_computations.
 * 
 *  @param transa Transposition flag for matrix A: 'N' or 'n' means A is not
 *      transposed, and 'T' or 't' means that A is transposed.
 *  @param transb Transposition flag for matrix B: 'N' or 'n' means B is not
 *      transposed, and 'T' or 't' means that B is transposed.
 *  @param offsetc Flag specifying how offsets should be applied to matrix C:
 *      - 'F' means that the same offset will be applied to each element of
 *          the matrix C,
 *      - 'C' means that individual offset will be applied to each element
 *          within each column,
 *      - 'R' means that individual offset will be applied to each element
 *          within each row.
 *  @param M The M dimension.
 *  @param N The N dimension.
 *  @param K The K dimension.
 *  @param alpha The alpha parameter that is used to scale the product of
 *      matrices A and B.
 *  @param A A pointer to the A matrix data.
 *  @param lda The leading dimension for the matrix A.
 *  @param ao The offset value for the matrix A.
 *  @param B A pointer to the B matrix data.
 *  @param ldb The leading dimension for the matrix B.
 *  @param bo The offset value for the matrix B.
 *  @param beta The beta parameter that is used to scale the matrix C.
 *  @param C A pointer to the C matrix data.
 *  @param ldc The leading dimension for the matrix C.
 *  @param co An array of offset values for the matrix C. The number of
 *      elements in the array depends on the value of \p offsetc.
 *  @return #dnnl_success/#dnnl::status::success on success and a status
 *      describing the error otherwise. */

///
///
///
///
///
///
///
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") BytePointer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") byte[] A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);

/** Performs integer matrix-matrix multiply on 8-bit signed matrix A, 8-bit
 *  signed matrix B, and 32-bit signed resulting matrix C.
 * 
 *  The operation is defined as:
 * 
 *  {@code C := alpha * (op(A) - A_offset) * (op(B) - B_offset) + beta * C + C_offset}
 * 
 *  where
 *   - {@code op( X ) = X} or {@code op( X ) = X**T},
 *   - {@code alpha} and {@code beta} are scalars, and
 *   - {@code A}, {@code B}, and {@code C} are matrices:
 *      - {@code op( A )} is an {@code MxK} matrix,
 *      - {@code op( B )} is an {@code KxN} matrix,
 *      - {@code C} is an {@code MxN} matrix.
 *   - {@code A_offset} is an {@code MxK} matrix with every element equal the {@code ao} value,
 *   - {@code B_offset} is an {@code KxN} matrix with every element equal the {@code bo} value,
 *   - {@code C_offset} is an {@code MxN} matrix which is defined by the {@code co} array of size {@code len}:
 *     - if {@code offsetc = F}: the {@code len} must be at least {@code 1},
 *     - if {@code offsetc = C}: the {@code len} must be at least {@code max(1, m)},
 *     - if {@code offsetc = R}: the {@code len} must be at least {@code max(1, n)},
 * 
 *  The matrices are assumed to be stored in row-major order (the elements in
 *  each of the matrix rows are contiguous in memory).
 * 
 *  \note
 *      This API does not support XERBLA. Instead, unlike the standard BLAS
 *      functions, this one returns a dnnl_status_t value to allow error
 *      handling.
 * 
 *  \warning
 *      On some architectures saturation may happen during intermediate
 *      computations, which would lead to unexpected results. For more
 *      details, refer to \ref dev_guide_int8_computations.
 * 
 *  @param transa Transposition flag for matrix A: 'N' or 'n' means A is not
 *      transposed, and 'T' or 't' means that A is transposed.
 *  @param transb Transposition flag for matrix B: 'N' or 'n' means B is not
 *      transposed, and 'T' or 't' means that B is transposed.
 *  @param offsetc Flag specifying how offsets should be applied to matrix C:
 *      - 'F' means that the same offset will be applied to each element of
 *          the matrix C,
 *      - 'C' means that individual offset will be applied to each element
 *          within each column,
 *      - 'R' means that individual offset will be applied to each element
 *          within each row.
 *  @param M The M dimension.
 *  @param N The N dimension.
 *  @param K The K dimension.
 *  @param alpha The alpha parameter that is used to scale the product of
 *      matrices A and B.
 *  @param A A pointer to the A matrix data.
 *  @param lda The leading dimension for the matrix A.
 *  @param ao The offset value for the matrix A.
 *  @param B A pointer to the B matrix data.
 *  @param ldb The leading dimension for the matrix B.
 *  @param bo The offset value for the matrix B.
 *  @param beta The beta parameter that is used to scale the matrix C.
 *  @param C A pointer to the C matrix data.
 *  @param ldc The leading dimension for the matrix C.
 *  @param co An array of offset values for the matrix C. The number of
 *      elements in the array depends on the value of \p offsetc.
 *  @return #dnnl_success/#dnnl::status::success on success and a status
 *      describing the error otherwise. */
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const BytePointer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
public static native @Cast("dnnl_status_t") int dnnl_gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc,
        @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const byte[] A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);

/** \} dnnl_api_blas
 <p>
 *  \} dnnl_api */

// #ifdef __cplusplus
// #endif

// #endif /* ONEAPI_DNNL_DNNL_H */


// Parsed from oneapi/dnnl/dnnl_common.hpp

/*******************************************************************************
* Copyright 2022-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C++ common API */

// #ifndef ONEAPI_DNNL_DNNL_COMMON_HPP
// #define ONEAPI_DNNL_DNNL_COMMON_HPP

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <algorithm>
// #include <cstdlib>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>
// #include <unordered_map>

// #include "oneapi/dnnl/dnnl_common.h"

/** \endcond */

// __cpp_exceptions is referred from
// https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_exceptions.html
// gcc < 5 does not define __cpp_exceptions but __EXCEPTIONS,
// Microsoft C++ Compiler does not provide an option to disable exceptions
// #ifndef DNNL_ENABLE_EXCEPTIONS
// #if __cpp_exceptions || __EXCEPTIONS
//         || (defined(_MSC_VER) && !defined(__clang__))
public static final int DNNL_ENABLE_EXCEPTIONS = 1;
// #else
// #endif
// #endif

// #if defined(__GNUC__) || defined(__clang__)
// #define DNNL_TRAP() __builtin_trap()
// #elif defined(__INTEL_COMPILER) || defined(_MSC_VER)
// #define DNNL_TRAP() __debugbreak()
// #else
// #error "unknown compiler"
// #endif

// #if DNNL_ENABLE_EXCEPTIONS
// #define DNNL_THROW_ERROR(status, msg) throw error(status, msg)
// #else
// #endif

/** \addtogroup dnnl_api oneDNN API
 *  \{
 <p>
 *  oneDNN namespace */
// Targeting ../error.java



/** A class that provides the destructor for a oneDNN C API handle. */
// Targeting ../dnnl_engine_handle.java


// Targeting ../dnnl_memory_handle.java


// Targeting ../dnnl_memory_desc_handle.java


// Targeting ../dnnl_primitive_desc_handle.java


// Targeting ../dnnl_primitive_attr_handle.java


// Targeting ../dnnl_post_ops_handle.java


// Targeting ../dnnl_primitive_handle.java


// Targeting ../dnnl_stream_handle.java



/** \} dnnl_api_utils
 <p>
 *  \addtogroup dnnl_api_engine Engine
 * 
 *  An abstraction of a computational device: a CPU, a specific GPU
 *  card in the system, etc. Most primitives are created to execute
 *  computations on one specific engine. The only exceptions are reorder
 *  primitives that transfer data between two different engines.
 * 
 *  @see \ref dev_guide_basic_concepts
 * 
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../engine.java



/** Converts engine kind enum value from C++ API to C API type.
 * 
 *  @param akind C++ API engine kind enum value.
 *  @return Corresponding C API engine kind enum value. */

///
///
///
@Namespace("dnnl") public static native @Cast("dnnl_engine_kind_t") int convert_to_c(engine.kind akind);
@Namespace("dnnl") public static native @Cast("dnnl_engine_kind_t") int convert_to_c(@Cast("dnnl::engine::kind") int akind);

/** \} dnnl_api_engine
 <p>
 *  \addtogroup dnnl_api_stream Stream
 * 
 *  An encapsulation of execution context tied to a particular engine.
 * 
 *  @see \ref dev_guide_basic_concepts
 * 
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../stream.java



// #define DNNL_DEFINE_BITMASK_OPS(enum_name)
//     inline enum_name operator|(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) | static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name operator&(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) & static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name operator^(enum_name lhs, enum_name rhs) {
//         return static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) ^ static_cast<unsigned>(rhs));
//     }
// 
//     inline enum_name &operator|=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) | static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name &operator&=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) & static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name &operator^=(enum_name &lhs, enum_name rhs) {
//         lhs = static_cast<enum_name>(
//                 static_cast<unsigned>(lhs) ^ static_cast<unsigned>(rhs));
//         return lhs;
//     }
// 
//     inline enum_name operator~(enum_name rhs) {
//         return static_cast<enum_name>(~static_cast<unsigned>(rhs));
//     }

/** \} dnnl_api_stream
 <p>
 *  \addtogroup dnnl_api_fpmath_mode Floating-point Math Mode
 *  \{
 <p>
 *  Floating-point math mode */
@Namespace("dnnl") public enum fpmath_mode {
    /** Default behavior, no downconversions allowed */
    strict(dnnl_fpmath_mode_strict),
    /** Implicit f32->bf16 conversions allowed */
    bf16(dnnl_fpmath_mode_bf16),
    /** Implicit f32->f16 conversions allowed */
    f16(dnnl_fpmath_mode_f16),
    /** Implicit f32->tf32 conversions allowed */
    tf32(dnnl_fpmath_mode_tf32),
    /** Implicit f32->f16, f32->tf32 or f32->bf16 conversions allowed */
    any(dnnl_fpmath_mode_any);

    public final int value;
    private fpmath_mode(int v) { this.value = v; }
    private fpmath_mode(fpmath_mode e) { this.value = e.value; }
    public fpmath_mode intern() { for (fpmath_mode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts an fpmath mode enum value from C++ API to C API type.
 * 
 *  @param mode C++ API fpmath mode enum value.
 *  @return Corresponding C API fpmath mode enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_fpmath_mode_t") int convert_to_c(fpmath_mode mode);

/** \} dnnl_api_fpmath_mode
 <p>
 *  \} dnnl_api_common */

 // namespace dnnl

/** \} dnnl_api */

// #endif


// Parsed from oneapi/dnnl/dnnl.hpp

/*******************************************************************************
* Copyright 2016-2023 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

/** \file
/** C++ API */

// #ifndef ONEAPI_DNNL_DNNL_HPP
// #define ONEAPI_DNNL_DNNL_HPP

// #include "oneapi/dnnl/dnnl_config.h"

/** \cond DO_NOT_DOCUMENT_THIS */
// #include <algorithm>
// #include <cstdlib>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>
// #include <unordered_map>

// #include "oneapi/dnnl/dnnl.h"
// #include "oneapi/dnnl/dnnl_common.hpp"

/** \endcond
 <p>
 *  \addtogroup dnnl_api oneDNN API
 *  \{
 <p>
 *  oneDNN namespace */

/** \addtogroup dnnl_api_utils Utilities
 *  Utility types and definitions.
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
/** \endcond
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */

/** \endcond
 <p>
 *  \} dnnl_api_utils */
// Targeting ../primitive.java



/** Converts primitive kind enum value from C++ API to C API type.
 * 
 *  @param akind C++ API primitive kind enum value.
 *  @return Corresponding C API primitive kind enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_primitive_kind_t") int convert_to_c(primitive.kind akind);







/** \} dnnl_api_primitives_common
 <p>
 *  \addtogroup dnnl_api_attributes
 * 
 *  A container for parameters that extend primitives behavior.
 * 
 *  Attributes can also contain Post-ops, which are computations executed
 *  after the primitive.
 * 
 *  @see \ref dev_guide_attributes
 *  @see \ref dev_guide_attributes_post_ops
 * 
 *  \{
 <p>
 *  Scratchpad mode */
@Namespace("dnnl") public enum scratchpad_mode {
    /** The library manages the scratchpad allocation according to the policy
     *  specified by the {@code DNNL_ENABLE_CONCURRENT_EXEC}
     *  [build option](\ref dev_guide_build_options) (default).
     * 
     *  When {@code DNNL_ENABLE_CONCURRENT_EXEC=OFF} (default), the library
     *  scratchpad is common to all primitives to reduce the memory footprint.
     *  This configuration comes with limited thread-safety properties, namely
     *  primitives can be created and executed in parallel but cannot migrate
     *  between threads (in other words, each primitive should be executed in
     *  the same thread it was created in).
     * 
     *  When {@code DNNL_ENABLE_CONCURRENT_EXEC=ON}, the library scratchpad is
     *  private to each primitive. The memory footprint is larger than when
     *  using {@code DNNL_ENABLE_CONCURRENT_EXEC=OFF} but different primitives can be
     *  created and run concurrently (the same primitive cannot be run
     *  concurrently from two different threads though). */
    library(dnnl_scratchpad_mode_library),
    /** The user manages the scratchpad allocation by querying and providing
     *  the scratchpad memory to primitives. This mode is thread-safe as long
     *  as the scratchpad buffers are not used concurrently by two primitive
     *  executions. */
    user(dnnl_scratchpad_mode_user);

    public final int value;
    private scratchpad_mode(int v) { this.value = v; }
    private scratchpad_mode(scratchpad_mode e) { this.value = e.value; }
    public scratchpad_mode intern() { for (scratchpad_mode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts a scratchpad mode enum value from C++ API to C API type.
 * 
 *  @param mode C++ API scratchpad mode enum value.
 *  @return Corresponding C API scratchpad mode enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_scratchpad_mode_t") int convert_to_c(scratchpad_mode mode);

/** Propagation kind. */
@Namespace("dnnl") public enum prop_kind {
    /** Undefined propagation kind. */
    undef(dnnl_prop_kind_undef),
    /** Forward data propagation (training mode). In this mode, primitives
     *  perform computations necessary for subsequent backward propagation. */
    forward_training(dnnl_forward_training),
    /** Forward data propagation (inference mode). In this mode, primitives
     *  perform only computations that are necessary for inference and omit
     *  computations that are necessary only for backward propagation. */
    forward_inference(dnnl_forward_inference),
    /** Forward data propagation,
     *  alias for #dnnl::prop_kind::forward_training. */
    forward(dnnl_forward),
    /** Backward propagation (with respect to all parameters). */
    backward(dnnl_backward),
    /** Backward data propagation. */
    backward_data(dnnl_backward_data),
    /** Backward weights propagation. */
    backward_weights(dnnl_backward_weights),
    /** Backward bias propagation. */
    backward_bias(dnnl_backward_bias);

    public final int value;
    private prop_kind(int v) { this.value = v; }
    private prop_kind(prop_kind e) { this.value = e.value; }
    public prop_kind intern() { for (prop_kind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts propagation kind enum value from C++ API to C API type.
 * 
 *  @param akind C++ API propagation kind enum value.
 *  @return Corresponding C API propagation kind enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_prop_kind_t") int convert_to_c(prop_kind akind);

/** Kinds of algorithms. */
@Namespace("dnnl") public enum algorithm {
    /** Undefined algorithm */
    undef(dnnl_alg_kind_undef),
    /** Convolution algorithm that is chosen to be either direct or Winograd
     *  automatically */
    convolution_auto(dnnl_convolution_auto),
    /** Direct convolution */
    convolution_direct(dnnl_convolution_direct),
    /** Winograd convolution */
    convolution_winograd(dnnl_convolution_winograd),
    /** Direct deconvolution */
    deconvolution_direct(dnnl_deconvolution_direct),
    /** Winograd deconvolution */
    deconvolution_winograd(dnnl_deconvolution_winograd),
    /** Elementwise: rectified linear unit (ReLU) */
    eltwise_relu(dnnl_eltwise_relu),
    /** Elementwise: hyperbolic tangent non-linearity (tanh) */
    eltwise_tanh(dnnl_eltwise_tanh),
    /** Elementwise: exponential linear unit (ELU) */
    eltwise_elu(dnnl_eltwise_elu),
    /** Elementwise: square */
    eltwise_square(dnnl_eltwise_square),
    /** Elementwise: abs */
    eltwise_abs(dnnl_eltwise_abs),
    /** Elementwise: square root */
    eltwise_sqrt(dnnl_eltwise_sqrt),
    /** Elementwise: swish ({@code x \cdot sigmoid(a \cdot x)}) */
    eltwise_swish(dnnl_eltwise_swish),
    /** Elementwise: linear */
    eltwise_linear(dnnl_eltwise_linear),
    /** Elementwise: soft_relu */
    eltwise_soft_relu(dnnl_eltwise_soft_relu),
    /** Elementwise: mish */
    eltwise_mish(dnnl_eltwise_mish),
    /** Elementwise: logistic */
    eltwise_logistic(dnnl_eltwise_logistic),
    /** Elementwise: exponent */
    eltwise_exp(dnnl_eltwise_exp),
    /** Elementwise: tanh-based gelu */
    eltwise_gelu_tanh(dnnl_eltwise_gelu_tanh),
    /** Elementwise: erf-based gelu */
    eltwise_gelu_erf(dnnl_eltwise_gelu_erf),
    /** Elementwise: natural logarithm */
    eltwise_log(dnnl_eltwise_log),
    /** Elementwise: clip */
    eltwise_clip(dnnl_eltwise_clip),
    /** Eltwise: clip version 2 */
    eltwise_clip_v2(dnnl_eltwise_clip_v2),
    /** Elementwise: pow */
    eltwise_pow(dnnl_eltwise_pow),
    /** Elementwise: round */
    eltwise_round(dnnl_eltwise_round),
    /** Elementwise: hardswish */
    eltwise_hardswish(dnnl_eltwise_hardswish),
    /** Elementwise: hardsigmoid */
    eltwise_hardsigmoid(dnnl_eltwise_hardsigmoid),
    /** Elementwise: rectified linar unit (ReLU) (dst for backward) */
    eltwise_relu_use_dst_for_bwd(dnnl_eltwise_relu_use_dst_for_bwd),
    /** Elementwise: hyperbolic tangent non-linearity (tanh) (dst for backward) */
    eltwise_tanh_use_dst_for_bwd(dnnl_eltwise_tanh_use_dst_for_bwd),
    /** Elementwise: exponential linear unit (ELU) (dst for backward) */
    eltwise_elu_use_dst_for_bwd(dnnl_eltwise_elu_use_dst_for_bwd),
    /** Elementwise: square root (dst for backward) */
    eltwise_sqrt_use_dst_for_bwd(dnnl_eltwise_sqrt_use_dst_for_bwd),
    /** Elementwise: logistic (dst for backward) */
    eltwise_logistic_use_dst_for_bwd(dnnl_eltwise_logistic_use_dst_for_bwd),
    /** Elementwise: exponent (dst for backward) */
    eltwise_exp_use_dst_for_bwd(dnnl_eltwise_exp_use_dst_for_bwd),
    /** Elementwise: clip version 2 (dst for backward) */
    eltwise_clip_v2_use_dst_for_bwd(dnnl_eltwise_clip_v2_use_dst_for_bwd),
    /** Local response normalization (LRN) across multiple channels */
    lrn_across_channels(dnnl_lrn_across_channels),
    /** LRN within a single channel */
    lrn_within_channel(dnnl_lrn_within_channel),
    /** Max pooling */
    pooling_max(dnnl_pooling_max),
    /** Average pooling include padding */
    pooling_avg_include_padding(dnnl_pooling_avg_include_padding),
    /** Average pooling exclude padding */
    pooling_avg_exclude_padding(dnnl_pooling_avg_exclude_padding),
    /** RNN cell */
    vanilla_rnn(dnnl_vanilla_rnn),
    /** LSTM cell */
    vanilla_lstm(dnnl_vanilla_lstm),
    /** GRU cell */
    vanilla_gru(dnnl_vanilla_gru),
    /** GRU cell with linear before reset. Differs from the vanilla GRU
     *  in how the new memory gate is calculated:
     *  {@code c_t = tanh(W_c*x_t + b_{c_x} + r_t*(U_c*h_{t-1}+b_{c_h})) }
     *  LRB GRU expects 4 bias tensors on input:
     *  {@code [b_{u}, b_{r}, b_{c_x}, b_{c_h}]} */
    lbr_gru(dnnl_lbr_gru),
    /** AUGRU cell */
    vanilla_augru(dnnl_vanilla_augru),
    /** AUGRU cell with linear before reset */
    lbr_augru(dnnl_lbr_augru),
    /** Binary add */
    binary_add(dnnl_binary_add),
    /** Binary mul */
    binary_mul(dnnl_binary_mul),
    /** Binary max */
    binary_max(dnnl_binary_max),
    /** Binary min */
    binary_min(dnnl_binary_min),
    /** Binary div */
    binary_div(dnnl_binary_div),
    /** Binary sub */
    binary_sub(dnnl_binary_sub),
    /** Binary greater than or equal */
    binary_ge(dnnl_binary_ge),
    /** Binary greater than */
    binary_gt(dnnl_binary_gt),
    /** Binary less than or equal */
    binary_le(dnnl_binary_le),
    /** Binary less than */
    binary_lt(dnnl_binary_lt),
    /** Binary equal */
    binary_eq(dnnl_binary_eq),
    /** Binary not equal */
    binary_ne(dnnl_binary_ne),
    /** Nearest Neighbor resampling method */
    resampling_nearest(dnnl_resampling_nearest),
    /** Linear (Bilinear, Trilinear) resampling method */
    resampling_linear(dnnl_resampling_linear),
    /** Reduction using max operation */
    reduction_max(dnnl_reduction_max),
    /** Reduction using min operation */
    reduction_min(dnnl_reduction_min),
    /** Reduction using sum operation */
    reduction_sum(dnnl_reduction_sum),
    /** Reduction using mul operation */
    reduction_mul(dnnl_reduction_mul),
    /** Reduction using mean operation */
    reduction_mean(dnnl_reduction_mean),
    /** Reduction using norm_lp_max operation */
    reduction_norm_lp_max(dnnl_reduction_norm_lp_max),
    /** Reduction using norm_lp_sum operation */
    reduction_norm_lp_sum(dnnl_reduction_norm_lp_sum),
    /** Reduction using norm_lp_power_p_max operation */
    reduction_norm_lp_power_p_max(dnnl_reduction_norm_lp_power_p_max),
    /** Reduction using norm_lp_power_p_sum operation */
    reduction_norm_lp_power_p_sum(dnnl_reduction_norm_lp_power_p_sum),
    /** Softmax, numerically stable */
    softmax_accurate(dnnl_softmax_accurate),
    /** LogSoftmax, numerically stable */
    softmax_log(dnnl_softmax_log);

    public final int value;
    private algorithm(int v) { this.value = v; }
    private algorithm(algorithm e) { this.value = e.value; }
    public algorithm intern() { for (algorithm e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts algorithm kind enum value from C++ API to C API type.
 *  @param aalgorithm C++ API algorithm kind enum value.
 *  @return Corresponding C API algorithm kind enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_alg_kind_t") int convert_to_c(algorithm aalgorithm);

/** \} dnnl_api_attributes
 <p>
 *  \addtogroup dnnl_api_primitives_common
 *  \{
 <p>
 *  Flags for normalization primitives. */
@Namespace("dnnl") public enum normalization_flags {
    /** Use no normalization flags. If specified, the library computes mean and
     *  variance on forward propagation for training and inference, outputs them
     *  on forward propagation for training, and computes the respective
     *  derivatives on backward propagation. */
    none(dnnl_normalization_flags_none),

    /** Use global statistics. If specified, the library uses mean and
     *  variance provided by the user as an input on forward propagation and
     *  does not compute their derivatives on backward propagation. Otherwise,
     *  the library computes mean and variance on forward propagation for
     *  training and inference, outputs them on forward propagation for
     *  training, and computes the respective derivatives on backward
     *  propagation. */
    use_global_stats(dnnl_use_global_stats),

    /** Use scale parameter. If specified, the user is expected to pass scale as
     *  input on forward propagation. On backward propagation of type
     *  #dnnl::prop_kind::backward, the library computes its derivative. */
    use_scale(dnnl_use_scale),

    /** Use shift parameter. If specified, the user is expected to pass shift as
     *  input on forward propagation. On backward propagation of type
     *  #dnnl::prop_kind::backward, the library computes its derivative. */
    use_shift(dnnl_use_shift),

    /** Fuse normalization with ReLU. On training, normalization will require
     *  the workspace to implement backward propagation. On inference, the
     *  workspace is not required and behavior is the same as when normalization
     *  is fused with ReLU using the post-ops API. */
    fuse_norm_relu(dnnl_fuse_norm_relu),

    /** Fuse normalization with elementwise binary Add and then fuse with ReLU.
     *  On training, normalization will require the workspace to implement
     *  backward propagation. On inference, the workspace is not required. */
    fuse_norm_add_relu(dnnl_fuse_norm_add_relu);

    public final int value;
    private normalization_flags(int v) { this.value = v; }
    private normalization_flags(normalization_flags e) { this.value = e.value; }
    public normalization_flags intern() { for (normalization_flags e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts normalization flags enum value from C++ API to C API type.
 *  @param flags C++ API normalization flags enum value.
 *  @return Corresponding C API normalization flags enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_normalization_flags_t") int convert_to_c(normalization_flags flags);

/** \} dnnl_api_primitives_common
 <p>
 *  \addtogroup dnnl_api_rnn
 *  \{
 <p>
 *  RNN cell flags. */
@Namespace("dnnl") public enum rnn_flags {
    /** Undefined RNN flags */
    undef(dnnl_rnn_flags_undef),
    /** Do not add weights gradient to existing diff_weights memory */
    diff_weights_overwrite(dnnl_rnn_flags_diff_weights_overwrite);

    public final int value;
    private rnn_flags(int v) { this.value = v; }
    private rnn_flags(rnn_flags e) { this.value = e.value; }
    public rnn_flags intern() { for (rnn_flags e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts RNN cell flags enum value from C++ API to C API type.
 *  @param flags C++ API RNN cell flags enum value.
 *  @return Corresponding C API RNN cell flags enum value. */
@Namespace("dnnl") public static native @Cast("dnnl_rnn_flags_t") int convert_to_c(rnn_flags flags);

/** A direction of RNN primitive execution */
@Namespace("dnnl") public enum rnn_direction {
    /** Undefined RNN direction. */
    undef(dnnl_rnn_direction_undef),
    /** Unidirectional execution of RNN primitive from left to right. */
    unidirectional_left2right(dnnl_unidirectional_left2right),
    /** Unidirectional execution of RNN primitive from right to left. */
    unidirectional_right2left(dnnl_unidirectional_right2left),
    /** Bidirectional execution of RNN primitive with concatenation of the
     *  results. */
    bidirectional_concat(dnnl_bidirectional_concat),
    /** Bidirectional execution of RNN primitive with summation of the
     *  results. */
    bidirectional_sum(dnnl_bidirectional_sum);

    public final int value;
    private rnn_direction(int v) { this.value = v; }
    private rnn_direction(rnn_direction e) { this.value = e.value; }
    public rnn_direction intern() { for (rnn_direction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts RNN direction enum value from C++ API to C API type.
 *  @param dir C++ API RNN direction enum value.
 *  @return Corresponding C API RNN direction enum value. */

///
///
@Namespace("dnnl") public static native @Cast("dnnl_rnn_direction_t") int convert_to_c(rnn_direction dir);

/** \} dnnl_api_rnn
 <p>
 *  \addtogroup dnnl_api_primitives_common
 *  \{
 <p>
 *  Primitive descriptor query specification.
 * 
 *  In general, queries are not used with the C++ API because most queries are
 *  implemented as class members.
 * 
 *  See \ref dnnl_query_t for more information. */
@Namespace("dnnl") public enum query {
    /** no query */
    undef(dnnl_query_undef),

    /** execution engine */
    engine(dnnl_query_engine),
    /** primitive kind */
    primitive_kind(dnnl_query_primitive_kind),

    /** number of inputs expected */
    num_of_inputs_s32(dnnl_query_num_of_inputs_s32),
    /** number of outputs expected */
    num_of_outputs_s32(dnnl_query_num_of_outputs_s32),

    /** runtime estimation (seconds), unimplemented */
    
///
    time_estimate_f64(dnnl_query_time_estimate_f64),
    /** memory required for scratchpad (bytes)
     * 
     *  @see \ref dev_guide_attributes_scratchpad */
    
///
    memory_consumption_s64(dnnl_query_memory_consumption_s64),

    /** scratchpad engine
     * 
     *  engine to be used for creating scratchpad memory */
    scratchpad_engine(dnnl_query_scratchpad_engine),

    /** reorder source engine */
    reorder_src_engine(dnnl_query_reorder_src_engine),
    /** reorder destination engine */
    reorder_dst_engine(dnnl_query_reorder_dst_engine),

    /** implementation name */
    impl_info_str(dnnl_query_impl_info_str),

    /** propagation kind */
    prop_kind(dnnl_query_prop_kind),

    /** size of cache blob ID in bytes */
    cache_blob_id_size_s64(dnnl_query_cache_blob_id_size_s64),

    /** cache blob ID (pointer to array) */
    cache_blob_id(dnnl_query_cache_blob_id),

    /** strides */
    strides(dnnl_query_strides),
    /** dilations */
    dilations(dnnl_query_dilations),
    /** left padding */
    padding_l(dnnl_query_padding_l),
    /** right padding */
    padding_r(dnnl_query_padding_r),
    /** epsilon */
    epsilon_f32(dnnl_query_epsilon_f32),
    /** flags */
    flags(dnnl_query_flags),
    /** algorithm kind */
    alg_kind(dnnl_query_alg_kind),
    /** alpha */
    alpha_f32(dnnl_query_alpha_f32),
    /** beta */
    beta_f32(dnnl_query_beta_f32),
    /** axis */
    axis_s32(dnnl_query_axis_s32),
    /** LRN parameter local size */
    local_size_s64(dnnl_query_local_size_s64),
    /** LRN parameter K */
    k_f32(dnnl_query_k_f32),
    /** Reduction parameter P */
    p_f32(dnnl_query_p_f32),
    /** Resampling parameter factors */
    factors(dnnl_query_factors),
    /** RNN parameter cell kind */
    cell_kind(dnnl_query_cell_kind),
    /** RNN parameter direction */
    direction(dnnl_query_direction),
    /** RNN parameter activation kind */
    activation_kind(dnnl_query_activation_kind),
    /** Pooling parameter kernel */
    kernel(dnnl_query_kernel),
    /** Shuffle parameter group size */
    group_size_s64(dnnl_query_group_size_s64),

    /** source memory desc */
    src_md(dnnl_query_src_md),
    /** source gradient (diff) memory desc */
    diff_src_md(dnnl_query_diff_src_md),
    /** weights memory descriptor desc */
    weights_md(dnnl_query_weights_md),
    /** weights gradient (diff) memory desc */
    diff_weights_md(dnnl_query_diff_weights_md),
    /** destination memory desc */
    dst_md(dnnl_query_dst_md),
    /** destination gradient (diff) memory desc */
    diff_dst_md(dnnl_query_diff_dst_md),
    /** workspace memory desc */
    workspace_md(dnnl_query_workspace_md),
    /** scratchpad memory desc */
    scratchpad_md(dnnl_query_scratchpad_md),
    /** memory desc of an execute argument */
    exec_arg_md(dnnl_query_exec_arg_md),

    /** number of dimensions */
    ndims_s32(dnnl_query_ndims_s32),
    /** vector of dimensions */
    dims(dnnl_query_dims),
    /** data type */
    data_type(dnnl_query_data_type),
    /** submemory offset */
    submemory_offset_s64(dnnl_query_submemory_offset_s64),
    /** vector of padded dimensions */
    padded_dims(dnnl_query_padded_dims),
    /** vector of padded offsets */
    padded_offsets(dnnl_query_padded_offsets),
    /** format kind */
    format_kind(dnnl_query_format_kind),
    /**  number of innermost blocks */
    inner_nblks_s32(dnnl_query_inner_nblks_s32),
    /** vector of sizes of the innermost blocks */
    inner_blks(dnnl_query_inner_blks),
    /** vector of logical indices of the blocks */
    inner_idxs(dnnl_query_inner_idxs);
// #ifdef DNNL_EXPERIMENTAL_SPARSE
// #endif

    public final int value;
    private query(int v) { this.value = v; }
    private query(query e) { this.value = e.value; }
    public query intern() { for (query e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Converts query enum value from C++ API to C API type.
 *  @param aquery C++ API query enum value.
 *  @return Corresponding C API query enum value. */

///
///
///
///
///
///
///
///
///
///
///
///
///
///
@Namespace("dnnl") public static native @Cast("dnnl_query_t") int convert_to_c(query aquery);
// Targeting ../memory.java



@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@Cast("dnnl_data_type_t") int a, memory.data_type b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Cast("dnnl_data_type_t") int a, memory.data_type b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(memory.data_type a, @Cast("dnnl_data_type_t") int b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(memory.data_type a, @Cast("dnnl_data_type_t") int b);

@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(@Cast("dnnl_format_tag_t") int a, memory.format_tag b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Cast("dnnl_format_tag_t") int a, memory.format_tag b);
@Namespace("dnnl") public static native @Cast("bool") @Name("operator ==") boolean equals(memory.format_tag a, @Cast("dnnl_format_tag_t") int b);

///
///
@Namespace("dnnl") public static native @Cast("bool") @Name("operator !=") boolean notEquals(memory.format_tag a, @Cast("dnnl_format_tag_t") int b);

/** \} dnnl_api_memory
 <p>
 *  \addtogroup dnnl_api_primitives
 *  \{
 *  \addtogroup dnnl_api_attributes Attributes
 * 
 *  A container for parameters that extend primitives behavior.
 * 
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../post_ops.java



/** \cond DO_NOT_DOCUMENT_THIS */
// Targeting ../primitive_attr.java


// Targeting ../primitive_desc_base.java


// Targeting ../reorder.java



/** \} dnnl_api_reorder
 <p>
 *  \addtogroup dnnl_api_concat Concat
 * 
 *  A primitive to concatenate data by arbitrary dimension.
 * 
 *  @see \ref dev_guide_concat in developer guide
 * 
 *  \{
 <p>
 *  \cond DO_NOT_DOCUMENT_THIS */
@Namespace("dnnl") public static native @ByVal @Cast("std::vector<const_dnnl_memory_desc_t>*") dnnl_memory_desc_vector convert_to_c(
        @Cast("const std::vector<dnnl::memory::desc>*") @ByRef memory_desc_vector mds);
// Targeting ../concat.java


// Targeting ../sum.java


// Targeting ../primitive_desc.java


// Targeting ../convolution_forward.java


// Targeting ../convolution_backward_data.java


// Targeting ../convolution_backward_weights.java


// Targeting ../deconvolution_forward.java


// Targeting ../deconvolution_backward_data.java


// Targeting ../deconvolution_backward_weights.java


// Targeting ../lrn_forward.java


// Targeting ../lrn_backward.java


// Targeting ../eltwise_forward.java


// Targeting ../eltwise_backward.java


// Targeting ../softmax_forward.java


// Targeting ../softmax_backward.java


// Targeting ../batch_normalization_forward.java


// Targeting ../batch_normalization_backward.java


// Targeting ../group_normalization_forward.java


// Targeting ../group_normalization_backward.java


// Targeting ../layer_normalization_forward.java


// Targeting ../layer_normalization_backward.java


// Targeting ../inner_product_forward.java


// Targeting ../inner_product_backward_data.java


// Targeting ../inner_product_backward_weights.java


// Targeting ../rnn_primitive_desc_base.java


// Targeting ../vanilla_rnn_forward.java


// Targeting ../vanilla_rnn_backward.java


// Targeting ../lstm_forward.java


// Targeting ../lstm_backward.java


// Targeting ../gru_forward.java


// Targeting ../gru_backward.java


// Targeting ../lbr_gru_forward.java


// Targeting ../lbr_gru_backward.java


// Targeting ../augru_forward.java


// Targeting ../augru_backward.java


// Targeting ../lbr_augru_forward.java


// Targeting ../lbr_augru_backward.java


// Targeting ../shuffle_forward.java


// Targeting ../shuffle_backward.java


// Targeting ../binary.java


// Targeting ../matmul.java


// Targeting ../resampling_forward.java


// Targeting ../resampling_backward.java


// Targeting ../pooling_forward.java


// Targeting ../pooling_backward.java


// Targeting ../prelu_forward.java


// Targeting ../prelu_backward.java


// Targeting ../reduction.java



/** \} dnnl_api_reduction
 <p>
 *  \} dnnl_api_primitives
 <p>
 *  \addtogroup dnnl_api_service Service
 * 
 *  A set of functions that aid in oneDNN debugging and profiling.
 * 
 *  \{
 <p>
 *  \copydoc dnnl_version_t */

/** Status values returned by the library functions. */
@Namespace("dnnl") public enum status {
    /** \copydoc dnnl_success */
    success(dnnl_success),
    /** \copydoc dnnl_out_of_memory */
    out_of_memory(dnnl_out_of_memory),
    /** \copydoc dnnl_invalid_arguments */
    invalid_arguments(dnnl_invalid_arguments),
    /** \copydoc dnnl_unimplemented */
    unimplemented(dnnl_unimplemented),
    /** \copydoc dnnl_last_impl_reached */
    last_impl_reached(dnnl_last_impl_reached),
    /** \copydoc dnnl_runtime_error */
    runtime_error(dnnl_runtime_error),
    /** \copydoc dnnl_not_required */
    not_required(dnnl_not_required);

    public final int value;
    private status(int v) { this.value = v; }
    private status(status e) { this.value = e.value; }
    public status intern() { for (status e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** \copydoc dnnl_set_verbose() */
@Namespace("dnnl") public static native status set_verbose(int level);

/** \copydoc dnnl_version() */

///
@Namespace("dnnl") public static native @Cast("const dnnl::version_t*") dnnl_version_t version();

/** Returns the floating-point math mode that will be used by default
 *  for all subsequently created primitives.
 * 
 *  @return Output FP math mode. */
@Namespace("dnnl") public static native fpmath_mode get_default_fpmath_mode();

/** \copydoc dnnl_set_default_fpmath_mode() */
@Namespace("dnnl") public static native status set_default_fpmath_mode(fpmath_mode mode);
@Namespace("dnnl") public static native @Cast("dnnl::status") int set_default_fpmath_mode(@Cast("dnnl::fpmath_mode") int mode);

/** \copydoc dnnl_set_jit_dump() */
@Namespace("dnnl") public static native status set_jit_dump(int enable);

/** \copydoc dnnl_set_jit_profiling_flags() */
@Namespace("dnnl") public static native status set_jit_profiling_flags(@Cast("unsigned") int flags);

/** \copydoc dnnl_set_jit_profiling_jitdumpdir() */
@Namespace("dnnl") public static native status set_jit_profiling_jitdumpdir(@StdString BytePointer dir);
@Namespace("dnnl") public static native @Cast("dnnl::status") int set_jit_profiling_jitdumpdir(@StdString String dir);

/** \copydoc dnnl_cpu_isa_t */
@Namespace("dnnl") public enum cpu_isa {
    /** \copydoc dnnl_cpu_isa_default */
    isa_default(dnnl_cpu_isa_default),
    /** \copydoc dnnl_cpu_isa_sse41 */
    sse41(dnnl_cpu_isa_sse41),
    /** \copydoc dnnl_cpu_isa_avx */
    avx(dnnl_cpu_isa_avx),
    /** \copydoc dnnl_cpu_isa_avx2 */
    avx2(dnnl_cpu_isa_avx2),
    /** \copydoc dnnl_cpu_isa_avx2_vnni */
    avx2_vnni(dnnl_cpu_isa_avx2_vnni),
    /** \copydoc dnnl_cpu_isa_avx2_vnni_2 */
    avx2_vnni_2(dnnl_cpu_isa_avx2_vnni_2),
    /** \copydoc dnnl_cpu_isa_avx512_core */
    avx512_core(dnnl_cpu_isa_avx512_core),
    /** \copydoc dnnl_cpu_isa_avx512_core_vnni */
    avx512_core_vnni(dnnl_cpu_isa_avx512_core_vnni),
    /** \copydoc dnnl_cpu_isa_avx512_core_bf16 */
    avx512_core_bf16(dnnl_cpu_isa_avx512_core_bf16),
    /** \copydoc dnnl_cpu_isa_avx512_core_fp16 */
    avx512_core_fp16(dnnl_cpu_isa_avx512_core_fp16),
    /** \copydoc dnnl_cpu_isa_avx512_core_amx */
    avx512_core_amx(dnnl_cpu_isa_avx512_core_amx),
    /** \copydoc dnnl_cpu_isa_avx512_core_amx_fp16 */
    avx512_core_amx_fp16(dnnl_cpu_isa_avx512_core_amx_fp16);

    public final int value;
    private cpu_isa(int v) { this.value = v; }
    private cpu_isa(cpu_isa e) { this.value = e.value; }
    public cpu_isa intern() { for (cpu_isa e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** \copydoc dnnl_set_max_cpu_isa() */
@Namespace("dnnl") public static native status set_max_cpu_isa(cpu_isa isa);
@Namespace("dnnl") public static native @Cast("dnnl::status") int set_max_cpu_isa(@Cast("dnnl::cpu_isa") int isa);

/** \copydoc dnnl_get_effective_cpu_isa() */
@Namespace("dnnl") public static native cpu_isa get_effective_cpu_isa();

/** \copydoc dnnl_cpu_isa_hints_t */
@Namespace("dnnl") public enum cpu_isa_hints {
    /** \copydoc dnnl_cpu_isa_no_hints */
    no_hints(dnnl_cpu_isa_no_hints),
    /** \copydoc dnnl_cpu_isa_prefer_ymm */
    prefer_ymm(dnnl_cpu_isa_prefer_ymm);

    public final int value;
    private cpu_isa_hints(int v) { this.value = v; }
    private cpu_isa_hints(cpu_isa_hints e) { this.value = e.value; }
    public cpu_isa_hints intern() { for (cpu_isa_hints e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** \copydoc dnnl_set_cpu_isa_hints() */
@Namespace("dnnl") public static native status set_cpu_isa_hints(cpu_isa_hints isa_hints);
@Namespace("dnnl") public static native @Cast("dnnl::status") int set_cpu_isa_hints(@Cast("dnnl::cpu_isa_hints") int isa_hints);

/** \copydoc dnnl_get_cpu_isa_hints() */

///
///
@Namespace("dnnl") public static native cpu_isa_hints get_cpu_isa_hints();

/** \} dnnl_api_service */

// #ifdef DNNL_EXPERIMENTAL_PROFILING
// #endif

/** \addtogroup dnnl_api_primitive_cache Primitive Cache
/**
/** A set of functions that provide primitive cache control.
/**
/** \{
<p>
/** Returns the number of primitives that can be held in the primitive cache
/** at the same time. */
@Namespace("dnnl") public static native int get_primitive_cache_capacity();

/** \copydoc dnnl_set_primitive_cache_capacity(int capacity) */

///
///
@Namespace("dnnl") public static native void set_primitive_cache_capacity(int _capacity);

/** \} dnnl_api_primitive_cache
 <p>
 *  \addtogroup dnnl_api_blas BLAS functions
 * 
 *  A subset of Basic Linear Algebra (BLAS) functions that perform
 *  matrix-matrix multiplication.
 * 
 *  \{
 <p>
 *  \copydoc dnnl_sgemm() */
@Namespace("dnnl") public static native status sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N,
        @Cast("dnnl_dim_t") long K, float alpha, @Const FloatPointer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatPointer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatPointer C, @Cast("dnnl_dim_t") long ldc);
@Namespace("dnnl") public static native @Cast("dnnl::status") int sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N,
        @Cast("dnnl_dim_t") long K, float alpha, @Const FloatBuffer A, @Cast("dnnl_dim_t") long lda,
        @Const FloatBuffer B, @Cast("dnnl_dim_t") long ldb, float beta, FloatBuffer C, @Cast("dnnl_dim_t") long ldc);
@Namespace("dnnl") public static native status sgemm(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("dnnl_dim_t") long M, @Cast("dnnl_dim_t") long N,
        @Cast("dnnl_dim_t") long K, float alpha, @Const float[] A, @Cast("dnnl_dim_t") long lda,
        @Const float[] B, @Cast("dnnl_dim_t") long ldb, float beta, float[] C, @Cast("dnnl_dim_t") long ldc);

/** \copydoc dnnl_gemm_u8s8s32() */
@Namespace("dnnl") public static native status gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") BytePointer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
@Namespace("dnnl") public static native @Cast("dnnl::status") int gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
@Namespace("dnnl") public static native status gemm_u8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Cast("const uint8_t*") byte[] A,
        @Cast("dnnl_dim_t") long lda, @Cast("uint8_t") byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);

/** \copydoc dnnl_gemm_s8s8s32() */
@Namespace("dnnl") public static native status gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const BytePointer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const BytePointer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntPointer C, @Cast("dnnl_dim_t") long ldc, @Const IntPointer co);
@Namespace("dnnl") public static native @Cast("dnnl::status") int gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const ByteBuffer A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const ByteBuffer B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, IntBuffer C, @Cast("dnnl_dim_t") long ldc, @Const IntBuffer co);
@Namespace("dnnl") public static native status gemm_s8s8s32(@Cast("char") byte transa, @Cast("char") byte transb, @Cast("char") byte offsetc, @Cast("dnnl_dim_t") long M,
        @Cast("dnnl_dim_t") long N, @Cast("dnnl_dim_t") long K, float alpha, @Const byte[] A,
        @Cast("dnnl_dim_t") long lda, byte ao, @Const byte[] B, @Cast("dnnl_dim_t") long ldb, byte bo,
        float beta, int[] C, @Cast("dnnl_dim_t") long ldc, @Const int[] co);

/** \} dnnl_api_blas */

// implementation section

/** \cond DO_NOT_DOCUMENT_THIS */









/** \endcond */

// #undef DNNL_DEFINE_BITMASK_OPS

 // namespace dnnl

/** oneAPI namespace
 <p>
 *  The oneAPI namespace.
 *  Contains the oneapi::dnnl namespace as an alias to the ::dnnl namespace. */
// Note: without this guard, doxygen warns of potentially recursive namespace
// #ifndef DOXYGEN_SHOULD_SKIP_THIS
// #endif
 // namespace oneapi

/** \} dnnl_api */

// #endif /* ONEAPI_DNNL_DNNL_HPP */


}

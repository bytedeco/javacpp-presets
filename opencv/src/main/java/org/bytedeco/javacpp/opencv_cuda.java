// Targeted by JavaCPP version 1.3.3-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.javacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.opencv_core.*;

public class opencv_cuda extends org.bytedeco.javacpp.presets.opencv_cuda {
    static { Loader.load(); }

// Parsed from <opencv2/core/cuda.hpp>

/*M///////////////////////////////////////////////////////////////////////////////////////
//
//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
//
//  By downloading, copying, installing or using the software you agree to this license.
//  If you do not agree to this license, do not download, install,
//  copy or use the software.
//
//
//                          License Agreement
//                For Open Source Computer Vision Library
//
// Copyright (C) 2000-2008, Intel Corporation, all rights reserved.
// Copyright (C) 2009, Willow Garage Inc., all rights reserved.
// Copyright (C) 2013, OpenCV Foundation, all rights reserved.
// Third party copyrights are property of their respective owners.
//
// Redistribution and use in source and binary forms, with or without modification,
// are permitted provided that the following conditions are met:
//
//   * Redistribution's of source code must retain the above copyright notice,
//     this list of conditions and the following disclaimer.
//
//   * Redistribution's in binary form must reproduce the above copyright notice,
//     this list of conditions and the following disclaimer in the documentation
//     and/or other materials provided with the distribution.
//
//   * The name of the copyright holders may not be used to endorse or promote products
//     derived from this software without specific prior written permission.
//
// This software is provided by the copyright holders and contributors "as is" and
// any express or implied warranties, including, but not limited to, the implied
// warranties of merchantability and fitness for a particular purpose are disclaimed.
// In no event shall the Intel Corporation or contributors be liable for any direct,
// indirect, incidental, special, exemplary, or consequential damages
// (including, but not limited to, procurement of substitute goods or services;
// loss of use, data, or profits; or business interruption) however caused
// and on any theory of liability, whether in contract, strict liability,
// or tort (including negligence or otherwise) arising in any way out of
// the use of this software, even if advised of the possibility of such damage.
//
//M*/

// #ifndef OPENCV_CORE_CUDA_HPP
// #define OPENCV_CORE_CUDA_HPP

// #ifndef __cplusplus
// #endif

// #include "opencv2/core.hpp"
// #include "opencv2/core/cuda_types.hpp"

/**
  \defgroup cuda CUDA-accelerated Computer Vision
  \{
    \defgroup cudacore Core part
    \{
      \defgroup cudacore_init Initalization and Information
      \defgroup cudacore_struct Data Structures
    \}
  \}
 */

/** \addtogroup cudacore_struct
 *  \{ */

//===================================================================================
// GpuMat
//===================================================================================

/** \brief Base storage class for GPU memory with reference counting.
<p>
Its interface matches the Mat interface with the following limitations:
<p>
-   no arbitrary dimensions support (only 2D)
-   no functions that return references to their data (because references on GPU are not valid for
    CPU)
-   no expression templates technique support
<p>
Beware that the latter limitation may lead to overloaded matrix operators that cause memory
allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be
passed directly to the kernel.
<p>
\note In contrast with Mat, in most cases GpuMat::isContinuous() == false . This means that rows are
aligned to a size depending on the hardware. Single-row GpuMat is always a continuous matrix.
<p>
\note You are not recommended to leave static or global GpuMat variables allocated, that is, to rely
on its destructor. The destruction order of such variables and CUDA context is undefined. GPU memory
release function returns error if the CUDA context has been destroyed before.
<p>
\sa Mat
 */
@Namespace("cv::cuda") @NoOffset public static class GpuMat extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public GpuMat(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public GpuMat(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public GpuMat position(long position) {
        return (GpuMat)super.position(position);
    }


    /** default allocator */

    /** default constructor */
    public GpuMat() { super((Pointer)null); allocate(); }
    private native void allocate();

    /** constructs GpuMat of the specified size and type */
    public GpuMat(int rows, int cols, int type) { super((Pointer)null); allocate(rows, cols, type); }
    private native void allocate(int rows, int cols, int type);
    public GpuMat(@ByVal Size size, int type) { super((Pointer)null); allocate(size, type); }
    private native void allocate(@ByVal Size size, int type);

    /** constucts GpuMat and fills it with the specified value _s */
    public GpuMat(int rows, int cols, int type, @ByVal Scalar s) { super((Pointer)null); allocate(rows, cols, type, s); }
    private native void allocate(int rows, int cols, int type, @ByVal Scalar s);
    public GpuMat(@ByVal Size size, int type, @ByVal Scalar s) { super((Pointer)null); allocate(size, type, s); }
    private native void allocate(@ByVal Size size, int type, @ByVal Scalar s);

    /** copy constructor */
    public GpuMat(@Const @ByRef GpuMat m) { super((Pointer)null); allocate(m); }
    private native void allocate(@Const @ByRef GpuMat m);

    /** constructor for GpuMat headers pointing to user-allocated data */
    public GpuMat(int rows, int cols, int type, Pointer data, @Cast("size_t") long step/*=cv::Mat::AUTO_STEP*/) { super((Pointer)null); allocate(rows, cols, type, data, step); }
    private native void allocate(int rows, int cols, int type, Pointer data, @Cast("size_t") long step/*=cv::Mat::AUTO_STEP*/);
    public GpuMat(int rows, int cols, int type, Pointer data) { super((Pointer)null); allocate(rows, cols, type, data); }
    private native void allocate(int rows, int cols, int type, Pointer data);
    public GpuMat(@ByVal Size size, int type, Pointer data, @Cast("size_t") long step/*=cv::Mat::AUTO_STEP*/) { super((Pointer)null); allocate(size, type, data, step); }
    private native void allocate(@ByVal Size size, int type, Pointer data, @Cast("size_t") long step/*=cv::Mat::AUTO_STEP*/);
    public GpuMat(@ByVal Size size, int type, Pointer data) { super((Pointer)null); allocate(size, type, data); }
    private native void allocate(@ByVal Size size, int type, Pointer data);

    /** creates a GpuMat header for a part of the bigger matrix */
    public GpuMat(@Const @ByRef GpuMat m, @ByVal Range rowRange, @ByVal Range colRange) { super((Pointer)null); allocate(m, rowRange, colRange); }
    private native void allocate(@Const @ByRef GpuMat m, @ByVal Range rowRange, @ByVal Range colRange);
    public GpuMat(@Const @ByRef GpuMat m, @ByVal Rect roi) { super((Pointer)null); allocate(m, roi); }
    private native void allocate(@Const @ByRef GpuMat m, @ByVal Rect roi);

    /** builds GpuMat from host memory (Blocking call) */
    public GpuMat(@ByVal Mat arr) { super((Pointer)null); allocate(arr); }
    private native void allocate(@ByVal Mat arr);
    public GpuMat(@ByVal UMat arr) { super((Pointer)null); allocate(arr); }
    private native void allocate(@ByVal UMat arr);

    /** destructor - calls release() */

    /** assignment operators */
    public native @ByRef @Name("operator =") GpuMat put(@Const @ByRef GpuMat m);

    /** allocates new GpuMat data unless the GpuMat already has specified size and type */
    public native void create(int rows, int cols, int type);
    public native void create(@ByVal Size size, int type);

    /** decreases reference counter, deallocate the data when reference counter reaches 0 */
    public native void release();

    /** swaps with other smart pointer */
    public native void swap(@ByRef GpuMat mat);

    /** pefroms upload data to GpuMat (Blocking call) */
    public native void upload(@ByVal Mat arr);
    public native void upload(@ByVal UMat arr);
    public native void upload(@ByVal GpuMat arr);

    /** pefroms upload data to GpuMat (Non-Blocking call) */
    public native void upload(@ByVal Mat arr, @ByRef Stream stream);
    public native void upload(@ByVal UMat arr, @ByRef Stream stream);
    public native void upload(@ByVal GpuMat arr, @ByRef Stream stream);

    /** pefroms download data from device to host memory (Blocking call) */
    public native void download(@ByVal Mat dst);
    public native void download(@ByVal UMat dst);
    public native void download(@ByVal GpuMat dst);

    /** pefroms download data from device to host memory (Non-Blocking call) */
    public native void download(@ByVal Mat dst, @ByRef Stream stream);
    public native void download(@ByVal UMat dst, @ByRef Stream stream);
    public native void download(@ByVal GpuMat dst, @ByRef Stream stream);

    /** returns deep copy of the GpuMat, i.e. the data is copied */
    public native @ByVal GpuMat clone();

    /** copies the GpuMat content to device memory (Blocking call) */
    public native void copyTo(@ByVal Mat dst);
    public native void copyTo(@ByVal UMat dst);
    public native void copyTo(@ByVal GpuMat dst);

    /** copies the GpuMat content to device memory (Non-Blocking call) */
    public native void copyTo(@ByVal Mat dst, @ByRef Stream stream);
    public native void copyTo(@ByVal UMat dst, @ByRef Stream stream);
    public native void copyTo(@ByVal GpuMat dst, @ByRef Stream stream);

    /** copies those GpuMat elements to "m" that are marked with non-zero mask elements (Blocking call) */
    public native void copyTo(@ByVal Mat dst, @ByVal Mat mask);
    public native void copyTo(@ByVal UMat dst, @ByVal UMat mask);
    public native void copyTo(@ByVal GpuMat dst, @ByVal GpuMat mask);

    /** copies those GpuMat elements to "m" that are marked with non-zero mask elements (Non-Blocking call) */
    public native void copyTo(@ByVal Mat dst, @ByVal Mat mask, @ByRef Stream stream);
    public native void copyTo(@ByVal UMat dst, @ByVal UMat mask, @ByRef Stream stream);
    public native void copyTo(@ByVal GpuMat dst, @ByVal GpuMat mask, @ByRef Stream stream);

    /** sets some of the GpuMat elements to s (Blocking call) */
    public native @ByRef GpuMat setTo(@ByVal Scalar s);

    /** sets some of the GpuMat elements to s (Non-Blocking call) */
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByRef Stream stream);

    /** sets some of the GpuMat elements to s, according to the mask (Blocking call) */
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal Mat mask);
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal UMat mask);
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal GpuMat mask);

    /** sets some of the GpuMat elements to s, according to the mask (Non-Blocking call) */
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal Mat mask, @ByRef Stream stream);
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal UMat mask, @ByRef Stream stream);
    public native @ByRef GpuMat setTo(@ByVal Scalar s, @ByVal GpuMat mask, @ByRef Stream stream);

    /** converts GpuMat to another datatype (Blocking call) */
    public native void convertTo(@ByVal Mat dst, int rtype);
    public native void convertTo(@ByVal UMat dst, int rtype);
    public native void convertTo(@ByVal GpuMat dst, int rtype);

    /** converts GpuMat to another datatype (Non-Blocking call) */
    public native void convertTo(@ByVal Mat dst, int rtype, @ByRef Stream stream);
    public native void convertTo(@ByVal UMat dst, int rtype, @ByRef Stream stream);
    public native void convertTo(@ByVal GpuMat dst, int rtype, @ByRef Stream stream);

    /** converts GpuMat to another datatype with scaling (Blocking call) */
    public native void convertTo(@ByVal Mat dst, int rtype, double alpha, double beta/*=0.0*/);
    public native void convertTo(@ByVal Mat dst, int rtype, double alpha);
    public native void convertTo(@ByVal UMat dst, int rtype, double alpha, double beta/*=0.0*/);
    public native void convertTo(@ByVal UMat dst, int rtype, double alpha);
    public native void convertTo(@ByVal GpuMat dst, int rtype, double alpha, double beta/*=0.0*/);
    public native void convertTo(@ByVal GpuMat dst, int rtype, double alpha);

    /** converts GpuMat to another datatype with scaling (Non-Blocking call) */
    public native void convertTo(@ByVal Mat dst, int rtype, double alpha, @ByRef Stream stream);
    public native void convertTo(@ByVal UMat dst, int rtype, double alpha, @ByRef Stream stream);
    public native void convertTo(@ByVal GpuMat dst, int rtype, double alpha, @ByRef Stream stream);

    /** converts GpuMat to another datatype with scaling (Non-Blocking call) */
    public native void convertTo(@ByVal Mat dst, int rtype, double alpha, double beta, @ByRef Stream stream);
    public native void convertTo(@ByVal UMat dst, int rtype, double alpha, double beta, @ByRef Stream stream);
    public native void convertTo(@ByVal GpuMat dst, int rtype, double alpha, double beta, @ByRef Stream stream);

    public native void assignTo(@ByRef GpuMat m, int type/*=-1*/);
    public native void assignTo(@ByRef GpuMat m);

    /** returns pointer to y-th row */
    public native @Cast("uchar*") BytePointer ptr(int y/*=0*/);
    public native @Cast("uchar*") BytePointer ptr();

    /** template version of the above method */

    /** returns a new GpuMat header for the specified row */
    public native @ByVal GpuMat row(int y);

    /** returns a new GpuMat header for the specified column */
    public native @ByVal GpuMat col(int x);

    /** ... for the specified row span */
    public native @ByVal GpuMat rowRange(int startrow, int endrow);
    public native @ByVal GpuMat rowRange(@ByVal Range r);

    /** ... for the specified column span */
    public native @ByVal GpuMat colRange(int startcol, int endcol);
    public native @ByVal GpuMat colRange(@ByVal Range r);

    /** extracts a rectangular sub-GpuMat (this is a generalized form of row, rowRange etc.) */
    public native @ByVal @Name("operator ()") GpuMat apply(@ByVal Range rowRange, @ByVal Range colRange);
    public native @ByVal @Name("operator ()") GpuMat apply(@ByVal Rect roi);

    /** creates alternative GpuMat header for the same data, with different
     *  number of channels and/or different number of rows */
    public native @ByVal GpuMat reshape(int cn, int rows/*=0*/);
    public native @ByVal GpuMat reshape(int cn);

    /** locates GpuMat header within a parent GpuMat */
    public native void locateROI(@ByRef Size wholeSize, @ByRef Point ofs);

    /** moves/resizes the current GpuMat ROI inside the parent GpuMat */
    public native @ByRef GpuMat adjustROI(int dtop, int dbottom, int dleft, int dright);

    /** returns true iff the GpuMat data is continuous
     *  (i.e. when there are no gaps between successive rows) */
    public native @Cast("bool") boolean isContinuous();

    /** returns element size in bytes */
    public native @Cast("size_t") long elemSize();

    /** returns the size of element channel in bytes */
    public native @Cast("size_t") long elemSize1();

    /** returns element type */
    public native int type();

    /** returns element type */
    public native int depth();

    /** returns number of channels */
    public native int channels();

    /** returns step/elemSize1() */
    public native @Cast("size_t") long step1();

    /** returns GpuMat size : width == number of columns, height == number of rows */
    public native @ByVal Size size();

    /** returns true if GpuMat data is NULL */
    public native @Cast("bool") boolean empty();

    /** includes several bit-fields:
    - the magic signature
    - continuity flag
    - depth
    - number of channels
    */
    public native int flags(); public native GpuMat flags(int flags);

    /** the number of rows and columns */
    public native int rows(); public native GpuMat rows(int rows);
    public native int cols(); public native GpuMat cols(int cols);

    /** a distance between successive rows in bytes; includes the gap if any */
    public native @Cast("size_t") long step(); public native GpuMat step(long step);

    /** pointer to the data */
    public native @Cast("uchar*") BytePointer data(); public native GpuMat data(BytePointer data);

    /** pointer to the reference counter;
     *  when GpuMat points to user-allocated data, the pointer is NULL */
    public native IntPointer refcount(); public native GpuMat refcount(IntPointer refcount);

    /** helper fields used in locateROI and adjustROI */
    public native @Cast("uchar*") BytePointer datastart(); public native GpuMat datastart(BytePointer datastart);
    @MemberGetter public native @Cast("const uchar*") BytePointer dataend();

    /** allocator */
}

/** \brief Creates a continuous matrix.
<p>
@param rows Row count.
@param cols Column count.
@param type Type of the matrix.
@param arr Destination matrix. This parameter changes only if it has a proper type and area (
\f$\texttt{rows} \times \texttt{cols}\f$ ).
<p>
Matrix is called continuous if its elements are stored continuously, that is, without gaps at the
end of each row.
 */
@Namespace("cv::cuda") public static native void createContinuous(int rows, int cols, int type, @ByVal Mat arr);
@Namespace("cv::cuda") public static native void createContinuous(int rows, int cols, int type, @ByVal UMat arr);
@Namespace("cv::cuda") public static native void createContinuous(int rows, int cols, int type, @ByVal GpuMat arr);

/** \brief Ensures that the size of a matrix is big enough and the matrix has a proper type.
<p>
@param rows Minimum desired number of rows.
@param cols Minimum desired number of columns.
@param type Desired matrix type.
@param arr Destination matrix.
<p>
The function does not reallocate memory if the matrix has proper attributes already.
 */
@Namespace("cv::cuda") public static native void ensureSizeIsEnough(int rows, int cols, int type, @ByVal Mat arr);
@Namespace("cv::cuda") public static native void ensureSizeIsEnough(int rows, int cols, int type, @ByVal UMat arr);
@Namespace("cv::cuda") public static native void ensureSizeIsEnough(int rows, int cols, int type, @ByVal GpuMat arr);

/** BufferPool management (must be called before Stream creation) */
@Namespace("cv::cuda") public static native void setBufferPoolUsage(@Cast("bool") boolean on);
@Namespace("cv::cuda") public static native void setBufferPoolConfig(int deviceId, @Cast("size_t") long stackSize, int stackCount);

//===================================================================================
// HostMem
//===================================================================================

/** \brief Class with reference counting wrapping special memory type allocation functions from CUDA.
<p>
Its interface is also Mat-like but with additional memory type parameters.
<p>
-   **PAGE_LOCKED** sets a page locked memory type used commonly for fast and asynchronous
    uploading/downloading data from/to GPU.
-   **SHARED** specifies a zero copy memory allocation that enables mapping the host memory to GPU
    address space, if supported.
-   **WRITE_COMBINED** sets the write combined buffer that is not cached by CPU. Such buffers are
    used to supply GPU with data when GPU only reads it. The advantage is a better CPU cache
    utilization.
<p>
\note Allocation size of such memory types is usually limited. For more details, see *CUDA 2.2
Pinned Memory APIs* document or *CUDA C Programming Guide*.
 */
@Namespace("cv::cuda") @NoOffset public static class HostMem extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public HostMem(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public HostMem(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public HostMem position(long position) {
        return (HostMem)super.position(position);
    }

    /** enum cv::cuda::HostMem::AllocType */
    public static final int PAGE_LOCKED = 1, SHARED = 2, WRITE_COMBINED = 4;

    public static native MatAllocator getAllocator(@Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public static native MatAllocator getAllocator();

    public HostMem(@Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(alloc_type); }
    private native void allocate(@Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem() { super((Pointer)null); allocate(); }
    private native void allocate();

    public HostMem(@Const @ByRef HostMem m) { super((Pointer)null); allocate(m); }
    private native void allocate(@Const @ByRef HostMem m);

    public HostMem(int rows, int cols, int type, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(rows, cols, type, alloc_type); }
    private native void allocate(int rows, int cols, int type, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem(int rows, int cols, int type) { super((Pointer)null); allocate(rows, cols, type); }
    private native void allocate(int rows, int cols, int type);
    public HostMem(@ByVal Size size, int type, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(size, type, alloc_type); }
    private native void allocate(@ByVal Size size, int type, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem(@ByVal Size size, int type) { super((Pointer)null); allocate(size, type); }
    private native void allocate(@ByVal Size size, int type);

    /** creates from host memory with coping data */
    public HostMem(@ByVal Mat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(arr, alloc_type); }
    private native void allocate(@ByVal Mat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem(@ByVal Mat arr) { super((Pointer)null); allocate(arr); }
    private native void allocate(@ByVal Mat arr);
    public HostMem(@ByVal UMat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(arr, alloc_type); }
    private native void allocate(@ByVal UMat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem(@ByVal UMat arr) { super((Pointer)null); allocate(arr); }
    private native void allocate(@ByVal UMat arr);
    public HostMem(@ByVal GpuMat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/) { super((Pointer)null); allocate(arr, alloc_type); }
    private native void allocate(@ByVal GpuMat arr, @Cast("cv::cuda::HostMem::AllocType") int alloc_type/*=cv::cuda::HostMem::PAGE_LOCKED*/);
    public HostMem(@ByVal GpuMat arr) { super((Pointer)null); allocate(arr); }
    private native void allocate(@ByVal GpuMat arr);

    public native @ByRef @Name("operator =") HostMem put(@Const @ByRef HostMem m);

    /** swaps with other smart pointer */
    public native void swap(@ByRef HostMem b);

    /** returns deep copy of the matrix, i.e. the data is copied */
    public native @ByVal HostMem clone();

    /** allocates new matrix data unless the matrix already has specified size and type. */
    public native void create(int rows, int cols, int type);
    public native void create(@ByVal Size size, int type);

    /** creates alternative HostMem header for the same data, with different
     *  number of channels and/or different number of rows */
    public native @ByVal HostMem reshape(int cn, int rows/*=0*/);
    public native @ByVal HostMem reshape(int cn);

    /** decrements reference counter and released memory if needed. */
    public native void release();

    /** returns matrix header with disabled reference counting for HostMem data. */
    public native @ByVal Mat createMatHeader();

    /** \brief Maps CPU memory to GPU address space and creates the cuda::GpuMat header without reference counting
    for it.
    <p>
    This can be done only if memory was allocated with the SHARED flag and if it is supported by the
    hardware. Laptops often share video and CPU memory, so address spaces can be mapped, which
    eliminates an extra copy.
     */
    public native @ByVal GpuMat createGpuMatHeader();

    // Please see cv::Mat for descriptions
    public native @Cast("bool") boolean isContinuous();
    public native @Cast("size_t") long elemSize();
    public native @Cast("size_t") long elemSize1();
    public native int type();
    public native int depth();
    public native int channels();
    public native @Cast("size_t") long step1();
    public native @ByVal Size size();
    public native @Cast("bool") boolean empty();

    // Please see cv::Mat for descriptions
    public native int flags(); public native HostMem flags(int flags);
    public native int rows(); public native HostMem rows(int rows);
    public native int cols(); public native HostMem cols(int cols);
    public native @Cast("size_t") long step(); public native HostMem step(long step);

    public native @Cast("uchar*") BytePointer data(); public native HostMem data(BytePointer data);
    public native IntPointer refcount(); public native HostMem refcount(IntPointer refcount);

    public native @Cast("uchar*") BytePointer datastart(); public native HostMem datastart(BytePointer datastart);
    @MemberGetter public native @Cast("const uchar*") BytePointer dataend();

    public native @Cast("cv::cuda::HostMem::AllocType") int alloc_type(); public native HostMem alloc_type(int alloc_type);
}

/** \brief Page-locks the memory of matrix and maps it for the device(s).
<p>
@param m Input matrix.
 */
@Namespace("cv::cuda") public static native void registerPageLocked(@ByRef Mat m);

/** \brief Unmaps the memory of matrix and makes it pageable again.
<p>
@param m Input matrix.
 */
@Namespace("cv::cuda") public static native void unregisterPageLocked(@ByRef Mat m);

//===================================================================================
// Stream
//===================================================================================

/** \brief This class encapsulates a queue of asynchronous calls.
<p>
\note Currently, you may face problems if an operation is enqueued twice with different data. Some
functions use the constant GPU memory, and next call may update the memory before the previous one
has been finished. But calling different operations asynchronously is safe because each operation
has its own constant buffer. Memory copy/upload/download/set operations to the buffers you hold are
also safe.
<p>
\note The Stream class is not thread-safe. Please use different Stream objects for different CPU threads.
<p>
<pre>{@code
void thread1()
{
    cv::cuda::Stream stream1;
    cv::cuda::func1(..., stream1);
}

void thread2()
{
    cv::cuda::Stream stream2;
    cv::cuda::func2(..., stream2);
}
}</pre>
<p>
\note By default all CUDA routines are launched in Stream::Null() object, if the stream is not specified by user.
In multi-threading environment the stream objects must be passed explicitly (see previous note).
 */
@Namespace("cv::cuda") @NoOffset public static class Stream extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Stream(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public Stream(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public Stream position(long position) {
        return (Stream)super.position(position);
    }

    public static class StreamCallback extends FunctionPointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public    StreamCallback(Pointer p) { super(p); }
        protected StreamCallback() { allocate(); }
        private native void allocate();
        public native void call(int status, Pointer userData);
    }

    /** creates a new asynchronous stream */
    public Stream() { super((Pointer)null); allocate(); }
    private native void allocate();

    /** \brief Returns true if the current stream queue is finished. Otherwise, it returns false.
    */
    public native @Cast("bool") boolean queryIfComplete();

    /** \brief Blocks the current CPU thread until all operations in the stream are complete.
    */
    public native void waitForCompletion();

    /** \brief Makes a compute stream wait on an event.
    */
    public native void waitEvent(@Const @ByRef Event event);

    /** \brief Adds a callback to be called on the host after all currently enqueued items in the stream have
    completed.
    <p>
    \note Callbacks must not make any CUDA API calls. Callbacks must not perform any synchronization
    that may depend on outstanding device work or other callbacks that are not mandated to run earlier.
    Callbacks without a mandated order (in independent streams) execute in undefined order and may be
    serialized.
     */
    public native void enqueueHostCallback(StreamCallback callback, Pointer userData);

    /** return Stream object for default CUDA stream */
    public static native @ByRef Stream Null();

    /** returns true if stream object is not default (!= 0) */
    

    @Opaque public static class Impl extends Pointer {
        /** Empty constructor. Calls {@code super((Pointer)null)}. */
        public Impl() { super((Pointer)null); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Impl(Pointer p) { super(p); }
    }
}

@Namespace("cv::cuda") @NoOffset public static class Event extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Event(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public Event(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public Event position(long position) {
        return (Event)super.position(position);
    }

    /** enum cv::cuda::Event::CreateFlags */
    public static final int
        /** Default event flag */
        DEFAULT        =  0x00,
        /** Event uses blocking synchronization */
        BLOCKING_SYNC  =  0x01,
        /** Event will not record timing data */
        DISABLE_TIMING =  0x02,
        /** Event is suitable for interprocess use. DisableTiming must be set */
        INTERPROCESS   =  0x04;

    public Event(@Cast("cv::cuda::Event::CreateFlags") int flags/*=cv::cuda::Event::DEFAULT*/) { super((Pointer)null); allocate(flags); }
    private native void allocate(@Cast("cv::cuda::Event::CreateFlags") int flags/*=cv::cuda::Event::DEFAULT*/);
    public Event() { super((Pointer)null); allocate(); }
    private native void allocate();

    /** records an event */
    public native void record(@ByRef(nullValue = "cv::cuda::Stream::Null()") Stream stream);
    public native void record();

    /** queries an event's status */
    public native @Cast("bool") boolean queryIfComplete();

    /** waits for an event to complete */
    public native void waitForCompletion();

    /** computes the elapsed time between events */
    public static native float elapsedTime(@Const @ByRef Event start, @Const @ByRef Event end);

    @Opaque public static class Impl extends Pointer {
        /** Empty constructor. Calls {@code super((Pointer)null)}. */
        public Impl() { super((Pointer)null); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Impl(Pointer p) { super(p); }
    }
}

/** \} cudacore_struct */

//===================================================================================
// Initialization & Info
//===================================================================================

/** \addtogroup cudacore_init
/** \{
<p>
/** \brief Returns the number of installed CUDA-enabled devices.
<p>
Use this function before any other CUDA functions calls. If OpenCV is compiled without CUDA support,
this function returns 0.
 */
@Namespace("cv::cuda") public static native int getCudaEnabledDeviceCount();

/** \brief Sets a device and initializes it for the current thread.
<p>
@param device System index of a CUDA device starting with 0.
<p>
If the call of this function is omitted, a default device is initialized at the fist CUDA usage.
 */
@Namespace("cv::cuda") public static native void setDevice(int device);

/** \brief Returns the current device index set by cuda::setDevice or initialized by default.
 */
@Namespace("cv::cuda") public static native int getDevice();

/** \brief Explicitly destroys and cleans up all resources associated with the current device in the current
process.
<p>
Any subsequent API call to this device will reinitialize the device.
 */
@Namespace("cv::cuda") public static native void resetDevice();

/** \brief Enumeration providing CUDA computing features.
 */
/** enum cv::cuda::FeatureSet */
public static final int
    FEATURE_SET_COMPUTE_10 = 10,
    FEATURE_SET_COMPUTE_11 = 11,
    FEATURE_SET_COMPUTE_12 = 12,
    FEATURE_SET_COMPUTE_13 = 13,
    FEATURE_SET_COMPUTE_20 = 20,
    FEATURE_SET_COMPUTE_21 = 21,
    FEATURE_SET_COMPUTE_30 = 30,
    FEATURE_SET_COMPUTE_32 = 32,
    FEATURE_SET_COMPUTE_35 = 35,
    FEATURE_SET_COMPUTE_50 = 50,

    GLOBAL_ATOMICS =  FEATURE_SET_COMPUTE_11,
    SHARED_ATOMICS =  FEATURE_SET_COMPUTE_12,
    NATIVE_DOUBLE =  FEATURE_SET_COMPUTE_13,
    WARP_SHUFFLE_FUNCTIONS =  FEATURE_SET_COMPUTE_30,
    DYNAMIC_PARALLELISM =  FEATURE_SET_COMPUTE_35;

/** checks whether current device supports the given feature */
@Namespace("cv::cuda") public static native @Cast("bool") boolean deviceSupports(@Cast("cv::cuda::FeatureSet") int feature_set);

/** \brief Class providing a set of static methods to check what NVIDIA\* card architecture the CUDA module was
built for.
<p>
According to the CUDA C Programming Guide Version 3.2: "PTX code produced for some specific compute
capability can always be compiled to binary code of greater or equal compute capability".
 */
@Namespace("cv::cuda") public static class TargetArchs extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public TargetArchs() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public TargetArchs(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public TargetArchs(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public TargetArchs position(long position) {
        return (TargetArchs)super.position(position);
    }

    /** \brief The following method checks whether the module was built with the support of the given feature:
    <p>
    @param feature_set Features to be checked. See :ocvcuda::FeatureSet.
     */
    public static native @Cast("bool") boolean builtWith(@Cast("cv::cuda::FeatureSet") int feature_set);

    /** \brief There is a set of methods to check whether the module contains intermediate (PTX) or binary CUDA
    code for the given architecture(s):
    <p>
    @param major Major compute capability version.
    @param minor Minor compute capability version.
     */
    public static native @Cast("bool") boolean has(int major, int minor);
    public static native @Cast("bool") boolean hasPtx(int major, int minor);
    public static native @Cast("bool") boolean hasBin(int major, int minor);

    public static native @Cast("bool") boolean hasEqualOrLessPtx(int major, int minor);
    public static native @Cast("bool") boolean hasEqualOrGreater(int major, int minor);
    public static native @Cast("bool") boolean hasEqualOrGreaterPtx(int major, int minor);
    public static native @Cast("bool") boolean hasEqualOrGreaterBin(int major, int minor);
}

/** \brief Class providing functionality for querying the specified GPU properties.
 */
@Namespace("cv::cuda") @NoOffset public static class DeviceInfo extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public DeviceInfo(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public DeviceInfo(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public DeviceInfo position(long position) {
        return (DeviceInfo)super.position(position);
    }

    /** creates DeviceInfo object for the current GPU */
    public DeviceInfo() { super((Pointer)null); allocate(); }
    private native void allocate();

    /** \brief The constructors.
    <p>
    @param device_id System index of the CUDA device starting with 0.
    <p>
    Constructs the DeviceInfo object for the specified device. If device_id parameter is missed, it
    constructs an object for the current device.
     */
    public DeviceInfo(int device_id) { super((Pointer)null); allocate(device_id); }
    private native void allocate(int device_id);

    /** \brief Returns system index of the CUDA device starting with 0.
    */
    public native int deviceID();

    /** ASCII string identifying device */
    public native @Cast("const char*") BytePointer name();

    /** global memory available on device in bytes */
    public native @Cast("size_t") long totalGlobalMem();

    /** shared memory available per block in bytes */
    public native @Cast("size_t") long sharedMemPerBlock();

    /** 32-bit registers available per block */
    public native int regsPerBlock();

    /** warp size in threads */
    public native int warpSize();

    /** maximum pitch in bytes allowed by memory copies */
    public native @Cast("size_t") long memPitch();

    /** maximum number of threads per block */
    public native int maxThreadsPerBlock();

    /** maximum size of each dimension of a block */
    public native @ByVal Point3i maxThreadsDim();

    /** maximum size of each dimension of a grid */
    public native @ByVal Point3i maxGridSize();

    /** clock frequency in kilohertz */
    public native int clockRate();

    /** constant memory available on device in bytes */
    public native @Cast("size_t") long totalConstMem();

    /** major compute capability */
    public native int majorVersion();

    /** minor compute capability */
    public native int minorVersion();

    /** alignment requirement for textures */
    public native @Cast("size_t") long textureAlignment();

    /** pitch alignment requirement for texture references bound to pitched memory */
    public native @Cast("size_t") long texturePitchAlignment();

    /** number of multiprocessors on device */
    public native int multiProcessorCount();

    /** specified whether there is a run time limit on kernels */
    public native @Cast("bool") boolean kernelExecTimeoutEnabled();

    /** device is integrated as opposed to discrete */
    public native @Cast("bool") boolean integrated();

    /** device can map host memory with cudaHostAlloc/cudaHostGetDevicePointer */
    public native @Cast("bool") boolean canMapHostMemory();

    /** enum cv::cuda::DeviceInfo::ComputeMode */
    public static final int
        /** default compute mode (Multiple threads can use cudaSetDevice with this device) */
        ComputeModeDefault = 0,
        /** compute-exclusive-thread mode (Only one thread in one process will be able to use cudaSetDevice with this device) */
        ComputeModeExclusive = 1,
        /** compute-prohibited mode (No threads can use cudaSetDevice with this device) */
        ComputeModeProhibited = 2,
        /** compute-exclusive-process mode (Many threads in one process will be able to use cudaSetDevice with this device) */
        ComputeModeExclusiveProcess = 3;

    /** compute mode */
    public native @Cast("cv::cuda::DeviceInfo::ComputeMode") int computeMode();

    /** maximum 1D texture size */
    public native int maxTexture1D();

    /** maximum 1D mipmapped texture size */
    public native int maxTexture1DMipmap();

    /** maximum size for 1D textures bound to linear memory */
    public native int maxTexture1DLinear();

    /** maximum 2D texture dimensions */
    public native @ByVal Point maxTexture2D();

    /** maximum 2D mipmapped texture dimensions */
    public native @ByVal Point maxTexture2DMipmap();

    /** maximum dimensions (width, height, pitch) for 2D textures bound to pitched memory */
    public native @ByVal Point3i maxTexture2DLinear();

    /** maximum 2D texture dimensions if texture gather operations have to be performed */
    public native @ByVal Point maxTexture2DGather();

    /** maximum 3D texture dimensions */
    public native @ByVal Point3i maxTexture3D();

    /** maximum Cubemap texture dimensions */
    public native int maxTextureCubemap();

    /** maximum 1D layered texture dimensions */
    public native @ByVal Point maxTexture1DLayered();

    /** maximum 2D layered texture dimensions */
    public native @ByVal Point3i maxTexture2DLayered();

    /** maximum Cubemap layered texture dimensions */
    public native @ByVal Point maxTextureCubemapLayered();

    /** maximum 1D surface size */
    public native int maxSurface1D();

    /** maximum 2D surface dimensions */
    public native @ByVal Point maxSurface2D();

    /** maximum 3D surface dimensions */
    public native @ByVal Point3i maxSurface3D();

    /** maximum 1D layered surface dimensions */
    public native @ByVal Point maxSurface1DLayered();

    /** maximum 2D layered surface dimensions */
    public native @ByVal Point3i maxSurface2DLayered();

    /** maximum Cubemap surface dimensions */
    public native int maxSurfaceCubemap();

    /** maximum Cubemap layered surface dimensions */
    public native @ByVal Point maxSurfaceCubemapLayered();

    /** alignment requirements for surfaces */
    public native @Cast("size_t") long surfaceAlignment();

    /** device can possibly execute multiple kernels concurrently */
    public native @Cast("bool") boolean concurrentKernels();

    /** device has ECC support enabled */
    public native @Cast("bool") boolean ECCEnabled();

    /** PCI bus ID of the device */
    public native int pciBusID();

    /** PCI device ID of the device */
    public native int pciDeviceID();

    /** PCI domain ID of the device */
    public native int pciDomainID();

    /** true if device is a Tesla device using TCC driver, false otherwise */
    public native @Cast("bool") boolean tccDriver();

    /** number of asynchronous engines */
    public native int asyncEngineCount();

    /** device shares a unified address space with the host */
    public native @Cast("bool") boolean unifiedAddressing();

    /** peak memory clock frequency in kilohertz */
    public native int memoryClockRate();

    /** global memory bus width in bits */
    public native int memoryBusWidth();

    /** size of L2 cache in bytes */
    public native int l2CacheSize();

    /** maximum resident threads per multiprocessor */
    public native int maxThreadsPerMultiProcessor();

    /** gets free and total device memory */
    public native void queryMemory(@Cast("size_t*") @ByRef SizeTPointer totalMemory, @Cast("size_t*") @ByRef SizeTPointer freeMemory);
    public native @Cast("size_t") long freeMemory();
    public native @Cast("size_t") long totalMemory();

    /** \brief Provides information on CUDA feature support.
    <p>
    @param feature_set Features to be checked. See cuda::FeatureSet.
    <p>
    This function returns true if the device has the specified CUDA feature. Otherwise, it returns false
     */
    public native @Cast("bool") boolean supports(@Cast("cv::cuda::FeatureSet") int feature_set);

    /** \brief Checks the CUDA module and device compatibility.
    <p>
    This function returns true if the CUDA module can be run on the specified device. Otherwise, it
    returns false .
     */
    public native @Cast("bool") boolean isCompatible();
}

@Namespace("cv::cuda") public static native void printCudaDeviceInfo(int device);
@Namespace("cv::cuda") public static native void printShortCudaDeviceInfo(int device);

/** \brief Converts an array to half precision floating number.
<p>
@param _src input array.
@param _dst output array.
@param stream Stream for the asynchronous version.
\sa convertFp16
*/
@Namespace("cv::cuda") public static native void convertFp16(@ByVal Mat _src, @ByVal Mat _dst, @ByRef(nullValue = "cv::cuda::Stream::Null()") Stream stream);
@Namespace("cv::cuda") public static native void convertFp16(@ByVal Mat _src, @ByVal Mat _dst);
@Namespace("cv::cuda") public static native void convertFp16(@ByVal UMat _src, @ByVal UMat _dst, @ByRef(nullValue = "cv::cuda::Stream::Null()") Stream stream);
@Namespace("cv::cuda") public static native void convertFp16(@ByVal UMat _src, @ByVal UMat _dst);
@Namespace("cv::cuda") public static native void convertFp16(@ByVal GpuMat _src, @ByVal GpuMat _dst, @ByRef(nullValue = "cv::cuda::Stream::Null()") Stream stream);
@Namespace("cv::cuda") public static native void convertFp16(@ByVal GpuMat _src, @ByVal GpuMat _dst);

/** \} cudacore_init */

 // namespace cv { namespace cuda {


// #include "opencv2/core/cuda.inl.hpp"

// #endif /* OPENCV_CORE_CUDA_HPP */


}

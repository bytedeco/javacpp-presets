// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.bytedeco.depthai;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;
import org.bytedeco.opencv.opencv_core.*;
import static org.bytedeco.opencv.global.opencv_core.*;
import org.bytedeco.opencv.opencv_imgproc.*;
import static org.bytedeco.opencv.global.opencv_imgproc.*;

import static org.bytedeco.depthai.global.depthai.*;


/**
 * \brief Camera node. Experimental node, for both mono and color types of sensors
 */
@Namespace("dai::node") @NoOffset @Properties(inherit = org.bytedeco.depthai.presets.depthai.class)
public class Camera extends CameraPropertiesNode {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Camera(Pointer p) { super(p); }

    @MemberGetter public static native @Cast("const char*") BytePointer NAME();
    /**
     * Constructs Camera node.
     */
    public Camera(@SharedPtr PipelineImpl par, @Cast("int64_t") long nodeId) { super((Pointer)null); allocate(par, nodeId); }
    private native void allocate(@SharedPtr PipelineImpl par, @Cast("int64_t") long nodeId);
    public Camera(@SharedPtr PipelineImpl par, @Cast("int64_t") long nodeId, @UniquePtr CameraProperties props) { super((Pointer)null); allocate(par, nodeId, props); }
    private native void allocate(@SharedPtr PipelineImpl par, @Cast("int64_t") long nodeId, @UniquePtr CameraProperties props);

    /**
     * Computes the scaled size given numerator and denominator
     */
    

    /**
     * Initial control options to apply to sensor
     */
    @MemberGetter public native @ByRef CameraControl initialControl();

    /**
     * Input for ImageManipConfig message, which can modify crop parameters in runtime
     *
     * Default queue is non-blocking with size 8
     */
    @MemberGetter public native @ByRef Input inputConfig();

    /**
     * Input for CameraControl message, which can modify camera parameters in runtime
     *
     * Default queue is blocking with size 8
     */
    @MemberGetter public native @ByRef Input inputControl();

    /**
     * Outputs ImgFrame message that carries NV12 encoded (YUV420, UV plane interleaved) frame data.
     *
     * Suitable for use with VideoEncoder node
     */
    @MemberGetter public native @ByRef Output video();

    /**
     * Outputs ImgFrame message that carries BGR/RGB planar/interleaved encoded frame data.
     *
     * Suitable for use with NeuralNetwork node
     */
    @MemberGetter public native @ByRef Output preview();

    /**
     * Outputs ImgFrame message that carries NV12 encoded (YUV420, UV plane interleaved) frame data.
     *
     * The message is sent only when a CameraControl message arrives to inputControl with captureStill command set.
     */
    @MemberGetter public native @ByRef Output still();

    /**
     * Outputs ImgFrame message that carries YUV420 planar (I420/IYUV) frame data.
     *
     * Generated by the ISP engine, and the source for the 'video', 'preview' and 'still' outputs
     */
    @MemberGetter public native @ByRef Output isp();

    /**
     * Outputs ImgFrame message that carries RAW10-packed (MIPI CSI-2 format) frame data.
     *
     * Captured directly from the camera sensor, and the source for the 'isp' output.
     */
    @MemberGetter public native @ByRef Output raw();

    /**
     * Outputs metadata-only ImgFrame message as an early indicator of an incoming frame.
     *
     * It's sent on the MIPI SoF (start-of-frame) event, just after the exposure of the current frame
     * has finished and before the exposure for next frame starts.
     * Could be used to synchronize various processes with camera capture.
     * Fields populated: camera id, sequence number, timestamp
     */
    @MemberGetter public native @ByRef Output frameEvent();

    // /**
    //  * Input for mocking 'isp' functionality.
    //  *
    //  * Default queue is non-blocking with size 8
    //  */
    // Input mockIsp{*this, "mockIsp", Input::Type::SReceiver, false, 8, {{DatatypeEnum::ImgFrame, false}}};

    /**
     * Specify which board socket to use
     * @param boardSocket Board socket to use
     */
    public native void setBoardSocket(CameraBoardSocket boardSocket);
    public native void setBoardSocket(@Cast("dai::CameraBoardSocket") int boardSocket);

    /**
     * Retrieves which board socket to use
     * @return Board socket to use
     */
    public native CameraBoardSocket getBoardSocket();

    /**
     * Specify which camera to use by name
     * @param name Name of the camera to use
     */
    public native void setCamera(@StdString BytePointer name);
    public native void setCamera(@StdString ByteBuffer name);
    public native void setCamera(@StdString String name);

    /**
     * Retrieves which camera to use by name
     * @return Name of the camera to use
     */
    public native @StdString BytePointer getCamera();

    /** Set camera image orientation */
    public native void setImageOrientation(CameraImageOrientation imageOrientation);
    public native void setImageOrientation(@Cast("dai::CameraImageOrientation") int imageOrientation);

    /** Get camera image orientation */
    public native CameraImageOrientation getImageOrientation();

    // TODO(themarpe) - add back
    // /// Set image type of preview output images.
    // void setPreviewType(ImgFrame::Type type);
    // /// Get image type of preview output frames.
    // ImgFrame::Type getPreviewType() const;
    // /// Set image type of video output images. Supported AUTO, GRAY, YUV420 and NV12.
    // void setVideoType(ImgFrame::Type type);
    // /// Get image type of video output frames. Supported AUTO, GRAY, YUV420 and NV12.
    // ImgFrame::Type getVideoType() const;

    /** Set desired resolution. Sets sensor size to best fit */
    public native void setSize(@ByVal @Cast("std::tuple<int,int>*") Pointer size);
    /** Set desired resolution. Sets sensor size to best fit */
    public native void setSize(int width, int height);

    /** Set preview output size */
    public native void setPreviewSize(int width, int height);

    /** Set preview output size, as a tuple <width, height> */
    public native void setPreviewSize(@ByVal @Cast("std::tuple<int,int>*") Pointer size);

    /** Set video output size */
    public native void setVideoSize(int width, int height);

    /** Set video output size, as a tuple <width, height> */
    public native void setVideoSize(@ByVal @Cast("std::tuple<int,int>*") Pointer size);

    /** Set still output size */
    public native void setStillSize(int width, int height);

    /** Set still output size, as a tuple <width, height> */
    public native void setStillSize(@ByVal @Cast("std::tuple<int,int>*") Pointer size);

    // /**
    //  * Set 'isp' output scaling (numerator/denominator), preserving the aspect ratio.
    //  * The fraction numerator/denominator is simplified first to a irreducible form,
    //  * then a set of hardware scaler constraints applies:
    //  * max numerator = 16, max denominator = 63
    //  */
    // void setIspScale(int numerator, int denominator);

    // /// Set 'isp' output scaling, as a tuple <numerator, denominator>
    // void setIspScale(std::tuple<int, int> scale);

    // /**
    //  * Set 'isp' output scaling, per each direction. If the horizontal scaling factor
    //  * (horizNum/horizDen) is different than the vertical scaling factor
    //  * (vertNum/vertDen), a distorted (stretched or squished) image is generated
    //  */
    // void setIspScale(int horizNum, int horizDenom, int vertNum, int vertDenom);

    // /// Set 'isp' output scaling, per each direction, as <numerator, denominator> tuples
    // void setIspScale(std::tuple<int, int> horizScale, std::tuple<int, int> vertScale);

    /**
     * Set rate at which camera should produce frames
     * @param fps Rate in frames per second
     */
    public native void setFps(float fps);

    /**
     * Isp 3A rate (auto focus, auto exposure, auto white balance, camera controls etc.).
     * Default (0) matches the camera FPS, meaning that 3A is running on each frame.
     * Reducing the rate of 3A reduces the CPU usage on CSS, but also increases the convergence rate of 3A.
     * Note that camera controls will be processed at this rate. E.g. if camera is running at 30 fps, and camera control is sent at every frame,
     * but 3A fps is set to 15, the camera control messages will be processed at 15 fps rate, which will lead to queueing.
     */
    public native void setIsp3aFps(int isp3aFps);

    /**
     * Get rate at which camera should produce frames
     * @return Rate in frames per second
     */
    public native float getFps();

    /** Get preview size as tuple */
    public native @ByVal @Cast("std::tuple<int,int>*") Pointer getPreviewSize();
    /** Get preview width */
    public native int getPreviewWidth();
    /** Get preview height */
    public native int getPreviewHeight();

    /** Get video size as tuple */
    public native @ByVal @Cast("std::tuple<int,int>*") Pointer getVideoSize();
    /** Get video width */
    public native int getVideoWidth();
    /** Get video height */
    public native int getVideoHeight();

    /** Get still size as tuple */
    public native @ByVal @Cast("std::tuple<int,int>*") Pointer getStillSize();
    /** Get still width */
    public native int getStillWidth();
    /** Get still height */
    public native int getStillHeight();

    /** Get sensor resolution as size */
    public native @ByVal @Cast("std::tuple<int,int>*") Pointer getSize();
    /** Get sensor resolution width */
    public native int getWidth();
    /** Get sensor resolution height */
    public native int getHeight();

    // /// Get 'isp' output resolution as size, after scaling
    // std::tuple<int, int> getIspSize() const;
    // /// Get 'isp' output width
    // int getIspWidth() const;
    // /// Get 'isp' output height
    // int getIspHeight() const;

    // /**
    //  * Specify sensor center crop.
    //  * Resolution size / video size
    //  */
    // void sensorCenterCrop();

    // /**
    //  * Specifies sensor crop rectangle
    //  * @param x Top left X coordinate
    //  * @param y Top left Y coordinate
    //  */
    // void setSensorCrop(float x, float y);

    // /**
    //  * @returns Sensor top left crop coordinates
    //  */
    // std::tuple<float, float> getSensorCrop() const;
    // /// Get sensor top left x crop coordinate
    // float getSensorCropX() const;
    // /// Get sensor top left y crop coordinate
    // float getSensorCropY() const;

    // /**
    //  * Specifies whether preview output should preserve aspect ratio,
    //  * after downscaling from video size or not.
    //  *
    //  * @param keep If true, a larger crop region will be considered to still be able to
    //  * create the final image in the specified aspect ratio. Otherwise video size is resized to fit preview size
    //  */
    // void setPreviewKeepAspectRatio(bool keep);

    // /**
    //  * @see setPreviewKeepAspectRatio
    //  * @returns Preview keep aspect ratio option
    //  */
    // bool getPreviewKeepAspectRatio();

    // /// Get number of frames in preview pool
    // int getPreviewNumFramesPool();
    // /// Get number of frames in video pool
    // int getVideoNumFramesPool();
    // /// Get number of frames in still pool
    // int getStillNumFramesPool();
    // /// Get number of frames in raw pool
    // int getRawNumFramesPool();
    // /// Get number of frames in isp pool
    // int getIspNumFramesPool();

    /** Set the source of the warp mesh or disable */
    public native void setMeshSource(@ByVal CameraProperties.WarpMeshSource source);
    /** Gets the source of the warp mesh */
    public native @ByVal CameraProperties.WarpMeshSource getMeshSource();

    /**
     * Specify local filesystem paths to the undistort mesh calibration files.
     *
     * When a mesh calibration is set, it overrides the camera intrinsics/extrinsics matrices.
     * Overrides useHomographyRectification behavior.
     * Mesh format: a sequence of (y,x) points as 'float' with coordinates from the input image
     * to be mapped in the output. The mesh can be subsampled, configured by {@code setMeshStep}.
     *
     * With a 1280x800 resolution and the default (16,16) step, the required mesh size is:
     *
     * width: 1280 / 16 + 1 = 81
     *
     * height: 800 / 16 + 1 = 51
     */
    public native void loadMeshFile(@Const @ByRef Path warpMesh);

    /**
     * Specify mesh calibration data for undistortion
     * See {@code loadMeshFiles} for the expected data format
     */

    /**
     * Set the distance between mesh points. Default: (32, 32)
     */
    public native void setMeshStep(int width, int height);
    /** Gets the distance between mesh points */
    public native @ByVal @Cast("std::tuple<int,int>*") Pointer getMeshStep();

    /** Set calibration alpha parameter that determines FOV of undistorted frames */
    public native void setCalibrationAlpha(float alpha);
    /** Get calibration alpha parameter that determines FOV of undistorted frames */
    public native @ByVal FloatOptional getCalibrationAlpha();

    /**
     * Configures whether the camera {@code raw} frames are saved as MIPI-packed to memory.
     * The packed format is more efficient, consuming less memory on device, and less data
     * to send to host: RAW10: 4 pixels saved on 5 bytes, RAW12: 2 pixels saved on 3 bytes.
     * When packing is disabled ({@code false}), data is saved lsb-aligned, e.g. a RAW10 pixel
     * will be stored as uint16, on bits 9..0: 0b0000'00pp'pppp'pppp.
     * Default is auto: enabled for standard color/monochrome cameras where ISP can work
     * with both packed/unpacked, but disabled for other cameras like ToF.
     */
    public native void setRawOutputPacked(@Cast("bool") boolean packed);
}

// Targeted by JavaCPP version 1.4.4-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.javacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class ngraph extends org.bytedeco.javacpp.presets.ngraph {
    static { Loader.load(); }

@Name("std::vector<std::string>") public static class StringVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public StringVector(Pointer p) { super(p); }
    public StringVector(BytePointer value) { this(1); put(0, value); }
    public StringVector(BytePointer ... array) { this(array.length); put(array); }
    public StringVector(String value) { this(1); put(0, value); }
    public StringVector(String ... array) { this(array.length); put(array); }
    public StringVector()       { allocate();  }
    public StringVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef StringVector put(@ByRef StringVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @StdString BytePointer get(@Cast("size_t") long i);
    public native StringVector put(@Cast("size_t") long i, BytePointer value);
    @ValueSetter @Index(function = "at") public native StringVector put(@Cast("size_t") long i, @StdString String value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @StdString BytePointer value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @StdString BytePointer get();
    }

    public BytePointer[] get() {
        BytePointer[] array = new BytePointer[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public BytePointer pop_back() {
        long size = size();
        BytePointer value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public StringVector push_back(BytePointer value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public StringVector put(BytePointer value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public StringVector put(BytePointer ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }

    public StringVector push_back(String value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public StringVector put(String value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public StringVector put(String ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<size_t>") public static class SizeTVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public SizeTVector(Pointer p) { super(p); }
    public SizeTVector(long ... array) { this(array.length); put(array); }
    public SizeTVector()       { allocate();  }
    public SizeTVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef SizeTVector put(@ByRef SizeTVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @Cast("size_t") long get(@Cast("size_t") long i);
    public native SizeTVector put(@Cast("size_t") long i, long value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @Cast("size_t") long value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @Cast("size_t") long get();
    }

    public long[] get() {
        long[] array = new long[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public long pop_back() {
        long size = size();
        long value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public SizeTVector push_back(long value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public SizeTVector put(long value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public SizeTVector put(long ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<std::shared_ptr<ngraph::op::Result> >") public static class NgraphResultVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public NgraphResultVector(Pointer p) { super(p); }
    public NgraphResultVector(Result value) { this(1); put(0, value); }
    public NgraphResultVector(Result ... array) { this(array.length); put(array); }
    public NgraphResultVector()       { allocate();  }
    public NgraphResultVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef NgraphResultVector put(@ByRef NgraphResultVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @SharedPtr Result get(@Cast("size_t") long i);
    public native NgraphResultVector put(@Cast("size_t") long i, Result value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @SharedPtr Result value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @SharedPtr @Const Result get();
    }

    public Result[] get() {
        Result[] array = new Result[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public Result pop_back() {
        long size = size();
        Result value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public NgraphResultVector push_back(Result value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public NgraphResultVector put(Result value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public NgraphResultVector put(Result ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<std::shared_ptr<ngraph::op::Parameter> >") public static class NgraphParameterVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public NgraphParameterVector(Pointer p) { super(p); }
    public NgraphParameterVector(Parameter value) { this(1); put(0, value); }
    public NgraphParameterVector(Parameter ... array) { this(array.length); put(array); }
    public NgraphParameterVector()       { allocate();  }
    public NgraphParameterVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef NgraphParameterVector put(@ByRef NgraphParameterVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @SharedPtr Parameter get(@Cast("size_t") long i);
    public native NgraphParameterVector put(@Cast("size_t") long i, Parameter value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @SharedPtr Parameter value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @SharedPtr @Const Parameter get();
    }

    public Parameter[] get() {
        Parameter[] array = new Parameter[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public Parameter pop_back() {
        long size = size();
        Parameter value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public NgraphParameterVector push_back(Parameter value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public NgraphParameterVector put(Parameter value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public NgraphParameterVector put(Parameter ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<std::shared_ptr<ngraph::Node> >") public static class NgraphNodeVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public NgraphNodeVector(Pointer p) { super(p); }
    public NgraphNodeVector(Node value) { this(1); put(0, value); }
    public NgraphNodeVector(Node ... array) { this(array.length); put(array); }
    public NgraphNodeVector()       { allocate();  }
    public NgraphNodeVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef NgraphNodeVector put(@ByRef NgraphNodeVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @SharedPtr Node get(@Cast("size_t") long i);
    public native NgraphNodeVector put(@Cast("size_t") long i, Node value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @SharedPtr Node value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @SharedPtr @Const Node get();
    }

    public Node[] get() {
        Node[] array = new Node[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public Node pop_back() {
        long size = size();
        Node value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public NgraphNodeVector push_back(Node value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public NgraphNodeVector put(Node value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public NgraphNodeVector put(Node ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<std::shared_ptr<ngraph::runtime::Tensor> >") public static class NgraphTensorVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public NgraphTensorVector(Pointer p) { super(p); }
    public NgraphTensorVector(Tensor value) { this(1); put(0, value); }
    public NgraphTensorVector(Tensor ... array) { this(array.length); put(array); }
    public NgraphTensorVector()       { allocate();  }
    public NgraphTensorVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef NgraphTensorVector put(@ByRef NgraphTensorVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @SharedPtr Tensor get(@Cast("size_t") long i);
    public native NgraphTensorVector put(@Cast("size_t") long i, Tensor value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @SharedPtr Tensor value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @SharedPtr @Const Tensor get();
    }

    public Tensor[] get() {
        Tensor[] array = new Tensor[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public Tensor pop_back() {
        long size = size();
        Tensor value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public NgraphTensorVector push_back(Tensor value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public NgraphTensorVector put(Tensor value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public NgraphTensorVector put(Tensor ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::vector<std::shared_ptr<ngraph::Function> >") public static class NgraphFunctionVector extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public NgraphFunctionVector(Pointer p) { super(p); }
    public NgraphFunctionVector(Function value) { this(1); put(0, value); }
    public NgraphFunctionVector(Function ... array) { this(array.length); put(array); }
    public NgraphFunctionVector()       { allocate();  }
    public NgraphFunctionVector(long n) { allocate(n); }
    private native void allocate();
    private native void allocate(@Cast("size_t") long n);
    public native @Name("operator=") @ByRef NgraphFunctionVector put(@ByRef NgraphFunctionVector x);

    public boolean empty() { return size() == 0; }
    public native long size();
    public void clear() { resize(0); }
    public native void resize(@Cast("size_t") long n);

    @Index(function = "at") public native @SharedPtr Function get(@Cast("size_t") long i);
    public native NgraphFunctionVector put(@Cast("size_t") long i, Function value);

    public native @ByVal Iterator insert(@ByVal Iterator pos, @SharedPtr Function value);
    public native @ByVal Iterator erase(@ByVal Iterator pos);
    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*") @SharedPtr @Const Function get();
    }

    public Function[] get() {
        Function[] array = new Function[size() < Integer.MAX_VALUE ? (int)size() : Integer.MAX_VALUE];
        for (int i = 0; i < array.length; i++) {
            array[i] = get(i);
        }
        return array;
    }
    @Override public String toString() {
        return java.util.Arrays.toString(get());
    }

    public Function pop_back() {
        long size = size();
        Function value = get(size - 1);
        resize(size - 1);
        return value;
    }
    public NgraphFunctionVector push_back(Function value) {
        long size = size();
        resize(size + 1);
        return put(size, value);
    }
    public NgraphFunctionVector put(Function value) {
        if (size() != 1) { resize(1); }
        return put(0, value);
    }
    public NgraphFunctionVector put(Function ... array) {
        if (size() != array.length) { resize(array.length); }
        for (int i = 0; i < array.length; i++) {
            put(i, array[i]);
        }
        return this;
    }
}

@Name("std::unordered_map<std::string,void*>") public static class StringVoidMap extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public StringVoidMap(Pointer p) { super(p); }
    public StringVoidMap()       { allocate();  }
    private native void allocate();
    public native @Name("operator=") @ByRef StringVoidMap put(@ByRef StringVoidMap x);

    public boolean empty() { return size() == 0; }
    public native long size();

    @Index public native Pointer get(@StdString BytePointer i);
    public native StringVoidMap put(@StdString BytePointer i, Pointer value);

    public native @ByVal Iterator begin();
    public native @ByVal Iterator end();
    @NoOffset @Name("iterator") public static class Iterator extends Pointer {
        public Iterator(Pointer p) { super(p); }
        public Iterator() { }

        public native @Name("operator++") @ByRef Iterator increment();
        public native @Name("operator==") boolean equals(@ByRef Iterator it);
        public native @Name("operator*().first") @MemberGetter @StdString BytePointer first();
        public native @Name("operator*().second") @MemberGetter @Const Pointer second();
    }
}

// Parsed from ngraph/backend.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>  // std::shared_ptr
// #include <string>  // std::string
// #include <utility> // std::move
// #include <vector>  // std::vector

// #include "ngraph/function.hpp"
// #include "ngraph/runtime/backend.hpp"
// #include "ngraph/runtime/tensor.hpp"
        /** \brief ONNXIFI extensions to nGraph backend */
        @Name("ngraph::onnxifi::Backend") public static class NgraphONNXIFIBackend extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public NgraphONNXIFIBackend(Pointer p) { super(p); }
        
//            Backend(const Backend&) = delete;
//            Backend& operator=(const Backend&) = delete;

//            Backend(Backend&&) = default;
//            Backend& operator=(Backend&&) = default;

//            Backend() = delete;

           
          
         
        

            public native @StdString BytePointer get_type();
            public native @Cast("bool") boolean compile(@Const @SharedPtr @ByRef Function function);

            public native @Cast("bool") boolean call(@Const @SharedPtr @ByRef Function function,
                                  @Const @ByRef NgraphTensorVector outputs,
                                  @Const @ByRef NgraphTensorVector inputs);

            public native @Cast("bool") boolean call_with_validate(
                            @Const @SharedPtr @ByRef Function function,
                            @Const @ByRef NgraphTensorVector outputs,
                            @Const @ByRef NgraphTensorVector inputs);
        }

     // namespace onnxifi

 // namespace ngraph


// Parsed from ngraph/backend_manager.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef> // std::size_t, std::uintptr_t
// #include <map>     // std::map
// #include <mutex>   // std::mutex

// #include "onnxifi.h"

// #include "ngraph/runtime/backend.hpp"

// #include "backend.hpp"
        /** \brief ONNXIFI backend manager */
        @Namespace("ngraph::onnxifi") @NoOffset public static class BackendManager extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public BackendManager(Pointer p) { super(p); }
        
            
            

            
            

            public static native void get_backend_ids(onnxBackendID backend_ids, @Cast("std::size_t*") LongPointer count);
            public static native void get_backend_ids(onnxBackendID backend_ids, @Cast("std::size_t*") LongBuffer count);
            public static native void get_backend_ids(onnxBackendID backend_ids, @Cast("std::size_t*") long[] count);

            

            
        }

     // namespace onnxifi

 // namespace ngraph


// Parsed from ngraph/descriptor/tensor.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <string>

// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/partial_shape.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/type/element_type.hpp"
            @Namespace("ngraph::descriptor::layout") @Opaque public static class TensorLayout extends Pointer {
                /** Empty constructor. Calls {@code super((Pointer)null)}. */
                public TensorLayout() { super((Pointer)null); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public TensorLayout(Pointer p) { super(p); }
            }
        

        /** \brief Compile-time descriptor of a first-class value that is a view of a tensor. */
        @Name("ngraph::descriptor::Tensor") public static class DescriptorTensor extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public DescriptorTensor(Pointer p) { super(p); }
        

            public native @StdString BytePointer get_name();

            public native @Const @ByRef Type get_element_type();
            public native @Const @ByRef Shape get_shape();
            public native @SharedPtr TensorLayout get_tensor_layout();

            

            public native void set_pool_offset(@Cast("size_t") long arg0);
            public native @Cast("size_t") long get_pool_offset();

            public native @Cast("size_t") long size();
        }

        @Namespace("ngraph::descriptor") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, @Const @ByRef DescriptorTensor arg1);
    



// Parsed from ngraph/runtime/tensor.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>

// #include "ngraph/descriptor/layout/tensor_layout.hpp"
// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/strides.hpp"
// #include "ngraph/type/element_type.hpp"
        @Namespace("ngraph::descriptor") @Opaque public static class Value extends Pointer {
            /** Empty constructor. Calls {@code super((Pointer)null)}. */
            public Value() { super((Pointer)null); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Value(Pointer p) { super(p); }
        }
    
        @Namespace("ngraph::runtime") @NoOffset public static class Tensor extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Tensor(Pointer p) { super(p); }
        
            public native @ByRef @Name("operator =") Tensor put(@Const @ByRef Tensor arg0);

            /** \brief Get tensor shape
             *  @return const reference to a Shape */
            public native @Const @ByRef Shape get_shape();

            /** \brief Get tensor strides
             *  @return Strides */
            public native @ByVal Strides get_strides();

            /** \brief Get tensor element type
             *  @return element::Type */
            public native @Const @ByRef Type get_element_type();

            /** \brief Get number of elements in the tensor
             *  @return number of elements in the tensor */
            public native @Cast("size_t") long get_element_count();

            /** \brief Get the size in bytes of the tensor
             *  @return number of bytes in tensor's allocation */
            public native @Cast("size_t") long get_size_in_bytes();

            /** \brief Get tensor's unique name
             *  @return tensor's name */
            public native @StdString BytePointer get_name();

            /** \brief Get tensor layout
             *  @return tensor layout */
            public native @SharedPtr TensorLayout get_tensor_layout();

            /** \brief Set tensor layout
             *  @param layout Layout to set */
            public native void set_tensor_layout(@SharedPtr TensorLayout layout);

            /** \brief Get the stale value of the tensor. A tensor is stale if its data is
             *  changed.
             *  @return true if there is new data in this tensor */
            public native @Cast("bool") boolean get_stale();

            /** \brief Set the stale value of the tensor. A tensor is stale if its data is
             *  changed. */
            public native void set_stale(@Cast("bool") boolean val);

            /** \brief Write bytes directly into the tensor
             *  @param p Pointer to source of data
             *  @param offset Offset into tensor storage to begin writing. Must be element-aligned.
             *  @param n Number of bytes to write, must be integral number of elements. */
            public native void write(@Const Pointer p, @Cast("size_t") long offset, @Cast("size_t") long n);

            /** \brief Read bytes directly from the tensor
             *  @param p Pointer to destination for data
             *  @param offset Offset into tensor storage to begin writing. Must be element-aligned.
             *  @param n Number of bytes to read, must be integral number of elements. */
            public native void read(Pointer p, @Cast("size_t") long offset, @Cast("size_t") long n);
        }
    



// Parsed from ngraph/runtime/backend.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/function.hpp"
// #include "ngraph/runtime/performance_counter.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/type/element_type.hpp"
        @Namespace("ngraph::runtime") @Opaque public static class ExternalFunction extends Pointer {
            /** Empty constructor. Calls {@code super((Pointer)null)}. */
            public ExternalFunction() { super((Pointer)null); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public ExternalFunction(Pointer p) { super(p); }
        }
    


/** \brief Interface to a generic backend.
 * 
 *  Backends are responsible for function execution and value allocation. */
@Name("ngraph::runtime::Backend") public static class Backend extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Backend(Pointer p) { super(p); }

    /** \brief Create a new Backend object
     *  @param type The name of a registered backend, such as "CPU" or "GPU".
     *    To select a subdevice use "GPU:N" where s{@code N} is the subdevice number.
     *  @return unique_ptr to a new Backend or nullptr if the named backend
     *    does not exist. */
    public static native @UniquePtr Backend create(@StdString BytePointer type);
    public static native @UniquePtr Backend create(@StdString String type);

    /** \brief Query the list of registered devices
     *  @return A vector of all registered devices. */
    public static native @ByVal StringVector get_registered_devices();

    /** \brief Create a tensor specific to this backend
     *  @param element_type The type of the tensor element
     *  @param shape The shape of the tensor
     *  @return shared_ptr to a new backend-specific tensor */
    public native @SharedPtr @ByVal Tensor create_tensor(@Const @ByRef Type element_type, @Const @ByRef Shape shape);

    /** \brief Create a tensor specific to this backend
     *  @param element_type The type of the tensor element
     *  @param shape The shape of the tensor
     *  @param memory_pointer A pointer to a buffer used for this tensor. The size of the buffer
     *      must be sufficient to contain the tensor. The lifetime of the buffer is the
     *      responsibility of the caller.
     *  @return shared_ptr to a new backend-specific tensor */
    public native @SharedPtr @ByVal Tensor create_tensor(
            @Const @ByRef Type element_type, @Const @ByRef Shape shape, Pointer memory_pointer);

    /** \brief Create a tensor of C type T specific to this backend
     *  @param shape The shape of the tensor
     *  @return shared_ptr to a new backend specific tensor */

    /** \brief Compiles a Function.
     *  @param func The function to compile
     *  @return true if compile is successful, false otherwise */
    public native @Cast("bool") boolean compile(@SharedPtr @ByVal Function func);

    /** \brief Executes a single iteration of a Function. If func is not compiled the call will
     *      compile it.
     *  @param func The function to execute
     *  @return true if iteration is successful, false otherwise */
    public native @Cast("bool") boolean call(@SharedPtr @ByVal Function func,
                          @Const @ByRef NgraphTensorVector outputs,
                          @Const @ByRef NgraphTensorVector inputs);

    /** \brief Executes a single iteration of a Function. If func is not compiled the call will
     *      compile it. Optionally validates the inputs and outputs against the function graph.
     *  @param func The function to execute
     *  @return true if iteration is successful, false otherwise */
    public native @Cast("bool") boolean call_with_validate(@SharedPtr @ByVal Function func,
                                @Const @ByRef NgraphTensorVector outputs,
                                @Const @ByRef NgraphTensorVector inputs);

    /** \brief Compiled functions may be cached. This function removes a compiled function
     *      from the cache.
     *  @param func The function to execute */
    public native void remove_compiled_function(@SharedPtr @ByVal Function func);

    /** \brief Enable the collection of per-op performance information on a specified Function.
     *      Data collection is via the {@code get_performance_data} method.
     *  @param func The function to collect perfomance data on.
     *  @param enable Set to true to enable or false to disable data collection */
    public native void enable_performance_data(@SharedPtr @ByVal Function func, @Cast("bool") boolean enable);
    /** \brief Collect performance information gathered on a Function.
     *  @param func The function to get collected data.
     *  @return Vector of PerformanceCounter information. */
    public native @StdVector PerformanceCounter get_performance_data(@SharedPtr @ByVal Function func);

    /** \brief Test if a backend is capable of supporting an op
     *  @param node is the op to test.
     *  @return true if the op is supported, false otherwise. */
    public native @Cast("bool") boolean is_supported(@Const @ByRef Node node);
}


// Parsed from ngraph/runtime/cpu/cpu_backend.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <map>
// #include <memory>

// #include "ngraph/runtime/backend.hpp"
            @Namespace("ngraph::runtime::cpu") @Opaque public static class CPU_ExternalFunction extends Pointer {
                /** Empty constructor. Calls {@code super((Pointer)null)}. */
                public CPU_ExternalFunction() { super((Pointer)null); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public CPU_ExternalFunction(Pointer p) { super(p); }
            }
            @Namespace("ngraph::runtime::cpu") @Opaque public static class CPU_CallFrame extends Pointer {
                /** Empty constructor. Calls {@code super((Pointer)null)}. */
                public CPU_CallFrame() { super((Pointer)null); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public CPU_CallFrame(Pointer p) { super(p); }
            }

            @Namespace("ngraph::runtime::cpu") @NoOffset public static class CPU_Backend extends Backend {
                static { Loader.load(); }
                /** Default native constructor. */
                public CPU_Backend() { super((Pointer)null); allocate(); }
                /** Native array allocator. Access with {@link Pointer#position(long)}. */
                public CPU_Backend(long size) { super((Pointer)null); allocateArray(size); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public CPU_Backend(Pointer p) { super(p); }
                private native void allocate();
                private native void allocateArray(long size);
                @Override public CPU_Backend position(long position) {
                    return (CPU_Backend)super.position(position);
                }
            
                

                public native @SharedPtr @ByVal Tensor create_tensor(@Const @ByRef Type element_type,
                                                  @Const @ByRef Shape shape,
                                                  Pointer memory_pointer);

                public native @SharedPtr @ByVal Tensor create_tensor(@Const @ByRef Type element_type,
                                                  @Const @ByRef Shape shape);

                public native @Cast("bool") boolean compile(@SharedPtr @ByVal Function func);

                public native @Cast("bool") boolean call(@SharedPtr @ByVal Function func,
                                          @Const @ByRef NgraphTensorVector outputs,
                                          @Const @ByRef NgraphTensorVector inputs);

                public native void remove_compiled_function(@SharedPtr @ByVal Function func);
                public native @SharedPtr CPU_CallFrame get_call_frame(@SharedPtr @ByVal Function func);

                public native void enable_performance_data(@SharedPtr @ByVal Function func, @Cast("bool") boolean enable);
                public native @StdVector PerformanceCounter get_performance_data(@SharedPtr @ByVal Function func);
            }
        
    



// Parsed from ngraph/runtime/performance_counter.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <string>
        @Namespace("ngraph::runtime") @NoOffset public static class PerformanceCounter extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public PerformanceCounter(Pointer p) { super(p); }
        
            public PerformanceCounter(@Cast("const char*") BytePointer n, @Cast("size_t") long us, @Cast("size_t") long calls) { super((Pointer)null); allocate(n, us, calls); }
            private native void allocate(@Cast("const char*") BytePointer n, @Cast("size_t") long us, @Cast("size_t") long calls);
            public PerformanceCounter(String n, @Cast("size_t") long us, @Cast("size_t") long calls) { super((Pointer)null); allocate(n, us, calls); }
            private native void allocate(String n, @Cast("size_t") long us, @Cast("size_t") long calls);
            public native @StdString BytePointer name();
            public native @Cast("size_t") long total_microseconds();
            public native @Cast("size_t") long microseconds();
            public native @Cast("size_t") long call_count();
            public native @StdString BytePointer m_name(); public native PerformanceCounter m_name(BytePointer m_name);
            public native @Cast("size_t") long m_total_microseconds(); public native PerformanceCounter m_total_microseconds(long m_total_microseconds);
            public native @Cast("size_t") long m_call_count(); public native PerformanceCounter m_call_count(long m_call_count);
        }
    



// Parsed from ngraph/type/element_type.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

//================================================================================================
// ElementType
//================================================================================================

// #pragma once

// #include <iostream>
// #include <memory>
// #include <string>
// #include <vector>

// #include "ngraph/except.hpp"
// #include "ngraph/type/bfloat16.hpp"

        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type dynamic();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef @Name("boolean") Type _boolean();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type bf16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type f32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type f64();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i8();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i64();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u8();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u64();

        @Namespace("ngraph::element") @NoOffset public static class Type extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Type(Pointer p) { super(p); }
            /** Native array allocator. Access with {@link Pointer#position(long)}. */
            public Type(long size) { super((Pointer)null); allocateArray(size); }
            private native void allocateArray(long size);
            @Override public Type position(long position) {
                return (Type)super.position(position);
            }
        
            public Type() { super((Pointer)null); allocate(); }
            private native void allocate();
            public Type(@Const @ByRef Type arg0) { super((Pointer)null); allocate(arg0); }
            private native void allocate(@Const @ByRef Type arg0);
            public Type(@Cast("size_t") long bitwidth,
                             @Cast("bool") boolean is_real,
                             @Cast("bool") boolean is_signed,
                             @Cast("bool") boolean is_quantized,
                             @StdString BytePointer cname) { super((Pointer)null); allocate(bitwidth, is_real, is_signed, is_quantized, cname); }
            private native void allocate(@Cast("size_t") long bitwidth,
                             @Cast("bool") boolean is_real,
                             @Cast("bool") boolean is_signed,
                             @Cast("bool") boolean is_quantized,
                             @StdString BytePointer cname);
            public Type(@Cast("size_t") long bitwidth,
                             @Cast("bool") boolean is_real,
                             @Cast("bool") boolean is_signed,
                             @Cast("bool") boolean is_quantized,
                             @StdString String cname) { super((Pointer)null); allocate(bitwidth, is_real, is_signed, is_quantized, cname); }
            private native void allocate(@Cast("size_t") long bitwidth,
                             @Cast("bool") boolean is_real,
                             @Cast("bool") boolean is_signed,
                             @Cast("bool") boolean is_quantized,
                             @StdString String cname);
            public native @ByRef @Name("operator =") Type put(@Const @ByRef Type arg0);
            public native @StdString BytePointer c_type_string();
            public native @Cast("size_t") long size();
            public native @Cast("size_t") long hash();
            public native @Cast("bool") boolean is_static();
            public native @Cast("bool") boolean is_dynamic();
            public native @Cast("bool") boolean is_real();
            public native @Cast("bool") boolean is_signed();
            public native @Cast("bool") boolean is_quantized();
            public native @Cast("size_t") long bitwidth();
            public native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Type other);
            public native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Type other);
            public native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef Type other);
            
            public static native @Cast("const ngraph::element::Type**") @StdVector PointerPointer get_known_types();

            /** Returns true if the type is floating point, else false. */
            public native @Cast("bool") boolean get_is_real();
            /** \brief Checks whether this element type is merge-compatible with {@code t}.
             *  @param t The element type to compare this element type to.
             *  @return {@code true} if this element type is compatible with {@code t}, else {@code false}. */
            
            ///
            ///
            ///
            ///
            ///
            public native @Cast("bool") boolean compatible(@ByVal Type t);

            /** \brief Merges two element types t1 and t2, writing the result into dst and
             *         returning true if successful, else returning false.
             * 
             *         To "merge" two element types t1 and t2 is to find the least restrictive
             *         element type t that is no more restrictive than t1 and t2, if t exists.
             *         More simply:
             * 
             *            merge(dst,element::Type::dynamic,t)
             *               writes t to dst and returns true
             * 
             *            merge(dst,t,element::Type::dynamic)
             *               writes t to dst and returns true
             * 
             *            merge(dst,t1,t2) where t1, t2 both static and equal
             *               writes t1 to dst and returns true
             * 
             *            merge(dst,t1,t2) where t1, t2 both static and unequal
             *               does nothing to dst, and returns false */
            public static native @Cast("bool") boolean merge(@ByRef Type dst, @Const @ByRef Type t1, @Const @ByRef Type t2);
        }

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<char>") Type fromChar();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<bool>") Type fromBool();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<float>") Type fromFloat();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<double>") Type fromDouble();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<int8_t>") Type fromInt8t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<int16_t>") Type fromInt16t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<int32_t>") Type fromInt32t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<int64_t>") Type fromInt64t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<uint8_t>") Type fromUInt8t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<uint16_t>") Type fromUInt16t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<uint32_t>") Type fromUInt32t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<uint64_t>") Type fromUInt64t();

        @Namespace("ngraph::element") public static native @Const @ByRef @Name("from<ngraph::bfloat16>") Type fromNGraphBFloat16();

        @Namespace("ngraph::element") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Type obj);
    



// Parsed from ngraph/shape.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstdio>
// #include <vector>

// #include "ngraph/axis_set.hpp"
// #include "ngraph/strides.hpp"
    /** \brief Shape for a tensor. */
    @Namespace("ngraph") public static class Shape extends SizeTVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Shape(Pointer p) { super(p); }
    

        public Shape(@Const @ByRef SizeTVector axis_lengths) { super((Pointer)null); allocate(axis_lengths); }
        private native void allocate(@Const @ByRef SizeTVector axis_lengths);

        public Shape(@Const @ByRef Shape axis_lengths) { super((Pointer)null); allocate(axis_lengths); }
        private native void allocate(@Const @ByRef Shape axis_lengths);

        public Shape(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/) { super((Pointer)null); allocate(n, initial_value); }
        private native void allocate(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/);
        public Shape(@Cast("size_t") long n) { super((Pointer)null); allocate(n); }
        private native void allocate(@Cast("size_t") long n);

        public Shape() { super((Pointer)null); allocate(); }
        private native void allocate();
        public native @ByRef @Name("operator =") Shape put(@Const @ByRef Shape v);
    }

    /** Number of elements in spanned by a shape */

    /** Row-major strides for a shape */

    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Shape shape);



// Parsed from ngraph/function.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <atomic>
// #include <initializer_list>
// #include <list>
// #include <memory>
// #include <string>
// #include <vector>

// #include "ngraph/node.hpp"
// #include "ngraph/parameter_vector.hpp"
// #include "ngraph/result_vector.hpp"
    /** A user-defined function. */
    @Namespace("ngraph") @NoOffset public static class Function extends Pointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Function(Pointer p) { super(p); }
    
        public Function(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/) { super((Pointer)null); allocate(results, parameters, name); }
        private native void allocate(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/);
        public Function(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters) { super((Pointer)null); allocate(results, parameters); }
        private native void allocate(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters);
        public Function(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/) { super((Pointer)null); allocate(results, parameters, name); }
        private native void allocate(@Const @ByRef NodeVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/);

        public Function(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/) { super((Pointer)null); allocate(result, parameters, name); }
        private native void allocate(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/);
        public Function(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters) { super((Pointer)null); allocate(result, parameters); }
        private native void allocate(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters);
        public Function(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/) { super((Pointer)null); allocate(result, parameters, name); }
        private native void allocate(@Const @SharedPtr @ByRef Node result,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/);

        public Function(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/) { super((Pointer)null); allocate(results, parameters, name); }
        private native void allocate(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString BytePointer name/*=""*/);
        public Function(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters) { super((Pointer)null); allocate(results, parameters); }
        private native void allocate(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters);
        public Function(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/) { super((Pointer)null); allocate(results, parameters, name); }
        private native void allocate(@Const @ByRef ResultVector results,
                         @Const @ByRef ParameterVector parameters,
                         @StdString String name/*=""*/);

        public native void init();
        /** Return the number of outputs for this function. */
        public native @Cast("size_t") long get_output_size();

        /** Return the op that generates output i */
        public native @SharedPtr @ByVal Node get_output_op(@Cast("size_t") long i);

        /** Return the element type of output i */
        public native @Const @ByRef Type get_output_element_type(@Cast("size_t") long i);

        /** Return the shape of element i */
        public native @Const @ByRef Shape get_output_shape(@Cast("size_t") long i);

        /** Return the partial shape of element i */

        /** Return the function parameters */
        public native @Const @ByRef ParameterVector get_parameters();
        /** Return a list of function's outputs */
        public native @Const @ByRef ResultVector get_results();
        /** Check that there is a single result and return it. */
        public native @SharedPtr @ByVal Node get_result();

        public native @StdString BytePointer get_friendly_name();
        public native @StdString BytePointer get_name();
        // so we can use `dynamic_cast` in FunctionCall to double check if we are dealing with
        //  an XLA or regular function
        public native void set_name(@StdString BytePointer name);
        public native void set_name(@StdString String name);
        
        public native @Cast("size_t") long get_instance_id();
        public native @Cast("size_t") long get_temporary_pool_size();
        public native void set_temporary_pool_size(@Cast("size_t") long arg0);
        // updates graph and m_results list
        public native void replace_node(@SharedPtr @ByVal Node old, @SharedPtr @ByVal Node repl);

        public native void validate_nodes_and_infer_types();
    }



// Parsed from ngraph/node_vector.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>
        @Namespace("ngraph::op") @Opaque public static class Result extends Pointer {
            /** Empty constructor. Calls {@code super((Pointer)null)}. */
            public Result() { super((Pointer)null); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Result(Pointer p) { super(p); }
        }
    

    /** \brief Zero or more nodes. */
    @Namespace("ngraph") public static class NodeVector extends NgraphNodeVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public NodeVector(Pointer p) { super(p); }
    

        public NodeVector(@Const @ByRef NgraphNodeVector nodes) { super((Pointer)null); allocate(nodes); }
        private native void allocate(@Const @ByRef NgraphNodeVector nodes);

        public NodeVector(@Const @ByRef NodeVector nodes) { super((Pointer)null); allocate(nodes); }
        private native void allocate(@Const @ByRef NodeVector nodes);

        public NodeVector(@Cast("size_t") long size) { super((Pointer)null); allocate(size); }
        private native void allocate(@Cast("size_t") long size);

        public native @ByRef @Name("operator =") NodeVector put(@Const @ByRef NodeVector other);

        public NodeVector() { super((Pointer)null); allocate(); }
        private native void allocate();
    }



// Parsed from ngraph/assertion.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <exception>
// #include <sstream>
// #include <vector>

// #include "ngraph/except.hpp"
    /** Base class for ngraph assertion failure exceptions. */
    @Namespace("ngraph") @NoOffset public static class AssertionFailure extends ngraph_error {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public AssertionFailure(Pointer p) { super(p); }
    
        public AssertionFailure(@StdString BytePointer what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString BytePointer what_arg);
        public AssertionFailure(@StdString String what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString String what_arg);

        public native @NoException @Cast("const char*") BytePointer what();
    }

    /**
     *  Helper class for failed assertions. Callers should not instantiate this class directly.
     *  This class is meant to be wrapped with a macro like NGRAPH_ASSERT. This class provides
     *  two main facilities: (1) an ostream accessible via get_stream(), to which a detailed
     *  error explanation can be written; and (2) throws an exception of type T when the
     *  AssertionHelper is destructed.
     * 
     * 
     *  Typical usage is via a wrapper around the NGRAPH_ASSERT_STREAM macro:
     * 
     *     class MyException : public AssertionFailure;
     * 
     *     #define MY_ASSERT(cond) NGRAPH_ASSERT_STREAM(::ngraph::MyException, cond)
     * 
     *     ...
     * 
     *     MY_ASSERT(42 != 43) << "Uh-oh. " << 42 << " is not " << 43 << ".";
     * 
     *  If the assertion fails, it will throw a CompileError exception with a what() string of:
     * 
     *    Assertion '42 != 43' failed at foo.cpp:123:
     *    Uh-oh. 42 is not 43.
     * 
     * 
     *  AssertionHelper also provides support for tagging the exception with a "location" string,
     *  reflecting things like the op that was being processed when the error occurred. For
     *  example:
     * 
     *    class CompileError : public AssertionFailure;
     * 
     *    #define COMPILE_ASSERT(node,cond)                                       <backslash>
     *       NGRAPH_ASSERT_STREAM_WITH_LOC(::ngraph::CompileError, cond,          <backslash>
     *                                     "While compiling node " + node->name())
     * 
     *    ...
     * 
     *    COMPILE_ASSERT(node, node->get_users().size != 0) << "Node has no users";
     * 
     *  If the assertion fails, it will throw a CompileError exception with a what() string
     *  similar to:
     * 
     *    While compiling node Add_123:
     *    Assertion 'node->get_users().size != 0' failed at foo.cpp:123:
     *    Node has no users
     *  */

    /**
     *  Class that returns a dummy ostream to absorb error strings for non-failed assertions.
     *  This is cheaper to construct than AssertionHelper, so the macros will produce a
     *  DummyAssertionHelper in lieu of an AssertionHelper if the condition is true.
     *  */
    @Namespace("ngraph") public static class DummyAssertionHelper extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public DummyAssertionHelper() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public DummyAssertionHelper(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public DummyAssertionHelper(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public DummyAssertionHelper position(long position) {
            return (DummyAssertionHelper)super.position(position);
        }
    
        /** Returns an ostream to which additional error details can be written. Anything written
         *  to this stream will be ignored. The returned stream has the lifetime of the
         *  DummyAssertionHelper. */
        public native @Cast("std::ostream*") @ByRef Pointer get_stream();
    }


/** Asserts condition "cond" with an exception class of "T", at location "loc". */
// #define NGRAPH_ASSERT_STREAM_WITH_LOC(T, cond, loc)
//     ((cond) ? ::ngraph::DummyAssertionHelper().get_stream()
//             : ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, #cond, loc).get_stream())
/** Asserts condition "cond" with an exception class of "T", and no location specified. */
// #define NGRAPH_ASSERT_STREAM(T, cond)
//     ((cond) ? ::ngraph::DummyAssertionHelper().get_stream()
//             : ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, #cond).get_stream())
/** Fails unconditionally with an exception class of "T", at location "loc". */
// #define NGRAPH_FAIL_STREAM_WITH_LOC(T, loc)
//     ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, "", loc).get_stream()
/** Fails unconditionally with an exception class of "T", and no location specified. */
// #define NGRAPH_FAIL_STREAM(T) ::ngraph::AssertionHelper<T>(__FILE__, __LINE__).get_stream()

// #define NGRAPH_ASSERT(cond) NGRAPH_ASSERT_STREAM(::ngraph::AssertionFailure, cond)
// #define NGRAPH_FAIL() NGRAPH_FAIL_STREAM(::ngraph::AssertionFailure)


// Parsed from ngraph/except.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <sstream>
// #include <stdexcept>
    /** Base error for ngraph runtime errors. */
    @Namespace("ngraph") public static class ngraph_error extends Pointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public ngraph_error(Pointer p) { super(p); }
    
        public ngraph_error(@StdString BytePointer what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString BytePointer what_arg);
        public ngraph_error(@StdString String what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString String what_arg);
    }

    @Namespace("ngraph") public static class unsupported_op extends Pointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public unsupported_op(Pointer p) { super(p); }
    
        public unsupported_op(@StdString BytePointer what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString BytePointer what_arg);
        public unsupported_op(@StdString String what_arg) { super((Pointer)null); allocate(what_arg); }
        private native void allocate(@StdString String what_arg);
    }



// Parsed from ngraph/placement.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>

// #include "ngraph/log.hpp"
    

    /** enum class ngraph::Placement */
    public static final int
        DEFAULT = 0,
        INTERPRETER = 1,
        CPU = 2,
        GPU = 3,
        NNP = 4,
        PLAIDML = 5;

    @Namespace("ngraph") public static native @StdString BytePointer placement_to_string(@Cast("ngraph::Placement") int placement);

    // Split function to function(s) with unique placement

    // Split function to function(s) with unique placement



// Parsed from ngraph/coordinate.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <algorithm>
// #include <vector>

// #include "ngraph/axis_set.hpp"
// #include "ngraph/shape.hpp"
    /** \brief Coordinates for a tensor element */
    @Namespace("ngraph") public static class Coordinate extends SizeTVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Coordinate(Pointer p) { super(p); }
    
        public Coordinate() { super((Pointer)null); allocate(); }
        private native void allocate();

        public Coordinate(@Const @ByRef Shape shape) { super((Pointer)null); allocate(shape); }
        private native void allocate(@Const @ByRef Shape shape);

        public Coordinate(@Const @ByRef SizeTVector axes) { super((Pointer)null); allocate(axes); }
        private native void allocate(@Const @ByRef SizeTVector axes);

        public Coordinate(@Const @ByRef Coordinate axes) { super((Pointer)null); allocate(axes); }
        private native void allocate(@Const @ByRef Coordinate axes);

        public Coordinate(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/) { super((Pointer)null); allocate(n, initial_value); }
        private native void allocate(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/);
        public Coordinate(@Cast("size_t") long n) { super((Pointer)null); allocate(n); }
        private native void allocate(@Cast("size_t") long n);

        public native @ByRef @Name("operator =") Coordinate put(@Const @ByRef Coordinate v);
    }

    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Coordinate coordinate);



// Parsed from ngraph/strides.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <ostream>
// #include <vector>
    /** \brief Strides for a tensor. */
    @Namespace("ngraph") public static class Strides extends SizeTVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Strides(Pointer p) { super(p); }
    

        public Strides(@Const @ByRef SizeTVector axis_strides) { super((Pointer)null); allocate(axis_strides); }
        private native void allocate(@Const @ByRef SizeTVector axis_strides);

        public Strides(@Const @ByRef Strides axis_strides) { super((Pointer)null); allocate(axis_strides); }
        private native void allocate(@Const @ByRef Strides axis_strides);

        public Strides(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/) { super((Pointer)null); allocate(n, initial_value); }
        private native void allocate(@Cast("size_t") long n, @Cast("size_t") long initial_value/*=0*/);
        public Strides(@Cast("size_t") long n) { super((Pointer)null); allocate(n); }
        private native void allocate(@Cast("size_t") long n);

        public Strides() { super((Pointer)null); allocate(); }
        private native void allocate();
        public native @ByRef @Name("operator =") Strides put(@Const @ByRef Strides v);
    }

    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Strides strides);



// Parsed from ngraph/descriptor/input.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/descriptor/tensor.hpp"

        // Describes a tensor that is an input to an op, directly or indirectly via a tuple
        @Namespace("ngraph::descriptor") @NoOffset public static class Input extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Input(Pointer p) { super(p); }
        
            /** @param node The node that owns this input
             *  @param index The position of this this tensor in all input tensors
             *  @param output The output that supplies a value for this input */
            public Input(Node node, @Cast("size_t") long index, @ByRef Output output) { super((Pointer)null); allocate(node, index, output); }
            private native void allocate(Node node, @Cast("size_t") long index, @ByRef Output output);

            /** @return the node that this is an input of */
            public native @SharedPtr Node get_node();

            /** @return the raw pointer to the node that this is an input of */
            public native Node get_raw_pointer_node();
            /** @return the position within all supplied tensors of this input */
            public native @Cast("size_t") long get_index();
            /** @return the connected output */
            /** @return the connected output */
            public native @ByRef Output get_output();
            /** @return the tensor of the connected output */

            /** @return the tensor of the connected output */
            public native @ByRef DescriptorTensor get_tensor();

            public native void replace_output(@SharedPtr Node node, @Cast("size_t") long i);
            public native void replace_output(@ByRef Output output);
            /** @return the shape of the connected output */
            public native @Const @ByRef Shape get_shape();

            /** @return the partial shape of the connected output */

            /** @return the element type of the connected output */
            public native @Const @ByRef Type get_element_type();
        }
    



// Parsed from ngraph/descriptor/output.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <set>

// #include "ngraph/descriptor/input.hpp"
// #include "ngraph/descriptor/tensor.hpp"
    // The forward declaration of Node is needed here because Node has a deque of
    // Outputs, and Output is an incomplete type at this point. STL containers of
    // incomplete type have undefined behavior according to the C++11 standard, and
    // in practice including node.hpp here was causing compilation errors on some
    // systems (namely macOS).
        // Describes an output tensor of an op
        @Namespace("ngraph::descriptor") @NoOffset public static class Output extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Output(Pointer p) { super(p); }
        
            /** @param node Node that owns this output.
             *  @param index Position of the output tensor in all output tensors
             *  @param tensor The view of this tensor; where the value will be written */
            public Output(Node node, @Cast("size_t") long index, @Const @SharedPtr @ByRef DescriptorTensor tensor) { super((Pointer)null); allocate(node, index, tensor); }
            private native void allocate(Node node, @Cast("size_t") long index, @Const @SharedPtr @ByRef DescriptorTensor tensor);

            public native @SharedPtr @ByVal Node get_node();
            public native @Cast("size_t") long get_index();
            public native @SharedPtr @ByVal DescriptorTensor get_tensor_ptr();
            public native void set_tensor_ptr(@Const @SharedPtr @ByRef DescriptorTensor tensor);
            public native void add_input(Input input);
            public native void remove_input(Input input);
            public native @ByRef DescriptorTensor get_tensor();

            /** @return the shape of the output */
            public native @Const @ByRef Shape get_shape();

            /** @return the partial shape of the output */

            /** @return the element type of the output */
            public native @Const @ByRef Type get_element_type();
        }
    



// Parsed from ngraph/op/op.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <string>

// #include "ngraph/node.hpp"
// #include "ngraph/op/util/op_annotations.hpp"
        /** Root of all actual ops */
        @Namespace("ngraph::op") @NoOffset public static class Op extends Node {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Op(Pointer p) { super(p); }
        
            public native void set_op_annotations(@SharedPtr OpAnnotations op_annotations);
            public native @SharedPtr OpAnnotations get_op_annotations();
        }
    



// Parsed from ngraph/parameter_vector.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>

// #include "ngraph/op/parameter.hpp"
    /** \brief Zero or more nodes. */
    @Namespace("ngraph") public static class ParameterVector extends NgraphParameterVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public ParameterVector(Pointer p) { super(p); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public ParameterVector(long size) { super((Pointer)null); allocateArray(size); }
        private native void allocateArray(long size);
        @Override public ParameterVector position(long position) {
            return (ParameterVector)super.position(position);
        }
    

        public ParameterVector(@Const @ByRef NgraphParameterVector parameters) { super((Pointer)null); allocate(parameters); }
        private native void allocate(@Const @ByRef NgraphParameterVector parameters);

        public ParameterVector(@Const @ByRef ParameterVector parameters) { super((Pointer)null); allocate(parameters); }
        private native void allocate(@Const @ByRef ParameterVector parameters);

        public native @ByRef @Name("operator =") ParameterVector put(@Const @ByRef ParameterVector parameters);

        public ParameterVector() { super((Pointer)null); allocate(); }
        private native void allocate();
    }



// Parsed from ngraph/op/parameter.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/op/op.hpp"
        /** \brief A function parameter.
         * 
         *  Parameters are nodes that represent the arguments that will be passed to user-defined functions.
         *  Function creation requires a sequence of parameters.
         *  Basic graph operations do not need parameters attached to a function. */
        @Namespace("ngraph::op") @NoOffset public static class Parameter extends Op {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Parameter(Pointer p) { super(p); }
        
            /** \brief Constructions a tensor view-typed parameter node.
             * 
             *  @param element_type The element type of the parameter.
             *  @param pshape The partial shape of the parameter.
             *  @param cacheable True if the parameter is not expected to be frequently updated. */

            public native void validate_and_infer_types();

            public native @Cast("bool") boolean get_cacheable();
            public native @SharedPtr @ByVal Node copy_with_new_args(@Const @ByRef NodeVector new_args);
        }
    



// Parsed from ngraph/result_vector.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>

// #include "ngraph/op/result.hpp"
    /** \brief Zero or more nodes. */
    @Namespace("ngraph") public static class ResultVector extends NgraphResultVector {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public ResultVector(Pointer p) { super(p); }
    
        public ResultVector(@Cast("size_t") long size) { super((Pointer)null); allocate(size); }
        private native void allocate(@Cast("size_t") long size);

        public ResultVector(@Const @ByRef NgraphResultVector nodes) { super((Pointer)null); allocate(nodes); }
        private native void allocate(@Const @ByRef NgraphResultVector nodes);

        public ResultVector(@Const @ByRef ResultVector nodes) { super((Pointer)null); allocate(nodes); }
        private native void allocate(@Const @ByRef ResultVector nodes);

        public native @ByRef @Name("operator =") ResultVector put(@Const @ByRef ResultVector arg0);

        public ResultVector() { super((Pointer)null); allocate(); }
        private native void allocate();
    }



// Parsed from ngraph/op/util/op_annotations.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/assertion.hpp"
            @Namespace("ngraph::op::util") public static class oi_pair extends Pointer {
                static { Loader.load(); }
                /** Default native constructor. */
                public oi_pair() { super((Pointer)null); allocate(); }
                /** Native array allocator. Access with {@link Pointer#position(long)}. */
                public oi_pair(long size) { super((Pointer)null); allocateArray(size); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public oi_pair(Pointer p) { super(p); }
                private native void allocate();
                private native void allocateArray(long size);
                @Override public oi_pair position(long position) {
                    return (oi_pair)super.position(position);
                }
            
                public native @Cast("size_t") long output(); public native oi_pair output(long output);
                public native @Cast("size_t") long input(); public native oi_pair input(long input);
                public native @Cast("bool") boolean destructive(); public native oi_pair destructive(boolean destructive);
            }

            /** \brief Base class for annotations added to graph ops */
            @Namespace("ngraph::op::util") public static class OpAnnotations extends Pointer {
                static { Loader.load(); }
                /** Default native constructor. */
                public OpAnnotations() { super((Pointer)null); allocate(); }
                /** Native array allocator. Access with {@link Pointer#position(long)}. */
                public OpAnnotations(long size) { super((Pointer)null); allocateArray(size); }
                /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
                public OpAnnotations(Pointer p) { super(p); }
                private native void allocate();
                private native void allocateArray(long size);
                @Override public OpAnnotations position(long position) {
                    return (OpAnnotations)super.position(position);
                }
            

                public native void add_in_place_oi_pair(@Const @ByRef oi_pair oi);

                public native @StdVector oi_pair get_in_place_oi_pairs();

                public native @Cast("bool") boolean is_cacheable();
                public native void set_cacheable(@Cast("bool") boolean val);
            }
        
    



// Parsed from ngraph/autodiff/adjoints.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <map>
// #include <memory>
// #include <unordered_map>

// #include "ngraph/coordinate.hpp"
// #include "ngraph/node_vector.hpp"
// #include "ngraph/strides.hpp"
        @Namespace("ngraph::runtime") @Opaque public static class Manager extends Pointer {
            /** Empty constructor. Calls {@code super((Pointer)null)}. */
            public Manager() { super((Pointer)null); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Manager(Pointer p) { super(p); }
        }
    
        @Namespace("ngraph::autodiff") @NoOffset public static class Adjoints extends Pointer {
            static { Loader.load(); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public Adjoints(Pointer p) { super(p); }
            /** Native array allocator. Access with {@link Pointer#position(long)}. */
            public Adjoints(long size) { super((Pointer)null); allocateArray(size); }
            private native void allocateArray(long size);
            @Override public Adjoints position(long position) {
                return (Adjoints)super.position(position);
            }
        
            /** \brief (dy/dx)(c) for all x used to compute y
             * 
             *  @param y The dependent value
             *  @param c An expression for where to evaluate the derivatives */
            public Adjoints(@Const @ByRef NodeVector y, @Const @ByRef NodeVector c) { super((Pointer)null); allocate(y, c); }
            private native void allocate(@Const @ByRef NodeVector y, @Const @ByRef NodeVector c);

            public Adjoints(@Const @ByRef Adjoints adjoints) { super((Pointer)null); allocate(adjoints); }
            private native void allocate(@Const @ByRef Adjoints adjoints);
            public native @ByRef @Name("operator =") Adjoints put(@Const @ByRef Adjoints adjoints);
            
            ///
            public Adjoints() { super((Pointer)null); allocate(); }
            private native void allocate();

            /** \brief (dy/dx)(c)
             * 
             *  @param x The node whose adjoint is desired. */
            
            ///
            public native @Const @ByRef NodeVector get(@Const @SharedPtr @ByRef Node x);

            /** \brief Add a backprop contribution to x's adjoint
             * 
             *  @param x The adjoint node
             *  @param delta A backprop contribution */
            
            ///
            public native void add_delta(@Const @SharedPtr @ByRef Node x,
                                       @Const @SharedPtr @ByRef Node delta,
                                       @Cast("size_t") long output_index/*=0*/);
            public native void add_delta(@Const @SharedPtr @ByRef Node x,
                                       @Const @SharedPtr @ByRef Node delta);

            /** \brief Add a backprop contribution to a slice of x's adjoint
             * 
             *  @param x The adjoint node
             *  @param delta A backprop contribution
             *  @param lower_bounds Lower bounds of slice to add to
             *  @param upper_bounds Upper bounds of slice to add to
             *  @param strides Strides of slice to add to */
            public native void add_delta_to_slice(@Const @SharedPtr @ByRef Node x,
                                                @Const @SharedPtr @ByRef Node delta,
                                                @Const @ByRef Coordinate lower_bounds,
                                                @Const @ByRef Coordinate upper_bounds,
                                                @Const @ByRef Strides strides);

            public native @SharedPtr @ByVal Node backprop_node(@Const @SharedPtr @ByRef Node x);
        }
    



// Parsed from ngraph/node.hpp

//*****************************************************************************
// Copyright 2017-2018 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <atomic>
// #include <deque>
// #include <iostream>
// #include <memory>
// #include <set>
// #include <string>
// #include <tuple>
// #include <typeindex>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>

// #include "ngraph/assertion.hpp"
// #include "ngraph/autodiff/adjoints.hpp"
// #include "ngraph/descriptor/input.hpp"
// #include "ngraph/descriptor/output.hpp"
// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/node_vector.hpp"
// #include "ngraph/placement.hpp"
        @Namespace("ngraph::pass") @Opaque public static class GetOutputElementElimination extends Pointer {
            /** Empty constructor. Calls {@code super((Pointer)null)}. */
            public GetOutputElementElimination() { super((Pointer)null); }
            /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
            public GetOutputElementElimination(Pointer p) { super(p); }
        }
    
    

    @Namespace("ngraph") public static native void replace_node_users_arguments(@SharedPtr @ByVal Node target,
                                          @SharedPtr @ByVal Node replacement);

    @Namespace("ngraph") public static native void insert_new_node_between(@Const @SharedPtr @ByRef Node src_node,
                                     @Const @SharedPtr @ByRef Node dst_node,
                                     @Const @SharedPtr @ByRef Node new_node);

    @Namespace("ngraph") public static native @StdString BytePointer node_validation_assertion_string(@Const Node node);

    @Namespace("ngraph") public static native @Const @SharedPtr @ByRef Node check_single_output_arg(@Const @SharedPtr @ByRef Node node,
                                                             @Cast("size_t") long i);
    @Namespace("ngraph") public static native @Const @ByRef NodeVector check_single_output_args(@Const @ByRef NodeVector args);

    /** Nodes are the backbone of the graph of Value dataflow. Every node has
     *  zero or more nodes as arguments and one value, which is either a tensor
     *  view or a (possibly empty) tuple of values. */
    @Namespace("ngraph") @NoOffset public static class Node extends Pointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public Node(Pointer p) { super(p); }
    
        public native void revalidate_and_infer_types();
        // Called after transition
        public native void delayed_validate_and_infer_types();

        /** The class name, must not contain spaces */
        public native @StdString BytePointer description();
        public native @StdString BytePointer get_friendly_name();
        public native @StdString BytePointer get_name();
        public native void set_name(@StdString BytePointer name);
        public native void set_name(@StdString String name);
        /** Return true if this has the same implementing class as node. This
         *  will be used by the pattern matcher when comparing a pattern
         *  graph against the graph. */
        public native @Cast("bool") boolean is_same_op_type(@Const @SharedPtr @ByRef Node node);

        public native @Cast("bool") boolean is_parameter();
        public native @Cast("bool") boolean is_output();
        public native @Cast("bool") boolean is_constant();
        public native @Cast("bool") boolean is_commutative();
        public native @Cast("size_t") long get_instance_id();
        
        public native @Cast("std::ostream*") @ByRef Pointer write_short_description(@Cast("std::ostream*") @ByRef Pointer arg0);
        public native @Cast("std::ostream*") @ByRef Pointer write_long_description(@Cast("std::ostream*") @ByRef Pointer arg0);

        // TODO: Deprecate
        // TODO: Deprecate
        // Deprecated
        // TODO: Remove from unit tests.
        // Deprecated
        // TODO: Remove from unit tests.

        /** Get control dependencies registered on the node */

        public native void add_control_dependency(@SharedPtr @ByVal Node node);

        public native void remove_control_dependency(@SharedPtr @ByVal Node node);

        /** Returns the number of outputs on the for the node. */
        public native @Cast("size_t") long get_output_size();

        /** Returns the element type for output i */
        public native @Const @ByRef Type get_output_element_type(@Cast("size_t") long i);

        /** Checks that there is exactly one output and returns its element type */
        public native @Const @ByRef Type get_element_type();

        /** Returns the shape for output i */
        public native @Const @ByRef Shape get_output_shape(@Cast("size_t") long i);

        /** Returns the partial shape for output i */

        /** Checks that there is exactly one output and returns its shape */
        public native @Const @ByRef Shape get_shape();

        /** Returns the tensor for output i */
        public native @ByRef DescriptorTensor get_output_tensor(@Cast("size_t") long i);

        /** Checks that there is exactly one output and returns its tensor. */
        public native @ByRef DescriptorTensor get_output_tensor();

        /** Returns the tensor view of output i */
        public native @SharedPtr @ByVal DescriptorTensor get_output_tensor_ptr(@Cast("size_t") long i);

        /** Checks that there is exactly one output and returns its tensor view. */
        public native @SharedPtr @ByVal DescriptorTensor get_output_tensor_ptr();

        /** Returns the set of inputs using output i */

        /** Returns the number of inputs for the op */
        public native @Cast("size_t") long get_input_size();

        /** Returns the element type of input i */
        public native @Const @ByRef Type get_input_element_type(@Cast("size_t") long i);

        /** Returns the shape of input i */
        public native @Const @ByRef Shape get_input_shape(@Cast("size_t") long i);

        /** Returns the partial shape of input i */

        public native @ByVal NodeVector get_arguments();

        public native @SharedPtr @ByVal Node get_argument(@Cast("size_t") long index);

        public native @SharedPtr @ByVal Node copy_with_new_args(@Const @ByRef NodeVector new_args);

        public native @ByVal NgraphFunctionVector get_functions();

        /** True if this and node have one output with same element type and shape */
        

        /** Get device placement */
        public native @Cast("ngraph::Placement") int get_placement();

        /** Set device placement */
        public native void set_placement(@Cast("ngraph::Placement") int placement);

        /** Get device placement */
        public native @Cast("size_t") long get_placement_size();

        /** Set device placement */
        public native void set_placement(@Cast("size_t") long placement);

        /** Get input descriptor that is connected to src */
        public native Input get_input_from(@Const @SharedPtr @ByRef Node src);

        /** Get ouput descriptor that outputs to dst */
        public native Output get_output_to(@Const @SharedPtr @ByRef Node dst);

        /** Get all the nodes that uses the current node */
        public native @ByVal NodeVector get_users(@Cast("bool") boolean check_is_used/*=false*/);
        public native @ByVal NodeVector get_users();

        public native @SharedPtr @ByVal Node get_default_value();
        /** Use instance ids for comparison instead of memory addresses to improve determinism */
        public native @Cast("bool") @Name("operator <") boolean lessThan(@Const @ByRef Node other);
    }

    @Namespace("ngraph") public static class NodeValidationError extends AssertionFailure {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public NodeValidationError(Pointer p) { super(p); }
    
        public NodeValidationError(@StdString BytePointer what) { super((Pointer)null); allocate(what); }
        private native void allocate(@StdString BytePointer what);
        public NodeValidationError(@StdString String what) { super((Pointer)null); allocate(what); }
        private native void allocate(@StdString String what);
    }

    @Namespace("ngraph") @NoOffset public static class NodeDescription extends Pointer {
        static { Loader.load(); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public NodeDescription(Pointer p) { super(p); }
    
        public NodeDescription(@Const @ByRef Node node, @Cast("bool") boolean is_short) { super((Pointer)null); allocate(node, is_short); }
        private native void allocate(@Const @ByRef Node node, @Cast("bool") boolean is_short);

        
        @MemberGetter public native @Const @ByRef Node m_node();
        public native @Cast("bool") boolean m_is_short(); public native NodeDescription m_is_short(boolean m_is_short);
    }

    @Namespace("ngraph") public static native void check_new_args_count(@Const Node node, @Const @ByRef NodeVector new_args);


// #define NODE_VALIDATION_ASSERT(node, cond)
//     NGRAPH_ASSERT_STREAM_WITH_LOC(
//         ::ngraph::NodeValidationError, cond, ::ngraph::node_validation_assertion_string(node))
// #define NODE_VALIDATION_FAIL(node)
//     NGRAPH_FAIL_STREAM_WITH_LOC(::ngraph::NodeValidationError,
//                                 ::ngraph::node_validation_assertion_string(node))


// Parsed from ngraph/onnxifi.h

// #ifndef ONNXIFI_H
public static final int ONNXIFI_H = 1;

// #ifdef __cplusplus
// #endif

// #if defined(_WIN32) && defined(_M_IX86)
/* Windows x86 */
// #define ONNXIFI_ABI __stdcall
// #elif defined(__i386__)
/* Linux x86 */
// #define ONNXIFI_ABI __attribute__((__cdecl__))
// #else
// #define ONNXIFI_ABI
// #endif

// #ifndef ONNXIFI_PUBLIC
// #if defined(__ELF__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(__MACH__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(_WIN32) && defined(__GNUC__)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __attribute__((__dllexport__))
// #else
// #define ONNXIFI_PUBLIC __attribute__((__dllimport__))
// #endif
// #elif defined(_WIN32)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __declspec(dllexport)
// #else
// #define ONNXIFI_PUBLIC __declspec(dllimport)
// #endif
// #else
// #define ONNXIFI_PUBLIC
// #endif
// #endif

// #ifndef ONNXIFI_CHECK_RESULT
//   #if defined(__GNUC__) && (__GNUC__ >= 4)
//     #define ONNXIFI_CHECK_RESULT __attribute__((__warn_unused_result__))
//   #elif defined(_MSC_VER) && (_MSC_VER >= 1700)
//     #define ONNXIFI_CHECK_RESULT _Check_return_
//   #else
//     #define ONNXIFI_CHECK_RESULT
//   #endif
// #endif

// #include <stddef.h>

// #if !defined(ONNXIFI_NO_STDINT_H)
// #if defined(_MSC_VER) && (_MSC_VER < 1600)
// #else
// #include <stdint.h>
// #endif
// #endif /* !defined(ONNXIFI_NO_STDINT_H) */

/**
 * Opaque ONNXIFI backend ID.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. Backend ID uniquely identifies a backend for the life-
 * time of the process (i.e. no two hardware devices, software layers, or
 * combinations of both can have the same backend ID). Backend ID stays valid
 * even if the hardware device used by the backend disconnects from the system.
 */
@Namespace @Name("void") @Opaque public static class onnxBackendID extends Pointer {
    /** Empty constructor. Calls {@code super((Pointer)null)}. */
    public onnxBackendID() { super((Pointer)null); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxBackendID(Pointer p) { super(p); }
}
/**
 * Opaque ONNXIFI backend handle.
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph.
 */
@Namespace @Name("void") @Opaque public static class onnxBackend extends Pointer {
    /** Empty constructor. Calls {@code super((Pointer)null)}. */
    public onnxBackend() { super((Pointer)null); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxBackend(Pointer p) { super(p); }
}
/** Opaque ONNXIFI graph handle. */
@Namespace @Name("void") @Opaque public static class onnxGraph extends Pointer {
    /** Empty constructor. Calls {@code super((Pointer)null)}. */
    public onnxGraph() { super((Pointer)null); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxGraph(Pointer p) { super(p); }
}
/** Opaque ONNXIFI even handle. */
@Namespace @Name("void") @Opaque public static class onnxEvent extends Pointer {
    /** Empty constructor. Calls {@code super((Pointer)null)}. */
    public onnxEvent() { super((Pointer)null); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxEvent(Pointer p) { super(p); }
}

/** Return code for ONNXIFI functions */
/**
 * Type for enumeration values.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for bit fields.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for pointers or handles for memory buffers.
 * This type is intended to work not only for CPU-addressable memory, but also
 * for device memory. uint64_t ensures the API can accomodate Vulkan buffers.
 */

public static final int ONNXIFI_STATUS_SUCCESS = 0x0000;
public static final int ONNXIFI_STATUS_FALLBACK = 0x0001;
public static final int ONNXIFI_STATUS_INVALID_ID = 0x0101;
public static final int ONNXIFI_STATUS_INVALID_SIZE = 0x0102;
public static final int ONNXIFI_STATUS_INVALID_POINTER = 0x0103;
public static final int ONNXIFI_STATUS_INVALID_PROTOBUF = 0x0104;
public static final int ONNXIFI_STATUS_INVALID_MODEL = 0x0105;
public static final int ONNXIFI_STATUS_INVALID_BACKEND = 0x0106;
public static final int ONNXIFI_STATUS_INVALID_GRAPH = 0x0107;
public static final int ONNXIFI_STATUS_INVALID_EVENT = 0x0108;
public static final int ONNXIFI_STATUS_INVALID_STATE = 0x0109;
public static final int ONNXIFI_STATUS_INVALID_NAME = 0x010A;
public static final int ONNXIFI_STATUS_INVALID_SHAPE = 0x010B;
public static final int ONNXIFI_STATUS_INVALID_DATATYPE = 0x010C;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_TYPE = 0x010D;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_LOCATION = 0x010E;
public static final int ONNXIFI_STATUS_INVALID_FENCE_TYPE = 0x010F;
public static final int ONNXIFI_STATUS_INVALID_PROPERTY = 0x0110;
public static final int ONNXIFI_STATUS_UNSUPPORTED_TAG = 0x0201;
public static final int ONNXIFI_STATUS_UNSUPPORTED_VERSION = 0x0202;
public static final int ONNXIFI_STATUS_UNSUPPORTED_OPERATOR = 0x0203;
public static final int ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE = 0x0204;
public static final int ONNXIFI_STATUS_UNSUPPORTED_SHAPE = 0x0205;
public static final int ONNXIFI_STATUS_UNSUPPORTED_DATATYPE = 0x0206;
public static final int ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE = 0x0207;
public static final int ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE = 0x0208;
public static final int ONNXIFI_STATUS_UNSUPPORTED_PROPERTY = 0x0209;
public static final int ONNXIFI_STATUS_UNIDENTIFIED_NAME = 0x0301;
public static final int ONNXIFI_STATUS_MISMATCHING_SHAPE = 0x0302;
public static final int ONNXIFI_STATUS_MISMATCHING_DATATYPE = 0x0303;
public static final int ONNXIFI_STATUS_NO_SYSTEM_MEMORY = 0x0401;
public static final int ONNXIFI_STATUS_NO_DEVICE_MEMORY = 0x0402;
public static final int ONNXIFI_STATUS_NO_SYSTEM_RESOURCES = 0x0403;
public static final int ONNXIFI_STATUS_NO_DEVICE_RESOURCES = 0x0404;
public static final int ONNXIFI_STATUS_BACKEND_UNAVAILABLE = 0x0405;
public static final int ONNXIFI_STATUS_INTERNAL_ERROR = 0x0406;

/**
 * State of an ONNXIFI event object.
 *
 * Possible values:
 *     ONNXIFI_EVENT_STATE_INVALID
 *     ONNXIFI_EVENT_STATE_NONSIGNALLED
 *     ONNXIFI_EVENT_STATE_SIGNALLED
 */

/**
 * State for an invalid onnxEvent.
 */
public static final int ONNXIFI_EVENT_STATE_INVALID = 0;
/**
 * Non-signalled onnxEvent state.
 * onnxInitEvent creates events in non-signalled state.
 */
public static final int ONNXIFI_EVENT_STATE_NONSIGNALLED = 0x16BD;
/**
 * Signalled onnxEvent state.
 * onnxSignalEvent changes event state to signalled.
 */
public static final int ONNXIFI_EVENT_STATE_SIGNALLED = 0x3395;

/** Special-purpose accelerator for neural network */
public static final int ONNXIFI_DEVICE_TYPE_NPU = 0x01;
/** Digital signal processor */
public static final int ONNXIFI_DEVICE_TYPE_DSP = 0x02;
/** Graphics accelerator */
public static final int ONNXIFI_DEVICE_TYPE_GPU = 0x04;
/** General-purpose central processor */
public static final int ONNXIFI_DEVICE_TYPE_CPU = 0x08;
/** Field-programmable gate array */
public static final int ONNXIFI_DEVICE_TYPE_FPGA = 0x10;
/**
 * Heterogeneous backend whichinternally arbitrates or distributes work between
 * multiple device types.
 */
public static final int ONNXIFI_DEVICE_TYPE_HETEROGENEOUS = 0x20;

/**
 * The backend supports multi-threaded access to ONNXIFI backend, graph, and
 * event objects. E.g. onnxInitGraph can be called on a different thread than
 * onnxInitBackend.
 *
 * If this capability it not indicated, ONNXIFI backend, graph, and event
 * objects that relate to the backend must always be used on the same thread
 * where the backend object was initialized.
 */
public static final int ONNXIFI_CAPABILITY_THREAD_SAFE = 0x01;
/**
 * The backend supports ONNX graphs with symbolic variables in the outer
 * shape dimension (batch size), using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * The exact numerical value of the  of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE = 0x02;
/**
 * The backend supports ONNX graphs with symbolic variables in the all
 * shape dimensions, using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE capability.
 *
 * The exact numerical shape of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS = 0x04;
/**
 * The backend supports ONNX graphs with data-dependent outer shape dimension
 * (batch size) of graph outputs. The ONNX graph would specify unknown outer
 * shape dimension (batch size) using symbolic variables, so this capability
 * requires ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE support.
 *
 * For outputs with data-dependent outer shape dimension (batch size) the value
 * specified in onnxSetGraphIO call is interpreted as the upper limit. The exact
 * numerical batch size of the output can be retrieved by attaching a Shape
 * operator to the tensor with data-dependent shape and reading its output
 * through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE = 0x08;
/**
 * The backend supports ONNX graphs with data-dependent output shapes.
 * The ONNX graph would specify unknown output shapes using symbolic variables,
 * so this capability requires ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS support.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE capability.
 *
 * For outputs with data-dependent shapes the shape specified in onnxSetGraphIO
 * call is interpreted as the upper limit. The exact numerical shape of the
 * output can be retrieved by attaching a Shape operator to the tensor with
 * data-dependent shape and reading its output through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS = 0x10;
/**
 * The backend uses a hot-pluggable device, and can be disconnected at any time.
 *
 * If the underlying device disconnects from the system, subsequent operations
 * with the backend, or objects created on the backend, will fail with
 * ONNXIFI_STATUS_BACKEND_UNAVAILABLE status code.
 */
public static final int ONNXIFI_CAPABILITY_HOT_PLUGGABLE = 0x20;

/**
 * Type of the backend information.
 *
 * Possible values:
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION
 *     ONNXIFI_BACKEND_NAME
 *     ONNXIFI_BACKEND_VENDOR
 *     ONNXIFI_BACKEND_VERSION
 *     ONNXIFI_BACKEND_EXTENSIONS
 *     ONNXIFI_BACKEND_DEVICE
 *     ONNXIFI_BACKEND_DEVICE_TYPE
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION
 *     ONNXIFI_BACKEND_OPSET_VERSION
 *     ONNXIFI_BACKEND_CAPABILITIES
 *     ONNXIFI_BACKEND_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_MEMORY_TYPES
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES
 *     ONNXIFI_BACKEND_MEMORY_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT
 *     ONNXIFI_BACKEND_MACS_FP32
 *     ONNXIFI_BACKEND_MACS_FP16
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH
 *     ONNXIFI_BACKEND_PCI_BUS_ID
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID
 *     ONNXIFI_BACKEND_DIRECTX_ID
 *     ONNXIFI_BACKEND_CUDA_INDEX
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID
 */

/**
 * Major and minor version of ONNXIFI specification implemented by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 *      The high 32 bits specify the major version.
 *      The low 32 bits specify the minor version.
 *
 * Possible values:
 *      UINT64_C(0x0000000100000000) for ONNXIFI 1.0
 */
public static final int ONNXIFI_BACKEND_ONNXIFI_VERSION = 0;

/**
 * Marketing name of the backend (excluding the vendor name).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Caffe2"
 *    "Glow"
 */
public static final int ONNXIFI_BACKEND_NAME = 1;

/**
 * Name of the backend vendor.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Facebook"
 *    "Marat Dukhan"
 */
public static final int ONNXIFI_BACKEND_VENDOR = 2;

/**
 * Version of the backend software. Exact format is vendor-specific, but MUST be
 * unique for the software release.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "1.2.3"
 *    "1.2.3.0"
 *    "1.2.3-db3a4439d233276e25681fb4735b7f8e674dda65"
 */
public static final int ONNXIFI_BACKEND_VERSION = 3;

/**
 * Space-separated list of vendor- or device-specific extensions supported on
 * this backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    ""
 *    "onnx_clone_graph"
 *    "onnx_clone_graph fb_maskrcnn"
 */
public static final int ONNXIFI_BACKEND_EXTENSIONS = 4;

/**
 * Descriptive name of the device (i.e. CPU, GPU, DSP, or NPU model).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "nnDuino 123"
 */
public static final int ONNXIFI_BACKEND_DEVICE = 5;

/**
 * Type of the device.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxEnum.
 * Possible values:
 *      ONNXIFI_DEVICE_TYPE_NPU
 *      ONNXIFI_DEVICE_TYPE_DSP
 *      ONNXIFI_DEVICE_TYPE_GPU
 *      ONNXIFI_DEVICE_TYPE_CPU
 *      ONNXIFI_DEVICE_TYPE_FPGA
 *      ONNXIFI_DEVICE_TYPE_HETEROGENEOUS
 */
public static final int ONNXIFI_BACKEND_DEVICE_TYPE = 6;

/**
 * List of supported ONNX IR versions.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "3" (IR version in ONNX 1.0)
 *
 * Possible values: space-separated list of supported ONNX IR versions,
 *     represented as decimal integers. ONNX IR versions must match values
 *     in ONNX Version enum.
 */
public static final int ONNXIFI_BACKEND_ONNX_IR_VERSION = 7;

/**
 * List of supported operator set domains and maximum supported operator set
 * version for each domain.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "ai.onnx:1" (only operators in version 1 of default ONNX operator set)
 *    "ai.onnx:7" (operators up to version 7 of default ONNX operator set)
 *    "org.pytorch:2 ai.onnx:6 ai.facebook:1"
 *
 * Possible values: space-separated list of domain:max_version pairs where
 *     domain corresponds to OperatorSetIdProto.domain and max_version
 *     corresponds to the maximum value of OperatorSetIdProto.version supported
 *     by the backend for this domain. The backend MUST support all previous
 *     operator set versions as well.
 */
public static final int ONNXIFI_BACKEND_OPSET_VERSION = 8;

/**
 * Optional features supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of the following flags:
 *      ONNXIFI_CAPABILITY_THREAD_SAFE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS
 *      ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS
 *      ONNXIFI_CAPABILITY_HOT_PLUGGABLE
 *      or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_CAPABILITIES = 10;

/**
 * Auxiliary initialization properties supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_INIT_PROPERTIES = 11;

/**
 * Memory types supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values are any combination of the following flags:
 *     ONNXIFI_MEMORY_TYPE_CPU (always supported)
 *     ONNXIFI_MEMORY_TYPE_CUDA_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D
 *     ONNXIFI_MEMORY_TYPE_D3D_RESOURCE
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_MEMORY_TYPES = 12;

/**
 * Auxiliary initialization properties supported by graphs on the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES = 13;

/**
 * Memory synchronization primitives supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Possible values are any combination of the following flags:
 *     ONNXIFI_SYNCHRONIZATION_EVENT    (onnxEvent, always supported)
 *     ONNXIFI_SYNCHRONIZATION_IMPLICIT
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES = 14;

/**
 * Maximum amount of memory, in bytes, available to the use by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_SIZE = 20;

/**
 * Maximum size of network parameters, in bytes.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_SIZE = 21;

/**
 * Maximum number of independent network graphs supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_COUNT = 22;

/**
 * Number of FP32 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP32 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP32 = 30;

/**
 * Number of FP16 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP16 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP16 = 31;

/**
 * Bandwidth, in bytes per second, of the global memory specific to the backend
 * device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_BANDWIDTH = 35;

/**
 * Bandwidth, in bytes per second, of transferring data from cacheable
 * CPU-allocated memory to the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH = 36;

/**
 * Bandwidth, in bytes per second, of transferring data to cacheable
 * CPU-allocated memory from the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH = 37;

/**
 * PCI bus ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_BUS_ID = 40;

/**
 * PCI device ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DEVICE_ID = 41;

/**
 * PCI domain/function ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DOMAIN_ID = 42;

/**
 * DirectX ID of the backend device.
 *
 * This is the value that would be returned by ID3D12Device::GetAdapterLuid()
 * for the hardware device used by the backend.
 *
 * Since ONNXIFI 1.0, DXGI-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: LUID (8 bytes).
 */
public static final int ONNXIFI_BACKEND_DIRECTX_ID = 43;

/**
 * CUDA index of the backend device.
 *
 * Since ONNXIFI 1.0, CUDA-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_INDEX = 44;

/**
 * OpenCL platform ID for the backend device.
 * This platform ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_platform_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_PLATFORM_ID = 45;

/**
 * OpenCL device ID for the backend device.
 * This device ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_device_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_DEVICE_ID = 46;

/* Note: the data type values match ONNX TensorProto.DataType enum */
public static final int ONNXIFI_DATATYPE_UNDEFINED = 0;
public static final int ONNXIFI_DATATYPE_FLOAT16 = 10;
public static final int ONNXIFI_DATATYPE_FLOAT32 = 1;
public static final int ONNXIFI_DATATYPE_FLOAT64 = 11;
public static final int ONNXIFI_DATATYPE_INT8 = 3;
public static final int ONNXIFI_DATATYPE_INT16 = 5;
public static final int ONNXIFI_DATATYPE_INT32 = 6;
public static final int ONNXIFI_DATATYPE_INT64 = 7;
public static final int ONNXIFI_DATATYPE_UINT8 = 2;
public static final int ONNXIFI_DATATYPE_UINT16 = 4;
public static final int ONNXIFI_DATATYPE_UINT32 = 12;
public static final int ONNXIFI_DATATYPE_UINT64 = 13;
public static final int ONNXIFI_DATATYPE_COMPLEX64 = 14;
public static final int ONNXIFI_DATATYPE_COMPLEX128 = 15;

/** Cacheable CPU memory */
public static final int ONNXIFI_MEMORY_TYPE_CPU = 0;
/** CUDA memory buffer (allocated via cudaMalloc/cuMalloc).  */
public static final int ONNXIFI_MEMORY_TYPE_CUDA_BUFFER = 1;
/** OpenCL cl_mem object for a buffer or sub-buffer. */
public static final int ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER = 2;
/** OpenGL ES 2.0+ 2D Texture. */
public static final int ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D = 4;
/** Direct3D resource. */
public static final int ONNXIFI_MEMORY_TYPE_D3D_RESOURCE = 8;

/**
 * Terminates the list of auxiliary backend initialization properties passed to
 * onnxInitBackend.
 */
public static final int ONNXIFI_BACKEND_PROPERTY_NONE = 0;
/**
 * Optimization target for graphs initialized on the backend.
 *
 * Possible values:
 *     ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT
 *     ONNXIFI_OPTIMIZATION_LOW_LATENCY
 *     ONNXIFI_OPTIMIZATION_LOW_POWER
 *     ONNXIFI_OPTIMIZATION_LOW_DELAY
 */
public static final int ONNXIFI_BACKEND_PROPERTY_OPTIMIZATION = 1;
/**
 * Logging verbosity level for the backend.
 *
 * If this property is not specified during initialization, the backend should
 * assume ONNXIFI_LOG_LEVEL_WARNING logging verbosity level.
 *
 * Possible values:
 *     ONNXIFI_LOG_LEVEL_ERROR
 *     ONNXIFI_LOG_LEVEL_WARNING
 *     ONNXIFI_LOG_LEVEL_INFO
 *     ONNXIFI_LOG_LEVEL_DEBUG
 */
public static final int ONNXIFI_BACKEND_PROPERTY_LOG_LEVEL = 2;
/**
 * CUDA stream to be used by the backend.
 * CUDA stream must be created on the CUDA device used by the ONNXIFI backend.
 * Users can query which CUDA device is used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_CUDA_INDEX info type.
 *
 * If this property is not specified during initialization, the backend can
 * create a new CUDA stream for the device, or use a default CUDA stream.
 *
 * Possible values: cudaStream_t or CUstream object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_STREAM = 4;
/**
 * OpenCL context to be used by the backend.
 * The context must be created with the OpenCL device ID and the OpenCL platform
 * ID used by the ONNXIFI backend. Users can query which OpenCL device ID and
 * OpenCL platform ID are used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 * and ONNXIFI_BACKEND_OPENCL_DEVICE_ID info types.
 *
 * If this property is not specified during initialization, the backend will
 * create a new OpenCL context for the device.
 *
 * Possible values: cl_context object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_OPENCL_CONTEXT = 8;

/**
 * Terminates the list of auxiliary graph initialization properties passed to
 * onnxInitGraph.
 */
public static final int ONNXIFI_GRAPH_PROPERTY_NONE = 0;

/**
 * Optimize graph representation and compilation for highest throughput.
 */
public static final int ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT = 0;

/**
 * Optimize graph representation and compilation for lowest latency.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_LATENCY = 1;

/**
 * Optimize graph representation and compilation for lowest power consumption.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_POWER = 2;

/**
 * Optimize graph representation and compilation for lowest delay until first
 * result.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_DELAY = 3;

/**
 * Log events which caused a failure in an ONNXIFI function call.
 */
public static final int ONNXIFI_LOG_LEVEL_ERROR = 4;

/**
 * Log events in ONNXIFI_LOG_LEVEL_ERROR and events which caused
 * a performance, accuracy, or quality of service degradation in a backend.
 * Enabling this logging level SHOULD NOT have a measurable effect on
 * performance.
 */
public static final int ONNXIFI_LOG_LEVEL_WARNING = 3;

/**
 * Log events in ONNXIFI_LOG_LEVEL_WARNING and high-level status information
 * about operation of a backend. Enabling this logging level MAY cause a small
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_INFO = 2;

/**
 * Log events in ONNXIFI_LOG_LEVEL_INFO and detailed status information about
 * operations of a backend. Enabling this logging level MAY cause a serious
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_DEBUG = 1;

/**
 * Tag for version 1 of tensor descriptor structure (onnxTensorDescriptorV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the tensor descriptor structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1 = 0x43DFBF69;

public static class onnxTensorDescriptorV1 extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public onnxTensorDescriptorV1() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public onnxTensorDescriptorV1(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxTensorDescriptorV1(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public onnxTensorDescriptorV1 position(long position) {
        return (onnxTensorDescriptorV1)super.position(position);
    }

  /**
   * 32-bit tag needed to distinguish different versions of a tensor descriptor
   * structure. In the onnxTensorDescriptorV1 structure, the tag MUST be set to
   * ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1. If ONNXIFI introduce a new version of the
   * tensor descriptor structure in the future, it WILL have 32-bit tag with a
   * different value as the first member of the structure.
   *
   * ONNXIFI implementations MUST validate tag before accessing any other
   * members of the structure.
   */
  public native @Cast("int32_t") int tag(); public native onnxTensorDescriptorV1 tag(int tag);
  /**
   * Name of the blob corresponding to this tensor in the ONNX model. The name
   * must exactly match the ValueInfoProto.name of one of the
   * ModelProto.graph.input or ModelProto.graph.output
   */
  @MemberGetter public native @Cast("const char*") BytePointer name();
  /**
   * Data type of the elements in the tensor.
   *
   * Possible values:
   *     ONNXIFI_DATATYPE_FLOAT16
   *     ONNXIFI_DATATYPE_FLOAT32
   *     ONNXIFI_DATATYPE_FLOAT64
   *     ONNXIFI_DATATYPE_INT8
   *     ONNXIFI_DATATYPE_INT16
   *     ONNXIFI_DATATYPE_INT32
   *     ONNXIFI_DATATYPE_INT64
   *     ONNXIFI_DATATYPE_UINT8
   *     ONNXIFI_DATATYPE_UINT16
   *     ONNXIFI_DATATYPE_UINT32
   *     ONNXIFI_DATATYPE_UINT64
   *     ONNXIFI_DATATYPE_COMPLEX64
   *     ONNXIFI_DATATYPE_COMPLEX128
   */
  public native @Cast("onnxEnum") int dataType(); public native onnxTensorDescriptorV1 dataType(int dataType);
  /**
   * Type of memory that stores the tensor.
   *
   * ONNXIFI_MEMORY_TYPE_CPU memory type is always supported by the backend, but
   * other memory types are optional. The use MUST call onnxGetBackendInfo with
   * ONNXIFI_BACKEND_MEMORY_TYPES to check if a particular memory type is
   * supported before using it.
   *
   * If the memory type is different than ONNXIFI_MEMORY_TYPE_CPU, it must be
   * allocated on the same device as the backend.
   *
   * Possible values:
   *     ONNXIFI_MEMORY_TYPE_CPU                 (always supported)
   *     ONNXIFI_MEMORY_TYPE_CUDA_BUFFER         (support is optional)
   *     ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER       (support is optional)
   *     ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D (support is optional)
   *     ONNXIFI_MEMORY_TYPE_D3D_RESOURCE        (support is optional)
   */
  public native @Cast("onnxEnum") int memoryType(); public native onnxTensorDescriptorV1 memoryType(int memoryType);
  /**
   * Number of dimensions in the tensor.
   * For a scalar, the number of dimensions is 0.
   */
  public native @Cast("uint32_t") int dimensions(); public native onnxTensorDescriptorV1 dimensions(int dimensions);
  /**
   * Dimensions of the tensor.
   * For a scalar, this pointer can be NULL.
   */
  @MemberGetter public native @Cast("const uint64_t*") IntPointer shape();
  /**
   * Pointers to tensor data.
   *
   * Interpretation depends on memoryType:
   *   - ONNXIFI_MEMORY_TYPE_CPU: buffer is a valid pointer to CPU memory.
   *   - ONNXIFI_MEMORY_TYPE_CUDA_BUFFER: buffer is a valid pointer to CUDA
   *     device memory, allocated via cudaMalloc or cuMalloc. CUDA device memory
   *     must be allocated on the same device as the backend.
   *   - ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER: buffer is a cl_mem handle for an
   *     OpenCL buffer or a sub-buffer. cl_mem object must be allocated on the
   *     same OpenCL context as the backend. The context must be specified via
   *     ONNXIFI_BACKEND_OPENCL_CONTEXT initialization property to
   *     onnxInitBackend.
   *   - ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D: buffer is a name of a 2D
   *     texture created by glGenTextures. The texture must be allocated on the
   *     same device as the backend.
   *   - ONNXIFI_MEMORY_TYPE_D3D_RESOURCE: TBD
   */
  public native @Cast("onnxPointer") int buffer(); public native onnxTensorDescriptorV1 buffer(int buffer);
}

/**
 * Synchronization using ONNXIFI event object (onnxEvent).
 */
public static final int ONNXIFI_SYNCHRONIZATION_EVENT = 0;
/**
 * Implicit synchronization of inputs and outputs access with the caller.
 * The details are backend-specific, and may involve extra parameters passed
 * during backend initialization.
 *
 * Examples:
 *  - CUDA-based backends could implicitly synchronize with the caller through
 *    the use of the same CUDA stream.
 *  - OpenCL-based backends could implicitly synchronize with the caller through
 *    the use of the same in-order OpenCL command queue.
 */
public static final int ONNXIFI_SYNCHRONIZATION_IMPLICIT = 2;

/**
 * Tag for version 1 of memory fence structure (onnxMemoryFenceV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the memory fence structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_MEMORY_FENCE_V1 = 0x23E08AAB;

public static class onnxMemoryFenceV1 extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public onnxMemoryFenceV1() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public onnxMemoryFenceV1(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public onnxMemoryFenceV1(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public onnxMemoryFenceV1 position(long position) {
        return (onnxMemoryFenceV1)super.position(position);
    }

  /**
   * 32-bit tag needed to distinguish different versions of a memory fence
   * structure. In the onnxMemoryFenceV1 structure, the tag MUST be set to
   * ONNXIFI_TAG_MEMORY_FENCE_V1. If ONNXIFI introduce a new version of the
   * memory fence structure in the future, it WILL have 32-bit tag with a
   * different value as the first member of the structure.
   *
   * ONNXIFI implementations MUST validate tag before accessing any other
   * members of the structure.
   */
  public native @Cast("int32_t") int tag(); public native onnxMemoryFenceV1 tag(int tag);
  /**
   * Type of memory synchronization primitive.
   *
   * Possible values:
   *      ONNXIFI_SYNCHRONIZATION_EVENT    (onnxEvent, always supported)
   *      ONNXIFI_SYNCHRONIZATION_IMPLICIT
   */
  public native @Cast("onnxEnum") int type(); public native onnxMemoryFenceV1 type(int type);
    /**
     * Handle for a single-shot ONNXIFI event used as a synchronization
     * primitive. Event for the input fence must be created by the caller to
     * onnxRunGraph. Event for the output fence is created by implementation of
     * onnxRunGraph, and stored into the output memory fence structure before
     * onnxRunGraph returns.
     */
    public native onnxEvent event(); public native onnxMemoryFenceV1 event(onnxEvent event);
}

/* Function pointer declarations for dynamic loading */
public static class onnxGetBackendIDsFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxGetBackendIDsFunction(Pointer p) { super(p); }
    protected onnxGetBackendIDsFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") LongPointer numBackends);
}
public static class onnxReleaseBackendIDFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxReleaseBackendIDFunction(Pointer p) { super(p); }
    protected onnxReleaseBackendIDFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackendID backendID);
}
public static class onnxGetBackendInfoFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxGetBackendInfoFunction(Pointer p) { super(p); }
    protected onnxGetBackendInfoFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") LongPointer infoValueSize);
}
public static class onnxGetBackendCompatibilityFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxGetBackendCompatibilityFunction(Pointer p) { super(p); }
    protected onnxGetBackendCompatibilityFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackendID backendID,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel);
}
public static class onnxInitBackendFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxInitBackendFunction(Pointer p) { super(p); }
    protected onnxInitBackendFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
}
public static class onnxReleaseBackendFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxReleaseBackendFunction(Pointer p) { super(p); }
    protected onnxReleaseBackendFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackend backend);
}
public static class onnxInitEventFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxInitEventFunction(Pointer p) { super(p); }
    protected onnxInitEventFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackend backend,
    @ByPtrPtr onnxEvent event);
}
public static class onnxSignalEventFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxSignalEventFunction(Pointer p) { super(p); }
    protected onnxSignalEventFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxEvent event);
}
public static class onnxGetEventStateFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxGetEventStateFunction(Pointer p) { super(p); }
    protected onnxGetEventStateFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxEvent event,
    @Cast("onnxEventState*") IntPointer state);
}
public static class onnxWaitEventFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxWaitEventFunction(Pointer p) { super(p); }
    protected onnxWaitEventFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxEvent event);
}
public static class onnxReleaseEventFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxReleaseEventFunction(Pointer p) { super(p); }
    protected onnxReleaseEventFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxEvent event);
}
public static class onnxInitGraphFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxInitGraphFunction(Pointer p) { super(p); }
    protected onnxInitGraphFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxBackend backend,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
}
public static class onnxSetGraphIOFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxSetGraphIOFunction(Pointer p) { super(p); }
    protected onnxSetGraphIOFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxGraph graph,
    @Cast("uint32_t") int inputsCount,
    @Const onnxTensorDescriptorV1 inputDescriptors,
    @Cast("uint32_t") int outputsCount,
    @Const onnxTensorDescriptorV1 outputDescriptors);
}
public static class onnxRunGraphFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxRunGraphFunction(Pointer p) { super(p); }
    protected onnxRunGraphFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxGraph graph,
    @Const onnxMemoryFenceV1 inputFence,
    onnxMemoryFenceV1 outputFence);
}
public static class onnxReleaseGraphFunction extends FunctionPointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public    onnxReleaseGraphFunction(Pointer p) { super(p); }
    protected onnxReleaseGraphFunction() { allocate(); }
    private native void allocate();
    public native @Cast("onnxStatus") int call(
    onnxGraph graph);
}

/**
 * Get stable IDs of available backends on the system.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software). Backends
 * implemented in the same software, but targeting different devices (e.g.
 * "MyNN" for CPU and "MyNN" for GPU) have different backend IDs.
 *
 * Note that some (hot-pluggable) backends can be connected and disconnected at
 * any time, and thus subsequent calls to this function may return different
 * number or set of backend IDs. The returned IDs, however, stay valid even if
 * the hardware device used by the backend disconnects from the system.
 *
 * To avoid resource leak, the backend ID MUST be released through a call to
 * onnxReleaseBackendID when it is no longer needed.
 *
 * @param backendIDs[out] - pointer to the memory location where the backend IDs
 *                          will be returned. If the pointer is NULL, it is
 *                          ignored, and the function returns only the number
 *                          of backend IDs through numBackendIDs pointer.
 * @param numBackendIDs[in,out] - pointer to a variable specifying number of
 *                                available backends. On function entry, the
 *                                variable MUST contain the capacity, in number
 *                                of backend IDs, of the memory buffer specified
 *                                by backendIDs. For successful completion, this
 *                                capacity must be at least as large as the
 *                                number of available backends. If the function
 *                                completes with either ONNXIFI_STATUS_SUCCESS
 *                                or ONNXIFI_STATUS_FALLBACK status codes, the
 *                                number of backend IDs written into backendIDs
 *                                buffer is stored in the variable specified by
 *                                this pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and backend IDs
 *                                are stored in the location specified by
 *                                backendIDs, and the number of the backends
 *                                is stored in the location specified by
 *                                numBackends.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 backend IDs were not stored in the
 *                                 location specified by backendIDs, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient to store
 *                                 all available backend IDs. The number of
 *                                 available backends is stored in the
 *                                 location specified by numBackends.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        numBackends is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         system failed to allocate memory
 *                                         to store backend ID information.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendIDs(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") LongPointer numBackends);
public static native @Cast("onnxStatus") int onnxGetBackendIDs(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") LongBuffer numBackends);
public static native @Cast("onnxStatus") int onnxGetBackendIDs(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") long[] numBackends);

/**
 * Deinitialize ONNXIFI backend IDs and release associated resources.
 *
 * The user MUST deinitialize all objects created with this backend ID
 * (onnxBackend, onnxGraph, onnxEvent) before calling this function to
 * deinitialize the backend ID.
 *
 * @param backendID - backend ID returned by onnxGetBackendIDs.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the resources
 *                                associated to the backend ID were released to
 *                                the operating system.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackendID(
    onnxBackendID backendID);

/**
 * Query high-level information about the backend and its target device.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * The content, data type, and availability of information provided by this
 * function depends on infoType value as specified below:
 *
 *         infoType value                           data type      support
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION               uint64_t     required
 *     ONNXIFI_BACKEND_NAME                           char[]      required
 *     ONNXIFI_BACKEND_VENDOR                         char[]      required
 *     ONNXIFI_BACKEND_VERSION                        char[]      required
 *     ONNXIFI_BACKEND_EXTENSIONS                     char[]      required
 *     ONNXIFI_BACKEND_DEVICE                         char[]      required
 *     ONNXIFI_BACKEND_DEVICE_TYPE                   onnxEnum     required
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION                char[]      required
 *     ONNXIFI_BACKEND_OPSET_VERSION                  char[]      required
 *     ONNXIFI_BACKEND_CAPABILITIES                onnxBitfield   required
 *     ONNXIFI_BACKEND_INIT_PROPERTIES             onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_TYPES                onnxBitfield   required
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES       onnxBitfield   required
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES       onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_SIZE                   uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE                uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT               uint64_t     required
 *     ONNXIFI_BACKEND_MACS_FP32                     uint64_t     optional
 *     ONNXIFI_BACKEND_MACS_FP16                     uint64_t     optional
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH              uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH     uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_BUS_ID                    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_DIRECTX_ID                      LUID       optional
 *     ONNXIFI_BACKEND_CUDA_INDEX                    uint64_t     optional
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID         cl_platform_id  optional
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID            cl_device_id   optional
 *
 * @param backendID - ID of the backend to query.
 * @param infoType - type of the backend information to query. Must be one of
 *                   the ONNXIFI_BACKEND_* constants. If this value is not
 *                   supported by the backend, the function will fail with
 *                   ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE.
 * @param infoValue[out] - pointer to the memory location where the backend
 *                         information value will be returned. If the pointer is
 *                         NULL, is it ignored.
 * @param infoValueSize[in,out] - pointer to a variable specifying size, in
 *                                bytes, of the information value. On function
 *                                entry, the variable MUST contain the size of
 *                                the memory buffer specified by infoValue.
 *                                For successful completion, this size must be
 *                                at least as large as the queried value. If the
 *                                function completes with either
 *                                ONNXIFI_STATUS_SUCCESS or
 *                                ONNXIFI_STATUS_FALLBACK status codes, the
 *                                actual size of the value queried in the call
 *                                is stored in the variable specified by this
 *                                pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and requested
 *                                value is stored in the location specified by
 *                                infoValue, and the actual size of the
 *                                requested value is stored in the location
 *                                specified by infoValueSize.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 requested value was not stored in the
 *                                 location specified by infoValue, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient for the
 *                                 value. The actual size of the requested value
 *                                 is stored in the location specified by
 *                                 infoValueSize.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        infoValueSize is NULL.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the value of infoType is not
 *                                              supported by the backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 */
public static native @Cast("onnxStatus") int onnxGetBackendInfo(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") LongPointer infoValueSize);
public static native @Cast("onnxStatus") int onnxGetBackendInfo(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") LongBuffer infoValueSize);
public static native @Cast("onnxStatus") int onnxGetBackendInfo(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") long[] infoValueSize);

/**
 * Query if an ONNX model graph is compatible with the backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, the validation of symbolic dimension should be
 * deferred until graph inputs and outputs are specified in onnxSetGraphIO.
 *
 * Commonly, the serialized ModelProto message passed to this function would
 * not include the static weights (ModelProto.graph.initializer is empty), and
 * the backend implementation MUST NOT rely on the weights to determine if the
 * graph is supported.
 *
 * An important use-case is a ModelProto containing only a single NodeProto in
 * ModelProto.graph.node, which happens when a high-level framework checks
 * operators one-by-one to find a connected subgraph that can be offloaded to
 * the backend. Backend implementations SHOULD optimize performance for this
 * use-case.
 *
 * @param backend - ID of the backend to query.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param [in] onnxModel - pointer to serialized ONNX ModelProto message
 *                        representing the model graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph can efficiently run on the backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph can run on the backend through some
 *                                 emulation layer with some efficiency loss. If
 *                                 a backend decomposes this operator into
 *                                 multiple sub-operators, it should return this
 *                                 code. E.g. if a backend does not natively
 *                                 support grouped or depthwise convolution, but
 *                                 can execute it as multiple unit-group
 *                                 convolution operators, it must returns this
 *                                 code.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel is NULL.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 *                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, or inferred
 *                                          from the inputs by the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, or
 *                                             inferred from the inputs by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             output or intermediate data types
 *                                             specified in the ONNX model graph
 *                                             do not match the data types
 *                                             inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse and analyze
 *                                         the model graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendCompatibility(
    onnxBackendID backendID,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel);

/**
 * Initialize an ONNXIFI backend.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNXIFI graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * @param backendID - ID of the backend to initialize.
 * @param [in] auxPropertiesList - optional list of backend initialization
 *                                properties, terminated by
 *                                ONNXIFI_BACKEND_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param [out] backend - pointer to an opaque handle for the initialized ONNXIFI
 *                       backend. If the function fails, the handle is
 *                       initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        backend pointer is NULL.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the backend initialization
 *                                         property values is invalid.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize backend.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @ByPtrPtr onnxBackend backend);

/**
 * Deinitialize an ONNXIFI backend and release associated resources.
 *
 * The user MUST deinitialize all objects created on this backend (onnxGraph,
 * onnxEvent) before calling this function to deinitialize the backend.
 *
 * @param backend - ONNXIFI backend handle created by onnxInitBackend.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackend(
    onnxBackend backend);

/**
 * Initialize a single-shot ONNXIFI event.
 *
 * The newly created event is in non-signalled state.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to initialize the event.
 * @param [out] event - pointer to the opaque handle for the created ONNXIFI
 *                     event. If the function fails, the handle is initialized
 *                     to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        event pointer is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize the event.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            event.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitEvent(
    onnxBackend backend,
    @ByPtrPtr onnxEvent event);

/**
 * Change the state of an ONNXIFI event to signalled.
 *
 * @param event - event handle created by onnxInitEvent. While it is technically
 *                possible to use this function for output memory fence event
 *                created by onnxRunGraph, users SHOULD NOT do that.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was changed to signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_STATE The function call failed because event
 *                                      is already in the signalled state.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxSignalEvent(
    onnxEvent event);

/**
 * Query ONNXIFI event state without blocking.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 * @param [out] state - pointer to the variable that will store the state of the
 *                     event. If the function fails, the variable is initialized
 *                     to ONNXIFI_EVENT_STATE_INVALID.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the state
 *                                variable was initialized to either
 *                                ONNXIFI_EVENT_STATE_SIGNALLED or
 *                                ONNXIFI_EVENT_STATE_NONSIGNALLED according
 *                                to the state of the event.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because state
 *                                        pointer is NULL.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntPointer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntBuffer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") int[] state);

/**
 * Wait until an ONNXIFI event transitions to signalled state.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the function
 *                                returned because event transitioned to
 *                                signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxWaitEvent(
    onnxEvent event);

/**
 * Deinitialize an ONNXIFI event and release associated resources.
 *
 * @param event - event handle created by either onnxInitEvent or onnxRunGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseEvent(
    onnxEvent event);

/**
 * Parse an ONNXIFI graph and convert it for a particular backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, their validation should be deferred until a later
 * call to onnxSetGraphIO.
 *
 * Values of all static weights of the graph must be specified either in
 * ModelProto.graph.initializer, or through the weightDescriptors parameters,
 * but not through any combination of the two methods. If the caller creates the
 * graph on the fly, it SHOULD pass weights through weightDescriptors as it
 * involves less overhead.
 *
 * Blobs and operators in this graph are independent of the blobs and operators
 * of other graphs on the same backend.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to setup and run the model graph.
 * @param [in] auxPropertiesList - optional list of graph initialization
 *                                properties, terminated by
 *                                ONNXIFI_GRAPH_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param [in] onnxModel - pointer to serialized ONNX ModelProto message
 *                        representing the model graph. The backend MUST not
 *                        assume that the serialized ModelProto message is
 *                        present at this address after the function returns.
 * @param weightsCount - number of weights specified in this function call
 *                       through tensor descriptors. Alternatively, the weights
 *                       can be specified in ModelProto.graph.initializer.
 *                       If weightsCount is non-zero, weightDescriptors must be
 *                       non-NULL.
 * @param [in] weightDescriptors - descriptors of static input tensors for the
 *                                graph. Elements of this array provide location
 *                                for blobs identified by ValueInfoProto.name
 *                                listed in ModelProto.graph.input of the ONNX
 *                                graph. If this parameter is non-NULL,
 *                                all static inputs must be specified through
 *                                the tensor descriptors, and the
 *                                ModelProto.graph.initilizer list must be
 *                                empty. The tensor descriptors
 *                                must use ONNXIFI_MEMORY_TYPE_CPU memory type,
 *                                and the backend must copy the values of the
 *                                tensors and all metadata, including shape,
 *                                into its own memory before the function
 *                                returns.
 * @param [out] graph - pointer to the opaque handle for the created ONNXIFI
 *                     graph. If the function fails, and this pointer is
 *                     non-NULL, the handle is initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph was successfully initialized on the
 *                                backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph was initialized for the backend through
 *                                 an emulation layer with substantial
 *                                 efficiency loss. If a backend decomposes an
 *                                 operator into multiple sub-operators, it
 *                                 MUST return this code. E.g. if a backend
 *                                 does not natively support grouped or
 *                                 depthwise convolution, but can execute it as
 *                                 multiple unit-group convolution operators, it
 *                                 should return this code.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the graph initialization property
 *                                         values is invalid.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel or graph pointer is NULL, or
 *                                        weightDescriptors pointer is NULL
 *                                        while weightsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions in
 *                                      weightDescriptors is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         weightDescriptors is unknown to the
 *                                         backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            weightDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in weightDescriptors
 *                                                is invalid (NULL pointer).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the graph initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or
 *                                          output of one of the operators.
 *                                          The problematic tensor shapes could
 *                                          be directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 &                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, through
 *                                          weightDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, through
 *                                             weightDescriptors argument,
 *                                             or inferred from the inputs by
 *                                             the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because one of the memory
 *                                                types in weightDescriptors is
 *                                                different from
 *                                                ONNXIFI_MEMORY_TYPE_CPU.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified in weight
 *                                          descriptors do not match the shapes
 *                                          specified in the ONNX model graph,
 *                                          or output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified in weight
 *                                             descriptors do not match the data
 *                                             types specified in ONNX model
 *                                             graph, or output or intermediate
 *                                             data types specified in the ONNX
 *                                             model graph do not match the data
 *                                             types inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the model graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);

/**
 * Set locations for inputs and outputs of an ONNXIFI graph.
 *
 * The caller MUST ensure that the memory buffers specified for input and output
 * tensors remain accessible until all in-flight graph executions which use
 * specified buffer locations complete AND
 * - Either a next call to onnxSetGraphIO specifies different buffer locations
 * - Or the graph is deinitialized via onnxReleaseGraph
 * The caller can invalidate other data in tensor descriptors, including shape,
 * once the function returns.
 *
 * Calls to onnxRunGraph WILL use input and output locations specified in the
 * preceeding onnxSetGraphIO on the same graph. Asynchronous graph executions
 * that were in-flight before onnxSetGraphIO call will continue to use buffer
 * locations that were current when these graph executions started. An ONNXIFI
 * implementation MAY block inside onnxSetGraphIO until all in-flight graph
 * executions that started before the call complete.
 *
 * If a call to onnxSetGraphIO fails, it invalidates input and output locations
 * for the graph, and a subsequent call to onnxRunGraph will fail with
 * ONNXIFI_STATUS_UNIDENTIFIED_NAME.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param inputsCount - number of elements in the inputDescriptors array.
 * @param [in] inputDescriptors - descriptors of input tensors for the graph.
 *                               Elements of this array must provide a location
 *                               for each ValueInfoProto.name listed in
 *                               ModelProto.graph.input of the ONNX graph.
 *                               If inputsCount is non-zero, inputDescriptors
 *                               pointer must be non-NULL.
 * @param outputsCount - number of elements in the outputDescriptors array.
 *                       Must be greater than zero.
 * @param [in] outputDescriptors - descriptors of output tensors for the graph.
 *                                outputDescriptors pointer must be non-NULL.
 *                                Elements of this array must provide a location
 *                                for each ValueInfoProto.name listed in
 *                                ModelProto.graph.output of the ONNX graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        outputDescriptors pointer is NULL or
 *                                        inputDescriptors pointer is NULL while
 *                                        inputsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_NAME The function call failed because one of
 *                                     the names in tensor descriptors doesn't
 *                                     match blob name in ModelProto.graph.input
 *                                     or ModelProto.graph.output, or the same
 *                                     name appears in more than one tensor
 *                                     descriptor.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         inputDescriptors or outputDescriptors
 *                                         is unknown to the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            inputDescriptors or
 *                                            outputDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in inputDescriptors
 *                                                or outputDescriptors is not
 *                                                valid for the specified
 *                                                memory type (e.g. NULL pointer
 *                                                for ONNXIFI_MEMORY_TYPE_CPU).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because one
 *                                        of the tags in inputDescriptors or
 *                                        outputDescriptors is unknown to the
 *                                        backend (tag does not match
 *                                        ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend. This error code can be
 *                                          returned when the backend supports
 *                                          variable-size inputs and outputs,
 *                                          and the problematic tensor shape was
 *                                          provided in the ValueInfoProto as a
 *                                          symbolic variable.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because the backend does not
 *                                                support one of the memory
 *                                                types in inputDescriptors or
 *                                                outputDescriptors.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because one
 *                                          of the ValueInfoProto.name value in
 *                                          ModelProto.graph.input or
 *                                          ModelProto.graph.output doesn't have
 *                                          a match in the inputDescriptors or
 *                                          outputDescriptors.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument are
 *                                          inconsistent with the shapes
 *                                          specified in the ONNX model graph.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified through
 *                                             inputDescriptors or
 *                                             outputDescriptors argument are
 *                                             inconsistent with the data types
 *                                             specified in the ONNX model
 *                                             graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            tensor locations.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxSetGraphIO(
    onnxGraph graph,
    @Cast("uint32_t") int inputsCount,
    @Const onnxTensorDescriptorV1 inputDescriptors,
    @Cast("uint32_t") int outputsCount,
    @Const onnxTensorDescriptorV1 outputDescriptors);

/**
 * Asynchronously execute operations in an ONNXIFI graph using pre-specified
 * locations for inputs and outputs.
 *
 * This function operates asynchronously: it doesn't require that the locations
 * for graph inputs graph inputs hold valid values before the function is
 * called, and doesn't guarantee that the locations for graph outputs hold
 * valid values when the function returns. Instead, two synchronization
 * primitives are used to signal to the backend when inputs are ready to use,
 * and to signal to the caller when outputs are ready to use. The only
 * synchronization primitive that is always available is onnxEvent
 * (ONNXIFI_SYNCHRONIZATION_EVENT memory fence type). If a backend supports
 * additional types of synchronization primitives, it must indicate them in
 * ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES information query.
 *
 * The caller must successfully specify locations of input and output tensors
 * for the graph through onnxSetGraphIO before calling this function.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param [in] inputFence - synchronization primitive that signals when graph
 *                         inputs are ready to use by the backend. The
 *                         synchronization primitive always must be initialized
 *                         by the caller.
 * @param [out] outputFence - synchronization primitive that signals when graph
 *                           outputs are ready to use by the caller. The type
 *                           of the synchronization primitive always must be
 *                           initialized by the caller. The type of the
 *                           synchronization primitive determines whether it
 *                           is initialized by the user before the call or by
 *                           the backend as a result of this call. Single-shot
 *                           synchronizatiom objects are initialized as a result
 *                           of the call. Reusable synchronization objects are
 *                           generally initialized by the user prior to the
 *                           call.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        inputFence or outputFence pointer is
 *                                        NULL.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_FENCE_TYPE The function call failed because
 *                                           the type of synchronization
 *                                           primitive specified in inputFence
 *                                           or outputFence is unknown to the
 *                                           backend.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because
 *                                      the memory synchronization primitive
 *                                      specified in inputFence or outputFence
 *                                      is not valid (e.g. NULL onnxEvent).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because a tag
 *                                        in inputFence or outputFence is
 *                                        unknown to the backend (tag does not
 *                                        match ONNXIFI_TAG_MEMORY_FENCE_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE The function call failed
 *                                               because the backend does not
 *                                               support the type of
 *                                               synchronization primitive
 *                                               specified in inputFence or
 *                                               outputFence.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because
 *                                          some of the ValueInfoProto.name
 *                                          value in ModelProto.graph.input or
 *                                          ModelProto.graph.output were not
 *                                          specified in a call to
 *                                          onnxSetGraphIO.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to execute the model
 *                                         graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            execute the model graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to execute the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to execute the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxRunGraph(
    onnxGraph graph,
    @Const onnxMemoryFenceV1 inputFence,
    onnxMemoryFenceV1 outputFence);

/**
 * Deinitialize an ONNXIFI graph and release associated resources.
 *
 * If there are in-flight asynchronous inference operations on this graph,
 * the function MUST block until all outstanding operations complete.
 *
 * @param graph - graph handle created by onnxInitGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the graph
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because graph
 *                                      is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       graph backend experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseGraph(
    onnxGraph graph);

// #ifdef __cplusplus /* extern "C" */
// #endif

// #endif /* !defined(ONNXIFI_H) */


}

// Targeted by JavaCPP version 1.5.7: DO NOT EDIT THIS FILE

package org.bytedeco.ngraph.global;

import org.bytedeco.ngraph.*;

import org.bytedeco.ngraph.Allocator;
import org.bytedeco.ngraph.Function;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class ngraph extends org.bytedeco.ngraph.presets.ngraph {
    static { Loader.load(); }

// Targeting ../StringBoolMap.java


// Targeting ../StringStringMap.java


// Targeting ../SizeTSet.java


// Targeting ../DescriptorInputVector.java


// Targeting ../NodeInputVector.java


// Targeting ../NodeOutputVector.java


// Targeting ../PtrDiffTVector.java


// Targeting ../StringVector.java


// Targeting ../SizeTVector.java


// Targeting ../ResultVector.java


// Targeting ../ParameterVector.java


// Targeting ../NodeVector.java


// Targeting ../OpConstantVector.java


// Targeting ../TensorVector.java


// Targeting ../FunctionVector.java


// Targeting ../StringVoidMap.java


// Parsed from ngraph/descriptor/tensor.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <string>

// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/partial_shape.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/type/element_type.hpp"
// Targeting ../TensorLayout.java


        
// Targeting ../DescriptorTensor.java



        @Namespace("ngraph::descriptor") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer arg0, @Const @ByRef DescriptorTensor arg1);
    



// Parsed from ngraph/pass/pass_config.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <map>
// #include <string>
    

// Targeting ../PassConfig.java




// Parsed from ngraph/type/element_type.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

//================================================================================================
// ElementType
//================================================================================================

// #pragma once

// #include <iostream>
// #include <limits>
// #include <memory>
// #include <string>
// #include <vector>

// #include "ngraph/deprecated.hpp"
// #include "ngraph/except.hpp"
// #include "ngraph/ngraph_visibility.hpp"
// #include "ngraph/type/bfloat16.hpp"
// #include "ngraph/type/float16.hpp"
        /** enum class ngraph::element::Type_t */
        public static final int
            undefined = 0,
            dynamic = 1,
            boolean_type = 2,
            bf16 = 3,
            f16 = 4,
            f32 = 5,
            f64 = 6,
            i8 = 7,
            i16 = 8,
            i32 = 9,
            i64 = 10,
            u8 = 11,
            u16 = 12,
            u32 = 13,
            u64 = 14;
// Targeting ../Type.java



        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type dynamic();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef @Name("boolean") Type _boolean();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type bf16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type f16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type f32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type f64();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i8();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type i64();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u8();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u16();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u32();
        @Namespace("ngraph::element") @MemberGetter public static native @Const @ByRef Type u64();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<char>") Type fromChar();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<bool>") Type fromBool();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<float>") Type fromFloat();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<double>") Type fromDouble();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<int8_t>") Type fromInt8t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<int16_t>") Type fromInt16t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<int32_t>") Type fromInt32t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<int64_t>") Type fromInt64t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<uint8_t>") Type fromUInt8t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<uint16_t>") Type fromUInt16t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<uint32_t>") Type fromUInt32t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<uint64_t>") Type fromUInt64t();

        @Namespace("ngraph::element") public static native @ByVal @Name("from<ngraph::bfloat16>") Type fromNGraphBFloat16();
        

        @Namespace("ngraph::element") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Type obj);
    



// Parsed from ngraph/axis_set.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <ostream>
// #include <set>
// #include <vector>
// Targeting ../AxisSet.java



    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef AxisSet axis_set);



// Parsed from ngraph/axis_vector.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <ostream>
// #include <vector>
// Targeting ../AxisVector.java



    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef AxisVector axis_vector);



// Parsed from ngraph/coordinate_diff.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <ostream>
// #include <vector>
// Targeting ../CoordinateDiff.java



    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef CoordinateDiff coordinate_diff);



// Parsed from ngraph/shape.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstdio>
// #include <vector>

// #include "ngraph/axis_set.hpp"
// #include "ngraph/strides.hpp"
// Targeting ../Shape.java



    /** Number of elements in spanned by a shape */

    /** Row-major strides for a shape */

    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Shape shape);



// Parsed from ngraph/assertion.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <exception>
// #include <sstream>
// #include <vector>

// #include "ngraph/except.hpp"

// ****************************
// Do not use this in new code!
// ****************************
//
// The replacement is in ngraph/check.hpp. The system in check.hpp is much more efficient, since
// the macros/classes here require construction of the error message string even if the assertion
// does not fail.
//
// * If you have code that is calling NGRAPH_ASSERT, please replace it with NGRAPH_CHECK.
// * If you are defining a custom exception macro, please convert it to use NGRAPH_CHECK_HELPER
//   from check.hpp. See check.hpp for details on the new interface. In particular, note that the
//   expected signatures for the exception class constructor is slightly different.
//
// TODO: remove this.
// Targeting ../AssertionFailure.java



    /**
     *  NOTE: This is a legacy class likely to be retired soon. Documentation here is preserved
     *  for posterity.
     * 
     *  Helper class for failed assertions. Callers should not instantiate this class directly.
     *  This class is meant to be wrapped with a macro like NGRAPH_ASSERT. This class provides
     *  two main facilities: (1) an ostream accessible via get_stream(), to which a detailed
     *  error explanation can be written; and (2) throws an exception of type T when the
     *  AssertionHelper is destructed.
     * 
     * 
     *  Typical usage is via a wrapper around the NGRAPH_ASSERT_STREAM macro:
     * 
     *     class MyException : public AssertionFailure;
     * 
     *     #define MY_ASSERT(cond) NGRAPH_ASSERT_STREAM(::ngraph::MyException, cond)
     * 
     *     ...
     * 
     *     MY_ASSERT(42 != 43) << "Uh-oh. " << 42 << " is not " << 43 << ".";
     * 
     *  If the assertion fails, it will throw a CompileError exception with a what() string of:
     * 
     *    Assertion '42 != 43' failed at foo.cpp:123:
     *    Uh-oh. 42 is not 43.
     * 
     * 
     *  AssertionHelper also provides support for tagging the exception with a "location" string,
     *  reflecting things like the op that was being processed when the error occurred. For
     *  example:
     * 
     *    class CompileError : public AssertionFailure;
     * 
     *    #define COMPILE_ASSERT(node,cond)                                       <backslash>
     *       NGRAPH_ASSERT_STREAM_WITH_LOC(::ngraph::CompileError, cond,          <backslash>
     *                                     "While compiling node " + node->name())
     * 
     *    ...
     * 
     *    COMPILE_ASSERT(node, node->get_users().size != 0) << "Node has no users";
     * 
     *  If the assertion fails, it will throw a CompileError exception with a what() string
     *  similar to:
     * 
     *    While compiling node Add_123:
     *    Assertion 'node->get_users().size != 0' failed at foo.cpp:123:
     *    Node has no users
     *  */
// Targeting ../DummyAssertionHelper.java




/** Asserts condition "cond" with an exception class of "T", at location "loc". */
// #define NGRAPH_ASSERT_STREAM_WITH_LOC_DO_NOT_USE_IN_NEW_CODE(T, cond, loc)
//     ((cond) ? ::ngraph::DummyAssertionHelper().get_stream()
//             : ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, #cond, loc).get_stream())
/** Asserts condition "cond" with an exception class of "T", and no location specified. */
// #define NGRAPH_ASSERT_STREAM_DO_NOT_USE_IN_NEW_CODE(T, cond)
//     ((cond) ? ::ngraph::DummyAssertionHelper().get_stream()
//             : ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, #cond).get_stream())
/** Fails unconditionally with an exception class of "T", at location "loc". */
// #define NGRAPH_FAIL_STREAM_WITH_LOC_DO_NOT_USE_IN_NEW_CODE(T, loc)
//     ::ngraph::AssertionHelper<T>(__FILE__, __LINE__, "", loc).get_stream()
/** Fails unconditionally with an exception class of "T", and no location specified. */
// #define NGRAPH_FAIL_STREAM_DO_NOT_USE_IN_NEW_CODE(T)
//     ::ngraph::AssertionHelper<T>(__FILE__, __LINE__).get_stream()

/** DEPRECATED. Use NGRAPH_CHECK instead. */
// #define NGRAPH_ASSERT(cond)
//     NGRAPH_ASSERT_STREAM_DO_NOT_USE_IN_NEW_CODE(::ngraph::AssertionFailure, cond)
/** DEPRECATED. Use NGRAPH_CHECK instead, with a condition of {@code false}. */
// #define NGRAPH_FAIL() NGRAPH_FAIL_STREAM_DO_NOT_USE_IN_NEW_CODE(::ngraph::AssertionFailure)


// Parsed from ngraph/except.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <sstream>
// #include <stdexcept>
// Targeting ../ngraph_error.java


// Targeting ../unsupported_op.java





// Parsed from ngraph/placement.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>

// #include "ngraph/log.hpp"
    /** enum class ngraph::Placement */
    public static final int
        DEFAULT = 0,
        INTERPRETER = 1,
        CPU = 2,
        GPU = 3,
        NNP = 4,
        PLAIDML = 5;

    @Namespace("ngraph") public static native @StdString BytePointer placement_to_string(@Cast("ngraph::Placement") int placement);



// Parsed from ngraph/coordinate.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <algorithm>
// #include <vector>

// #include "ngraph/axis_set.hpp"
// #include "ngraph/shape.hpp"
// Targeting ../Coordinate.java



    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Coordinate coordinate);



// Parsed from ngraph/strides.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <ostream>
// #include <vector>
// Targeting ../Strides.java



    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer s, @Const @ByRef Strides strides);



// Parsed from ngraph/descriptor/input.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/descriptor/tensor.hpp"
// Targeting ../Input.java


    



// Parsed from ngraph/descriptor/output.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>

// #include "ngraph/descriptor/input.hpp"
// #include "ngraph/descriptor/tensor.hpp"
    // The forward declaration of Node is needed here because Node has a deque of
    // Outputs, and Output is an incomplete type at this point. STL containers of
    // incomplete type have undefined behavior according to the C++11 standard, and
    // in practice including node.hpp here was causing compilation errors on some
    // systems (namely macOS).
// Targeting ../Output.java


    



// Parsed from ngraph/dimension.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <limits>
// #include <stddef.h>
// #include <stdexcept>
// Targeting ../Dimension.java



    /** \brief Insert a human-readable representation of a dimension into an output stream.
     *  @param str The output stream targeted for insertion.
     *  @param dimension The dimension to be inserted into {@code str}.
     *  @return A reference to {@code str} after insertion.
     * 
     *  Inserts the string {@code ?} if {@code dimension} is dynamic; else inserts {@code int64_t(dimension)}. */
    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer str, @Const @ByRef Dimension dimension);



// Parsed from ngraph/rank.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/dimension.hpp"
    /** \brief Alias for Dimension, used when the value represents the number of axes in a shape,
     *         rather than the size of one dimension in a shape.
     * 
     *  XXX: THIS TYPE IS EXPERIMENTAL AND THE ENTIRE DESIGN IS SUBJECT TO CHANGE. */



// Parsed from ngraph/partial_shape.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <stddef.h>

// #include "ngraph/dimension.hpp"
// #include "ngraph/op/util/attr_types.hpp"
// #include "ngraph/rank.hpp"
// #include "ngraph/shape.hpp"
// Targeting ../PartialShape.java



    /** \brief Elementwise addition of two PartialShape objects.
     *  @param s1 Left operand for addition.
     *  @param s2 Right operand for addition.
     *  @return The result of elementwise adding {@code s1} to {@code s2} (see description).
     *  @throws std::invalid_argument If {@code s1} and {@code s2} have inconsistent ranks.
     * 
     *  \li If {@code s1} or {@code s2} has dynamic rank, returns PartialShape::dynamic().
     *  \li If {@code s1 and }s2{@code  both have static rank, and their ranks are unequal, throws
     *      std::invalid_argument.
     *  \li If }s1{@code  and }s2{@code  both have static rank, and their ranks are equal,
     *      returns a new shape whose }i{@code th dimension is }s1[i] + s2[i]{@code . */
    
    ///
    ///
    ///
    ///
    ///
    ///
    @Namespace("ngraph") public static native @ByVal @Name("operator +") PartialShape add(@Const @ByRef PartialShape s1, @Const @ByRef PartialShape s2);

    /** \brief Inserts a human-readable representation of a PartialShape into an output stream.
     *  @param str The output stream targeted for insertion.
     *  @param shape The shape to be inserted into {@code str}.
     *  @return A reference to {@code str} after insertion.
     * 
     *  The output to the stream is in "informal" notation. In other words:
     * 
     *  \li If {@code shape} has dynamic rank, inserts the string {@code ?}.
     *  \li If {@code shape} has static rank, inserts the string {@code {}, then inserts each dimension
     *      of {@code shape} into the output stream separated by commas, then inserts {@code }}.
     * 
     *  Example:
     * 
     *  <pre>{@code {.cpp}
     *  PartialShape s1{PartialShape::dynamic())};
     *  PartialShape s2{};
     *  PartialShape s3{1,Dimension::dynamic(),2,3};
     *  PartialShape s4{2,3,4};
     *  std::cout << s1 << std::endl
     *            << s2 << std::endl
     *            << s3 << std::endl
     *            << s4 << std::endl;
     *  }</pre>
     * 
     *  Output:
     * 
     *  <pre>{@code
     *  ?
     *  {}
     *  {1,?,2,3}
     *  {2,3,4}
     *  }</pre> */
    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer str, @Const @ByRef PartialShape shape);



// Parsed from ngraph/check.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <exception>
// #include <sstream>
// #include <vector>

// #include "ngraph/except.hpp"
    @Namespace("ngraph") public static native @Cast("std::ostream*") @ByRef Pointer write_all_to_stream(@Cast("std::ostream*") @ByRef Pointer str);
// Targeting ../CheckLocInfo.java


// Targeting ../CheckFailure.java




//
// Helper macro for defining custom check macros, which throw custom exception classes and provide
// useful context information (the check condition, source filename, line number, and any domain-
// specific context information [e.g., a summary of the node that was being processed at the time
// of the check]).
//
// For example (actually implemented in node.cpp), let's say we want to define a macro for
// checking conditions during node validation, usable as follows:
//
//    NODE_VALIDATION_CHECK(node_being_checked,
//                          node_being_checked->get_input_shape(0).size() == 1,
//                          "Node must have an input rank of 1, but got ",
//                          node_being_checked->get_input_shape(0).size(), ".");
//
// In case of failure, this will throw an exception of type NodeValidationFailure with a what()
// string something like:
//
//      Check 'node_being_checked->get_input_shape(0).size() == 1' failed at foo.cpp:123:
//      While validating node 'Broadcast[Broadcast_10](Reshape_9: float{1,3,4,5}) -> (??)':
//      Node must have an input of rank 1, but got 2.
//
// To implement this, he first step is to define a subclass of CheckFailure (let's say it's called
// MyFailure), which must have a constructor of the form:
//
//      MyFailure(const CheckLocInfo& check_loc_info,
//                T context_info, // "T" can be any type; you'll supply a function to convert "T"
//                                // to std::string
//                const std::string& explanation)
//
// Here, we define a custom class for node validation failures as follows:
//
//    static std::string node_validation_failure_loc_string(const Node* node)
//    {
//        std::stringstream ss;
//        ss << "While validating node '" << *node << "'";
//        return ss.str();
//    }
//
//    class NodeValidationFailure : public CheckFailure
//    {
//    public:
//        NodeValidationFailure(const CheckLocInfo& check_loc_info,
//                              const Node* node,
//                              const std::string& explanation)
//            : CheckFailure(check_loc_info, node_validation_failure_loc_string(node), explanation)
//        {
//        }
//    };
//
// Then, we define the macro NODE_VALIDATION_CHECK as follows:
//
// #define NODE_VALIDATION_CHECK(node, cond, ...) <backslash>
//     NGRAPH_CHECK_HELPER(::ngraph::NodeValidationFailure, (node), (cond), ##__VA_ARGS__)
//
// The macro NODE_VALIDATION_CHECK can now be called on any condition, with a Node* pointer
// supplied to generate an informative error message via node_validation_failure_loc_string().
//
// Take care to fully qualify the exception class name in the macro body.
//
// The "..." may be filled with expressions of any type that has an "operator<<" overload for
// insertion into std::ostream.
//
// TODO(amprocte): refactor NGRAPH_CHECK_HELPER so we don't have to introduce a locally-scoped
// variable (ss___) and risk shadowing.
//
// #define NGRAPH_CHECK_HELPER(exc_class, ctx, check, ...)
//     do
//     {
//         if (!(check))
//         {
//             ::std::stringstream ss___;
//             ::ngraph::write_all_to_stream(ss___, ##__VA_ARGS__);
//             throw exc_class(
//                 (::ngraph::CheckLocInfo{__FILE__, __LINE__, #check}), (ctx), ss___.str());
//         }
//     } while (0)

/** \brief Macro to check whether a boolean condition holds.
 *  @param cond Condition to check
 *  @param ... Additional error message info to be added to the error message via the {@code <<}
 *             stream-insertion operator. Note that the expressions here will be evaluated lazily,
 *             i.e., only if the {@code cond} evalutes to {@code false}.
 *  @throws ::ngraph::CheckFailure if {@code cond} is false. */
// #define NGRAPH_CHECK(cond, ...)
//     NGRAPH_CHECK_HELPER(::ngraph::CheckFailure, "", (cond), ##__VA_ARGS__)

/** \brief Macro to signal a code path that is unreachable in a successful execution. It's
 *  implemented with NGRAPH_CHECK macro.
 *  @param ... Additional error message that should describe why that execution path is unreachable.
 *  @throws ::ngraph::CheckFailure if the macro is executed. */
// #define NGRAPH_UNREACHABLE(...) NGRAPH_CHECK(false, "Unreachable: ", ##__VA_ARGS__)


// Parsed from ngraph/node.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <atomic>
// #include <deque>
// #include <iostream>
// #include <memory>
// #include <set>
// #include <string>
// #include <tuple>
// #include <typeindex>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>

// #include "ngraph/autodiff/adjoints.hpp"
// #include "ngraph/check.hpp"
// #include "ngraph/coordinate.hpp"
// #include "ngraph/deprecated.hpp"
// #include "ngraph/descriptor/input.hpp"
// #include "ngraph/descriptor/output.hpp"
// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/op/util/attr_types.hpp"
// #include "ngraph/placement.hpp"
// #include "ngraph/strides.hpp"
     // namespace op
    

    @Namespace("ngraph") public static native @StdString BytePointer node_validation_failure_loc_string(@Const Node node);

    @Namespace("ngraph") public static native @Const @SharedPtr @ByRef Node check_single_output_arg(@Const @SharedPtr @ByRef Node node,
                                                             @Cast("size_t") long i);
    @Namespace("ngraph") public static native @Const @ByRef NodeVector check_single_output_args(@Const @ByRef NodeVector args);

    @Namespace("ngraph") public static native @ByVal @Cast("ngraph::OutputVector*") NodeOutputVector as_output_vector(@Const @ByRef NodeVector args);
    @Namespace("ngraph") public static native @ByVal NodeVector as_node_vector(@Cast("const ngraph::OutputVector*") @ByRef NodeOutputVector values);

    /** Alias useful for cloning */
// Targeting ../Node.java


// Targeting ../NodeInput.java


// Targeting ../NodeOutput.java



    

    

    

    

    

    

    

    

    

    

    

    

    

    
// Targeting ../NodeValidationFailure.java


// Targeting ../NodeDescription.java



// #define NODE_VALIDATION_CHECK(node, cond, ...)
//     NGRAPH_CHECK_HELPER(::ngraph::NodeValidationFailure, (node), (cond), ##__VA_ARGS__)

 // namespace ngraph


// Parsed from ngraph/op/op.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <string>

// #include "ngraph/autodiff/adjoints.hpp"
// #include "ngraph/node.hpp"
// #include "ngraph/op/util/op_annotations.hpp"
// Targeting ../Op.java


    



// Parsed from ngraph/op/parameter.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/op/op.hpp"
// Targeting ../Parameter.java


    



// Parsed from ngraph/op/result.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/op/op.hpp"
// Targeting ../Result.java


    



// Parsed from ngraph/op/constant.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstring>
// #include <sstream>

// #include "ngraph/coordinate_diff.hpp"
// #include "ngraph/node.hpp"
// #include "ngraph/runtime/aligned_buffer.hpp"
// #include "ngraph/type/element_type.hpp"
// #include "ngraph/util.hpp"
// Targeting ../Constant.java


// Targeting ../ScalarConstantLikeBase.java


// Targeting ../ScalarConstantLike.java


    



// Parsed from ngraph/op/util/attr_types.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
        /** \brief Modes for the {@code Pad} operator. */
        /** enum class ngraph::op::PadMode */
        public static final int
            CONSTANT = 0,
            EDGE = 1,
            REFLECT = 2,
            SYMMETRIC = 3;

        /** \brief Padding Type used for {@code Convolution} and {@code Pooling}
         * 
         *  Follows ONNX padding type definitions
         *  EXPLICIT   - Pad dimensions are explicity specified
         *  SAME_LOWER - Pad dimensions computed to match input shape
         *               Ceil(num_dims/2) at the beginning and
         *               Floor(num_dims/2) at the end
         *  SAME_UPPER - Pad dimensions computed to match input shape
         *               Floor(num_dims/2) at the beginning and
         *               Ceil(num_dims/2) at the end
         *  VALID      - No padding
         *  */
        /** enum class ngraph::op::PadType */
        public static final int
            EXPLICIT = 0,
            SAME_LOWER = 1,
            SAME_UPPER = 2,
            VALID = 3,
            AUTO = SAME_UPPER,
            NOTSET = EXPLICIT;

        /** \brief Specifies the algorithm to use for implicit broadcasting of a tensor
         *         to align with another tensor
         * 
         *  NONE  - No implicit broadcasting of tensor
         *  NUMPY - Numpy-style implicit broadcasting
         *          (https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
         *          Right-align dimensions of the two tensors, with missing dimensions
         *          treated as size 1 dimensions. After alignment, for each dimension,
         *          their sizes should either match or one of them should be of size 1.
         *          Size 1 dimension will be implicitly broadcast to match the other
         *          size.
         * 
         *          E.g.,
         *               A: Shape(2, 1, 6)
         *               B: Shape(   3, 1)
         *          Result: Shape(2, 3, 6)
         * 
         *               A: Shape(2, 1, 6)
         *               B: Shape(   3, 1)
         *          Result: Shape(2, 3, 6)
         * 
         *  TODO: Add more implicit broadcast modes used by frameworks */
        /** enum class ngraph::op::AutoBroadcastType */
        public static final int
            NONE = 0,
            NUMPY = 1;

        /** \brief Specifies how eps is combined with L2 value */
        /** enum class ngraph::op::EpsMode */
        public static final int
            // Add bias to norm
            ADD = 0,
            // Calculate max of norm and bias
            MAX = 1;
// Targeting ../AutoBroadcastSpec.java


    



// Parsed from ngraph/op/util/binary_elementwise_arithmetic.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/op/op.hpp"
// #include "ngraph/op/util/attr_types.hpp"
// Targeting ../BinaryElementwiseArithmetic.java


        
    



// Parsed from ngraph/op/add.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/op/util/binary_elementwise_arithmetic.hpp"
// Targeting ../Add.java


    

    @Namespace("ngraph") public static native @SharedPtr @ByVal @Name("operator +") Node add(@Const @ByRef NodeOutput arg0, @Const @ByRef NodeOutput arg1);



// Parsed from ngraph/op/multiply.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include "ngraph/op/util/binary_elementwise_arithmetic.hpp"
// Targeting ../Multiply.java


    

    @Namespace("ngraph") public static native @SharedPtr @ByVal @Name("operator *") Node multiply(@Const @ByRef NodeOutput arg0, @Const @ByRef NodeOutput arg1);



// Parsed from ngraph/op/util/op_annotations.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <vector>

// #include "ngraph/except.hpp"
// Targeting ../oi_pair.java


// Targeting ../OpAnnotations.java


        
    



// Parsed from ngraph/function.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <atomic>
// #include <initializer_list>
// #include <list>
// #include <memory>
// #include <string>
// #include <vector>

// #include "ngraph/node.hpp"
// #include "ngraph/op/parameter.hpp"
// #include "ngraph/op/result.hpp"
// Targeting ../Function.java





// Parsed from ngraph/autodiff/adjoints.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <map>
// #include <memory>
// #include <unordered_map>

// #include "ngraph/coordinate.hpp"
// #include "ngraph/strides.hpp"
    // Need duplicate definition here to avoid g++ issues
    // Keep consistent with version in node.hpp
// Targeting ../Adjoints.java


    



// Parsed from ngraph/runtime/allocator.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <cstdint>
// #include <cstdlib>
// #include "ngraph/except.hpp"
// #include "ngraph/util.hpp"
// Targeting ../DefaultAllocator.java


        /** \brief Create a default allocator that calls into system
         *         allocation libraries */
        @Namespace("ngraph::runtime") public static native Allocator get_default_allocator();
    

// Targeting ../Allocator.java




// Parsed from ngraph/runtime/executable.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>

// #include "ngraph/function.hpp"
// #include "ngraph/runtime/performance_counter.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/type/element_type.hpp"
    

// Targeting ../Executable.java




// Parsed from ngraph/runtime/tensor.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <vector>

// #include "ngraph/descriptor/layout/tensor_layout.hpp"
// #include "ngraph/descriptor/tensor.hpp"
// #include "ngraph/runtime/backend.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/strides.hpp"
// #include "ngraph/type/element_type.hpp"
// Targeting ../Value.java


    
// Targeting ../Tensor.java


    



// Parsed from ngraph/runtime/backend.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>
// #include <mutex>

// #include "ngraph/function.hpp"
// #include "ngraph/pass/pass_config.hpp"
// #include "ngraph/runtime/allocator.hpp"
// #include "ngraph/runtime/executable.hpp"
// #include "ngraph/runtime/performance_counter.hpp"
// #include "ngraph/shape.hpp"
// #include "ngraph/type/element_type.hpp"
// #include "ngraph/util.hpp"
    

// Targeting ../Backend.java




// Parsed from ngraph/runtime/backend_manager.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <functional>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #ifdef _WIN32
// #include <windows.h>
// #define DL_HANDLE HMODULE
// #else
// #define DL_HANDLE void*
// #endif
    

// Targeting ../BackendConstructor.java


// Targeting ../BackendManager.java




// Parsed from ngraph/runtime/cpu/cpu_backend.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <map>
// #include <memory>
// #include <mutex>

// #include "cpu_backend_visibility.h"
// #include "ngraph/pass/pass_config.hpp"
// #include "ngraph/runtime/allocator.hpp"
// #include "ngraph/runtime/backend.hpp"
// #include "ngraph/runtime/backend_manager.hpp"
// Targeting ../CPU_ExternalFunction.java


// Targeting ../CPU_CallFrame.java


            @Namespace("ngraph::runtime::cpu") public static native BackendConstructor get_backend_constructor_pointer();
// Targeting ../CPU_Backend.java


// Targeting ../CPU_Executable.java


        
    



// Parsed from ngraph/runtime/performance_counter.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef>
// #include <string>

// #include "ngraph/node.hpp"
// Targeting ../PerformanceCounter.java


    



// Parsed from ngraph/frontend/onnxifi/backend.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <memory>  // std::shared_ptr
// #include <string>  // std::string
// #include <utility> // std::move
// #include <vector>  // std::vector

// #include "ngraph/function.hpp"
// #include "ngraph/runtime/backend.hpp"
// #include "ngraph/runtime/tensor.hpp"
// Targeting ../ONNXIFIBackend.java



     // namespace onnxifi

 // namespace ngraph


// Parsed from ngraph/frontend/onnxifi/backend_manager.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstddef> // std::size_t, std::uintptr_t
// #include <map>     // std::map
// #include <mutex>   // std::mutex
// #include "onnxifi.h"

// #include "backend.hpp"
// #include "ngraph/runtime/backend.hpp"
        /** \brief ONNXIFI backend manager */

     // namespace onnxifi

 // namespace ngraph


// Parsed from ngraph/frontend/onnxifi/onnxifi.h

// #ifndef ONNXIFI_H
public static final int ONNXIFI_H = 1;

// #ifdef __cplusplus
// #endif

// #if defined(_WIN32) && defined(_M_IX86)
/* Windows x86 */
// #define ONNXIFI_ABI __stdcall
// #elif defined(__i386__)
/* Linux x86 */
// #define ONNXIFI_ABI __attribute__((__cdecl__))
// #else
// #define ONNXIFI_ABI
// #endif

// #ifndef ONNXIFI_PUBLIC
// #if defined(__ELF__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(__MACH__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(_WIN32) && defined(__GNUC__)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __attribute__((__dllexport__))
// #else
// #define ONNXIFI_PUBLIC __attribute__((__dllimport__))
// #endif
// #elif defined(_WIN32)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __declspec(dllexport)
// #else
// #define ONNXIFI_PUBLIC __declspec(dllimport)
// #endif
// #else
// #define ONNXIFI_PUBLIC
// #endif
// #endif

// #ifndef ONNXIFI_CHECK_RESULT
//   #if defined(__GNUC__) && (__GNUC__ >= 4)
//     #define ONNXIFI_CHECK_RESULT __attribute__((__warn_unused_result__))
//   #elif defined(_MSC_VER) && (_MSC_VER >= 1700)
//     #define ONNXIFI_CHECK_RESULT _Check_return_
//   #else
//     #define ONNXIFI_CHECK_RESULT
//   #endif
// #endif

// #include <stddef.h>

// #if !defined(ONNXIFI_NO_STDINT_H)
// #if defined(_MSC_VER) && (_MSC_VER < 1600)
// #else
// #include <stdint.h>
// Targeting ../onnxBackendID.java


// Targeting ../onnxBackend.java


// Targeting ../onnxGraph.java


// Targeting ../onnxEvent.java



/** Return code for ONNXIFI functions */
/**
 * Type for enumeration values.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for bit fields.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for pointers or handles for memory buffers.
 * This type is intended to work not only for CPU-addressable memory, but also
 * for device memory. uint64_t ensures the API can accomodate Vulkan buffers.
 */

public static final int ONNXIFI_STATUS_SUCCESS = 0x0000;
public static final int ONNXIFI_STATUS_FALLBACK = 0x0001;
public static final int ONNXIFI_STATUS_INVALID_ID = 0x0101;
public static final int ONNXIFI_STATUS_INVALID_SIZE = 0x0102;
public static final int ONNXIFI_STATUS_INVALID_POINTER = 0x0103;
public static final int ONNXIFI_STATUS_INVALID_PROTOBUF = 0x0104;
public static final int ONNXIFI_STATUS_INVALID_MODEL = 0x0105;
public static final int ONNXIFI_STATUS_INVALID_BACKEND = 0x0106;
public static final int ONNXIFI_STATUS_INVALID_GRAPH = 0x0107;
public static final int ONNXIFI_STATUS_INVALID_EVENT = 0x0108;
public static final int ONNXIFI_STATUS_INVALID_STATE = 0x0109;
public static final int ONNXIFI_STATUS_INVALID_NAME = 0x010A;
public static final int ONNXIFI_STATUS_INVALID_SHAPE = 0x010B;
public static final int ONNXIFI_STATUS_INVALID_DATATYPE = 0x010C;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_TYPE = 0x010D;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_LOCATION = 0x010E;
public static final int ONNXIFI_STATUS_INVALID_FENCE_TYPE = 0x010F;
public static final int ONNXIFI_STATUS_INVALID_PROPERTY = 0x0110;
public static final int ONNXIFI_STATUS_UNSUPPORTED_TAG = 0x0201;
public static final int ONNXIFI_STATUS_UNSUPPORTED_VERSION = 0x0202;
public static final int ONNXIFI_STATUS_UNSUPPORTED_OPERATOR = 0x0203;
public static final int ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE = 0x0204;
public static final int ONNXIFI_STATUS_UNSUPPORTED_SHAPE = 0x0205;
public static final int ONNXIFI_STATUS_UNSUPPORTED_DATATYPE = 0x0206;
public static final int ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE = 0x0207;
public static final int ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE = 0x0208;
public static final int ONNXIFI_STATUS_UNSUPPORTED_PROPERTY = 0x0209;
public static final int ONNXIFI_STATUS_UNIDENTIFIED_NAME = 0x0301;
public static final int ONNXIFI_STATUS_MISMATCHING_SHAPE = 0x0302;
public static final int ONNXIFI_STATUS_MISMATCHING_DATATYPE = 0x0303;
public static final int ONNXIFI_STATUS_NO_SYSTEM_MEMORY = 0x0401;
public static final int ONNXIFI_STATUS_NO_DEVICE_MEMORY = 0x0402;
public static final int ONNXIFI_STATUS_NO_SYSTEM_RESOURCES = 0x0403;
public static final int ONNXIFI_STATUS_NO_DEVICE_RESOURCES = 0x0404;
public static final int ONNXIFI_STATUS_BACKEND_UNAVAILABLE = 0x0405;
public static final int ONNXIFI_STATUS_INTERNAL_ERROR = 0x0406;

/**
 * State of an ONNXIFI event object.
 *
 * Possible values:
 *     ONNXIFI_EVENT_STATE_INVALID
 *     ONNXIFI_EVENT_STATE_NONSIGNALLED
 *     ONNXIFI_EVENT_STATE_SIGNALLED
 */

/**
 * State for an invalid onnxEvent.
 */
public static final int ONNXIFI_EVENT_STATE_INVALID = 0;
/**
 * Non-signalled onnxEvent state.
 * onnxInitEvent creates events in non-signalled state.
 */
public static final int ONNXIFI_EVENT_STATE_NONSIGNALLED = 0x16BD;
/**
 * Signalled onnxEvent state.
 * onnxSignalEvent changes event state to signalled.
 */
public static final int ONNXIFI_EVENT_STATE_SIGNALLED = 0x3395;

/** Special-purpose accelerator for neural network */
public static final int ONNXIFI_DEVICE_TYPE_NPU = 0x01;
/** Digital signal processor */
public static final int ONNXIFI_DEVICE_TYPE_DSP = 0x02;
/** Graphics accelerator */
public static final int ONNXIFI_DEVICE_TYPE_GPU = 0x04;
/** General-purpose central processor */
public static final int ONNXIFI_DEVICE_TYPE_CPU = 0x08;
/** Field-programmable gate array */
public static final int ONNXIFI_DEVICE_TYPE_FPGA = 0x10;
/**
 * Heterogeneous backend whichinternally arbitrates or distributes work between
 * multiple device types.
 */
public static final int ONNXIFI_DEVICE_TYPE_HETEROGENEOUS = 0x20;

/**
 * The backend supports multi-threaded access to ONNXIFI backend, graph, and
 * event objects. E.g. onnxInitGraph can be called on a different thread than
 * onnxInitBackend.
 *
 * If this capability it not indicated, ONNXIFI backend, graph, and event
 * objects that relate to the backend must always be used on the same thread
 * where the backend object was initialized.
 */
public static final int ONNXIFI_CAPABILITY_THREAD_SAFE = 0x01;
/**
 * The backend supports ONNX graphs with symbolic variables in the outer
 * shape dimension (batch size), using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * The exact numerical value of the  of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE = 0x02;
/**
 * The backend supports ONNX graphs with symbolic variables in the all
 * shape dimensions, using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE capability.
 *
 * The exact numerical shape of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS = 0x04;
/**
 * The backend supports ONNX graphs with data-dependent outer shape dimension
 * (batch size) of graph outputs. The ONNX graph would specify unknown outer
 * shape dimension (batch size) using symbolic variables, so this capability
 * requires ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE support.
 *
 * For outputs with data-dependent outer shape dimension (batch size) the value
 * specified in onnxSetGraphIO call is interpreted as the upper limit. The exact
 * numerical batch size of the output can be retrieved by attaching a Shape
 * operator to the tensor with data-dependent shape and reading its output
 * through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE = 0x08;
/**
 * The backend supports ONNX graphs with data-dependent output shapes.
 * The ONNX graph would specify unknown output shapes using symbolic variables,
 * so this capability requires ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS support.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE capability.
 *
 * For outputs with data-dependent shapes the shape specified in onnxSetGraphIO
 * call is interpreted as the upper limit. The exact numerical shape of the
 * output can be retrieved by attaching a Shape operator to the tensor with
 * data-dependent shape and reading its output through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS = 0x10;
/**
 * The backend uses a hot-pluggable device, and can be disconnected at any time.
 *
 * If the underlying device disconnects from the system, subsequent operations
 * with the backend, or objects created on the backend, will fail with
 * ONNXIFI_STATUS_BACKEND_UNAVAILABLE status code.
 */
public static final int ONNXIFI_CAPABILITY_HOT_PLUGGABLE = 0x20;

/**
 * Type of the backend information.
 *
 * Possible values:
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION
 *     ONNXIFI_BACKEND_NAME
 *     ONNXIFI_BACKEND_VENDOR
 *     ONNXIFI_BACKEND_VERSION
 *     ONNXIFI_BACKEND_EXTENSIONS
 *     ONNXIFI_BACKEND_DEVICE
 *     ONNXIFI_BACKEND_DEVICE_TYPE
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION
 *     ONNXIFI_BACKEND_OPSET_VERSION
 *     ONNXIFI_BACKEND_CAPABILITIES
 *     ONNXIFI_BACKEND_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_MEMORY_TYPES
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES
 *     ONNXIFI_BACKEND_MEMORY_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT
 *     ONNXIFI_BACKEND_MACS_FP32
 *     ONNXIFI_BACKEND_MACS_FP16
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH
 *     ONNXIFI_BACKEND_PCI_BUS_ID
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID
 *     ONNXIFI_BACKEND_DIRECTX_ID
 *     ONNXIFI_BACKEND_CUDA_INDEX
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID
 */

/**
 * Major and minor version of ONNXIFI specification implemented by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 *      The high 32 bits specify the major version.
 *      The low 32 bits specify the minor version.
 *
 * Possible values:
 *      UINT64_C(0x0000000100000000) for ONNXIFI 1.0
 */
public static final int ONNXIFI_BACKEND_ONNXIFI_VERSION = 0;

/**
 * Marketing name of the backend (excluding the vendor name).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Caffe2"
 *    "Glow"
 */
public static final int ONNXIFI_BACKEND_NAME = 1;

/**
 * Name of the backend vendor.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Facebook"
 *    "Marat Dukhan"
 */
public static final int ONNXIFI_BACKEND_VENDOR = 2;

/**
 * Version of the backend software. Exact format is vendor-specific, but MUST be
 * unique for the software release.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "1.2.3"
 *    "1.2.3.0"
 *    "1.2.3-db3a4439d233276e25681fb4735b7f8e674dda65"
 */
public static final int ONNXIFI_BACKEND_VERSION = 3;

/**
 * Space-separated list of vendor- or device-specific extensions supported on
 * this backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    ""
 *    "onnx_clone_graph"
 *    "onnx_clone_graph fb_maskrcnn"
 */
public static final int ONNXIFI_BACKEND_EXTENSIONS = 4;

/**
 * Descriptive name of the device (i.e. CPU, GPU, DSP, or NPU model).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "nnDuino 123"
 */
public static final int ONNXIFI_BACKEND_DEVICE = 5;

/**
 * Type of the device.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxEnum.
 * Possible values:
 *      ONNXIFI_DEVICE_TYPE_NPU
 *      ONNXIFI_DEVICE_TYPE_DSP
 *      ONNXIFI_DEVICE_TYPE_GPU
 *      ONNXIFI_DEVICE_TYPE_CPU
 *      ONNXIFI_DEVICE_TYPE_FPGA
 *      ONNXIFI_DEVICE_TYPE_HETEROGENEOUS
 */
public static final int ONNXIFI_BACKEND_DEVICE_TYPE = 6;

/**
 * List of supported ONNX IR versions.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "3" (IR version in ONNX 1.0)
 *
 * Possible values: space-separated list of supported ONNX IR versions,
 *     represented as decimal integers. ONNX IR versions must match values
 *     in ONNX Version enum.
 */
public static final int ONNXIFI_BACKEND_ONNX_IR_VERSION = 7;

/**
 * List of supported operator set domains and maximum supported operator set
 * version for each domain.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "ai.onnx:1" (only operators in version 1 of default ONNX operator set)
 *    "ai.onnx:7" (operators up to version 7 of default ONNX operator set)
 *    "org.pytorch:2 ai.onnx:6 ai.facebook:1"
 *
 * Possible values: space-separated list of domain:max_version pairs where
 *     domain corresponds to OperatorSetIdProto.domain and max_version
 *     corresponds to the maximum value of OperatorSetIdProto.version supported
 *     by the backend for this domain. The backend MUST support all previous
 *     operator set versions as well.
 */
public static final int ONNXIFI_BACKEND_OPSET_VERSION = 8;

/**
 * Optional features supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of the following flags:
 *      ONNXIFI_CAPABILITY_THREAD_SAFE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS
 *      ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS
 *      ONNXIFI_CAPABILITY_HOT_PLUGGABLE
 *      or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_CAPABILITIES = 10;

/**
 * Auxiliary initialization properties supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_INIT_PROPERTIES = 11;

/**
 * Memory types supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values are any combination of the following flags:
 *     ONNXIFI_MEMORY_TYPE_CPU (always supported)
 *     ONNXIFI_MEMORY_TYPE_CUDA_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D
 *     ONNXIFI_MEMORY_TYPE_D3D_RESOURCE
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_MEMORY_TYPES = 12;

/**
 * Auxiliary initialization properties supported by graphs on the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES = 13;

/**
 * Memory synchronization primitives supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Possible values are any combination of the following flags:
 *     ONNXIFI_SYNCHRONIZATION_EVENT    (onnxEvent, always supported)
 *     ONNXIFI_SYNCHRONIZATION_IMPLICIT
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES = 14;

/**
 * Maximum amount of memory, in bytes, available to the use by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_SIZE = 20;

/**
 * Maximum size of network parameters, in bytes.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_SIZE = 21;

/**
 * Maximum number of independent network graphs supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_COUNT = 22;

/**
 * Number of FP32 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP32 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP32 = 30;

/**
 * Number of FP16 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP16 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP16 = 31;

/**
 * Bandwidth, in bytes per second, of the global memory specific to the backend
 * device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_BANDWIDTH = 35;

/**
 * Bandwidth, in bytes per second, of transferring data from cacheable
 * CPU-allocated memory to the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH = 36;

/**
 * Bandwidth, in bytes per second, of transferring data to cacheable
 * CPU-allocated memory from the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH = 37;

/**
 * PCI bus ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_BUS_ID = 40;

/**
 * PCI device ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DEVICE_ID = 41;

/**
 * PCI domain/function ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DOMAIN_ID = 42;

/**
 * DirectX ID of the backend device.
 *
 * This is the value that would be returned by ID3D12Device::GetAdapterLuid()
 * for the hardware device used by the backend.
 *
 * Since ONNXIFI 1.0, DXGI-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: LUID (8 bytes).
 */
public static final int ONNXIFI_BACKEND_DIRECTX_ID = 43;

/**
 * CUDA index of the backend device.
 *
 * Since ONNXIFI 1.0, CUDA-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_INDEX = 44;

/**
 * OpenCL platform ID for the backend device.
 * This platform ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_platform_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_PLATFORM_ID = 45;

/**
 * OpenCL device ID for the backend device.
 * This device ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_device_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_DEVICE_ID = 46;

/* Note: the data type values match ONNX TensorProto.DataType enum */
public static final int ONNXIFI_DATATYPE_UNDEFINED = 0;
public static final int ONNXIFI_DATATYPE_FLOAT16 = 10;
public static final int ONNXIFI_DATATYPE_FLOAT32 = 1;
public static final int ONNXIFI_DATATYPE_FLOAT64 = 11;
public static final int ONNXIFI_DATATYPE_INT8 = 3;
public static final int ONNXIFI_DATATYPE_INT16 = 5;
public static final int ONNXIFI_DATATYPE_INT32 = 6;
public static final int ONNXIFI_DATATYPE_INT64 = 7;
public static final int ONNXIFI_DATATYPE_UINT8 = 2;
public static final int ONNXIFI_DATATYPE_UINT16 = 4;
public static final int ONNXIFI_DATATYPE_UINT32 = 12;
public static final int ONNXIFI_DATATYPE_UINT64 = 13;
public static final int ONNXIFI_DATATYPE_COMPLEX64 = 14;
public static final int ONNXIFI_DATATYPE_COMPLEX128 = 15;
public static final int ONNXIFI_DATATYPE_BFLOAT16 = 16;

/** Cacheable CPU memory */
public static final int ONNXIFI_MEMORY_TYPE_CPU = 0;
/** CUDA memory buffer (allocated via cudaMalloc/cuMalloc).  */
public static final int ONNXIFI_MEMORY_TYPE_CUDA_BUFFER = 1;
/** OpenCL cl_mem object for a buffer or sub-buffer. */
public static final int ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER = 2;
/** OpenGL ES 2.0+ 2D Texture. */
public static final int ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D = 4;
/** Direct3D resource. */
public static final int ONNXIFI_MEMORY_TYPE_D3D_RESOURCE = 8;

/**
 * Terminates the list of auxiliary backend initialization properties passed to
 * onnxInitBackend.
 */
public static final int ONNXIFI_BACKEND_PROPERTY_NONE = 0;
/**
 * Optimization target for graphs initialized on the backend.
 *
 * Possible values:
 *     ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT
 *     ONNXIFI_OPTIMIZATION_LOW_LATENCY
 *     ONNXIFI_OPTIMIZATION_LOW_POWER
 *     ONNXIFI_OPTIMIZATION_LOW_DELAY
 */
public static final int ONNXIFI_BACKEND_PROPERTY_OPTIMIZATION = 1;
/**
 * Logging verbosity level for the backend.
 *
 * If this property is not specified during initialization, the backend should
 * assume ONNXIFI_LOG_LEVEL_WARNING logging verbosity level.
 *
 * Possible values:
 *     ONNXIFI_LOG_LEVEL_ERROR
 *     ONNXIFI_LOG_LEVEL_WARNING
 *     ONNXIFI_LOG_LEVEL_INFO
 *     ONNXIFI_LOG_LEVEL_DEBUG
 */
public static final int ONNXIFI_BACKEND_PROPERTY_LOG_LEVEL = 2;
/**
 * CUDA stream to be used by the backend.
 * CUDA stream must be created on the CUDA device used by the ONNXIFI backend.
 * Users can query which CUDA device is used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_CUDA_INDEX info type.
 *
 * If this property is not specified during initialization, the backend can
 * create a new CUDA stream for the device, or use a default CUDA stream.
 *
 * Possible values: cudaStream_t or CUstream object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_STREAM = 4;
/**
 * OpenCL context to be used by the backend.
 * The context must be created with the OpenCL device ID and the OpenCL platform
 * ID used by the ONNXIFI backend. Users can query which OpenCL device ID and
 * OpenCL platform ID are used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 * and ONNXIFI_BACKEND_OPENCL_DEVICE_ID info types.
 *
 * If this property is not specified during initialization, the backend will
 * create a new OpenCL context for the device.
 *
 * Possible values: cl_context object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_OPENCL_CONTEXT = 8;

/**
 * Terminates the list of auxiliary graph initialization properties passed to
 * onnxInitGraph.
 */
public static final int ONNXIFI_GRAPH_PROPERTY_NONE = 0;

/**
 * Optimize graph representation and compilation for highest throughput.
 */
public static final int ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT = 0;

/**
 * Optimize graph representation and compilation for lowest latency.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_LATENCY = 1;

/**
 * Optimize graph representation and compilation for lowest power consumption.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_POWER = 2;

/**
 * Optimize graph representation and compilation for lowest delay until first
 * result.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_DELAY = 3;

/**
 * Log events which caused a failure in an ONNXIFI function call.
 */
public static final int ONNXIFI_LOG_LEVEL_ERROR = 4;

/**
 * Log events in ONNXIFI_LOG_LEVEL_ERROR and events which caused
 * a performance, accuracy, or quality of service degradation in a backend.
 * Enabling this logging level SHOULD NOT have a measurable effect on
 * performance.
 */
public static final int ONNXIFI_LOG_LEVEL_WARNING = 3;

/**
 * Log events in ONNXIFI_LOG_LEVEL_WARNING and high-level status information
 * about operation of a backend. Enabling this logging level MAY cause a small
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_INFO = 2;

/**
 * Log events in ONNXIFI_LOG_LEVEL_INFO and detailed status information about
 * operations of a backend. Enabling this logging level MAY cause a serious
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_DEBUG = 1;

/**
 * Tag for version 1 of tensor descriptor structure (onnxTensorDescriptorV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the tensor descriptor structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1 = 0x43DFBF69;
// Targeting ../onnxTensorDescriptorV1.java



/**
 * Synchronization using ONNXIFI event object (onnxEvent).
 */
public static final int ONNXIFI_SYNCHRONIZATION_EVENT = 0;
/**
 * Implicit synchronization of inputs and outputs access with the caller.
 * The details are backend-specific, and may involve extra parameters passed
 * during backend initialization.
 *
 * Examples:
 *  - CUDA-based backends could implicitly synchronize with the caller through
 *    the use of the same CUDA stream.
 *  - OpenCL-based backends could implicitly synchronize with the caller through
 *    the use of the same in-order OpenCL command queue.
 */
public static final int ONNXIFI_SYNCHRONIZATION_IMPLICIT = 2;

/**
 * Tag for version 1 of memory fence structure (onnxMemoryFenceV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the memory fence structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_MEMORY_FENCE_V1 = 0x23E08AAB;
// Targeting ../onnxMemoryFenceV1.java


// Targeting ../onnxGetBackendIDsFunction.java


// Targeting ../onnxReleaseBackendIDFunction.java


// Targeting ../onnxGetBackendInfoFunction.java


// Targeting ../onnxGetBackendCompatibilityFunction.java


// Targeting ../onnxInitBackendFunction.java


// Targeting ../onnxReleaseBackendFunction.java


// Targeting ../onnxInitEventFunction.java


// Targeting ../onnxSignalEventFunction.java


// Targeting ../onnxGetEventStateFunction.java


// Targeting ../onnxWaitEventFunction.java


// Targeting ../onnxReleaseEventFunction.java


// Targeting ../onnxInitGraphFunction.java


// Targeting ../onnxSetGraphIOFunction.java


// Targeting ../onnxRunGraphFunction.java


// Targeting ../onnxReleaseGraphFunction.java



/**
 * Get stable IDs of available backends on the system.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software). Backends
 * implemented in the same software, but targeting different devices (e.g.
 * "MyNN" for CPU and "MyNN" for GPU) have different backend IDs.
 *
 * Note that some (hot-pluggable) backends can be connected and disconnected at
 * any time, and thus subsequent calls to this function may return different
 * number or set of backend IDs. The returned IDs, however, stay valid even if
 * the hardware device used by the backend disconnects from the system.
 *
 * To avoid resource leak, the backend ID MUST be released through a call to
 * onnxReleaseBackendID when it is no longer needed.
 *
 * @param backendIDs[out] - pointer to the memory location where the backend IDs
 *                          will be returned. If the pointer is NULL, it is
 *                          ignored, and the function returns only the number
 *                          of backend IDs through numBackendIDs pointer.
 * @param numBackendIDs[in,out] - pointer to a variable specifying number of
 *                                available backends. On function entry, the
 *                                variable MUST contain the capacity, in number
 *                                of backend IDs, of the memory buffer specified
 *                                by backendIDs. For successful completion, this
 *                                capacity must be at least as large as the
 *                                number of available backends. If the function
 *                                completes with either ONNXIFI_STATUS_SUCCESS
 *                                or ONNXIFI_STATUS_FALLBACK status codes, the
 *                                number of backend IDs written into backendIDs
 *                                buffer is stored in the variable specified by
 *                                this pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and backend IDs
 *                                are stored in the location specified by
 *                                backendIDs, and the number of the backends
 *                                is stored in the location specified by
 *                                numBackends.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 backend IDs were not stored in the
 *                                 location specified by backendIDs, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient to store
 *                                 all available backend IDs. The number of
 *                                 available backends is stored in the
 *                                 location specified by numBackends.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        numBackends is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         system failed to allocate memory
 *                                         to store backend ID information.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendIDs(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") SizeTPointer numBackends);

/**
 * Deinitialize ONNXIFI backend IDs and release associated resources.
 *
 * The user MUST deinitialize all objects created with this backend ID
 * (onnxBackend, onnxGraph, onnxEvent) before calling this function to
 * deinitialize the backend ID.
 *
 * @param backendID - backend ID returned by onnxGetBackendIDs.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the resources
 *                                associated to the backend ID were released to
 *                                the operating system.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackendID(
    onnxBackendID backendID);

/**
 * Query high-level information about the backend and its target device.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * The content, data type, and availability of information provided by this
 * function depends on infoType value as specified below:
 *
 *         infoType value                           data type      support
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION               uint64_t     required
 *     ONNXIFI_BACKEND_NAME                           char[]      required
 *     ONNXIFI_BACKEND_VENDOR                         char[]      required
 *     ONNXIFI_BACKEND_VERSION                        char[]      required
 *     ONNXIFI_BACKEND_EXTENSIONS                     char[]      required
 *     ONNXIFI_BACKEND_DEVICE                         char[]      required
 *     ONNXIFI_BACKEND_DEVICE_TYPE                   onnxEnum     required
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION                char[]      required
 *     ONNXIFI_BACKEND_OPSET_VERSION                  char[]      required
 *     ONNXIFI_BACKEND_CAPABILITIES                onnxBitfield   required
 *     ONNXIFI_BACKEND_INIT_PROPERTIES             onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_TYPES                onnxBitfield   required
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES       onnxBitfield   required
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES       onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_SIZE                   uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE                uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT               uint64_t     required
 *     ONNXIFI_BACKEND_MACS_FP32                     uint64_t     optional
 *     ONNXIFI_BACKEND_MACS_FP16                     uint64_t     optional
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH              uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH     uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_BUS_ID                    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_DIRECTX_ID                      LUID       optional
 *     ONNXIFI_BACKEND_CUDA_INDEX                    uint64_t     optional
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID         cl_platform_id  optional
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID            cl_device_id   optional
 *
 * @param backendID - ID of the backend to query.
 * @param infoType - type of the backend information to query. Must be one of
 *                   the ONNXIFI_BACKEND_* constants. If this value is not
 *                   supported by the backend, the function will fail with
 *                   ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE.
 * @param infoValue[out] - pointer to the memory location where the backend
 *                         information value will be returned. If the pointer is
 *                         NULL, is it ignored.
 * @param infoValueSize[in,out] - pointer to a variable specifying size, in
 *                                bytes, of the information value. On function
 *                                entry, the variable MUST contain the size of
 *                                the memory buffer specified by infoValue.
 *                                For successful completion, this size must be
 *                                at least as large as the queried value. If the
 *                                function completes with either
 *                                ONNXIFI_STATUS_SUCCESS or
 *                                ONNXIFI_STATUS_FALLBACK status codes, the
 *                                actual size of the value queried in the call
 *                                is stored in the variable specified by this
 *                                pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and requested
 *                                value is stored in the location specified by
 *                                infoValue, and the actual size of the
 *                                requested value is stored in the location
 *                                specified by infoValueSize.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 requested value was not stored in the
 *                                 location specified by infoValue, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient for the
 *                                 value. The actual size of the requested value
 *                                 is stored in the location specified by
 *                                 infoValueSize.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        infoValueSize is NULL.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the value of infoType is not
 *                                              supported by the backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 */
public static native @Cast("onnxStatus") int onnxGetBackendInfo(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") SizeTPointer infoValueSize);

/**
 * Query if an ONNX model graph is compatible with the backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, the validation of symbolic dimension should be
 * deferred until graph inputs and outputs are specified in onnxSetGraphIO.
 *
 * Commonly, the serialized ModelProto message passed to this function would
 * not include the static weights (ModelProto.graph.initializer is empty), and
 * the backend implementation MUST NOT rely on the weights to determine if the
 * graph is supported.
 *
 * An important use-case is a ModelProto containing only a single NodeProto in
 * ModelProto.graph.node, which happens when a high-level framework checks
 * operators one-by-one to find a connected subgraph that can be offloaded to
 * the backend. Backend implementations SHOULD optimize performance for this
 * use-case.
 *
 * @param backend - ID of the backend to query.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param onnxModel [in] - pointer to serialized ONNX ModelProto message
 *                        representing the model graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph can efficiently run on the backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph can run on the backend through some
 *                                 emulation layer with some efficiency loss. If
 *                                 a backend decomposes this operator into
 *                                 multiple sub-operators, it should return this
 *                                 code. E.g. if a backend does not natively
 *                                 support grouped or depthwise convolution, but
 *                                 can execute it as multiple unit-group
 *                                 convolution operators, it must returns this
 *                                 code.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel is NULL.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 *                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, or inferred
 *                                          from the inputs by the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, or
 *                                             inferred from the inputs by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             output or intermediate data types
 *                                             specified in the ONNX model graph
 *                                             do not match the data types
 *                                             inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse and analyze
 *                                         the model graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendCompatibility(
    onnxBackendID backendID,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel);

/**
 * Initialize an ONNXIFI backend.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNXIFI graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * @param backendID - ID of the backend to initialize.
 * @param auxPropertiesList [in] - optional list of backend initialization
 *                                properties, terminated by
 *                                ONNXIFI_BACKEND_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param backend [out] - pointer to an opaque handle for the initialized ONNXIFI
 *                       backend. If the function fails, the handle is
 *                       initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        backend pointer is NULL.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the backend initialization
 *                                         property values is invalid.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize backend.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @ByPtrPtr onnxBackend backend);

/**
 * Deinitialize an ONNXIFI backend and release associated resources.
 *
 * The user MUST deinitialize all objects created on this backend (onnxGraph,
 * onnxEvent) before calling this function to deinitialize the backend.
 *
 * @param backend - ONNXIFI backend handle created by onnxInitBackend.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackend(
    onnxBackend backend);

/**
 * Initialize a single-shot ONNXIFI event.
 *
 * The newly created event is in non-signalled state.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to initialize the event.
 * @param event [out] - pointer to the opaque handle for the created ONNXIFI
 *                     event. If the function fails, the handle is initialized
 *                     to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        event pointer is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize the event.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            event.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitEvent(
    onnxBackend backend,
    @ByPtrPtr onnxEvent event);

/**
 * Change the state of an ONNXIFI event to signalled.
 *
 * @param event - event handle created by onnxInitEvent. While it is technically
 *                possible to use this function for output memory fence event
 *                created by onnxRunGraph, users SHOULD NOT do that.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was changed to signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_STATE The function call failed because event
 *                                      is already in the signalled state.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxSignalEvent(
    onnxEvent event);

/**
 * Query ONNXIFI event state without blocking.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 * @param state [out] - pointer to the variable that will store the state of the
 *                     event. If the function fails, the variable is initialized
 *                     to ONNXIFI_EVENT_STATE_INVALID.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the state
 *                                variable was initialized to either
 *                                ONNXIFI_EVENT_STATE_SIGNALLED or
 *                                ONNXIFI_EVENT_STATE_NONSIGNALLED according
 *                                to the state of the event.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because state
 *                                        pointer is NULL.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntPointer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntBuffer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") int[] state);

/**
 * Wait until an ONNXIFI event transitions to signalled state.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the function
 *                                returned because event transitioned to
 *                                signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxWaitEvent(
    onnxEvent event);

/**
 * Deinitialize an ONNXIFI event and release associated resources.
 *
 * @param event - event handle created by either onnxInitEvent or onnxRunGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseEvent(
    onnxEvent event);

/**
 * Parse an ONNXIFI graph and convert it for a particular backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, their validation should be deferred until a later
 * call to onnxSetGraphIO.
 *
 * Values of all static weights of the graph must be specified either in
 * ModelProto.graph.initializer, or through the weightDescriptors parameters,
 * but not through any combination of the two methods. If the caller creates the
 * graph on the fly, it SHOULD pass weights through weightDescriptors as it
 * involves less overhead.
 *
 * Blobs and operators in this graph are independent of the blobs and operators
 * of other graphs on the same backend.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to setup and run the model graph.
 * @param auxPropertiesList [in] - optional list of graph initialization
 *                                properties, terminated by
 *                                ONNXIFI_GRAPH_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param onnxModel [in] - pointer to serialized ONNX ModelProto message
 *                        representing the model graph. The backend MUST not
 *                        assume that the serialized ModelProto message is
 *                        present at this address after the function returns.
 * @param weightsCount - number of weights specified in this function call
 *                       through tensor descriptors. Alternatively, the weights
 *                       can be specified in ModelProto.graph.initializer.
 *                       If weightsCount is non-zero, weightDescriptors must be
 *                       non-NULL.
 * @param weightDescriptors [in] - descriptors of static input tensors for the
 *                                graph. Elements of this array provide location
 *                                for blobs identified by ValueInfoProto.name
 *                                listed in ModelProto.graph.input of the ONNX
 *                                graph. If this parameter is non-NULL,
 *                                all static inputs must be specified through
 *                                the tensor descriptors, and the
 *                                ModelProto.graph.initilizer list must be
 *                                empty. The tensor descriptors
 *                                must use ONNXIFI_MEMORY_TYPE_CPU memory type,
 *                                and the backend must copy the values of the
 *                                tensors and all metadata, including shape,
 *                                into its own memory before the function
 *                                returns.
 * @param graph [out] - pointer to the opaque handle for the created ONNXIFI
 *                     graph. If the function fails, and this pointer is
 *                     non-NULL, the handle is initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph was successfully initialized on the
 *                                backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph was initialized for the backend through
 *                                 an emulation layer with substantial
 *                                 efficiency loss. If a backend decomposes an
 *                                 operator into multiple sub-operators, it
 *                                 MUST return this code. E.g. if a backend
 *                                 does not natively support grouped or
 *                                 depthwise convolution, but can execute it as
 *                                 multiple unit-group convolution operators, it
 *                                 should return this code.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the graph initialization property
 *                                         values is invalid.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel or graph pointer is NULL, or
 *                                        weightDescriptors pointer is NULL
 *                                        while weightsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions in
 *                                      weightDescriptors is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         weightDescriptors is unknown to the
 *                                         backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            weightDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in weightDescriptors
 *                                                is invalid (NULL pointer).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the graph initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or
 *                                          output of one of the operators.
 *                                          The problematic tensor shapes could
 *                                          be directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 &                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, through
 *                                          weightDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, through
 *                                             weightDescriptors argument,
 *                                             or inferred from the inputs by
 *                                             the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because one of the memory
 *                                                types in weightDescriptors is
 *                                                different from
 *                                                ONNXIFI_MEMORY_TYPE_CPU.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified in weight
 *                                          descriptors do not match the shapes
 *                                          specified in the ONNX model graph,
 *                                          or output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified in weight
 *                                             descriptors do not match the data
 *                                             types specified in ONNX model
 *                                             graph, or output or intermediate
 *                                             data types specified in the ONNX
 *                                             model graph do not match the data
 *                                             types inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the model graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);

/**
 * Set locations for inputs and outputs of an ONNXIFI graph.
 *
 * The caller MUST ensure that the memory buffers specified for input and output
 * tensors remain accessible until all in-flight graph executions which use
 * specified buffer locations complete AND
 * - Either a next call to onnxSetGraphIO specifies different buffer locations
 * - Or the graph is deinitialized via onnxReleaseGraph
 * The caller can invalidate other data in tensor descriptors, including shape,
 * once the function returns.
 *
 * Calls to onnxRunGraph WILL use input and output locations specified in the
 * preceeding onnxSetGraphIO on the same graph. Asynchronous graph executions
 * that were in-flight before onnxSetGraphIO call will continue to use buffer
 * locations that were current when these graph executions started. An ONNXIFI
 * implementation MAY block inside onnxSetGraphIO until all in-flight graph
 * executions that started before the call complete.
 *
 * If a call to onnxSetGraphIO fails, it invalidates input and output locations
 * for the graph, and a subsequent call to onnxRunGraph will fail with
 * ONNXIFI_STATUS_UNIDENTIFIED_NAME.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param inputsCount - number of elements in the inputDescriptors array.
 * @param inputDescriptors [in] - descriptors of input tensors for the graph.
 *                               Elements of this array must provide a location
 *                               for each ValueInfoProto.name listed in
 *                               ModelProto.graph.input of the ONNX graph.
 *                               If inputsCount is non-zero, inputDescriptors
 *                               pointer must be non-NULL.
 * @param outputsCount - number of elements in the outputDescriptors array.
 *                       Must be greater than zero.
 * @param outputDescriptors [in] - descriptors of output tensors for the graph.
 *                                outputDescriptors pointer must be non-NULL.
 *                                Elements of this array must provide a location
 *                                for each ValueInfoProto.name listed in
 *                                ModelProto.graph.output of the ONNX graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        outputDescriptors pointer is NULL or
 *                                        inputDescriptors pointer is NULL while
 *                                        inputsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_NAME The function call failed because one of
 *                                     the names in tensor descriptors doesn't
 *                                     match blob name in ModelProto.graph.input
 *                                     or ModelProto.graph.output, or the same
 *                                     name appears in more than one tensor
 *                                     descriptor.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         inputDescriptors or outputDescriptors
 *                                         is unknown to the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            inputDescriptors or
 *                                            outputDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in inputDescriptors
 *                                                or outputDescriptors is not
 *                                                valid for the specified
 *                                                memory type (e.g. NULL pointer
 *                                                for ONNXIFI_MEMORY_TYPE_CPU).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because one
 *                                        of the tags in inputDescriptors or
 *                                        outputDescriptors is unknown to the
 *                                        backend (tag does not match
 *                                        ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend. This error code can be
 *                                          returned when the backend supports
 *                                          variable-size inputs and outputs,
 *                                          and the problematic tensor shape was
 *                                          provided in the ValueInfoProto as a
 *                                          symbolic variable.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because the backend does not
 *                                                support one of the memory
 *                                                types in inputDescriptors or
 *                                                outputDescriptors.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because one
 *                                          of the ValueInfoProto.name value in
 *                                          ModelProto.graph.input or
 *                                          ModelProto.graph.output doesn't have
 *                                          a match in the inputDescriptors or
 *                                          outputDescriptors.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument are
 *                                          inconsistent with the shapes
 *                                          specified in the ONNX model graph.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified through
 *                                             inputDescriptors or
 *                                             outputDescriptors argument are
 *                                             inconsistent with the data types
 *                                             specified in the ONNX model
 *                                             graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            tensor locations.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxSetGraphIO(
    onnxGraph graph,
    @Cast("uint32_t") int inputsCount,
    @Const onnxTensorDescriptorV1 inputDescriptors,
    @Cast("uint32_t") int outputsCount,
    @Const onnxTensorDescriptorV1 outputDescriptors);

/**
 * Asynchronously execute operations in an ONNXIFI graph using pre-specified
 * locations for inputs and outputs.
 *
 * This function operates asynchronously: it doesn't require that the locations
 * for graph inputs graph inputs hold valid values before the function is
 * called, and doesn't guarantee that the locations for graph outputs hold
 * valid values when the function returns. Instead, two synchronization
 * primitives are used to signal to the backend when inputs are ready to use,
 * and to signal to the caller when outputs are ready to use. The only
 * synchronization primitive that is always available is onnxEvent
 * (ONNXIFI_SYNCHRONIZATION_EVENT memory fence type). If a backend supports
 * additional types of synchronization primitives, it must indicate them in
 * ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES information query.
 *
 * The caller must successfully specify locations of input and output tensors
 * for the graph through onnxSetGraphIO before calling this function.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param inputFence [in] - synchronization primitive that signals when graph
 *                         inputs are ready to use by the backend. The
 *                         synchronization primitive always must be initialized
 *                         by the caller.
 * @param outputFence [out] - synchronization primitive that signals when graph
 *                           outputs are ready to use by the caller. The type
 *                           of the synchronization primitive always must be
 *                           initialized by the caller. The type of the
 *                           synchronization primitive determines whether it
 *                           is initialized by the user before the call or by
 *                           the backend as a result of this call. Single-shot
 *                           synchronizatiom objects are initialized as a result
 *                           of the call. Reusable synchronization objects are
 *                           generally initialized by the user prior to the
 *                           call.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        inputFence or outputFence pointer is
 *                                        NULL.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_FENCE_TYPE The function call failed because
 *                                           the type of synchronization
 *                                           primitive specified in inputFence
 *                                           or outputFence is unknown to the
 *                                           backend.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because
 *                                      the memory synchronization primitive
 *                                      specified in inputFence or outputFence
 *                                      is not valid (e.g. NULL onnxEvent).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because a tag
 *                                        in inputFence or outputFence is
 *                                        unknown to the backend (tag does not
 *                                        match ONNXIFI_TAG_MEMORY_FENCE_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE The function call failed
 *                                               because the backend does not
 *                                               support the type of
 *                                               synchronization primitive
 *                                               specified in inputFence or
 *                                               outputFence.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because
 *                                          some of the ValueInfoProto.name
 *                                          value in ModelProto.graph.input or
 *                                          ModelProto.graph.output were not
 *                                          specified in a call to
 *                                          onnxSetGraphIO.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to execute the model
 *                                         graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            execute the model graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to execute the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to execute the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxRunGraph(
    onnxGraph graph,
    @Const onnxMemoryFenceV1 inputFence,
    onnxMemoryFenceV1 outputFence);

/**
 * Deinitialize an ONNXIFI graph and release associated resources.
 *
 * If there are in-flight asynchronous inference operations on this graph,
 * the function MUST block until all outstanding operations complete.
 *
 * @param graph - graph handle created by onnxInitGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the graph
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because graph
 *                                      is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       graph backend experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseGraph(
    onnxGraph graph);

// #ifdef __cplusplus /* extern "C" */
// #endif

// #endif /* !defined(ONNXIFI_H) */


// Parsed from ngraph/frontend/onnx_import/core/weight.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <string>
// #include <unordered_map>
// #include <vector>

// #include "ngraph/runtime/backend.hpp"
// #include "ngraph/runtime/tensor.hpp"
// Targeting ../Weight.java


    



// Parsed from ngraph/frontend/onnx_import/onnx.hpp

//*****************************************************************************
// Copyright 2017-2019 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//*****************************************************************************

// #pragma once

// #include <cstdint>
// #include <iostream>
// #include <set>
// #include <string>

// #include "core/operator_set.hpp"
// #include "core/weight.hpp"
// #include "ngraph/function.hpp"
        /** \brief Registers ONNX custom operator
         *  Performs the registration of external ONNX operator. This means the code
         *  of the operator is not part of ONNX importer. The operator shall be registered
         *  before calling {@code load_onnx_model()} or {@code import_onnx_function()} functions.
         *  @param name    name of the operator,
         *  @param version  version of the operator (opset),
         *  @param domain  domain the operator belongs to,
         *  @param fn       function providing the implementation of the operator. */
        

        /** \brief      Return the set of names of supported operators.
         * 
         *  @param version [in]  The requested version of ONNX operators set.
         *  @param domain [in]   The requested domain the operators where registered for.
         * 
         *  @return     The set containing names of supported operators.
         *  */

        /** \brief      Determines whether ONNX operator is supported.
         * 
         *  @param op_name [in]  The ONNX operator name.
         *  @param version [in]  The ONNX operator set version.
         *  @param domain [in]   The domain the ONNX operator is registered to.
         * 
         *  @return     True if operator is supported, False otherwise.
         *  */
        @Namespace("ngraph::onnx_import") public static native @Cast("bool") boolean is_operator_supported(@StdString BytePointer op_name,
                                           @Cast("std::int64_t") long version,
                                           @StdString BytePointer domain/*="ai.onnx"*/);
        @Namespace("ngraph::onnx_import") public static native @Cast("bool") boolean is_operator_supported(@StdString BytePointer op_name,
                                           @Cast("std::int64_t") long version);
        @Namespace("ngraph::onnx_import") public static native @Cast("bool") boolean is_operator_supported(@StdString String op_name,
                                           @Cast("std::int64_t") long version,
                                           @StdString String domain/*="ai.onnx"*/);
        @Namespace("ngraph::onnx_import") public static native @Cast("bool") boolean is_operator_supported(@StdString String op_name,
                                           @Cast("std::int64_t") long version);

        /** \brief Convert an ONNX model to nGraph function
         *  The function translated serialized ONNX model to nGraph function. The serialized
         *  ONNX model is read from input stream.
         *  @param sin       input stream (e.g. file stream, memory stream, etc),
         *  @param weights  weights associated with the model. If weights are embedded into
         *                    the model this parameter shall be empty. Having weights in a model
         *                    and providing through this parameters is invalid (the weights from
         *                    the model  will take precedence).
         *  @return The function returns a nGraph function representing single output from graph. */
        @Namespace("ngraph::onnx_import") public static native @SharedPtr @ByVal Function import_onnx_model(@StdString BytePointer data, @Cast("const ngraph::onnx_import::Weights*") @ByRef(nullValue = "ngraph::onnx_import::Weights{}") StringVoidMap weights);
        @Namespace("ngraph::onnx_import") public static native @SharedPtr @ByVal Function import_onnx_model(@StdString BytePointer data);
        @Namespace("ngraph::onnx_import") public static native @SharedPtr @ByVal Function import_onnx_model(@StdString String data, @Cast("const ngraph::onnx_import::Weights*") @ByRef(nullValue = "ngraph::onnx_import::Weights{}") StringVoidMap weights);
        @Namespace("ngraph::onnx_import") public static native @SharedPtr @ByVal Function import_onnx_model(@StdString String data);

     // namespace onnx_import

 // namespace ngraph


}

diff --git a/src/target/llvm/codegen_llvm.cc b/src/target/llvm/codegen_llvm.cc
index 40ec2cc9e0b..9bc56dc9145 100644
--- a/src/target/llvm/codegen_llvm.cc
+++ b/src/target/llvm/codegen_llvm.cc
@@ -487,7 +487,7 @@ llvm::Value* CodeGenLLVM::CreateBroadcast(llvm::Value* value, int lanes) {
 }
 
 llvm::Value* CodeGenLLVM::CreateVecSlice(llvm::Value* vec, int begin, int extent) {
-  int num_elems = llvm::cast<llvm::VectorType>(vec->getType())->getNumElements();
+  int num_elems = GetVectorNumElements(vec);
   if (extent == num_elems && begin == 0) return vec;
   CHECK(begin >= 0 && extent <= num_elems) << "Slicing out of bound!\n";
   std::vector<llvm::Constant*> indices;
@@ -503,7 +503,7 @@ llvm::Value* CodeGenLLVM::CreateVecSlice(llvm::Value* vec, int begin, int extent
 }
 
 llvm::Value* CodeGenLLVM::CreateVecFlip(llvm::Value* vec) {
-  int num_elems = llvm::cast<llvm::VectorType>(vec->getType())->getNumElements();
+  int num_elems = GetVectorNumElements(vec);
 #if TVM_LLVM_VERSION >= 110
   std::vector<int> indices;
 #else
@@ -517,7 +517,7 @@ llvm::Value* CodeGenLLVM::CreateVecFlip(llvm::Value* vec) {
 
 llvm::Value* CodeGenLLVM::CreateVecPad(llvm::Value* vec, int target_lanes) {
   llvm::Value* mask = llvm::UndefValue::get(DTypeToLLVMType(DataType::Int(32, target_lanes)));
-  int num_elems = llvm::cast<llvm::VectorType>(vec->getType())->getNumElements();
+  int num_elems = GetVectorNumElements(vec);
   if (num_elems == target_lanes) return vec;
   CHECK_LT(num_elems, target_lanes);
   for (int i = 0; i < num_elems; ++i) {
@@ -531,15 +531,15 @@ llvm::Value* CodeGenLLVM::CreateVecConcat(std::vector<llvm::Value*> vecs) {
   int total_lanes = 0;
 
   for (llvm::Value* v : vecs) {
-    total_lanes += llvm::cast<llvm::VectorType>(v->getType())->getNumElements();
+    total_lanes += GetVectorNumElements(v);
   }
   while (vecs.size() > 1) {
     std::vector<llvm::Value*> new_vecs;
     for (size_t i = 0; i < vecs.size() - 1; i += 2) {
       llvm::Value* lhs = vecs[i];
       llvm::Value* rhs = vecs[i + 1];
-      const size_t lhs_lanes = llvm::cast<llvm::VectorType>(lhs->getType())->getNumElements();
-      const size_t rhs_lanes = llvm::cast<llvm::VectorType>(rhs->getType())->getNumElements();
+      const size_t lhs_lanes = GetVectorNumElements(lhs);
+      const size_t rhs_lanes = GetVectorNumElements(rhs);
       if (lhs_lanes < rhs_lanes) {
         lhs = CreateVecPad(lhs, rhs_lanes);
       } else if (rhs_lanes < lhs_lanes) {
@@ -843,16 +843,16 @@ llvm::Value* CodeGenLLVM::CreateIntrinsic(const CallNode* op) {
     return builder_->CreateFCmpUNO(a, a);
   } else if (op->op.same_as(builtin::vectorlow())) {
     llvm::Value* v = MakeValue(op->args[0]);
-    int l = llvm::cast<llvm::VectorType>(v->getType())->getNumElements();
+    int l = GetVectorNumElements(v);
     return CreateVecSlice(v, 0, l / 2);
   } else if (op->op.same_as(builtin::vectorhigh())) {
     llvm::Value* v = MakeValue(op->args[0]);
-    int l = llvm::cast<llvm::VectorType>(v->getType())->getNumElements();
+    int l = GetVectorNumElements(v);
     return CreateVecSlice(v, l / 2, l / 2);
   } else if (op->op.same_as(builtin::vectorcombine())) {
     llvm::Value* v0 = MakeValue(op->args[0]);
     llvm::Value* v1 = MakeValue(op->args[1]);
-    int num_elems = llvm::cast<llvm::VectorType>(v0->getType())->getNumElements() * 2;
+    int num_elems = GetVectorNumElements(v0) * 2;
 #if TVM_LLVM_VERSION >= 110
     std::vector<int> indices;
 #else
diff --git a/src/target/llvm/codegen_llvm.h b/src/target/llvm/codegen_llvm.h
index 3b0ce10534f..78eb5e2dcac 100644
--- a/src/target/llvm/codegen_llvm.h
+++ b/src/target/llvm/codegen_llvm.h
@@ -242,6 +242,11 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
    */
   llvm::Function* GetIntrinsicDecl(llvm::Intrinsic::ID id, llvm::Type* ret_type,
                                    llvm::ArrayRef<llvm::Type*> arg_types);
+  /*!
+   * \brief Get the number of elements in the given vector value.
+   * \param vec The value, must be of a vector type.
+   */
+  inline int GetVectorNumElements(llvm::Value* vec);
   // initialize the function state.
   void InitFuncState();
   // Get alignment given index.
@@ -348,6 +353,15 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
    */
   static std::unique_ptr<DebugInfo> CreateDebugInfo(llvm::Module* module);
 };
+
+inline int CodeGenLLVM::GetVectorNumElements(llvm::Value* vec) {
+#if TVM_LLVM_VERSION >= 120
+  return llvm::cast<llvm::FixedVectorType>(vec->getType())->getNumElements();
+#else
+  return llvm::cast<llvm::VectorType>(vec->getType())->getNumElements();
+#endif
+}
+
 }  // namespace codegen
 }  // namespace tvm
 #endif  // LLVM_VERSION
diff --git a/src/target/llvm/codegen_x86_64.cc b/src/target/llvm/codegen_x86_64.cc
index f3362fb0f1e..a71a0226c95 100644
--- a/src/target/llvm/codegen_x86_64.cc
+++ b/src/target/llvm/codegen_x86_64.cc
@@ -117,7 +117,11 @@ llvm::Value* CodeGenX86_64::CallVectorIntrin(llvm::Intrinsic::ID id, size_t intr
                                              llvm::Type* result_ty,
                                              const std::vector<llvm::Value*>& args) {
   llvm::Function* f = llvm::Intrinsic::getDeclaration(module_.get(), id, {});
+#if TVM_LLVM_VERSION >= 120
+  size_t num_elems = llvm::cast<llvm::FixedVectorType>(result_ty)->getNumElements();
+#else
   size_t num_elems = llvm::cast<llvm::VectorType>(result_ty)->getNumElements();
+#endif
   if (intrin_lanes == num_elems) {
     return builder_->CreateCall(f, args);
   }
@@ -130,7 +134,7 @@ llvm::Value* CodeGenX86_64::CallVectorIntrin(llvm::Intrinsic::ID id, size_t intr
     std::vector<llvm::Value*> split_args;
     for (const auto& v : args) {
       if (v->getType()->isVectorTy()) {
-        CHECK_EQ(llvm::cast<llvm::VectorType>(v->getType())->getNumElements(), num_elems);
+        CHECK_EQ(GetVectorNumElements(v), num_elems);
         split_args.push_back(CreateVecSlice(v, i, intrin_lanes));
       } else {
         split_args.push_back(v);
diff -ruN apache-tvm-src-v0.7.0.rc0-incubating/python/tvm/relay/op/strategy/x86.py apache-tvm-src-v0.7.0.rc0-incubating-patch/python/tvm/relay/op/strategy/x86.py
--- apache-tvm-src-v0.7.0.rc0-incubating/python/tvm/relay/op/strategy/x86.py	2020-10-03 03:13:38.000000000 +0900
+++ apache-tvm-src-v0.7.0.rc0-incubating-patch/python/tvm/relay/op/strategy/x86.py	2020-12-08 20:52:58.377336269 +0900
@@ -368,6 +368,13 @@
             name="batch_matmul_cblas.x86",
             plevel=15,
         )
+    if "mkl" in target.libs:
+        strategy.add_implementation(
+            wrap_compute_batch_matmul(topi.x86.batch_matmul_mkl),
+            wrap_topi_schedule(topi.x86.schedule_batch_matmul_mkl),
+            name="batch_matmul_mkl.x86",
+            plevel=15,
+        )
     return strategy
 
 
diff -ruN apache-tvm-src-v0.7.0.rc0-incubating/python/tvm/topi/x86/batch_matmul.py apache-tvm-src-v0.7.0.rc0-incubating-patch/python/tvm/topi/x86/batch_matmul.py
--- apache-tvm-src-v0.7.0.rc0-incubating/python/tvm/topi/x86/batch_matmul.py	2020-10-03 03:13:38.000000000 +0900
+++ apache-tvm-src-v0.7.0.rc0-incubating-patch/python/tvm/topi/x86/batch_matmul.py	2020-12-08 20:57:52.883989301 +0900
@@ -19,7 +19,7 @@
 from tvm import te
 from tvm import autotvm
 from tvm.autotvm.task.space import SplitEntity
-from tvm.contrib import cblas
+from tvm.contrib import cblas, mkl
 from .. import generic
 from ..util import traverse_inline, get_const_tuple, get_max_power2_factor
 
@@ -128,10 +128,9 @@
     cfg["tile_y"] = SplitEntity([M // y_bn, y_bn])
 
 
-@autotvm.register_topi_compute("batch_matmul_cblas.x86")
-def batch_matmul_cblas(cfg, x, y):
+def batch_matmul_blas_common(cfg, x, y, lib):
     """Computes batch matrix multiplication of `x` and `y` when `x` and `y` are
-    data in batch.
+    data in batch, using one of BLAS libraries.
 
     Parameters
     ----------
@@ -141,6 +140,8 @@
         3-D with shape [batch, M, K]
     y : tvm.te.Tensor
         3-D with shape [batch, N, K]
+    lib : A contrib module which implements batch_matmul funtion
+        cblas and mkl are supported
     Returns
     -------
     output : tvm.te.Tensor
@@ -152,9 +153,28 @@
     assert XB == YB, "batch dimension doesn't match"
     assert XK == YK, "shapes of x and y is inconsistant"
     cfg.add_flop(XB * M * N * XK * 2)
-    return cblas.batch_matmul(x, y, False, True)
+    return lib.batch_matmul(x, y, False, True)
+
+
+@autotvm.register_topi_compute("batch_matmul_cblas.x86")
+def batch_matmul_cblas(cfg, x, y):
+    """Compute batch_matmul using cblas"""
+    return batch_matmul_blas_common(cfg, x, y, cblas)
 
 
 @autotvm.register_topi_schedule("batch_matmul_cblas.x86")
 def schedule_batch_matmul_cblas(_, outs):
+    """Create schedule for batch_matmul_cblas"""
+    return generic.schedule_extern(outs)
+
+
+@autotvm.register_topi_compute("batch_matmul_mkl.x86")
+def batch_matmul_mkl(cfg, x, y):
+    """Compute batch_matmul using mkl"""
+    return batch_matmul_blas_common(cfg, x, y, mkl)
+
+
+@autotvm.register_topi_schedule("batch_matmul_mkl.x86")
+def schedule_batch_matmul_mkl(_, outs):
+    """Create schedule for batch_matmul_mul"""
     return generic.schedule_extern(outs)

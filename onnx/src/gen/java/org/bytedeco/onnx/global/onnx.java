// Targeted by JavaCPP version 1.5.5-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.onnx.global;

import org.bytedeco.onnx.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;

public class onnx extends org.bytedeco.onnx.presets.onnx {
    static { Loader.load(); }

// Targeting ../StringAttributeMap.java


// Targeting ../StringPassMap.java


// Targeting ../IntSet.java


// Targeting ../StringSet.java


// Targeting ../TypeConstraintParamVector.java


// Targeting ../FloatVector.java


// Targeting ../LongVector.java


// Targeting ../StringVector.java


// Targeting ../OpSchemaVector.java


// Targeting ../FormalParameterVector.java


// Targeting ../TypeProtoVector.java


// Targeting ../TensorShapeProtoVector.java


// Targeting ../LongLongPair.java


// Targeting ../UseTypeIntPair.java


// Targeting ../StringIntMap.java


// Targeting ../StringTypeProtoMap.java


// Targeting ../StringTensorProtoMap.java


// Targeting ../StringAttributeProtoMap.java


// Targeting ../StringIntIntPairMap.java


// Targeting ../IntIntMap.java


// Targeting ../UnorderedStringSet.java


// Targeting ../DataTypeSet.java


// Parsed from onnx/defs/schema.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #include <climits>
// #include <cstring>
// #include <functional>
// #include <initializer_list>
// #include <iostream>
// #include <limits>
// #include <memory>
// #include <ostream>
// #include <set>
// #include <string>
// #include <tuple>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>

// #include "data_type_utils.h"
// #include "onnx/common/common.h"
// #include "onnx/common/constants.h"
// #include "onnx/defs/shape_inference.h"
// #include "onnx/onnx-operators_pb.h"
// Targeting ../FunctionBodyBuildContext.java


// Targeting ../FunctionBodyBuildContextImpl.java


// Targeting ../SchemaError.java



// #define fail_schema(...)
//   throw ONNX_NAMESPACE::SchemaError(ONNX_NAMESPACE::MakeString(__VA_ARGS__));

// Type constraint map. Key is type string. Value is data type set and
// description.
// Targeting ../OpSchema.java



// Map type to store operator schemas. The format is,
// <OpName, <Domain, <OperatorSetVersion, OpSchema>>>.
// Targeting ../ISchemaRegistry.java


// Targeting ../OpSchemaRegistry.java





// Registers all schema of a given operator set

// Forward declaration for the non-specialized GetOpSchema method.  This
// enforces a consistent signature on functions that query individual schema,
// which are defined as specializations of this function.

// #define ONNX_OPERATOR_SET_SCHEMA(name, ver, impl)
//   ONNX_OPERATOR_SET_SCHEMA_EX(name, Onnx, ONNX_DOMAIN, ver, true, impl)

// #define ONNX_ML_OPERATOR_SET_SCHEMA(name, ver, impl)
//   ONNX_OPERATOR_SET_SCHEMA_EX(name, OnnxML, AI_ONNX_ML_DOMAIN, ver, true, impl)

// #define ONNX_TRAINING_OPERATOR_SET_SCHEMA(name, ver, impl)
//   ONNX_OPERATOR_SET_SCHEMA_EX(
//       name, OnnxTraining, AI_ONNX_TRAINING_DOMAIN, ver, true, impl)

// #define ONNX_PREVIEW_TRAINING_OPERATOR_SET_SCHEMA(name, ver, impl)
//   ONNX_OPERATOR_SET_SCHEMA_EX(
//       name, OnnxPreview, AI_ONNX_PREVIEW_TRAINING_DOMAIN, ver, true, impl)

// Defines specialization of GetOpSchema for a class whose name is determined
// based on a convention using name, domain, and version.  Operator schema are
// normally included in operator sets and registered in OpSchemaRegistry::map().
// In this case, callers should set dbg_included_in_static_opset to true.  This
// assists with runtime validation in in DEBUG builds ensuring the intended set
// of operator schema is registered.
// #define ONNX_OPERATOR_SET_SCHEMA_EX(
//     name, domain, domain_str, ver, dbg_included_in_static_opset, impl)
//   class ONNX_OPERATOR_SET_SCHEMA_CLASS_NAME(domain, ver, name);
//   template <>
//   OpSchema
//   GetOpSchema<ONNX_OPERATOR_SET_SCHEMA_CLASS_NAME(domain, ver, name)>() {
//     return impl.SetName(#name)
//         .SetDomain(domain_str)
//         .SinceVersion(ver)
//         .SetLocation(__FILE__, __LINE__);
//   }
//   size_t dbg_count_check_##name##_##domain##_ver##ver =
//       (dbg_included_in_static_opset) ? ONNX_DBG_INCREMENT_COUNT_IN_OPSETS()
//                                      : 0;

// #ifdef NDEBUG
// #define ONNX_DBG_INCREMENT_COUNT_IN_OPSETS() 0
// #else
// #define ONNX_DBG_INCREMENT_COUNT_IN_OPSETS()
//   DbgOperatorSetTracker::Instance().IncrementCount()
// #define ONNX_DBG_GET_COUNT_IN_OPSETS()
//   DbgOperatorSetTracker::Instance().GetCount()
// Targeting ../DbgOperatorSetTracker.java


// #endif

// Naming convention for operator schema classes
// #define ONNX_OPERATOR_SET_SCHEMA_CLASS_NAME(domain, ver, name)
//   name##_##domain##_ver##ver

// Naming convention for preview operator schema classes
// #define ONNX_PREVIEW_OPERATOR_SET_SCHEMA_CLASS_NAME(ver, name)
//   ONNX_OPERATOR_SET_SCHEMA_CLASS_NAME(OnnxPreview, ver, name)

// Helper function


// #ifdef __GNUC__
// #define ONNX_UNUSED __attribute__((__unused__))
// #else
// #define ONNX_UNUSED
// #endif

// Legacy macros to register schema at static initialization
// #define ONNX_OPERATOR_SCHEMA(name)
//   ONNX_OPERATOR_SCHEMA_UNIQ_HELPER(__COUNTER__, name)
// #define ONNX_OPERATOR_SCHEMA_UNIQ_HELPER(Counter, name)
//   ONNX_OPERATOR_SCHEMA_UNIQ(Counter, name)
// #define ONNX_OPERATOR_SCHEMA_UNIQ(Counter, name)
//   static ONNX_NAMESPACE::OpSchemaRegistry::OpSchemaRegisterOnce(
//       op_schema_register_once##name##Counter) ONNX_UNUSED =
//       OpSchema(#name, __FILE__, __LINE__)

// Helper function


@Namespace("onnx") public static native @StdString BytePointer GenerateOptionalArgumentsDoc();

@Namespace("onnx") public static native @StdString BytePointer GenerateBroadcastingDocMul();

@Namespace("onnx") public static native @StdString BytePointer GenerateBroadcastingDocUni(
    @Cast("const char*") BytePointer from,
    @Cast("const char*") BytePointer to);
@Namespace("onnx") public static native @StdString String GenerateBroadcastingDocUni(
    String from,
    String to);

/*
 * Macros for setting operator documentation
 * Use this macro for simple SetDoc() calls that generate documentation
 * directly. This is the macro to use in almost all cases.
 * Sample usage guidelines:
 * const char* doc_str = "foo";
 * SetDoc(GET_OP_DOC_STR(doc_str))
 *
 * SetDoc(GET_OP_DOC_STR(
            std::string(BitShift_ver11_doc) + GenerateBroadcastingDocMul()))
 */
// #ifndef __ONNX_NO_DOC_STRINGS
// #define GET_OP_DOC_STR(doc_str) (doc_str)
// #else
// #define GET_OP_DOC_STR(doc_str) ("")
// #endif

/*
 * Use this macro when the documentation needs to be populated in some
 * complicated way like string substitutions, etc before calling SetDoc.
 * Sample usage guidelines:
    std::string doc;
    POPULATE_OP_DOC_STR(
        doc = R"DOC(
Returns the tensor resulted from performing the `{name}` logical operation
elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
support).

{broadcast_doc}
)DOC";
        ReplaceAll(doc, "{name}", name);
        ReplaceAll(
            doc, "{broadcast_doc}", GenerateBroadcastingDocMul().c_str()););
    schema.SetDoc(doc);
 *
 */
// #ifndef __ONNX_NO_DOC_STRINGS
// #define POPULATE_OP_DOC_STR(DocPopulatorCode)
//   do {
//     DocPopulatorCode
//   } while (0)
// #else
// #define POPULATE_OP_DOC_STR(DocPopulatorCode)
// #endif

 // namespace ONNX_NAMESPACE


// Parsed from onnx/defs/operator_sets.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #include "onnx/defs/schema.h"
// Targeting ../Abs_Onnx_ver1.java


// Targeting ../Add_Onnx_ver1.java


// Targeting ../And_Onnx_ver1.java


// Targeting ../ArgMax_Onnx_ver1.java


// Targeting ../ArgMin_Onnx_ver1.java


// Targeting ../AveragePool_Onnx_ver1.java


// Targeting ../BatchNormalization_Onnx_ver1.java


// Targeting ../Cast_Onnx_ver1.java


// Targeting ../Ceil_Onnx_ver1.java


// Targeting ../Clip_Onnx_ver1.java


// Targeting ../Concat_Onnx_ver1.java


// Targeting ../Constant_Onnx_ver1.java


// Targeting ../Conv_Onnx_ver1.java


// Targeting ../ConvTranspose_Onnx_ver1.java


// Targeting ../DepthToSpace_Onnx_ver1.java


// Targeting ../Div_Onnx_ver1.java


// Targeting ../Dropout_Onnx_ver1.java


// Targeting ../Elu_Onnx_ver1.java


// Targeting ../Equal_Onnx_ver1.java


// Targeting ../Exp_Onnx_ver1.java


// Targeting ../Flatten_Onnx_ver1.java


// Targeting ../Floor_Onnx_ver1.java


// Targeting ../GRU_Onnx_ver1.java


// Targeting ../Gather_Onnx_ver1.java


// Targeting ../Gemm_Onnx_ver1.java


// Targeting ../GlobalAveragePool_Onnx_ver1.java


// Targeting ../GlobalLpPool_Onnx_ver1.java


// Targeting ../GlobalMaxPool_Onnx_ver1.java


// Targeting ../Greater_Onnx_ver1.java


// Targeting ../HardSigmoid_Onnx_ver1.java


// Targeting ../Hardmax_Onnx_ver1.java


// Targeting ../Identity_Onnx_ver1.java


// Targeting ../If_Onnx_ver1.java


// Targeting ../InstanceNormalization_Onnx_ver1.java


// Targeting ../LRN_Onnx_ver1.java


// Targeting ../LSTM_Onnx_ver1.java


// Targeting ../LeakyRelu_Onnx_ver1.java


// Targeting ../Less_Onnx_ver1.java


// Targeting ../Log_Onnx_ver1.java


// Targeting ../LogSoftmax_Onnx_ver1.java


// Targeting ../Loop_Onnx_ver1.java


// Targeting ../LpNormalization_Onnx_ver1.java


// Targeting ../LpPool_Onnx_ver1.java


// Targeting ../MatMul_Onnx_ver1.java


// Targeting ../Max_Onnx_ver1.java


// Targeting ../MaxPool_Onnx_ver1.java


// Targeting ../MaxRoiPool_Onnx_ver1.java


// Targeting ../Mean_Onnx_ver1.java


// Targeting ../Min_Onnx_ver1.java


// Targeting ../Mul_Onnx_ver1.java


// Targeting ../Neg_Onnx_ver1.java


// Targeting ../Not_Onnx_ver1.java


// Targeting ../Or_Onnx_ver1.java


// Targeting ../PRelu_Onnx_ver1.java


// Targeting ../Pad_Onnx_ver1.java


// Targeting ../Pow_Onnx_ver1.java


// Targeting ../RNN_Onnx_ver1.java


// Targeting ../RandomNormal_Onnx_ver1.java


// Targeting ../RandomNormalLike_Onnx_ver1.java


// Targeting ../RandomUniform_Onnx_ver1.java


// Targeting ../RandomUniformLike_Onnx_ver1.java


// Targeting ../Reciprocal_Onnx_ver1.java


// Targeting ../ReduceL1_Onnx_ver1.java


// Targeting ../ReduceL2_Onnx_ver1.java


// Targeting ../ReduceLogSum_Onnx_ver1.java


// Targeting ../ReduceLogSumExp_Onnx_ver1.java


// Targeting ../ReduceMax_Onnx_ver1.java


// Targeting ../ReduceMean_Onnx_ver1.java


// Targeting ../ReduceMin_Onnx_ver1.java


// Targeting ../ReduceProd_Onnx_ver1.java


// Targeting ../ReduceSum_Onnx_ver1.java


// Targeting ../ReduceSumSquare_Onnx_ver1.java


// Targeting ../Relu_Onnx_ver1.java


// Targeting ../Reshape_Onnx_ver1.java


// Targeting ../Selu_Onnx_ver1.java


// Targeting ../Shape_Onnx_ver1.java


// Targeting ../Sigmoid_Onnx_ver1.java


// Targeting ../Size_Onnx_ver1.java


// Targeting ../Slice_Onnx_ver1.java


// Targeting ../Softmax_Onnx_ver1.java


// Targeting ../Softplus_Onnx_ver1.java


// Targeting ../Softsign_Onnx_ver1.java


// Targeting ../SpaceToDepth_Onnx_ver1.java


// Targeting ../Split_Onnx_ver1.java


// Targeting ../Sqrt_Onnx_ver1.java


// Targeting ../Squeeze_Onnx_ver1.java


// Targeting ../Sub_Onnx_ver1.java


// Targeting ../Sum_Onnx_ver1.java


// Targeting ../Tanh_Onnx_ver1.java


// Targeting ../Tile_Onnx_ver1.java


// Targeting ../TopK_Onnx_ver1.java


// Targeting ../Transpose_Onnx_ver1.java


// Targeting ../Unsqueeze_Onnx_ver1.java


// Targeting ../Upsample_Onnx_ver1.java


// Targeting ../Xor_Onnx_ver1.java


// Targeting ../OpSet_Onnx_ver1.java


// Targeting ../GlobalLpPool_Onnx_ver2.java


// Targeting ../LpPool_Onnx_ver2.java


// Targeting ../Pad_Onnx_ver2.java


// Targeting ../Split_Onnx_ver2.java


// Targeting ../OpSet_Onnx_ver2.java


// Targeting ../GRU_Onnx_ver3.java


// Targeting ../OpSet_Onnx_ver3.java


// Targeting ../Concat_Onnx_ver4.java


// Targeting ../OpSet_Onnx_ver4.java


// Targeting ../Reshape_Onnx_ver5.java


// Targeting ../OpSet_Onnx_ver5.java


// Targeting ../Abs_Onnx_ver6.java


// Targeting ../Add_Onnx_ver6.java


// Targeting ../BatchNormalization_Onnx_ver6.java


// Targeting ../Cast_Onnx_ver6.java


// Targeting ../Ceil_Onnx_ver6.java


// Targeting ../Clip_Onnx_ver6.java


// Targeting ../Div_Onnx_ver6.java


// Targeting ../Dropout_Onnx_ver6.java


// Targeting ../Elu_Onnx_ver6.java


// Targeting ../Exp_Onnx_ver6.java


// Targeting ../Floor_Onnx_ver6.java


// Targeting ../Gemm_Onnx_ver6.java


// Targeting ../HardSigmoid_Onnx_ver6.java


// Targeting ../InstanceNormalization_Onnx_ver6.java


// Targeting ../LeakyRelu_Onnx_ver6.java


// Targeting ../Log_Onnx_ver6.java


// Targeting ../Max_Onnx_ver6.java


// Targeting ../Mean_Onnx_ver6.java


// Targeting ../Min_Onnx_ver6.java


// Targeting ../Mul_Onnx_ver6.java


// Targeting ../Neg_Onnx_ver6.java


// Targeting ../PRelu_Onnx_ver6.java


// Targeting ../Reciprocal_Onnx_ver6.java


// Targeting ../Relu_Onnx_ver6.java


// Targeting ../Selu_Onnx_ver6.java


// Targeting ../Sigmoid_Onnx_ver6.java


// Targeting ../Sqrt_Onnx_ver6.java


// Targeting ../Sub_Onnx_ver6.java


// Targeting ../Sum_Onnx_ver6.java


// Targeting ../Tanh_Onnx_ver6.java


// Targeting ../Tile_Onnx_ver6.java


// Targeting ../OpSet_Onnx_ver6.java


// Targeting ../Acos_Onnx_ver7.java


// Targeting ../Add_Onnx_ver7.java


// Targeting ../And_Onnx_ver7.java


// Targeting ../Asin_Onnx_ver7.java


// Targeting ../Atan_Onnx_ver7.java


// Targeting ../AveragePool_Onnx_ver7.java


// Targeting ../BatchNormalization_Onnx_ver7.java


// Targeting ../Cos_Onnx_ver7.java


// Targeting ../Div_Onnx_ver7.java


// Targeting ../Dropout_Onnx_ver7.java


// Targeting ../Equal_Onnx_ver7.java


// Targeting ../Gemm_Onnx_ver7.java


// Targeting ../Greater_Onnx_ver7.java


// Targeting ../GRU_Onnx_ver7.java


// Targeting ../Less_Onnx_ver7.java


// Targeting ../LSTM_Onnx_ver7.java


// Targeting ../Mul_Onnx_ver7.java


// Targeting ../Or_Onnx_ver7.java


// Targeting ../Pow_Onnx_ver7.java


// Targeting ../RNN_Onnx_ver7.java


// Targeting ../Sin_Onnx_ver7.java


// Targeting ../Sub_Onnx_ver7.java


// Targeting ../Tan_Onnx_ver7.java


// Targeting ../Upsample_Onnx_ver7.java


// Targeting ../Multinomial_Onnx_ver7.java


// Targeting ../Xor_Onnx_ver7.java


// Targeting ../PRelu_Onnx_ver7.java


// Targeting ../OpSet_Onnx_ver7.java


// Targeting ../Expand_Onnx_ver8.java


// Targeting ../Max_Onnx_ver8.java


// Targeting ../Min_Onnx_ver8.java


// Targeting ../Sum_Onnx_ver8.java


// Targeting ../Mean_Onnx_ver8.java


// Targeting ../MaxPool_Onnx_ver8.java


// Targeting ../Scan_Onnx_ver8.java


// Targeting ../OpSet_Onnx_ver8.java


// Targeting ../BatchNormalization_Onnx_ver9.java


// Targeting ../Compress_Onnx_ver9.java


// Targeting ../ConstantOfShape_Onnx_ver9.java


// Targeting ../EyeLike_Onnx_ver9.java


// Targeting ../Greater_Onnx_ver9.java


// Targeting ../Less_Onnx_ver9.java


// Targeting ../Upsample_Onnx_ver9.java


// Targeting ../MaxUnpool_Onnx_ver9.java


// Targeting ../Constant_Onnx_ver9.java


// Targeting ../MatMul_Onnx_ver9.java


// Targeting ../OneHot_Onnx_ver9.java


// Targeting ../PRelu_Onnx_ver9.java


// Targeting ../Gemm_Onnx_ver9.java


// Targeting ../Flatten_Onnx_ver9.java


// Targeting ../Sinh_Onnx_ver9.java


// Targeting ../Cosh_Onnx_ver9.java


// Targeting ../Asinh_Onnx_ver9.java


// Targeting ../Acosh_Onnx_ver9.java


// Targeting ../Atanh_Onnx_ver9.java


// Targeting ../Shrink_Onnx_ver9.java


// Targeting ../IsNaN_Onnx_ver9.java


// Targeting ../Sign_Onnx_ver9.java


// Targeting ../Scan_Onnx_ver9.java


// Targeting ../Erf_Onnx_ver9.java


// Targeting ../Scatter_Onnx_ver9.java


// Targeting ../Where_Onnx_ver9.java


// Targeting ../Cast_Onnx_ver9.java


// Targeting ../NonZero_Onnx_ver9.java


// Targeting ../TfIdfVectorizer_Onnx_ver9.java


// Targeting ../MeanVarianceNormalization_Onnx_ver9.java


// Targeting ../OpSet_Onnx_ver9.java


// Targeting ../StringNormalizer_Onnx_ver10.java


// Targeting ../Upsample_Onnx_ver10.java


// Targeting ../Resize_Onnx_ver10.java


// Targeting ../TopK_Onnx_ver10.java


// Targeting ../MaxPool_Onnx_ver10.java


// Targeting ../Mod_Onnx_ver10.java


// Targeting ../AveragePool_Onnx_ver10.java


// Targeting ../Slice_Onnx_ver10.java


// Targeting ../ThresholdedRelu_Onnx_ver10.java


// Targeting ../Dropout_Onnx_ver10.java


// Targeting ../MatMulInteger_Onnx_ver10.java


// Targeting ../QLinearMatMul_Onnx_ver10.java


// Targeting ../ConvInteger_Onnx_ver10.java


// Targeting ../QLinearConv_Onnx_ver10.java


// Targeting ../QuantizeLinear_Onnx_ver10.java


// Targeting ../DequantizeLinear_Onnx_ver10.java


// Targeting ../IsInf_Onnx_ver10.java


// Targeting ../NonMaxSuppression_Onnx_ver10.java


// Targeting ../ReverseSequence_Onnx_ver10.java


// Targeting ../RoiAlign_Onnx_ver10.java


// Targeting ../OpSet_Onnx_ver10.java


// Targeting ../Loop_Onnx_ver11.java


// Targeting ../CumSum_Onnx_ver11.java


// Targeting ../Round_Onnx_ver11.java


// Targeting ../BitShift_Onnx_ver11.java


// Targeting ../Unique_Onnx_ver11.java


// Targeting ../TopK_Onnx_ver11.java


// Targeting ../DepthToSpace_Onnx_ver11.java


// Targeting ../Equal_Onnx_ver11.java


// Targeting ../Constant_Onnx_ver11.java


// Targeting ../DynamicQuantizeLinear_Onnx_ver11.java


// Targeting ../GatherElements_Onnx_ver11.java


// Targeting ../ScatterElements_Onnx_ver11.java


// Targeting ../Scatter_Onnx_ver11.java


// Targeting ../Clip_Onnx_ver11.java


// Targeting ../Resize_Onnx_ver11.java


// Targeting ../Range_Onnx_ver11.java


// Targeting ../Det_Onnx_ver11.java


// Targeting ../ScatterND_Onnx_ver11.java


// Targeting ../GatherND_Onnx_ver11.java


// Targeting ../Gather_Onnx_ver11.java


// Targeting ../OneHot_Onnx_ver11.java


// Targeting ../Slice_Onnx_ver11.java


// Targeting ../Squeeze_Onnx_ver11.java


// Targeting ../Unsqueeze_Onnx_ver11.java


// Targeting ../Flatten_Onnx_ver11.java


// Targeting ../ArgMax_Onnx_ver11.java


// Targeting ../ArgMin_Onnx_ver11.java


// Targeting ../ReduceL1_Onnx_ver11.java


// Targeting ../ReduceL2_Onnx_ver11.java


// Targeting ../ReduceLogSum_Onnx_ver11.java


// Targeting ../ReduceLogSumExp_Onnx_ver11.java


// Targeting ../ReduceMax_Onnx_ver11.java


// Targeting ../ReduceMean_Onnx_ver11.java


// Targeting ../ReduceMin_Onnx_ver11.java


// Targeting ../ReduceProd_Onnx_ver11.java


// Targeting ../ReduceSum_Onnx_ver11.java


// Targeting ../ReduceSumSquare_Onnx_ver11.java


// Targeting ../Compress_Onnx_ver11.java


// Targeting ../Concat_Onnx_ver11.java


// Targeting ../Hardmax_Onnx_ver11.java


// Targeting ../LogSoftmax_Onnx_ver11.java


// Targeting ../Softmax_Onnx_ver11.java


// Targeting ../Scan_Onnx_ver11.java


// Targeting ../Split_Onnx_ver11.java


// Targeting ../AveragePool_Onnx_ver11.java


// Targeting ../MaxPool_Onnx_ver11.java


// Targeting ../MaxUnpool_Onnx_ver11.java


// Targeting ../LpPool_Onnx_ver11.java


// Targeting ../Conv_Onnx_ver11.java


// Targeting ../ConvTranspose_Onnx_ver11.java


// Targeting ../SequenceEmpty_Onnx_ver11.java


// Targeting ../SequenceConstruct_Onnx_ver11.java


// Targeting ../SequenceInsert_Onnx_ver11.java


// Targeting ../SequenceAt_Onnx_ver11.java


// Targeting ../SequenceErase_Onnx_ver11.java


// Targeting ../SequenceLength_Onnx_ver11.java


// Targeting ../SplitToSequence_Onnx_ver11.java


// Targeting ../ConcatFromSequence_Onnx_ver11.java


// Targeting ../Pad_Onnx_ver11.java


// Targeting ../Gemm_Onnx_ver11.java


// Targeting ../If_Onnx_ver11.java


// Targeting ../NonMaxSuppression_Onnx_ver11.java


// Targeting ../OpSet_Onnx_ver11.java


// Targeting ../ArgMax_Onnx_ver12.java


// Targeting ../ArgMin_Onnx_ver12.java


// Targeting ../Clip_Onnx_ver12.java


// Targeting ../Einsum_Onnx_ver12.java


// Targeting ../MaxPool_Onnx_ver12.java


// Targeting ../ReduceMax_Onnx_ver12.java


// Targeting ../ReduceMin_Onnx_ver12.java


// Targeting ../GatherND_Onnx_ver12.java


// Targeting ../NegativeLogLikelihoodLoss_Onnx_ver12.java


// Targeting ../Dropout_Onnx_ver12.java


// Targeting ../Constant_Onnx_ver12.java


// Targeting ../Celu_Onnx_ver12.java


// Targeting ../Max_Onnx_ver12.java


// Targeting ../Min_Onnx_ver12.java


// Targeting ../LessOrEqual_Onnx_ver12.java


// Targeting ../GreaterOrEqual_Onnx_ver12.java


// Targeting ../SoftmaxCrossEntropyLoss_Onnx_ver12.java


// Targeting ../Pow_Onnx_ver12.java


// Targeting ../OpSet_Onnx_ver12.java


// Targeting ../Constant_Onnx_ver13.java


// Targeting ../Greater_Onnx_ver13.java


// Targeting ../Less_Onnx_ver13.java


// Targeting ../Equal_Onnx_ver13.java


// Targeting ../Add_Onnx_ver13.java


// Targeting ../Sub_Onnx_ver13.java


// Targeting ../Mul_Onnx_ver13.java


// Targeting ../Div_Onnx_ver13.java


// Targeting ../Softmax_Onnx_ver13.java


// Targeting ../LogSoftmax_Onnx_ver13.java


// Targeting ../Hardmax_Onnx_ver13.java


// Targeting ../Mod_Onnx_ver13.java


// Targeting ../Neg_Onnx_ver13.java


// Targeting ../Abs_Onnx_ver13.java


// Targeting ../Reciprocal_Onnx_ver13.java


// Targeting ../Floor_Onnx_ver13.java


// Targeting ../Ceil_Onnx_ver13.java


// Targeting ../Sqrt_Onnx_ver13.java


// Targeting ../Relu_Onnx_ver13.java


// Targeting ../Exp_Onnx_ver13.java


// Targeting ../Log_Onnx_ver13.java


// Targeting ../Tanh_Onnx_ver13.java


// Targeting ../Pow_Onnx_ver13.java


// Targeting ../Sigmoid_Onnx_ver13.java


// Targeting ../Max_Onnx_ver13.java


// Targeting ../Min_Onnx_ver13.java


// Targeting ../Sum_Onnx_ver13.java


// Targeting ../Mean_Onnx_ver13.java


// Targeting ../Clip_Onnx_ver13.java


// Targeting ../Gemm_Onnx_ver13.java


// Targeting ../MatMul_Onnx_ver13.java


// Targeting ../Expand_Onnx_ver13.java


// Targeting ../Sign_Onnx_ver13.java


// Targeting ../Erf_Onnx_ver13.java


// Targeting ../SoftmaxCrossEntropyLoss_Onnx_ver13.java


// Targeting ../NegativeLogLikelihoodLoss_Onnx_ver13.java


// Targeting ../Dropout_Onnx_ver13.java


// Targeting ../Flatten_Onnx_ver13.java


// Targeting ../LRN_Onnx_ver13.java


// Targeting ../MeanVarianceNormalization_Onnx_ver13.java


// Targeting ../ReduceMax_Onnx_ver13.java


// Targeting ../ReduceMin_Onnx_ver13.java


// Targeting ../ReduceSum_Onnx_ver13.java


// Targeting ../ReduceSumSquare_Onnx_ver13.java


// Targeting ../ReduceMean_Onnx_ver13.java


// Targeting ../ReduceProd_Onnx_ver13.java


// Targeting ../ReduceLogSum_Onnx_ver13.java


// Targeting ../ReduceLogSumExp_Onnx_ver13.java


// Targeting ../ReduceL1_Onnx_ver13.java


// Targeting ../ReduceL2_Onnx_ver13.java


// Targeting ../ArgMax_Onnx_ver13.java


// Targeting ../ArgMin_Onnx_ver13.java


// Targeting ../Cast_Onnx_ver13.java


// Targeting ../Reshape_Onnx_ver13.java


// Targeting ../Shape_Onnx_ver13.java


// Targeting ../Size_Onnx_ver13.java


// Targeting ../Concat_Onnx_ver13.java


// Targeting ../Split_Onnx_ver13.java


// Targeting ../Slice_Onnx_ver13.java


// Targeting ../Transpose_Onnx_ver13.java


// Targeting ../ScatterND_Onnx_ver13.java


// Targeting ../ScatterElements_Onnx_ver13.java


// Targeting ../Gather_Onnx_ver13.java


// Targeting ../GatherElements_Onnx_ver13.java


// Targeting ../Squeeze_Onnx_ver13.java


// Targeting ../Unsqueeze_Onnx_ver13.java


// Targeting ../SpaceToDepth_Onnx_ver13.java


// Targeting ../DepthToSpace_Onnx_ver13.java


// Targeting ../Tile_Onnx_ver13.java


// Targeting ../Resize_Onnx_ver13.java


// Targeting ../Identity_Onnx_ver13.java


// Targeting ../IsNaN_Onnx_ver13.java


// Targeting ../NonZero_Onnx_ver13.java


// Targeting ../GatherND_Onnx_ver13.java


// Targeting ../Pad_Onnx_ver13.java


// Targeting ../QuantizeLinear_Onnx_ver13.java


// Targeting ../DequantizeLinear_Onnx_ver13.java


// Targeting ../Loop_Onnx_ver13.java


// Targeting ../If_Onnx_ver13.java


// Targeting ../OpSet_Onnx_ver13.java



@Namespace("onnx") public static native void RegisterOnnxOperatorSetSchema();

 // namespace ONNX_NAMESPACE


// Parsed from onnx/defs/operator_sets_ml.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #ifdef ONNX_ML

// #include "onnx/defs/schema.h"
// Targeting ../ArrayFeatureExtractor_OnnxML_ver1.java


// Targeting ../Binarizer_OnnxML_ver1.java


// Targeting ../CastMap_OnnxML_ver1.java


// Targeting ../CategoryMapper_OnnxML_ver1.java


// Targeting ../DictVectorizer_OnnxML_ver1.java


// Targeting ../FeatureVectorizer_OnnxML_ver1.java


// Targeting ../Imputer_OnnxML_ver1.java


// Targeting ../LabelEncoder_OnnxML_ver1.java


// Targeting ../LinearClassifier_OnnxML_ver1.java


// Targeting ../LinearRegressor_OnnxML_ver1.java


// Targeting ../Normalizer_OnnxML_ver1.java


// Targeting ../OneHotEncoder_OnnxML_ver1.java


// Targeting ../SVMClassifier_OnnxML_ver1.java


// Targeting ../SVMRegressor_OnnxML_ver1.java


// Targeting ../Scaler_OnnxML_ver1.java


// Targeting ../TreeEnsembleClassifier_OnnxML_ver1.java


// Targeting ../TreeEnsembleRegressor_OnnxML_ver1.java


// Targeting ../ZipMap_OnnxML_ver1.java


// Targeting ../OpSet_OnnxML_ver1.java


// Targeting ../LabelEncoder_OnnxML_ver2.java


// Targeting ../OpSet_OnnxML_ver2.java



@Namespace("onnx") public static native void RegisterOnnxMLOperatorSetSchema();
 // namespace ONNX_NAMESPACE

// #endif


// Parsed from onnx/defs/operator_sets_training.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #include "onnx/defs/schema.h"
// Targeting ../OpSet_OnnxTraining_ver1.java



// Register training operators.
@Namespace("onnx") public static native void RegisterOnnxTrainingOperatorSetSchema();

 // namespace ONNX_NAMESPACE

// Parsed from onnx/defs/data_type_utils.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #ifndef ONNX_DATA_TYPE_UTILS_H
// #define ONNX_DATA_TYPE_UTILS_H

// #include <mutex>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include "onnx/onnx_pb.h"
// String pointer as unique TypeProto identifier.
// Targeting ../DataTypeUtils.java


 // namespace Utils
 // namespace ONNX_NAMESPACE

// #endif // ! ONNX_DATA_TYPE_UTILS_H


// Parsed from onnx/defs/shape_inference.h

// #pragma once

// #include "onnx/defs/data_type_utils.h"
// #include "onnx/proto_utils.h"
// #include "onnx/string_utils.h"
// Targeting ../InferenceError.java



// #define fail_type_inference(...)
//   throw ONNX_NAMESPACE::InferenceError(
//       ONNX_NAMESPACE::MakeString("[TypeInferenceError] ", __VA_ARGS__));

// #define fail_shape_inference(...)
//   throw ONNX_NAMESPACE::InferenceError(
//       ONNX_NAMESPACE::MakeString("[ShapeInferenceError] ", __VA_ARGS__));
// Targeting ../InferenceContext.java



// This no-op inference function is used for operators without an
// inference implementation.
@Namespace("onnx") public static native void dummyInferenceFunction(@ByRef InferenceContext arg0);

@Namespace("onnx") public static native @Cast("int64_t") long getAttribute(
    @ByRef InferenceContext ctx,
    @StdString BytePointer attributeName,
    @Cast("int64_t") long defaultValue);
@Namespace("onnx") public static native @Cast("int64_t") long getAttribute(
    @ByRef InferenceContext ctx,
    @StdString String attributeName,
    @Cast("int64_t") long defaultValue);

@Namespace("onnx") public static native @StdString BytePointer getAttribute(
    @ByRef InferenceContext ctx,
    @StdString BytePointer attributeName,
    @StdString BytePointer defaultValue);
@Namespace("onnx") public static native @StdString String getAttribute(
    @ByRef InferenceContext ctx,
    @StdString String attributeName,
    @StdString String defaultValue);

@Namespace("onnx") public static native @ByVal @Name("operator *") Dimension multiply(
    @ByVal Dimension dim1,
    @ByVal Dimension dim2);

@Namespace("onnx") public static native @ByVal @Name("operator *") Dimension multiply(
    @ByVal Dimension dim1,
    @Cast("int64_t") long dim2);

@Namespace("onnx") public static native @ByVal @Name("operator /") Dimension divide(
    @ByVal Dimension dim1,
    @Cast("int64_t") long dim2);

// if from >= upto_exclusive, return 1.
// Caller must make sure upto_exclusive is less than or equal to shape.size()
// Caller must make sure from>=0
@Namespace("onnx") public static native @ByVal Dimension multiplyDims(@Const @ByRef TensorShapeProto shape, int from, int upto_exclusive);

@Namespace("onnx") public static native void propagateElemTypeWithValidation(
    @Const TypeProto input_type,
    TypeProto output_type);

@Namespace("onnx") public static native void propagateTensorElemTypeWithValidation(
    @Const TypeProto input_type,
    TypeProto output_type);

@Namespace("onnx") public static native void propagateSequenceElemTypeWithValidation(
    @Const TypeProto input_type,
    TypeProto output_type);

// propagate the element type from an input type to an output type.
// if an existing output element type exists, validate it matches.

// Note: for all methods below for propagating type or shape, callers are
// responsible to handle optional inputs/outputs and ensure that the specified
// index value is less than NumInputs/NumOutputs.

@Namespace("onnx") public static native void propagateElemTypeFromTensorInputToOutput(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long inputIndex,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void propagateElemTypeFromSequenceInputToOutput(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long inputIndex,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void propagateElemTypeFromInputToOutput(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long inputIndex,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void propagateElemTypeFromDtypeToOutput(
    @ByRef InferenceContext ctx,
    int data_type,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void propagateElemTypeFromDtypeToOutput(
    @ByRef InferenceContext ctx,
    @Const AttributeProto attr,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native @Cast("bool") boolean hasShape(@Const @ByRef TypeProto type);

@Namespace("onnx") public static native @Cast("bool") boolean hasInputShape(@ByRef InferenceContext ctx, @Cast("size_t") long n);

@Namespace("onnx") public static native @Cast("bool") boolean hasNInputShapes(@ByRef InferenceContext ctx, @Cast("size_t") long n);

@Namespace("onnx") public static native @Const @ByRef TensorShapeProto getInputShape(@ByRef InferenceContext ctx, @Cast("size_t") long n);

// Caller must make sure fromDimIndex is strictly less than shape.dim_size()
@Namespace("onnx") public static native void appendSingleDimCopiedFromInputTypeToOutputType(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long inputIndex,
    @Cast("size_t") long outputIndex,
    @Cast("size_t") long fromDimIndex);

@Namespace("onnx") public static native void propagateShapeFromInputToOutput(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long inputIndex,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void propagateShapeAndTypeFromFirstInput(@ByRef InferenceContext ctx);

@Namespace("onnx") public static native void updateOutputElemType(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long outputIndex,
    int elemType);

// Infer type of an output from the value of a specified attribute, which is
// expected to have a valid value representing a TensorProto_DataType.
@Namespace("onnx") public static native void propagateElemTypeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString BytePointer attributeName,
    @Cast("size_t") long outputIndex,
    @Cast("onnx::TensorProto_DataType") int default_value/*=TensorProto::UNDEFINED*/);
@Namespace("onnx") public static native void propagateElemTypeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString BytePointer attributeName,
    @Cast("size_t") long outputIndex);
@Namespace("onnx") public static native void propagateElemTypeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString String attributeName,
    @Cast("size_t") long outputIndex,
    @Cast("onnx::TensorProto_DataType") int default_value/*=TensorProto::UNDEFINED*/);
@Namespace("onnx") public static native void propagateElemTypeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString String attributeName,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native TensorShapeProto getOutputShape(@ByRef InferenceContext ctx, @Cast("size_t") long n);

@Namespace("onnx") public static native void appendDim(TensorShapeProto shape, @Cast("int64_t") long dim_value);

@Namespace("onnx") public static native void updateOutputShape(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long outputIndex,
    @Const @ByRef TensorShapeProto shape);

@Namespace("onnx") public static native void updateOutputShape(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long outputIndex,
    @Const @ByRef TensorProto tensorProto);

// Infer shape of an output from the value of a specified attribute, which is
// expected to be a list of integers specifying a valid shape.
@Namespace("onnx") public static native void propagateShapeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString BytePointer attributeName,
    @Cast("size_t") long outputIndex);
@Namespace("onnx") public static native void propagateShapeFromAttributeToOutput(
    @ByRef InferenceContext ctx,
    @StdString String attributeName,
    @Cast("size_t") long outputIndex);

@Namespace("onnx") public static native void multidirectionalBroadcastShapeInference(
    @Const @ByRef TensorShapeProtoVector shapes,
    @ByRef TensorShapeProto resultShape);

@Namespace("onnx") public static native void bidirectionalBroadcastShapeInference(
    @Const @ByRef TensorShapeProto shapeL,
    @Const @ByRef TensorShapeProto shapeR,
    @ByRef TensorShapeProto resultShape);

/*
Merge the dimension information from two TensorShapeProto_Dimension instances.
Values are merged into target from source.
If target has no dimension information, copy from source.
If source has no dimension information, ignore source.
If both have dimension information:
 - Prefer values over params. If both have values, values must match.
 - Prefer target param over source param if mismatched.
Fail if there are mismatches in dimension values.
Currently, there is no way to refine/update dimension information for the
source from information available in the target.
*/
@Namespace("onnx") public static native void mergeInDimensionInfo(
    @Const @ByRef Dimension source_dim,
    @ByRef Dimension target_dim,
    int dim_index);

/*
Merge shape information from a source shape into a target shape.
* merges each TensorShapeProto_Dimension separately.
* prefer values over params.
* If both have values, values must match.
* prefer target param over source param if mismatched.
* Fail if there are mismatches in number of dimensions or dimension values.
*/
@Namespace("onnx") public static native void mergeInShapeInfo(
    @Const @ByRef TensorShapeProto source,
    @ByRef TensorShapeProto target);

@Namespace("onnx") public static native void mergeInShapeInfo(
    @Const @ByRef TensorShapeProto source_shape,
    @ByRef TypeProto_Tensor target_type);

/*
Merge the shape information from two TypeProto_Tensor instances.
Values are merged into target from source.
If target has no shape information, copy from source.
If source has no shape information, ignore source.
If both have shape information:
- merge each TensorShapeProto_Dimension separately.
- Prefer values over params. If both have values, values must match.
- Prefer target param over source param if mismatched.
Fail if there are mismatches in number of dimensions or dimension values.
*/
@Namespace("onnx") public static native void mergeInShapeInfo(
    @Const @ByRef TypeProto_Tensor source,
    @ByRef TypeProto_Tensor target);

// Return a copy of a type, with a specified dimension removed from its shape.
@Namespace("onnx") public static native @ByVal TypeProto RemoveIthDimensionFromShape(
    @Const @ByRef TypeProto proto,
    int removed_dim);

// Return a copy of a type, with specified number of dimensions removed from the
// beginning.
@Namespace("onnx") public static native @ByVal TypeProto RemoveDimensionsFromShape(
    @Const @ByRef TypeProto proto,
    int num_dimensions);

// copied from GSL:
// https://github.com/Microsoft/GSL/blob/master/include/gsl/gsl_util

@Namespace("onnx") public static native void checkInputRank(@ByRef InferenceContext ctx, @Cast("size_t") long input_index, int expected_rank);

// Unification (between dimensions and/or shapes) is at the heart of
// shape-inference. The current inference algorithm can check input
// shapes/dimensions of a node and update the output shapes/dimensions. It
// cannot currently update input shapes and dimensions (even though in some
// contexts this inference is possible). Hence, we have the variants below to
// support "const" and "mutable" dimensions/shapes in unification.

@Namespace("onnx") public static native void checkDimEquality(@Cast("int64_t") long value1, @Cast("int64_t") long value2);

@Namespace("onnx") public static native void unifyDim(@Cast("const onnx::Dim*") @ByRef Dimension dim1, @Cast("const onnx::Dim*") @ByRef Dimension dim2);

// TODO: The functionality of unifyDim is similar to that of
// mergeInDimensionInfo. However, the error messages are different. Leaving this
// duplication in-place to preserve error message content.

@Namespace("onnx") public static native void unifyInputDim(
    @ByRef InferenceContext ctx,
    @Cast("size_t") long input_index,
    int dim_index,
    @Cast("onnx::Dim*") @ByRef Dimension dim);

// unifyDim: unifies a dimension with a constant value. If the dimension
// already has a value, we check for equality of new value with old value.
@Namespace("onnx") public static native void unifyDim(@Cast("onnx::Dim*") @ByRef Dimension dim, @Cast("int64_t") long value);

// target-shape = Union (target-shape, source_shape)
// Example 1: same rank, different dimensions
//    input1 shape: (2, 3, 4, 'x')
//    input2 shape: (2, 'y', 5, 'x')
//    output shape: (2, None, None, 'x')
// Example 2: different rank
//    input1 shape: (2, 3, 4, 'x')
//    input2 shape: (2, 3, 4)
//    output shape: None
@Namespace("onnx") public static native void UnionShapeInfo(
    @Const @ByRef TensorShapeProto source_shape,
    @ByRef TypeProto_Tensor target_type);

// target-type = Union (target-type, source-type)
// target and source are required to have the same type.
// Example 1: same tensor type, different shape
//    source: tensor elem_type: int64, shape: (2, 3, 4, 'x')
//    target: tensor elem_type: int64, shape: (2, 'y', 5, 'x')
//    output: tensor elem_type: int64, shape: (2, None, None, 'x')
// Example 2: same sequence type, different shape
//    source: sequence of tensor, elem_type: float, shape: (2, 3, 4)
//    target: sequence of tensor, elem_type: float, shape: None
//    output: sequence of tensor, elem_type: float, shape: None
@Namespace("onnx") public static native void UnionTypeInfo(
    @Const @ByRef TypeProto source_type,
    @ByRef TypeProto target_type);

 // namespace ONNX_NAMESPACE


// Parsed from onnx/onnx-data.pb.h

// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: onnx/onnx-data.proto

// #ifndef PROTOBUF_INCLUDED_onnx_2fonnx_2ddata_2eproto
// #define PROTOBUF_INCLUDED_onnx_2fonnx_2ddata_2eproto

// #include <limits>
// #include <string>

// #include <google/protobuf/port_def.inc>
// #if PROTOBUF_VERSION < 3007000
// #error This file was generated by a newer version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please update
// #error your headers.
// #endif
// #if 3007001 < PROTOBUF_MIN_PROTOC_VERSION
// #error This file was generated by an older version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please
// #error regenerate this file with a newer version of protoc.
// #endif

// #include <google/protobuf/port_undef.inc>
// #include <google/protobuf/io/coded_stream.h>
// #include <google/protobuf/arena.h>
// #include <google/protobuf/arenastring.h>
// #include <google/protobuf/generated_message_table_driven.h>
// #include <google/protobuf/generated_message_util.h>
// #include <google/protobuf/inlined_string_field.h>
// #include <google/protobuf/metadata.h>
// #include <google/protobuf/message.h>
// #include <google/protobuf/repeated_field.h>  // IWYU pragma: export
// #include <google/protobuf/extension_set.h>  // IWYU pragma: export
// #include <google/protobuf/generated_enum_reflection.h>
// #include <google/protobuf/unknown_field_set.h>
// #include "onnx/onnx-ml.pb.h"
// @@protoc_insertion_point(includes)
// #include <google/protobuf/port_def.inc>
// #define PROTOBUF_INTERNAL_EXPORT_onnx_2fonnx_2ddata_2eproto ONNX_API
// Targeting ../TableStruct_onnx_2fonnx_2ddata_2eproto.java


public static native void AddDescriptors_onnx_2fonnx_2ddata_2eproto();
// Targeting ../MapProtoDefaultTypeInternal.java



// Targeting ../SequenceProtoDefaultTypeInternal.java



  // namespace onnx


  // namespace protobuf
  // namespace google

/** enum onnx::SequenceProto_DataType */
public static final int
  SequenceProto_DataType_UNDEFINED = 0,
  SequenceProto_DataType_TENSOR = 1,
  SequenceProto_DataType_SPARSE_TENSOR = 2,
  SequenceProto_DataType_SEQUENCE = 3,
  SequenceProto_DataType_MAP = 4;
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::SequenceProto_DataType") int SequenceProto_DataType_DataType_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::SequenceProto_DataType") int SequenceProto_DataType_DataType_MAX();
@Namespace("onnx") @MemberGetter public static native int SequenceProto_DataType_DataType_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor SequenceProto_DataType_descriptor();
@Namespace("onnx") public static native @StdString BytePointer SequenceProto_DataType_Name(@Cast("onnx::SequenceProto_DataType") int value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::SequenceProto_DataType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString String name, @Cast("onnx::SequenceProto_DataType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::SequenceProto_DataType*") int[] value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString String name, @Cast("onnx::SequenceProto_DataType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::SequenceProto_DataType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean SequenceProto_DataType_Parse(
    @StdString String name, @Cast("onnx::SequenceProto_DataType*") int[] value);
// Targeting ../SequenceProto.java


// Targeting ../MapProto.java


// ===================================================================


// ===================================================================

// #ifdef __GNUC__
//   #pragma GCC diagnostic push
//   #pragma GCC diagnostic ignored "-Wstrict-aliasing"
// #endif  // __GNUC__
// SequenceProto

// optional string name = 1;




// #if LANG_CXX11

// #endif






// optional int32 elem_type = 2;





// repeated .onnx.TensorProto tensor_values = 3;







// repeated .onnx.SparseTensorProto sparse_tensor_values = 4;







// repeated .onnx.SequenceProto sequence_values = 5;








// repeated .onnx.MapProto map_values = 6;








// -------------------------------------------------------------------

// MapProto

// optional string name = 1;




// #if LANG_CXX11

// #endif






// optional int32 key_type = 2;





// repeated int64 keys = 3;








// repeated bytes string_keys = 4;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// optional .onnx.SequenceProto values = 5;







// #ifdef __GNUC__
//   #pragma GCC diagnostic pop
// #endif  // __GNUC__
// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

  // namespace onnx


  // namespace protobuf
  // namespace google

// @@protoc_insertion_point(global_scope)

// #include <google/protobuf/port_undef.inc>
// #endif  // PROTOBUF_INCLUDED_onnx_2fonnx_2ddata_2eproto


// Parsed from onnx/onnx-operators-ml.pb.h

// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: onnx/onnx-operators-ml.proto

// #ifndef PROTOBUF_INCLUDED_onnx_2fonnx_2doperators_2dml_2eproto
// #define PROTOBUF_INCLUDED_onnx_2fonnx_2doperators_2dml_2eproto

// #include <limits>
// #include <string>

// #include <google/protobuf/port_def.inc>
// #if PROTOBUF_VERSION < 3007000
// #error This file was generated by a newer version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please update
// #error your headers.
// #endif
// #if 3007001 < PROTOBUF_MIN_PROTOC_VERSION
// #error This file was generated by an older version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please
// #error regenerate this file with a newer version of protoc.
// #endif

// #include <google/protobuf/port_undef.inc>
// #include <google/protobuf/io/coded_stream.h>
// #include <google/protobuf/arena.h>
// #include <google/protobuf/arenastring.h>
// #include <google/protobuf/generated_message_table_driven.h>
// #include <google/protobuf/generated_message_util.h>
// #include <google/protobuf/inlined_string_field.h>
// #include <google/protobuf/metadata.h>
// #include <google/protobuf/message.h>
// #include <google/protobuf/repeated_field.h>  // IWYU pragma: export
// #include <google/protobuf/extension_set.h>  // IWYU pragma: export
// #include <google/protobuf/generated_enum_reflection.h>
// #include <google/protobuf/unknown_field_set.h>
// #include "onnx/onnx-ml.pb.h"
// @@protoc_insertion_point(includes)
// #include <google/protobuf/port_def.inc>
// #define PROTOBUF_INTERNAL_EXPORT_onnx_2fonnx_2doperators_2dml_2eproto ONNX_API
// Targeting ../TableStruct_onnx_2fonnx_2doperators_2dml_2eproto.java


public static native void AddDescriptors_onnx_2fonnx_2doperators_2dml_2eproto();
// Targeting ../FunctionProtoDefaultTypeInternal.java



// Targeting ../OperatorProtoDefaultTypeInternal.java



// Targeting ../OperatorSetProtoDefaultTypeInternal.java



  // namespace onnx



  // namespace protobuf
  // namespace google

/** enum onnx::OperatorStatus */
public static final int
  EXPERIMENTAL = 0,
  STABLE = 1;
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::OperatorStatus") int OperatorStatus_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::OperatorStatus") int OperatorStatus_MAX();
@Namespace("onnx") @MemberGetter public static native int OperatorStatus_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor OperatorStatus_descriptor();
@Namespace("onnx") public static native @StdString BytePointer OperatorStatus_Name(@Cast("onnx::OperatorStatus") int value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString BytePointer name, @Cast("onnx::OperatorStatus*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString String name, @Cast("onnx::OperatorStatus*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString BytePointer name, @Cast("onnx::OperatorStatus*") int[] value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString String name, @Cast("onnx::OperatorStatus*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString BytePointer name, @Cast("onnx::OperatorStatus*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean OperatorStatus_Parse(
    @StdString String name, @Cast("onnx::OperatorStatus*") int[] value);
// Targeting ../FunctionProto.java


// Targeting ../OperatorProto.java


// Targeting ../OperatorSetProto.java


// ===================================================================


// ===================================================================

// #ifdef __GNUC__
//   #pragma GCC diagnostic push
//   #pragma GCC diagnostic ignored "-Wstrict-aliasing"
// #endif  // __GNUC__
// FunctionProto

// optional string name = 1;




// #if LANG_CXX11

// #endif






// optional int64 since_version = 2;





// optional .onnx.OperatorStatus status = 3;





// repeated string input = 4;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated string output = 5;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated string attribute = 6;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated .onnx.NodeProto node = 7;







// optional string doc_string = 8;




// #if LANG_CXX11

// #endif






// repeated .onnx.OperatorSetIdProto opset_import = 9;







// -------------------------------------------------------------------

// OperatorProto

// optional string op_type = 1;




// #if LANG_CXX11

// #endif






// optional int64 since_version = 2;





// optional .onnx.OperatorStatus status = 3;





// optional string doc_string = 10;




// #if LANG_CXX11

// #endif






// -------------------------------------------------------------------

// OperatorSetProto

// optional string magic = 1;




// #if LANG_CXX11

// #endif






// optional int64 ir_version = 2;





// optional string ir_version_prerelease = 3;




// #if LANG_CXX11

// #endif






// optional string ir_build_metadata = 7;




// #if LANG_CXX11

// #endif






// optional string domain = 4;




// #if LANG_CXX11

// #endif






// optional int64 opset_version = 5;





// optional string doc_string = 6;




// #if LANG_CXX11

// #endif






// repeated .onnx.OperatorProto operator = 8;








// repeated .onnx.FunctionProto functions = 9;








// #ifdef __GNUC__
//   #pragma GCC diagnostic pop
// #endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

  // namespace onnx


  // namespace protobuf
  // namespace google

// @@protoc_insertion_point(global_scope)

// #include <google/protobuf/port_undef.inc>
// #endif  // PROTOBUF_INCLUDED_onnx_2fonnx_2doperators_2dml_2eproto


// Parsed from onnx/onnx-ml.pb.h

// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: onnx/onnx-ml.proto

// #ifndef PROTOBUF_INCLUDED_onnx_2fonnx_2dml_2eproto
// #define PROTOBUF_INCLUDED_onnx_2fonnx_2dml_2eproto

// #include <limits>
// #include <string>

// #include <google/protobuf/port_def.inc>
// #if PROTOBUF_VERSION < 3007000
// #error This file was generated by a newer version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please update
// #error your headers.
// #endif
// #if 3007001 < PROTOBUF_MIN_PROTOC_VERSION
// #error This file was generated by an older version of protoc which is
// #error incompatible with your Protocol Buffer headers. Please
// #error regenerate this file with a newer version of protoc.
// #endif

// #include <google/protobuf/port_undef.inc>
// #include <google/protobuf/io/coded_stream.h>
// #include <google/protobuf/arena.h>
// #include <google/protobuf/arenastring.h>
// #include <google/protobuf/generated_message_table_driven.h>
// #include <google/protobuf/generated_message_util.h>
// #include <google/protobuf/inlined_string_field.h>
// #include <google/protobuf/metadata.h>
// #include <google/protobuf/message.h>
// #include <google/protobuf/repeated_field.h>  // IWYU pragma: export
// #include <google/protobuf/extension_set.h>  // IWYU pragma: export
// #include <google/protobuf/generated_enum_reflection.h>
// #include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
// #include <google/protobuf/port_def.inc>
// #define PROTOBUF_INTERNAL_EXPORT_onnx_2fonnx_2dml_2eproto ONNX_API
// Targeting ../TableStruct_onnx_2fonnx_2dml_2eproto.java


public static native void AddDescriptors_onnx_2fonnx_2dml_2eproto();
// Targeting ../AttributeProtoDefaultTypeInternal.java



// Targeting ../GraphProtoDefaultTypeInternal.java



// Targeting ../ModelProtoDefaultTypeInternal.java



// Targeting ../NodeProtoDefaultTypeInternal.java



// Targeting ../OperatorSetIdProtoDefaultTypeInternal.java



// Targeting ../SparseTensorProtoDefaultTypeInternal.java



// Targeting ../StringStringEntryProtoDefaultTypeInternal.java



// Targeting ../TensorAnnotationDefaultTypeInternal.java



// Targeting ../TensorProtoDefaultTypeInternal.java



// Targeting ../TensorProto_SegmentDefaultTypeInternal.java



// Targeting ../TensorShapeProtoDefaultTypeInternal.java



// Targeting ../TensorShapeProto_DimensionDefaultTypeInternal.java



// Targeting ../TrainingInfoProtoDefaultTypeInternal.java



// Targeting ../TypeProtoDefaultTypeInternal.java



// Targeting ../TypeProto_MapDefaultTypeInternal.java



// Targeting ../TypeProto_OpaqueDefaultTypeInternal.java



// Targeting ../TypeProto_SequenceDefaultTypeInternal.java



// Targeting ../TypeProto_SparseTensorDefaultTypeInternal.java



// Targeting ../TypeProto_TensorDefaultTypeInternal.java



// Targeting ../ValueInfoProtoDefaultTypeInternal.java



  // namespace onnx




















  // namespace protobuf
  // namespace google

/** enum onnx::AttributeProto_AttributeType */
public static final int
  AttributeProto_AttributeType_UNDEFINED = 0,
  AttributeProto_AttributeType_FLOAT = 1,
  AttributeProto_AttributeType_INT = 2,
  AttributeProto_AttributeType_STRING = 3,
  AttributeProto_AttributeType_TENSOR = 4,
  AttributeProto_AttributeType_GRAPH = 5,
  AttributeProto_AttributeType_SPARSE_TENSOR = 11,
  AttributeProto_AttributeType_FLOATS = 6,
  AttributeProto_AttributeType_INTS = 7,
  AttributeProto_AttributeType_STRINGS = 8,
  AttributeProto_AttributeType_TENSORS = 9,
  AttributeProto_AttributeType_GRAPHS = 10,
  AttributeProto_AttributeType_SPARSE_TENSORS = 12;
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::AttributeProto_AttributeType") int AttributeProto_AttributeType_AttributeType_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::AttributeProto_AttributeType") int AttributeProto_AttributeType_AttributeType_MAX();
@Namespace("onnx") @MemberGetter public static native int AttributeProto_AttributeType_AttributeType_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor AttributeProto_AttributeType_descriptor();
@Namespace("onnx") public static native @StdString BytePointer AttributeProto_AttributeType_Name(@Cast("onnx::AttributeProto_AttributeType") int value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString BytePointer name, @Cast("onnx::AttributeProto_AttributeType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString String name, @Cast("onnx::AttributeProto_AttributeType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString BytePointer name, @Cast("onnx::AttributeProto_AttributeType*") int[] value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString String name, @Cast("onnx::AttributeProto_AttributeType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString BytePointer name, @Cast("onnx::AttributeProto_AttributeType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean AttributeProto_AttributeType_Parse(
    @StdString String name, @Cast("onnx::AttributeProto_AttributeType*") int[] value);
/** enum onnx::TensorProto_DataType */
public static final int
  TensorProto_DataType_UNDEFINED = 0,
  TensorProto_DataType_FLOAT = 1,
  TensorProto_DataType_UINT8 = 2,
  TensorProto_DataType_INT8 = 3,
  TensorProto_DataType_UINT16 = 4,
  TensorProto_DataType_INT16 = 5,
  TensorProto_DataType_INT32 = 6,
  TensorProto_DataType_INT64 = 7,
  TensorProto_DataType_STRING = 8,
  TensorProto_DataType_BOOL = 9,
  TensorProto_DataType_FLOAT16 = 10,
  TensorProto_DataType_DOUBLE = 11,
  TensorProto_DataType_UINT32 = 12,
  TensorProto_DataType_UINT64 = 13,
  TensorProto_DataType_COMPLEX64 = 14,
  TensorProto_DataType_COMPLEX128 = 15,
  TensorProto_DataType_BFLOAT16 = 16;
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::TensorProto_DataType") int TensorProto_DataType_DataType_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::TensorProto_DataType") int TensorProto_DataType_DataType_MAX();
@Namespace("onnx") @MemberGetter public static native int TensorProto_DataType_DataType_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor TensorProto_DataType_descriptor();
@Namespace("onnx") public static native @StdString BytePointer TensorProto_DataType_Name(@Cast("onnx::TensorProto_DataType") int value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataType*") int... value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataType*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataType*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataType_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataType*") int... value);
/** enum onnx::TensorProto_DataLocation */
public static final int
  TensorProto_DataLocation_DEFAULT = 0,
  TensorProto_DataLocation_EXTERNAL = 1;
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::TensorProto_DataLocation") int TensorProto_DataLocation_DataLocation_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::TensorProto_DataLocation") int TensorProto_DataLocation_DataLocation_MAX();
@Namespace("onnx") @MemberGetter public static native int TensorProto_DataLocation_DataLocation_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor TensorProto_DataLocation_descriptor();
@Namespace("onnx") public static native @StdString BytePointer TensorProto_DataLocation_Name(@Cast("onnx::TensorProto_DataLocation") int value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataLocation*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataLocation*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataLocation*") int[] value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataLocation*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString BytePointer name, @Cast("onnx::TensorProto_DataLocation*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean TensorProto_DataLocation_Parse(
    @StdString String name, @Cast("onnx::TensorProto_DataLocation*") int[] value);
/** enum onnx::Version */
public static final int
  _START_VERSION = 0,
  IR_VERSION_2017_10_10 = 1,
  IR_VERSION_2017_10_30 = 2,
  IR_VERSION_2017_11_3 = 3,
  IR_VERSION_2019_1_22 = 4,
  IR_VERSION_2019_3_18 = 5,
  IR_VERSION_2019_9_19 = 6,
  IR_VERSION = 7;
@Namespace("onnx") public static native @Cast("bool") boolean Version_IsValid(int value);
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::Version") int Version_MIN();
@Namespace("onnx") @MemberGetter public static native @Cast("const onnx::Version") int Version_MAX();
@Namespace("onnx") @MemberGetter public static native int Version_ARRAYSIZE();

@Namespace("onnx") public static native @Const EnumDescriptor Version_descriptor();
@Namespace("onnx") public static native @StdString BytePointer Version_Name(@Cast("onnx::Version") int value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString BytePointer name, @Cast("onnx::Version*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString String name, @Cast("onnx::Version*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString BytePointer name, @Cast("onnx::Version*") int[] value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString String name, @Cast("onnx::Version*") IntPointer value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString BytePointer name, @Cast("onnx::Version*") IntBuffer value);
@Namespace("onnx") public static native @Cast("bool") boolean Version_Parse(
    @StdString String name, @Cast("onnx::Version*") int[] value);
// Targeting ../AttributeProto.java


// Targeting ../ValueInfoProto.java


// Targeting ../NodeProto.java


// Targeting ../TrainingInfoProto.java


// Targeting ../ModelProto.java


// Targeting ../StringStringEntryProto.java


// Targeting ../TensorAnnotation.java


// Targeting ../GraphProto.java


// Targeting ../TensorProto_Segment.java


// Targeting ../TensorProto.java


// Targeting ../SparseTensorProto.java


// Targeting ../Dimension.java


// Targeting ../TensorShapeProto.java


// Targeting ../TypeProto_Tensor.java


// Targeting ../TypeProto_Sequence.java


// Targeting ../TypeProto_Map.java


// Targeting ../TypeProto_SparseTensor.java


// Targeting ../TypeProto_Opaque.java


// Targeting ../TypeProto.java


// Targeting ../OperatorSetIdProto.java


// ===================================================================


// ===================================================================

// #ifdef __GNUC__
//   #pragma GCC diagnostic push
//   #pragma GCC diagnostic ignored "-Wstrict-aliasing"
// #endif  // __GNUC__
// AttributeProto

// optional string name = 1;




// #if LANG_CXX11

// #endif






// optional string ref_attr_name = 21;




// #if LANG_CXX11

// #endif






// optional string doc_string = 13;




// #if LANG_CXX11

// #endif






// optional .onnx.AttributeProto.AttributeType type = 20;





// optional float f = 2;





// optional int64 i = 3;





// optional bytes s = 4;




// #if LANG_CXX11

// #endif






// optional .onnx.TensorProto t = 5;







// optional .onnx.GraphProto g = 6;







// optional .onnx.SparseTensorProto sparse_tensor = 22;







// repeated float floats = 7;








// repeated int64 ints = 8;








// repeated bytes strings = 9;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated .onnx.TensorProto tensors = 10;








// repeated .onnx.GraphProto graphs = 11;








// repeated .onnx.SparseTensorProto sparse_tensors = 23;








// -------------------------------------------------------------------

// ValueInfoProto

// optional string name = 1;




// #if LANG_CXX11

// #endif






// optional .onnx.TypeProto type = 2;







// optional string doc_string = 3;




// #if LANG_CXX11

// #endif






// -------------------------------------------------------------------

// NodeProto

// repeated string input = 1;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated string output = 2;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// optional string name = 3;




// #if LANG_CXX11

// #endif






// optional string op_type = 4;




// #if LANG_CXX11

// #endif






// optional string domain = 7;




// #if LANG_CXX11

// #endif






// repeated .onnx.AttributeProto attribute = 5;








// optional string doc_string = 6;




// #if LANG_CXX11

// #endif






// -------------------------------------------------------------------

// TrainingInfoProto

// optional .onnx.GraphProto initialization = 1;







// optional .onnx.GraphProto algorithm = 2;







// repeated .onnx.StringStringEntryProto initialization_binding = 3;








// repeated .onnx.StringStringEntryProto update_binding = 4;








// -------------------------------------------------------------------

// ModelProto

// optional int64 ir_version = 1;





// repeated .onnx.OperatorSetIdProto opset_import = 8;








// optional string producer_name = 2;




// #if LANG_CXX11

// #endif






// optional string producer_version = 3;




// #if LANG_CXX11

// #endif






// optional string domain = 4;




// #if LANG_CXX11

// #endif






// optional int64 model_version = 5;





// optional string doc_string = 6;




// #if LANG_CXX11

// #endif






// optional .onnx.GraphProto graph = 7;







// repeated .onnx.StringStringEntryProto metadata_props = 14;








// repeated .onnx.TrainingInfoProto training_info = 20;








// -------------------------------------------------------------------

// StringStringEntryProto

// optional string key = 1;




// #if LANG_CXX11

// #endif






// optional string value = 2;




// #if LANG_CXX11

// #endif






// -------------------------------------------------------------------

// TensorAnnotation

// optional string tensor_name = 1;




// #if LANG_CXX11

// #endif






// repeated .onnx.StringStringEntryProto quant_parameter_tensor_names = 2;








// -------------------------------------------------------------------

// GraphProto

// repeated .onnx.NodeProto node = 1;








// optional string name = 2;




// #if LANG_CXX11

// #endif






// repeated .onnx.TensorProto initializer = 5;








// repeated .onnx.SparseTensorProto sparse_initializer = 15;








// optional string doc_string = 10;




// #if LANG_CXX11

// #endif






// repeated .onnx.ValueInfoProto input = 11;








// repeated .onnx.ValueInfoProto output = 12;








// repeated .onnx.ValueInfoProto value_info = 13;








// repeated .onnx.TensorAnnotation quantization_annotation = 14;








// -------------------------------------------------------------------

// TensorProto_Segment

// optional int64 begin = 1;





// optional int64 end = 2;





// -------------------------------------------------------------------

// TensorProto

// repeated int64 dims = 1;








// optional int32 data_type = 2;





// optional .onnx.TensorProto.Segment segment = 3;







// repeated float float_data = 4 [packed = true];








// repeated int32 int32_data = 5 [packed = true];








// repeated bytes string_data = 6;





// #if LANG_CXX11

// #endif




// #if LANG_CXX11

// #endif





// repeated int64 int64_data = 7 [packed = true];








// optional string name = 8;




// #if LANG_CXX11

// #endif






// optional string doc_string = 12;




// #if LANG_CXX11

// #endif






// optional bytes raw_data = 9;




// #if LANG_CXX11

// #endif






// repeated .onnx.StringStringEntryProto external_data = 13;








// optional .onnx.TensorProto.DataLocation data_location = 14;





// repeated double double_data = 10 [packed = true];








// repeated uint64 uint64_data = 11 [packed = true];








// -------------------------------------------------------------------

// SparseTensorProto

// optional .onnx.TensorProto values = 1;







// optional .onnx.TensorProto indices = 2;







// repeated int64 dims = 3;








// -------------------------------------------------------------------

// TensorShapeProto_Dimension

// optional int64 dim_value = 1;






// optional string dim_param = 2;





// #if LANG_CXX11

// #endif






// optional string denotation = 3;




// #if LANG_CXX11

// #endif









// -------------------------------------------------------------------

// TensorShapeProto

// repeated .onnx.TensorShapeProto.Dimension dim = 1;








// -------------------------------------------------------------------

// TypeProto_Tensor

// optional int32 elem_type = 1;





// optional .onnx.TensorShapeProto shape = 2;







// -------------------------------------------------------------------

// TypeProto_Sequence

// optional .onnx.TypeProto elem_type = 1;







// -------------------------------------------------------------------

// TypeProto_Map

// optional int32 key_type = 1;





// optional .onnx.TypeProto value_type = 2;







// -------------------------------------------------------------------

// TypeProto_SparseTensor

// optional int32 elem_type = 1;





// optional .onnx.TensorShapeProto shape = 2;







// -------------------------------------------------------------------

// TypeProto_Opaque

// optional string domain = 1;




// #if LANG_CXX11

// #endif






// optional string name = 2;




// #if LANG_CXX11

// #endif






// -------------------------------------------------------------------

// TypeProto

// optional .onnx.TypeProto.Tensor tensor_type = 1;







// optional .onnx.TypeProto.Sequence sequence_type = 4;







// optional .onnx.TypeProto.Map map_type = 5;







// optional .onnx.TypeProto.SparseTensor sparse_tensor_type = 8;







// optional .onnx.TypeProto.Opaque opaque_type = 7;







// optional string denotation = 6;




// #if LANG_CXX11

// #endif









// -------------------------------------------------------------------

// OperatorSetIdProto

// optional string domain = 1;




// #if LANG_CXX11

// #endif






// optional int64 version = 2;





// #ifdef __GNUC__
//   #pragma GCC diagnostic pop
// #endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

  // namespace onnx





  // namespace protobuf
  // namespace google

// @@protoc_insertion_point(global_scope)

// #include <google/protobuf/port_undef.inc>
// #endif  // PROTOBUF_INCLUDED_onnx_2fonnx_2dml_2eproto


// Parsed from google/protobuf/arena.h

// Protocol Buffers - Google's data interchange format
// Copyright 2008 Google Inc.  All rights reserved.
// https://developers.google.com/protocol-buffers/
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
//     * Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//     * Redistributions in binary form must reproduce the above
// copyright notice, this list of conditions and the following disclaimer
// in the documentation and/or other materials provided with the
// distribution.
//     * Neither the name of Google Inc. nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// This file defines an Arena allocator for better allocation performance.

// #ifndef GOOGLE_PROTOBUF_ARENA_H__
// #define GOOGLE_PROTOBUF_ARENA_H__

// #include <limits>
// #ifdef max
// #undef max  // Visual Studio defines this macro
// #endif
// #if defined(_MSC_VER) && !defined(_LIBCPP_STD_VER) && !_HAS_EXCEPTIONS
// Work around bugs in MSVC <typeinfo> header when _HAS_EXCEPTIONS=0.
// #include <exception>
// #include <typeinfo>

// #else
// #include <typeinfo>
// #endif

// #include <google/protobuf/arena_impl.h>
// #include <google/protobuf/port.h>
// #include <type_traits>

// #include <google/protobuf/port_def.inc>

// #ifdef SWIG
// #error "You cannot SWIG proto headers"
// #endif  // defined below

  // namespace protobuf
  // namespace google          // defined below        // defined in message.h

@Namespace("google::protobuf::arena_metrics") public static native void EnableArenaMetrics(ArenaOptions options);


// Targeting ../ArenaStringPtr.java


// Targeting ../LazyField.java

           // defined in lazy_field.h  // defined in repeated_field.h

// Templated cleanup methods.
@Namespace("google::protobuf::internal") public static native void arena_free(Pointer object, @Cast("size_t") long size);


// Targeting ../ArenaOptions.java



// Support for non-RTTI environments. (The metrics hooks API uses type
// information.)
// #if PROTOBUF_RTTI
// #define RTTI_TYPE_ID(type) (&typeid(type))
// #else
// #define RTTI_TYPE_ID(type) (NULL)
// Targeting ../Arena.java



// Defined above for supporting environments without RTTI.
// #undef RTTI_TYPE_ID

  // namespace protobuf
  // namespace google

// #include <google/protobuf/port_undef.inc>

// #endif  // GOOGLE_PROTOBUF_ARENA_H__


// Parsed from google/protobuf/message_lite.h

// Protocol Buffers - Google's data interchange format
// Copyright 2008 Google Inc.  All rights reserved.
// https://developers.google.com/protocol-buffers/
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
//     * Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//     * Redistributions in binary form must reproduce the above
// copyright notice, this list of conditions and the following disclaimer
// in the documentation and/or other materials provided with the
// distribution.
//     * Neither the name of Google Inc. nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// Authors: wink@google.com (Wink Saville),
//          kenton@google.com (Kenton Varda)
//  Based on original Protocol Buffers design by
//  Sanjay Ghemawat, Jeff Dean, and others.
//
// Defines MessageLite, the abstract interface implemented by all (lite
// and non-lite) protocol message objects.

// #ifndef GOOGLE_PROTOBUF_MESSAGE_LITE_H__
// #define GOOGLE_PROTOBUF_MESSAGE_LITE_H__

// #include <climits>
// #include <string>
// #include <google/protobuf/stubs/common.h>
// #include <google/protobuf/stubs/logging.h>
// #include <google/protobuf/arena.h>
// #include <google/protobuf/stubs/once.h>
// #include <google/protobuf/port.h>
// #include <google/protobuf/stubs/strutil.h>


// #include <google/protobuf/port_def.inc>

// #ifdef SWIG
// #error "You cannot SWIG proto headers"
// #endif
// Targeting ../CodedInputStream.java


// Targeting ../CodedOutputStream.java


// Targeting ../ZeroCopyInputStream.java


// Targeting ../ZeroCopyOutputStream.java



  // namespace io
// Targeting ../RepeatedPtrFieldBase.java


// Targeting ../WireFormatLite.java


// Targeting ../WeakFieldMap.java



// We compute sizes as size_t but cache them as int.  This function converts a
// computed size to a cached size.  Since we don't proceed with serialization
// if the total size was > INT_MAX, it is not important what this function
// returns for inputs > INT_MAX.  However this case should not error or
// GOOGLE_CHECK-fail, because the full size_t resolution is still returned from
// ByteSizeLong() and checked against INT_MAX; we can catch the overflow
// there.
@Namespace("google::protobuf::internal") public static native int ToCachedSize(@Cast("size_t") long size);

// We mainly calculate sizes in terms of size_t, but some functions that
// compute sizes return "int".  These int sizes are expected to always be
// positive. This function is more efficient than casting an int to size_t
// directly on 64-bit platforms because it avoids making the compiler emit a
// sign extending instruction, which we don't want and don't want to pay for.
@Namespace("google::protobuf::internal") public static native @Cast("size_t") long FromIntSize(int size);

// For cases where a legacy function returns an integer size.  We GOOGLE_DCHECK()
// that the conversion will fit within an integer; if this is false then we
// are losing information.
@Namespace("google::protobuf::internal") public static native int ToIntSize(@Cast("size_t") long size);

// This type wraps a variable whose constructor and destructor are explicitly
// called. It is particularly useful for a global variable, without its
// constructor and destructor run on start and end of the program lifetime.
// This circumvents the initial construction order fiasco, while keeping
// the address of the empty string a compile time constant.
//
// Pay special attention to the initialization state of the object.
// 1. The object is "uninitialized" to begin with.
// 2. Call DefaultConstruct() only if the object is uninitialized.
//    After the call, the object becomes "initialized".
// 3. Call get() and get_mutable() only if the object is initialized.
// 4. Call Destruct() only if the object is initialized.
//    After the call, the object becomes uninitialized.

// Default empty string object. Don't use this directly. Instead, call
// GetEmptyString() to get the reference.


@Namespace("google::protobuf::internal") public static native @StdString BytePointer GetEmptyStringAlreadyInited();

@Namespace("google::protobuf::internal") public static native @Cast("size_t") long StringSpaceUsedExcludingSelfLong(@StdString BytePointer str);
@Namespace("google::protobuf::internal") public static native @Cast("size_t") long StringSpaceUsedExcludingSelfLong(@StdString String str);


// Targeting ../MessageLite.java










// Targeting ../BoundedZCIS.java









  // namespace internal



  // namespace protobuf
  // namespace google

// #include <google/protobuf/port_undef.inc>

// #endif  // GOOGLE_PROTOBUF_MESSAGE_LITE_H__


// Parsed from google/protobuf/unknown_field_set.h

// Protocol Buffers - Google's data interchange format
// Copyright 2008 Google Inc.  All rights reserved.
// https://developers.google.com/protocol-buffers/
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
//     * Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//     * Redistributions in binary form must reproduce the above
// copyright notice, this list of conditions and the following disclaimer
// in the documentation and/or other materials provided with the
// distribution.
//     * Neither the name of Google Inc. nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// Author: kenton@google.com (Kenton Varda)
//  Based on original Protocol Buffers design by
//  Sanjay Ghemawat, Jeff Dean, and others.
//
// Contains classes used to keep track of unrecognized fields seen while
// parsing a protocol message.

// #ifndef GOOGLE_PROTOBUF_UNKNOWN_FIELD_SET_H__
// #define GOOGLE_PROTOBUF_UNKNOWN_FIELD_SET_H__

// #include <assert.h>
// #include <string>
// #include <vector>
// #include <google/protobuf/stubs/common.h>
// #include <google/protobuf/stubs/logging.h>
// #include <google/protobuf/parse_context.h>
// #include <google/protobuf/io/coded_stream.h>
// #include <google/protobuf/message_lite.h>
// #include <google/protobuf/port.h>

// #include <google/protobuf/port_def.inc>

// #ifdef SWIG
// #error "You cannot SWIG proto headers"
// #endif         // coded_stream.h        // coded_stream.h      // zero_copy_stream.h
  
// Targeting ../InternalMetadataWithArena.java


// Targeting ../WireFormat.java


// Targeting ../MessageSetFieldSkipperUsingCord.java


                                    // extension_set_heavy.cc
                        // message.h
// Targeting ../UnknownFieldSet.java


// Targeting ../UnknownField.java



// ===================================================================
// inline implementations











































  // namespace protobuf
  // namespace google

// #include <google/protobuf/port_undef.inc>
// #endif  // GOOGLE_PROTOBUF_UNKNOWN_FIELD_SET_H__


// Parsed from google/protobuf/descriptor.h

// Protocol Buffers - Google's data interchange format
// Copyright 2008 Google Inc.  All rights reserved.
// https://developers.google.com/protocol-buffers/
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
//     * Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//     * Redistributions in binary form must reproduce the above
// copyright notice, this list of conditions and the following disclaimer
// in the documentation and/or other materials provided with the
// distribution.
//     * Neither the name of Google Inc. nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// Author: kenton@google.com (Kenton Varda)
//  Based on original Protocol Buffers design by
//  Sanjay Ghemawat, Jeff Dean, and others.
//
// This file contains classes which describe a type of protocol message.
// You can use a message's descriptor to learn at runtime what fields
// it contains and what the types of those fields are.  The Message
// interface also allows you to dynamically access and modify individual
// fields by passing the FieldDescriptor of the field you are interested
// in.
//
// Most users will not care about descriptors, because they will write
// code specific to certain protocol types and will simply use the classes
// generated by the protocol compiler directly.  Advanced users who want
// to operate on arbitrary types (not known at compile time) may want to
// read descriptors in order to learn about the contents of a message.
// A very small number of users will want to construct their own
// Descriptors, either because they are implementing Message manually or
// because they are writing something like the protocol compiler.
//
// For an example of how you might use descriptors, see the code example
// at the top of message.h.

// #ifndef GOOGLE_PROTOBUF_DESCRIPTOR_H__
// #define GOOGLE_PROTOBUF_DESCRIPTOR_H__

// #include <memory>
// #include <set>
// #include <string>
// #include <vector>
// #include <google/protobuf/stubs/common.h>
// #include <google/protobuf/stubs/mutex.h>
// #include <google/protobuf/stubs/once.h>

// #include <google/protobuf/port_def.inc>

// TYPE_BOOL is defined in the MacOS's ConditionalMacros.h.
// #ifdef TYPE_BOOL
// #undef TYPE_BOOL
// #endif  // TYPE_BOOL

// #ifdef SWIG
// #define PROTOBUF_EXPORT
// #endif
// Targeting ../DescriptorDatabase.java


// Targeting ../DescriptorProto.java


// Targeting ../DescriptorProto_ExtensionRange.java


// Targeting ../FieldDescriptorProto.java


// Targeting ../OneofDescriptorProto.java


// Targeting ../EnumDescriptorProto.java


// Targeting ../EnumValueDescriptorProto.java


// Targeting ../ServiceDescriptorProto.java


// Targeting ../MethodDescriptorProto.java


// Targeting ../FileDescriptorProto.java


// Targeting ../MessageOptions.java


// Targeting ../FieldOptions.java


// Targeting ../OneofOptions.java


// Targeting ../EnumOptions.java


// Targeting ../EnumValueOptions.java


// Targeting ../ExtensionRangeOptions.java


// Targeting ../ServiceOptions.java


// Targeting ../MethodOptions.java


// Targeting ../FileOptions.java


// Targeting ../UninterpretedOption.java


// Targeting ../SourceCodeInfo.java



// Defined in message.h
// Targeting ../DescriptorBuilder.java


// Targeting ../FileDescriptorTables.java


// Targeting ../Symbol.java



// Defined in unknown_field_set.h.
// Targeting ../GeneratedMessageReflection.java



// Targeting ../CommandLineInterface.java


// Targeting ../Formatter.java


  // namespace cpp

// Targeting ../DescriptorTest.java



// Targeting ../Printer.java



// Targeting ../SourceLocation.java


// Targeting ../DebugStringOptions.java


// Targeting ../LazyDescriptor.java


  // namespace internal

// Describes a type of protocol message, or a particular group within a
// message.  To obtain the Descriptor for a given message object, call
// Message::GetDescriptor().  Generated message classes also have a
// static method called descriptor() which returns the type's descriptor.
// Use DescriptorPool to construct your own descriptors.
// Targeting ../FieldDescriptor.java


// Targeting ../OneofDescriptor.java


// Targeting ../EnumDescriptor.java


// Targeting ../EnumValueDescriptor.java


// Targeting ../ServiceDescriptor.java


// Targeting ../MethodDescriptor.java


// Targeting ../FileDescriptor.java


// Targeting ../DescriptorPool.java




// inline methods ====================================================

// These macros makes this repetitive code more readable.
// #define PROTOBUF_DEFINE_ACCESSOR(CLASS, FIELD, TYPE)
//   inline TYPE CLASS::FIELD() const { return FIELD##_; }

// Strings fields are stored as pointers but returned as const references.
// #define PROTOBUF_DEFINE_STRING_ACCESSOR(CLASS, FIELD)
//   inline const std::string& CLASS::FIELD() const { return *FIELD##_; }

// Arrays take an index parameter, obviously.
// #define PROTOBUF_DEFINE_ARRAY_ACCESSOR(CLASS, FIELD, TYPE)
//   inline TYPE CLASS::FIELD(int index) const { return FIELD##s_ + index; }

// #define PROTOBUF_DEFINE_OPTIONS_ACCESSOR(CLASS, TYPE)
//   inline const TYPE& CLASS::options() const { return *options_; }













































































































// #undef PROTOBUF_DEFINE_ACCESSOR
// #undef PROTOBUF_DEFINE_STRING_ACCESSOR
// #undef PROTOBUF_DEFINE_ARRAY_ACCESSOR

// A few accessors differ from the macros...







// Can't use PROTOBUF_DEFINE_ARRAY_ACCESSOR because reserved_names_ is actually
// an array of pointers rather than the usual array of objects.






// Can't use PROTOBUF_DEFINE_ARRAY_ACCESSOR because reserved_names_ is actually
// an array of pointers rather than the usual array of objects.














// To save space, index() is computed by looking at the descriptor's position
// in the parent's array of children.








































// Can't use PROTOBUF_DEFINE_ARRAY_ACCESSOR because fields_ is actually an array
// of pointers rather than the usual array of objects.


  // namespace protobuf
  // namespace google

// #include <google/protobuf/port_undef.inc>

// #endif  // GOOGLE_PROTOBUF_DESCRIPTOR_H__


// Parsed from onnx/proto_utils.h

// #pragma once

// #include <google/protobuf/io/coded_stream.h>
// #include <google/protobuf/io/zero_copy_stream_impl_lite.h>

// #include "onnx/onnx_pb.h"

// #ifdef ONNX_USE_LITE_PROTO
// #include <google/protobuf/message_lite.h>
// #else // ONNX_USE_LITE_PROTO
// #include <google/protobuf/message.h>
// #endif  // !ONNX_USE_LITE_PROTO

// #ifdef ONNX_USE_LITE_PROTO
@Namespace("onnx") public static native @StdString BytePointer ProtoDebugString(@Const @ByRef MessageLite proto);
// #else
// #endif

@Namespace("onnx") public static native @Cast("bool") boolean ParseProtoFromBytes(MessageLite proto, @Cast("const char*") BytePointer buffer, @Cast("size_t") long length);
@Namespace("onnx") public static native @Cast("bool") boolean ParseProtoFromBytes(MessageLite proto, String buffer, @Cast("size_t") long length);

@Namespace("onnx") public static native @ByVal @Name("RetrieveValues<int64_t>") LongVector RetrieveValuesLong(@Const @ByRef AttributeProto attr);

@Namespace("onnx") public static native @ByVal @Name("RetrieveValues<std::string>") StringVector RetrieveValuesString(@Const @ByRef AttributeProto attr);

 // namespace ONNX_NAMESPACE


// Parsed from onnx/checker.h

// #pragma once

// #include <stdexcept>
// #include <unordered_map>
// #include <unordered_set>
// #include "onnx/defs/function.h"
// #include "onnx/defs/schema.h"
// #include "onnx/onnx-operators_pb.h"
// #include "onnx/onnx-data_pb.h"
// #include "onnx/onnx_pb.h"
// #include "onnx/string_utils.h"
// Targeting ../ValidationError.java



// #define fail_check(...)
//   throw ONNX_NAMESPACE::checker::ValidationError(
//       ONNX_NAMESPACE::MakeString(__VA_ARGS__));
// Targeting ../CheckerContext.java


// Targeting ../LexicalScopeContext.java


@Namespace("onnx::checker") public static native void check_value_info(@Const @ByRef ValueInfoProto value_info, @Const @ByRef CheckerContext arg1);
@Namespace("onnx::checker") public static native void check_tensor(@Const @ByRef TensorProto tensor, @Const @ByRef CheckerContext arg1);
@Namespace("onnx::checker") public static native void check_sparse_tensor(
    @Const @ByRef SparseTensorProto sparse_tensor,
    @Const @ByRef CheckerContext arg1);
@Namespace("onnx::checker") public static native void check_sequence(
    @Const @ByRef SequenceProto sequence,
    @Const @ByRef CheckerContext arg1);
@Namespace("onnx::checker") public static native void check_map(
    @Const @ByRef MapProto map,
    @Const @ByRef CheckerContext arg1);
@Namespace("onnx::checker") public static native void check_attribute(
    @Const @ByRef AttributeProto attr,
    @Const @ByRef CheckerContext arg1,
    @Const @ByRef LexicalScopeContext arg2);
@Namespace("onnx::checker") public static native void check_node(
    @Const @ByRef NodeProto node,
    @Const @ByRef CheckerContext arg1,
    @Const @ByRef LexicalScopeContext arg2);
@Namespace("onnx::checker") public static native void check_graph(
    @Const @ByRef GraphProto graph,
    @Const @ByRef CheckerContext arg1,
    @Const @ByRef LexicalScopeContext arg2);
@Namespace("onnx::checker") public static native void check_function(
    @Const @ByRef FunctionProto function,
    @Const @ByRef CheckerContext arg1,
    @Const @ByRef LexicalScopeContext arg2);

@Namespace("onnx::checker") public static native void check_model(@Const @ByRef ModelProto model);
@Namespace("onnx::checker") public static native void check_model(@StdString BytePointer model_path);
@Namespace("onnx::checker") public static native void check_model(@StdString String model_path);

@Namespace("onnx::checker") public static native @Cast("bool") boolean check_is_experimental_op(@StdString BytePointer node_op_type);
@Namespace("onnx::checker") public static native @Cast("bool") boolean check_is_experimental_op(@StdString String node_op_type);

 // namespace checker
 // namespace ONNX_NAMESPACE


// Parsed from onnx/shape_inference/implementation.h

// #pragma once

// #include "onnx/defs/function.h"
// #include "onnx/defs/schema.h"
// #include "onnx/proto_utils.h"
// #include "onnx/string_utils.h"

@Namespace("onnx::shape_inference") public static native void checkShapesAndTypes(
    @Const @ByRef TypeProto_Tensor inferredType,
    @Const @ByRef TypeProto_Tensor existingType);

@Namespace("onnx::shape_inference") public static native void checkShapesAndTypes(
    @Const @ByRef TypeProto_Sequence inferredType,
    @Const @ByRef TypeProto_Sequence existingType);

@Namespace("onnx::shape_inference") public static native void checkShapesAndTypes(
    @Const @ByRef TypeProto inferredType,
    @Const @ByRef TypeProto existingType);

@Namespace("onnx::shape_inference") public static native void mergeShapesAndTypes(
    @Const @ByRef TypeProto_Tensor inferredType,
    TypeProto_Tensor existingType);

@Namespace("onnx::shape_inference") public static native void mergeShapesAndTypes(
    @Const @ByRef TypeProto_Sequence inferredType,
    TypeProto_Tensor existingType);

@Namespace("onnx::shape_inference") public static native void mergeShapesAndTypes(
    @Const @ByRef TypeProto inferredType,
    TypeProto existingType);

@Namespace("onnx::shape_inference") public static native void InferShapes(
    @ByRef ModelProto m,
    @Cast("const bool") boolean check_type/*=false*/,
    @Const ISchemaRegistry schema_registry/*=onnx::OpSchemaRegistry::Instance()*/,
    int error_mode/*=0*/
    );
@Namespace("onnx::shape_inference") public static native void InferShapes(
    @ByRef ModelProto m
    );

@Namespace("onnx::shape_inference") public static native void InferShapes(
    GraphProto g,
    @Const @ByRef StringIntMap opset_imports,
    @Cast("const bool") boolean check_type/*=false*/,
    @Const ISchemaRegistry schema_registry/*=onnx::OpSchemaRegistry::Instance()*/,
    int error_mode/*=0*/
    );
@Namespace("onnx::shape_inference") public static native void InferShapes(
    GraphProto g,
    @Const @ByRef StringIntMap opset_imports
    );

@Namespace("onnx::shape_inference") public static native void InferShapes(
    @StdString BytePointer model_path,
    @Cast("const bool") boolean check_type/*=false*/,
    @StdString BytePointer save_path/*=""*/,
    @Const ISchemaRegistry schema_registry/*=onnx::OpSchemaRegistry::Instance()*/,
    int error_mode/*=0*/
    );
@Namespace("onnx::shape_inference") public static native void InferShapes(
    @StdString BytePointer model_path
    );
@Namespace("onnx::shape_inference") public static native void InferShapes(
    @StdString String model_path,
    @Cast("const bool") boolean check_type/*=false*/,
    @StdString String save_path/*=""*/,
    @Const ISchemaRegistry schema_registry/*=onnx::OpSchemaRegistry::Instance()*/,
    int error_mode/*=0*/
    );
@Namespace("onnx::shape_inference") public static native void InferShapes(
    @StdString String model_path
    );

@Namespace("onnx::shape_inference") public static native void InferShapeForFunctionNode(
    @Const FunctionProto func,
    @Const ISchemaRegistry schema_registry,
    @ByRef InferenceContext ctx);

@Namespace("onnx::shape_inference") public static native @StdString BytePointer getErrorWithNodeInfo(@ByVal NodeProto n, @ByVal @Cast("std::runtime_error*") Pointer err);

@Namespace("onnx::shape_inference") public static native void deleteCreatedTypes(@ByVal TypeProtoVector initializerTypeList);

 // namespace shape_inference
 // namespace ONNX_NAMESPACE

// Parsed from onnx/onnxifi.h

// #ifndef ONNXIFI_H
public static final int ONNXIFI_H = 1;

// #ifdef __cplusplus
// #endif

// #if defined(_WIN32) && defined(_M_IX86)
/* Windows x86 */
// #define ONNXIFI_ABI __stdcall
// #elif defined(__i386__)
/* Linux x86 */
// #define ONNXIFI_ABI __attribute__((__cdecl__))
// #else
// #define ONNXIFI_ABI
// #endif

// #ifndef ONNXIFI_PUBLIC
// #if defined(__ELF__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(__MACH__)
// #define ONNXIFI_PUBLIC __attribute__((__visibility__("default")))
// #elif defined(_WIN32) && defined(__GNUC__)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __attribute__((__dllexport__))
// #else
// #define ONNXIFI_PUBLIC __attribute__((__dllimport__))
// #endif
// #elif defined(_WIN32)
// #ifdef ONNXIFI_BUILD_LIBRARY
// #define ONNXIFI_PUBLIC __declspec(dllexport)
// #else
// #define ONNXIFI_PUBLIC __declspec(dllimport)
// #endif
// #else
// #define ONNXIFI_PUBLIC
// #endif
// #endif

// #ifndef ONNXIFI_CHECK_RESULT
//   #if defined(__GNUC__) && (__GNUC__ >= 4)
//     #define ONNXIFI_CHECK_RESULT __attribute__((__warn_unused_result__))
//   #elif defined(_MSC_VER) && (_MSC_VER >= 1700)
//     #define ONNXIFI_CHECK_RESULT _Check_return_
//   #else
//     #define ONNXIFI_CHECK_RESULT
//   #endif
// #endif

// #include <stddef.h>

// #if !defined(ONNXIFI_NO_STDINT_H)
// #if defined(_MSC_VER) && (_MSC_VER < 1600)
// #else
// #include <stdint.h>
// Targeting ../onnxBackendID.java


// Targeting ../onnxBackend.java


// Targeting ../onnxGraph.java


// Targeting ../onnxEvent.java



/** Return code for ONNXIFI functions */
/**
 * Type for enumeration values.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for bit fields.
 *
 * The low 32 bits are reserved for standardized ONNXIFI values.
 * The high 32 bits are reserved for vendor-specific extensions. Applications
 * must check for specific vendor extensions before interpreting these bits.
 */
/**
 * Type for pointers or handles for memory buffers.
 * This type is intended to work not only for CPU-addressable memory, but also
 * for device memory. uint64_t ensures the API can accommodate Vulkan buffers.
 */

public static final int ONNXIFI_STATUS_SUCCESS = 0x0000;
public static final int ONNXIFI_STATUS_FALLBACK = 0x0001;
public static final int ONNXIFI_STATUS_INVALID_ID = 0x0101;
public static final int ONNXIFI_STATUS_INVALID_SIZE = 0x0102;
public static final int ONNXIFI_STATUS_INVALID_POINTER = 0x0103;
public static final int ONNXIFI_STATUS_INVALID_PROTOBUF = 0x0104;
public static final int ONNXIFI_STATUS_INVALID_MODEL = 0x0105;
public static final int ONNXIFI_STATUS_INVALID_BACKEND = 0x0106;
public static final int ONNXIFI_STATUS_INVALID_GRAPH = 0x0107;
public static final int ONNXIFI_STATUS_INVALID_EVENT = 0x0108;
public static final int ONNXIFI_STATUS_INVALID_STATE = 0x0109;
public static final int ONNXIFI_STATUS_INVALID_NAME = 0x010A;
public static final int ONNXIFI_STATUS_INVALID_SHAPE = 0x010B;
public static final int ONNXIFI_STATUS_INVALID_DATATYPE = 0x010C;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_TYPE = 0x010D;
public static final int ONNXIFI_STATUS_INVALID_MEMORY_LOCATION = 0x010E;
public static final int ONNXIFI_STATUS_INVALID_FENCE_TYPE = 0x010F;
public static final int ONNXIFI_STATUS_INVALID_PROPERTY = 0x0110;
public static final int ONNXIFI_STATUS_UNSUPPORTED_TAG = 0x0201;
public static final int ONNXIFI_STATUS_UNSUPPORTED_VERSION = 0x0202;
public static final int ONNXIFI_STATUS_UNSUPPORTED_OPERATOR = 0x0203;
public static final int ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE = 0x0204;
public static final int ONNXIFI_STATUS_UNSUPPORTED_SHAPE = 0x0205;
public static final int ONNXIFI_STATUS_UNSUPPORTED_DATATYPE = 0x0206;
public static final int ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE = 0x0207;
public static final int ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE = 0x0208;
public static final int ONNXIFI_STATUS_UNSUPPORTED_PROPERTY = 0x0209;
public static final int ONNXIFI_STATUS_UNIDENTIFIED_NAME = 0x0301;
public static final int ONNXIFI_STATUS_MISMATCHING_SHAPE = 0x0302;
public static final int ONNXIFI_STATUS_MISMATCHING_DATATYPE = 0x0303;
public static final int ONNXIFI_STATUS_NO_SYSTEM_MEMORY = 0x0401;
public static final int ONNXIFI_STATUS_NO_DEVICE_MEMORY = 0x0402;
public static final int ONNXIFI_STATUS_NO_SYSTEM_RESOURCES = 0x0403;
public static final int ONNXIFI_STATUS_NO_DEVICE_RESOURCES = 0x0404;
public static final int ONNXIFI_STATUS_BACKEND_UNAVAILABLE = 0x0405;
public static final int ONNXIFI_STATUS_INTERNAL_ERROR = 0x0406;

/**
 * State of an ONNXIFI event object.
 *
 * Possible values:
 *     ONNXIFI_EVENT_STATE_INVALID
 *     ONNXIFI_EVENT_STATE_NONSIGNALLED
 *     ONNXIFI_EVENT_STATE_SIGNALLED
 */

/**
 * State for an invalid onnxEvent.
 */
public static final int ONNXIFI_EVENT_STATE_INVALID = 0;
/**
 * Non-signalled onnxEvent state.
 * onnxInitEvent creates events in non-signalled state.
 */
public static final int ONNXIFI_EVENT_STATE_NONSIGNALLED = 0x16BD;
/**
 * Signalled onnxEvent state.
 * onnxSignalEvent changes event state to signalled.
 */
public static final int ONNXIFI_EVENT_STATE_SIGNALLED = 0x3395;

/** Special-purpose accelerator for neural network */
public static final int ONNXIFI_DEVICE_TYPE_NPU = 0x01;
/** Digital signal processor */
public static final int ONNXIFI_DEVICE_TYPE_DSP = 0x02;
/** Graphics accelerator */
public static final int ONNXIFI_DEVICE_TYPE_GPU = 0x04;
/** General-purpose central processor */
public static final int ONNXIFI_DEVICE_TYPE_CPU = 0x08;
/** Field-programmable gate array */
public static final int ONNXIFI_DEVICE_TYPE_FPGA = 0x10;
/**
 * Heterogeneous backend whichinternally arbitrates or distributes work between
 * multiple device types.
 */
public static final int ONNXIFI_DEVICE_TYPE_HETEROGENEOUS = 0x20;

/**
 * The backend supports multi-threaded access to ONNXIFI backend, graph, and
 * event objects. E.g. onnxInitGraph can be called on a different thread than
 * onnxInitBackend.
 *
 * If this capability it not indicated, ONNXIFI backend, graph, and event
 * objects that relate to the backend must always be used on the same thread
 * where the backend object was initialized.
 */
public static final int ONNXIFI_CAPABILITY_THREAD_SAFE = 0x01;
/**
 * The backend supports ONNX graphs with symbolic variables in the outer
 * shape dimension (batch size), using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * The exact numerical value of the  of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE = 0x02;
/**
 * The backend supports ONNX graphs with symbolic variables in the all
 * shape dimensions, using TensorShapeProto.dim_param for
 * ModelProto.graph.input.type.shape or ModelProto.graph.output.type.shape.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE capability.
 *
 * The exact numerical shape of all input and output tensors must be specified
 * in the onnxSetGraphIO call(s).
 */
public static final int ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS = 0x04;
/**
 * The backend supports ONNX graphs with data-dependent outer shape dimension
 * (batch size) of graph outputs. The ONNX graph would specify unknown outer
 * shape dimension (batch size) using symbolic variables, so this capability
 * requires ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE support.
 *
 * For outputs with data-dependent outer shape dimension (batch size) the value
 * specified in onnxSetGraphIO call is interpreted as the upper limit. The exact
 * numerical batch size of the output can be retrieved by attaching a Shape
 * operator to the tensor with data-dependent shape and reading its output
 * through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE = 0x08;
/**
 * The backend supports ONNX graphs with data-dependent output shapes.
 * The ONNX graph would specify unknown output shapes using symbolic variables,
 * so this capability requires ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS support.
 *
 * Backends with this capability also MUST support
 * ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE capability.
 *
 * For outputs with data-dependent shapes the shape specified in onnxSetGraphIO
 * call is interpreted as the upper limit. The exact numerical shape of the
 * output can be retrieved by attaching a Shape operator to the tensor with
 * data-dependent shape and reading its output through ONNXIFI.
 */
public static final int ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS = 0x10;
/**
 * The backend uses a hot-pluggable device, and can be disconnected at any time.
 *
 * If the underlying device disconnects from the system, subsequent operations
 * with the backend, or objects created on the backend, will fail with
 * ONNXIFI_STATUS_BACKEND_UNAVAILABLE status code.
 */
public static final int ONNXIFI_CAPABILITY_HOT_PLUGGABLE = 0x20;

/**
 * Type of the backend information.
 *
 * Possible values:
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION
 *     ONNXIFI_BACKEND_NAME
 *     ONNXIFI_BACKEND_VENDOR
 *     ONNXIFI_BACKEND_VERSION
 *     ONNXIFI_BACKEND_EXTENSIONS
 *     ONNXIFI_BACKEND_DEVICE
 *     ONNXIFI_BACKEND_DEVICE_TYPE
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION
 *     ONNXIFI_BACKEND_OPSET_VERSION
 *     ONNXIFI_BACKEND_CAPABILITIES
 *     ONNXIFI_BACKEND_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_MEMORY_TYPES
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES
 *     ONNXIFI_BACKEND_MEMORY_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT
 *     ONNXIFI_BACKEND_MACS_FP32
 *     ONNXIFI_BACKEND_MACS_FP16
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH
 *     ONNXIFI_BACKEND_PCI_BUS_ID
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID
 *     ONNXIFI_BACKEND_DIRECTX_ID
 *     ONNXIFI_BACKEND_CUDA_INDEX
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID
 */

/**
 * Major and minor version of ONNXIFI specification implemented by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 *      The high 32 bits specify the major version.
 *      The low 32 bits specify the minor version.
 *
 * Possible values:
 *      UINT64_C(0x0000000100000000) for ONNXIFI 1.0
 */
public static final int ONNXIFI_BACKEND_ONNXIFI_VERSION = 0;

/**
 * Marketing name of the backend (excluding the vendor name).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Caffe2"
 *    "Glow"
 */
public static final int ONNXIFI_BACKEND_NAME = 1;

/**
 * Name of the backend vendor.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "Facebook"
 *    "Marat Dukhan"
 */
public static final int ONNXIFI_BACKEND_VENDOR = 2;

/**
 * Version of the backend software. Exact format is vendor-specific, but MUST be
 * unique for the software release.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "1.2.3"
 *    "1.2.3.0"
 *    "1.2.3-db3a4439d233276e25681fb4735b7f8e674dda65"
 */
public static final int ONNXIFI_BACKEND_VERSION = 3;

/**
 * Space-separated list of vendor- or device-specific extensions supported on
 * this backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in US-ASCII encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    ""
 *    "onnx_clone_graph"
 *    "onnx_clone_graph fb_maskrcnn"
 */
public static final int ONNXIFI_BACKEND_EXTENSIONS = 4;

/**
 * Descriptive name of the device (i.e. CPU, GPU, DSP, or NPU model).
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * This string MUST be in UTF-8 encoding and NOT locale-sensitive.
 *
 * Value type: char[], e.g.:
 *    "nnDuino 123"
 */
public static final int ONNXIFI_BACKEND_DEVICE = 5;

/**
 * Type of the device.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxEnum.
 * Possible values:
 *      ONNXIFI_DEVICE_TYPE_NPU
 *      ONNXIFI_DEVICE_TYPE_DSP
 *      ONNXIFI_DEVICE_TYPE_GPU
 *      ONNXIFI_DEVICE_TYPE_CPU
 *      ONNXIFI_DEVICE_TYPE_FPGA
 *      ONNXIFI_DEVICE_TYPE_HETEROGENEOUS
 */
public static final int ONNXIFI_BACKEND_DEVICE_TYPE = 6;

/**
 * List of supported ONNX IR versions.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "3" (IR version in ONNX 1.0)
 *
 * Possible values: space-separated list of supported ONNX IR versions,
 *     represented as decimal integers. ONNX IR versions must match values
 *     in ONNX Version enum.
 */
public static final int ONNXIFI_BACKEND_ONNX_IR_VERSION = 7;

/**
 * List of supported operator set domains and maximum supported operator set
 * version for each domain.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: char[], e.g.:
 *    "ai.onnx:1" (only operators in version 1 of default ONNX operator set)
 *    "ai.onnx:7" (operators up to version 7 of default ONNX operator set)
 *    "org.pytorch:2 ai.onnx:6 ai.facebook:1"
 *
 * Possible values: space-separated list of domain:max_version pairs where
 *     domain corresponds to OperatorSetIdProto.domain and max_version
 *     corresponds to the maximum value of OperatorSetIdProto.version supported
 *     by the backend for this domain. The backend MUST support all previous
 *     operator set versions as well.
 */
public static final int ONNXIFI_BACKEND_OPSET_VERSION = 8;

/**
 * Optional features supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of the following flags:
 *      ONNXIFI_CAPABILITY_THREAD_SAFE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS
 *      ONNXIFI_CAPABILITY_VARIABLE_BATCH_SIZE
 *      ONNXIFI_CAPABILITY_VARIABLE_SIZE_OUTPUTS
 *      ONNXIFI_CAPABILITY_HOT_PLUGGABLE
 *      or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_CAPABILITIES = 10;

/**
 * Auxiliary initialization properties supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_INIT_PROPERTIES = 11;

/**
 * Memory types supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values are any combination of the following flags:
 *     ONNXIFI_MEMORY_TYPE_CPU (always supported)
 *     ONNXIFI_MEMORY_TYPE_CUDA_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER
 *     ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D
 *     ONNXIFI_MEMORY_TYPE_D3D_RESOURCE
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_MEMORY_TYPES = 12;

/**
 * Auxiliary initialization properties supported by graphs on the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: onnxBitfield.
 * Possible values: any combination of vendor-specific flags in high 32 bits of
 * the bit field.
 */
public static final int ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES = 13;

/**
 * Memory synchronization primitives supported for graph inputs and outputs.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Possible values are any combination of the following flags:
 *     ONNXIFI_SYNCHRONIZATION_EVENT    (onnxEvent, always supported)
 *     ONNXIFI_SYNCHRONIZATION_IMPLICIT
 *     or any vendor-specific flags in the high 32 bits of the bit field.
 */
public static final int ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES = 14;

/**
 * Maximum amount of memory, in bytes, available to the use by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_SIZE = 20;

/**
 * Maximum size of network parameters, in bytes.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_SIZE = 21;

/**
 * Maximum number of independent network graphs supported by the backend.
 *
 * Since ONNXIFI 1.0, backends MUST support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MAX_GRAPH_COUNT = 22;

/**
 * Number of FP32 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP32 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP32 = 30;

/**
 * Number of FP16 multiply-accumulate operations per second delivered by the
 * backend.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 * If the backend does not support FP16 computation, the value MUST be 0.
 */
public static final int ONNXIFI_BACKEND_MACS_FP16 = 31;

/**
 * Bandwidth, in bytes per second, of the global memory specific to the backend
 * device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_MEMORY_BANDWIDTH = 35;

/**
 * Bandwidth, in bytes per second, of transferring data from cacheable
 * CPU-allocated memory to the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH = 36;

/**
 * Bandwidth, in bytes per second, of transferring data to cacheable
 * CPU-allocated memory from the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH = 37;

/**
 * PCI bus ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_BUS_ID = 40;

/**
 * PCI device ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DEVICE_ID = 41;

/**
 * PCI domain/function ID of the backend device.
 *
 * Since ONNXIFI 1.0, backends are recommended, but not required to support this
 * information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_PCI_DOMAIN_ID = 42;

/**
 * DirectX ID of the backend device.
 *
 * This is the value that would be returned by ID3D12Device::GetAdapterLuid()
 * for the hardware device used by the backend.
 *
 * Since ONNXIFI 1.0, DXGI-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: LUID (8 bytes).
 */
public static final int ONNXIFI_BACKEND_DIRECTX_ID = 43;

/**
 * CUDA index of the backend device.
 *
 * Since ONNXIFI 1.0, CUDA-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_INDEX = 44;

/**
 * OpenCL platform ID for the backend device.
 * This platform ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_platform_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_PLATFORM_ID = 45;

/**
 * OpenCL device ID for the backend device.
 * This device ID is guaranteed to remain valid for the lifetime of ONNXIFI
 * objects related to the same ONNXIFI backend (backend ID, backend, graph,
 * event).
 *
 * Since ONNXIFI 1.0, OpenCL-based backends are recommended, but not required to
 * support this information query.
 *
 * Value type: cl_device_id.
 */
public static final int ONNXIFI_BACKEND_OPENCL_DEVICE_ID = 46;

/* Note: the data type values match ONNX TensorProto.DataType enum */
public static final int ONNXIFI_DATATYPE_UNDEFINED = 0;
public static final int ONNXIFI_DATATYPE_FLOAT16 = 10;
public static final int ONNXIFI_DATATYPE_FLOAT32 = 1;
public static final int ONNXIFI_DATATYPE_FLOAT64 = 11;
public static final int ONNXIFI_DATATYPE_INT8 = 3;
public static final int ONNXIFI_DATATYPE_INT16 = 5;
public static final int ONNXIFI_DATATYPE_INT32 = 6;
public static final int ONNXIFI_DATATYPE_INT64 = 7;
public static final int ONNXIFI_DATATYPE_UINT8 = 2;
public static final int ONNXIFI_DATATYPE_UINT16 = 4;
public static final int ONNXIFI_DATATYPE_UINT32 = 12;
public static final int ONNXIFI_DATATYPE_UINT64 = 13;
public static final int ONNXIFI_DATATYPE_COMPLEX64 = 14;
public static final int ONNXIFI_DATATYPE_COMPLEX128 = 15;
public static final int ONNXIFI_DATATYPE_BFLOAT16 = 16;

/** Cacheable CPU memory */
public static final int ONNXIFI_MEMORY_TYPE_CPU = 0;
/** CUDA memory buffer (allocated via cudaMalloc/cuMalloc).  */
public static final int ONNXIFI_MEMORY_TYPE_CUDA_BUFFER = 1;
/** OpenCL cl_mem object for a buffer or sub-buffer. */
public static final int ONNXIFI_MEMORY_TYPE_OPENCL_BUFFER = 2;
/** OpenGL ES 2.0+ 2D Texture. */
public static final int ONNXIFI_MEMORY_TYPE_OPENGLES_TEXTURE_2D = 4;
/** Direct3D resource. */
public static final int ONNXIFI_MEMORY_TYPE_D3D_RESOURCE = 8;

/**
 * Terminates the list of auxiliary backend initialization properties passed to
 * onnxInitBackend.
 */
public static final int ONNXIFI_BACKEND_PROPERTY_NONE = 0;
/**
 * Optimization target for graphs initialized on the backend.
 *
 * Possible values:
 *     ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT
 *     ONNXIFI_OPTIMIZATION_LOW_LATENCY
 *     ONNXIFI_OPTIMIZATION_LOW_POWER
 *     ONNXIFI_OPTIMIZATION_LOW_DELAY
 */
public static final int ONNXIFI_BACKEND_PROPERTY_OPTIMIZATION = 1;
/**
 * Logging verbosity level for the backend.
 *
 * If this property is not specified during initialization, the backend should
 * assume ONNXIFI_LOG_LEVEL_WARNING logging verbosity level.
 *
 * Possible values:
 *     ONNXIFI_LOG_LEVEL_ERROR
 *     ONNXIFI_LOG_LEVEL_WARNING
 *     ONNXIFI_LOG_LEVEL_INFO
 *     ONNXIFI_LOG_LEVEL_DEBUG
 */
public static final int ONNXIFI_BACKEND_PROPERTY_LOG_LEVEL = 2;
/**
 * CUDA stream to be used by the backend.
 * CUDA stream must be created on the CUDA device used by the ONNXIFI backend.
 * Users can query which CUDA device is used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_CUDA_INDEX info type.
 *
 * If this property is not specified during initialization, the backend can
 * create a new CUDA stream for the device, or use a default CUDA stream.
 *
 * Possible values: cudaStream_t or CUstream object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_CUDA_STREAM = 4;
/**
 * OpenCL context to be used by the backend.
 * The context must be created with the OpenCL device ID and the OpenCL platform
 * ID used by the ONNXIFI backend. Users can query which OpenCL device ID and
 * OpenCL platform ID are used by the ONNXIFI backend by calling
 * onnxGetBackendInfo with ONNXIFI_BACKEND_OPENCL_PLATFORM_ID
 * and ONNXIFI_BACKEND_OPENCL_DEVICE_ID info types.
 *
 * If this property is not specified during initialization, the backend will
 * create a new OpenCL context for the device.
 *
 * Possible values: cl_context object, cast to uint64_t.
 */
public static final int ONNXIFI_BACKEND_OPENCL_CONTEXT = 8;

/**
 * Terminates the list of auxiliary graph initialization properties passed to
 * onnxInitGraph.
 */
public static final int ONNXIFI_GRAPH_PROPERTY_NONE = 0;

/**
 * Optimize graph representation and compilation for highest throughput.
 */
public static final int ONNXIFI_OPTIMIZATION_HIGH_THROUGHPUT = 0;

/**
 * Optimize graph representation and compilation for lowest latency.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_LATENCY = 1;

/**
 * Optimize graph representation and compilation for lowest power consumption.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_POWER = 2;

/**
 * Optimize graph representation and compilation for lowest delay until first
 * result.
 */
public static final int ONNXIFI_OPTIMIZATION_LOW_DELAY = 3;

/**
 * Log events which caused a failure in an ONNXIFI function call.
 */
public static final int ONNXIFI_LOG_LEVEL_ERROR = 4;

/**
 * Log events in ONNXIFI_LOG_LEVEL_ERROR and events which caused
 * a performance, accuracy, or quality of service degradation in a backend.
 * Enabling this logging level SHOULD NOT have a measurable effect on
 * performance.
 */
public static final int ONNXIFI_LOG_LEVEL_WARNING = 3;

/**
 * Log events in ONNXIFI_LOG_LEVEL_WARNING and high-level status information
 * about operation of a backend. Enabling this logging level MAY cause a small
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_INFO = 2;

/**
 * Log events in ONNXIFI_LOG_LEVEL_INFO and detailed status information about
 * operations of a backend. Enabling this logging level MAY cause a serious
 * degradation in performance.
 */
public static final int ONNXIFI_LOG_LEVEL_DEBUG = 1;

/**
 * Tag for version 1 of tensor descriptor structure (onnxTensorDescriptorV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the tensor descriptor structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1 = 0x43DFBF69;
// Targeting ../onnxTensorDescriptorV1.java



/**
 * Synchronization using ONNXIFI event object (onnxEvent).
 */
public static final int ONNXIFI_SYNCHRONIZATION_EVENT = 0;
/**
 * Implicit synchronization of inputs and outputs access with the caller.
 * The details are backend-specific, and may involve extra parameters passed
 * during backend initialization.
 *
 * Examples:
 *  - CUDA-based backends could implicitly synchronize with the caller through
 *    the use of the same CUDA stream.
 *  - OpenCL-based backends could implicitly synchronize with the caller through
 *    the use of the same in-order OpenCL command queue.
 */
public static final int ONNXIFI_SYNCHRONIZATION_IMPLICIT = 2;

/**
 * Tag for version 1 of memory fence structure (onnxMemoryFenceV1).
 *
 * The tag is unique for this version. If ONNXIFI introduce a new version of
 * the memory fence structure in the future, it will get a new tag value.
 */
public static final int ONNXIFI_TAG_MEMORY_FENCE_V1 = 0x23E08AAB;
// Targeting ../onnxMemoryFenceV1.java


// Targeting ../onnxGetBackendIDsFunction.java


// Targeting ../onnxReleaseBackendIDFunction.java


// Targeting ../onnxGetBackendInfoFunction.java


// Targeting ../onnxGetBackendCompatibilityFunction.java


// Targeting ../onnxInitBackendFunction.java


// Targeting ../onnxReleaseBackendFunction.java


// Targeting ../onnxInitEventFunction.java


// Targeting ../onnxSignalEventFunction.java


// Targeting ../onnxGetEventStateFunction.java


// Targeting ../onnxWaitEventFunction.java


// Targeting ../onnxReleaseEventFunction.java


// Targeting ../onnxInitGraphFunction.java


// Targeting ../onnxSetGraphIOFunction.java


// Targeting ../onnxRunGraphFunction.java


// Targeting ../onnxReleaseGraphFunction.java



/**
 * Get stable IDs of available backends on the system.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software). Backends
 * implemented in the same software, but targeting different devices (e.g.
 * "MyNN" for CPU and "MyNN" for GPU) have different backend IDs.
 *
 * Note that some (hot-pluggable) backends can be connected and disconnected at
 * any time, and thus subsequent calls to this function may return different
 * number or set of backend IDs. The returned IDs, however, stay valid even if
 * the hardware device used by the backend disconnects from the system.
 *
 * To avoid resource leak, the backend ID MUST be released through a call to
 * onnxReleaseBackendID when it is no longer needed.
 *
 * @param backendIDs[out] - pointer to the memory location where the backend IDs
 *                          will be returned. If the pointer is NULL, it is
 *                          ignored, and the function returns only the number
 *                          of backend IDs through numBackendIDs pointer.
 * @param numBackendIDs[in,out] - pointer to a variable specifying number of
 *                                available backends. On function entry, the
 *                                variable MUST contain the capacity, in number
 *                                of backend IDs, of the memory buffer specified
 *                                by backendIDs. For successful completion, this
 *                                capacity must be at least as large as the
 *                                number of available backends. If the function
 *                                completes with either ONNXIFI_STATUS_SUCCESS
 *                                or ONNXIFI_STATUS_FALLBACK status codes, the
 *                                number of backend IDs written into backendIDs
 *                                buffer is stored in the variable specified by
 *                                this pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and backend IDs
 *                                are stored in the location specified by
 *                                backendIDs, and the number of the backends
 *                                is stored in the location specified by
 *                                numBackends.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 backend IDs were not stored in the
 *                                 location specified by backendIDs, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient to store
 *                                 all available backend IDs. The number of
 *                                 available backends is stored in the
 *                                 location specified by numBackends.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        numBackends is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         system failed to allocate memory
 *                                         to store backend ID information.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendIDs(
    @ByPtrPtr onnxBackendID backendIDs,
    @Cast("size_t*") SizeTPointer numBackends);

/**
 * Deinitialize ONNXIFI backend IDs and release associated resources.
 *
 * The user MUST deinitialize all objects created with this backend ID
 * (onnxBackend, onnxGraph, onnxEvent) before calling this function to
 * deinitialize the backend ID.
 *
 * @param backendID - backend ID returned by onnxGetBackendIDs.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the resources
 *                                associated to the backend ID were released to
 *                                the operating system.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackendID(
    onnxBackendID backendID);

/**
 * Query high-level information about the backend and its target device.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNX graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * The content, data type, and availability of information provided by this
 * function depends on infoType value as specified below:
 *
 *         infoType value                           data type      support
 *     ONNXIFI_BACKEND_ONNXIFI_VERSION               uint64_t     required
 *     ONNXIFI_BACKEND_NAME                           char[]      required
 *     ONNXIFI_BACKEND_VENDOR                         char[]      required
 *     ONNXIFI_BACKEND_VERSION                        char[]      required
 *     ONNXIFI_BACKEND_EXTENSIONS                     char[]      required
 *     ONNXIFI_BACKEND_DEVICE                         char[]      required
 *     ONNXIFI_BACKEND_DEVICE_TYPE                   onnxEnum     required
 *     ONNXIFI_BACKEND_ONNX_IR_VERSION                char[]      required
 *     ONNXIFI_BACKEND_OPSET_VERSION                  char[]      required
 *     ONNXIFI_BACKEND_CAPABILITIES                onnxBitfield   required
 *     ONNXIFI_BACKEND_INIT_PROPERTIES             onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_TYPES                onnxBitfield   required
 *     ONNXIFI_BACKEND_GRAPH_INIT_PROPERTIES       onnxBitfield   required
 *     ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES       onnxBitfield   required
 *     ONNXIFI_BACKEND_MEMORY_SIZE                   uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_SIZE                uint64_t     required
 *     ONNXIFI_BACKEND_MAX_GRAPH_COUNT               uint64_t     required
 *     ONNXIFI_BACKEND_MACS_FP32                     uint64_t     optional
 *     ONNXIFI_BACKEND_MACS_FP16                     uint64_t     optional
 *     ONNXIFI_BACKEND_MEMORY_BANDWIDTH              uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_READ_BANDWIDTH     uint64_t     optional
 *     ONNXIFI_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_BUS_ID                    uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DEVICE_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_PCI_DOMAIN_ID                 uint64_t     optional
 *     ONNXIFI_BACKEND_DIRECTX_ID                      LUID       optional
 *     ONNXIFI_BACKEND_CUDA_INDEX                    uint64_t     optional
 *     ONNXIFI_BACKEND_OPENCL_PLATFORM_ID         cl_platform_id  optional
 *     ONNXIFI_BACKEND_OPENCL_DEVICE_ID            cl_device_id   optional
 *
 * @param backendID - ID of the backend to query.
 * @param infoType - type of the backend information to query. Must be one of
 *                   the ONNXIFI_BACKEND_* constants. If this value is not
 *                   supported by the backend, the function will fail with
 *                   ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE.
 * @param infoValue[out] - pointer to the memory location where the backend
 *                         information value will be returned. If the pointer is
 *                         NULL, is it ignored.
 * @param infoValueSize[in,out] - pointer to a variable specifying size, in
 *                                bytes, of the information value. On function
 *                                entry, the variable MUST contain the size of
 *                                the memory buffer specified by infoValue.
 *                                For successful completion, this size must be
 *                                at least as large as the queried value. If the
 *                                function completes with either
 *                                ONNXIFI_STATUS_SUCCESS or
 *                                ONNXIFI_STATUS_FALLBACK status codes, the
 *                                actual size of the value queried in the call
 *                                is stored in the variable specified by this
 *                                pointer.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded, and requested
 *                                value is stored in the location specified by
 *                                infoValue, and the actual size of the
 *                                requested value is stored in the location
 *                                specified by infoValueSize.
 * \retval ONNXIFI_STATUS_FALLBACK The function call completed, but the
 *                                 requested value was not stored in the
 *                                 location specified by infoValue, either
 *                                 because it is NULL, or because the size of
 *                                 the memory buffer is insufficient for the
 *                                 value. The actual size of the requested value
 *                                 is stored in the location specified by
 *                                 infoValueSize.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        infoValueSize is NULL.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the value of infoType is not
 *                                              supported by the backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 */
public static native @Cast("onnxStatus") int onnxGetBackendInfo(
    onnxBackendID backendID,
    @Cast("onnxBackendInfo") int infoType,
    Pointer infoValue,
    @Cast("size_t*") SizeTPointer infoValueSize);

/**
 * Query if an ONNX model graph is compatible with the backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, the validation of symbolic dimension should be
 * deferred until graph inputs and outputs are specified in onnxSetGraphIO.
 *
 * Commonly, the serialized ModelProto message passed to this function would
 * not include the static weights (ModelProto.graph.initializer is empty), and
 * the backend implementation MUST NOT rely on the weights to determine if the
 * graph is supported.
 *
 * An important use-case is a ModelProto containing only a single NodeProto in
 * ModelProto.graph.node, which happens when a high-level framework checks
 * operators one-by-one to find a connected subgraph that can be offloaded to
 * the backend. Backend implementations SHOULD optimize performance for this
 * use-case.
 *
 * @param backend - ID of the backend to query.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param onnxModel [in] - pointer to serialized ONNX ModelProto message
 *                        representing the model graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph can efficiently run on the backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph can run on the backend through some
 *                                 emulation layer with some efficiency loss. If
 *                                 a backend decomposes this operator into
 *                                 multiple sub-operators, it should return this
 *                                 code. E.g. if a backend does not natively
 *                                 support grouped or depthwise convolution, but
 *                                 can execute it as multiple unit-group
 *                                 convolution operators, it must returns this
 *                                 code.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel is NULL.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 *                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, or inferred
 *                                          from the inputs by the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, or
 *                                             inferred from the inputs by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             output or intermediate data types
 *                                             specified in the ONNX model graph
 *                                             do not match the data types
 *                                             inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse and analyze
 *                                         the model graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxGetBackendCompatibility(
    onnxBackendID backendID,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel);

/**
 * Initialize an ONNXIFI backend.
 *
 * ONNXIFI backend is a combination of software layer and hardware device used
 * to run an ONNXIFI graph. The same software layer may expose multiple backends
 * (e.g. one ONNXIFI backend for each GPU in the system, or one ONNXIFI backend
 * for GPU and another for CPU, both implemented in the same software).
 *
 * @param backendID - ID of the backend to initialize.
 * @param auxPropertiesList [in] - optional list of backend initialization
 *                                properties, terminated by
 *                                ONNXIFI_BACKEND_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param backend [out] - pointer to an opaque handle for the initialized ONNXIFI
 *                       backend. If the function fails, the handle is
 *                       initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_ID The function call failed because backendID
 *                                   is not an ONNXIFI backend ID.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        backend pointer is NULL.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the backend initialization
 *                                         property values is invalid.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize backend.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the backend.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            backend.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @ByPtrPtr onnxBackend backend);
public static native @Cast("onnxStatus") int onnxInitBackend(
    onnxBackendID backendID,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @ByPtrPtr onnxBackend backend);

/**
 * Deinitialize an ONNXIFI backend and release associated resources.
 *
 * The user MUST deinitialize all objects created on this backend (onnxGraph,
 * onnxEvent) before calling this function to deinitialize the backend.
 *
 * @param backend - ONNXIFI backend handle created by onnxInitBackend.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the backend
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseBackend(
    onnxBackend backend);

/**
 * Initialize a single-shot ONNXIFI event.
 *
 * The newly created event is in non-signalled state.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to initialize the event.
 * @param event [out] - pointer to the opaque handle for the created ONNXIFI
 *                     event. If the function fails, the handle is initialized
 *                     to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was successfully initialized.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        event pointer is NULL.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed due to
 *                                         insufficient system memory to
 *                                         initialize the event.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the event.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            event.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxInitEvent(
    onnxBackend backend,
    @ByPtrPtr onnxEvent event);

/**
 * Change the state of an ONNXIFI event to signalled.
 *
 * @param event - event handle created by onnxInitEvent. While it is technically
 *                possible to use this function for output memory fence event
 *                created by onnxRunGraph, users SHOULD NOT do that.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                was changed to signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_STATE The function call failed because event
 *                                      is already in the signalled state.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxSignalEvent(
    onnxEvent event);

/**
 * Query ONNXIFI event state without blocking.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 * @param state [out] - pointer to the variable that will store the state of the
 *                     event. If the function fails, the variable is initialized
 *                     to ONNXIFI_EVENT_STATE_INVALID.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the state
 *                                variable was initialized to either
 *                                ONNXIFI_EVENT_STATE_SIGNALLED or
 *                                ONNXIFI_EVENT_STATE_NONSIGNALLED according
 *                                to the state of the event.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because state
 *                                        pointer is NULL.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntPointer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") IntBuffer state);
public static native @Cast("onnxStatus") int onnxGetEventState(
    onnxEvent event,
    @Cast("onnxEventState*") int[] state);

/**
 * Wait until an ONNXIFI event transitions to signalled state.
 *
 * @param event - event handle created by onnxRunGraph. While it is technically
 *                possible to use this function to events created by
 *                onnxInitEvent, this is not the intended use-case.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the function
 *                                returned because event transitioned to
 *                                signalled state.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxWaitEvent(
    onnxEvent event);

/**
 * Deinitialize an ONNXIFI event and release associated resources.
 *
 * @param event - event handle created by either onnxInitEvent or onnxRunGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the event
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because event
 *                                      is not an ONNXIFI event handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseEvent(
    onnxEvent event);

/**
 * Parse an ONNXIFI graph and convert it for a particular backend.
 *
 * Model graph is passed as a serialized ModelProto message, where types and
 * dimensions of all inputs (including static weights) and outputs are specified
 * through ModelProto.graph.input and ModelProto.graph.output messages. If the
 * backend supports ONNXIFI_CAPABILITY_SYMBOLIC_SIZE_TENSORS, some of the shape
 * dimensions can be symbolic. If the backend supports
 * ONNXIFI_CAPABILITY_SYMBOLIC_BATCH_SIZE, the outer shape dimension can be
 * symbolic. In these cases, their validation should be deferred until a later
 * call to onnxSetGraphIO.
 *
 * Values of all static weights of the graph must be specified either in
 * ModelProto.graph.initializer, or through the weightDescriptors parameters,
 * but not through any combination of the two methods. If the caller creates the
 * graph on the fly, it SHOULD pass weights through weightDescriptors as it
 * involves less overhead.
 *
 * Blobs and operators in this graph are independent of the blobs and operators
 * of other graphs on the same backend.
 *
 * @param backend - backend handle created by onnxInitBackend. This backend
 *                  would be used to setup and run the model graph.
 * @param auxPropertiesList [in] - optional list of graph initialization
 *                                properties, terminated by
 *                                ONNXIFI_GRAPH_PROPERTY_NONE entry. Can be
 *                                NULL or empty.
 * @param onnxModelSize - size of the serialized ONNX ModelProto message,
 *                        in bytes.
 * @param onnxModel [in] - pointer to serialized ONNX ModelProto message
 *                        representing the model graph. The backend MUST not
 *                        assume that the serialized ModelProto message is
 *                        present at this address after the function returns.
 * @param weightsCount - number of weights specified in this function call
 *                       through tensor descriptors. Alternatively, the weights
 *                       can be specified in ModelProto.graph.initializer.
 *                       If weightsCount is non-zero, weightDescriptors must be
 *                       non-NULL.
 * @param weightDescriptors [in] - descriptors of static input tensors for the
 *                                graph. Elements of this array provide location
 *                                for blobs identified by ValueInfoProto.name
 *                                listed in ModelProto.graph.input of the ONNX
 *                                graph. If this parameter is non-NULL,
 *                                all static inputs must be specified through
 *                                the tensor descriptors, and the
 *                                ModelProto.graph.initilizer list must be
 *                                empty. The tensor descriptors
 *                                must use ONNXIFI_MEMORY_TYPE_CPU memory type,
 *                                and the backend must copy the values of the
 *                                tensors and all metadata, including shape,
 *                                into its own memory before the function
 *                                returns.
 * @param graph [out] - pointer to the opaque handle for the created ONNXIFI
 *                     graph. If the function fails, and this pointer is
 *                     non-NULL, the handle is initialized to NULL.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the model
 *                                graph was successfully initialized on the
 *                                backend.
 * \retval ONNXIFI_STATUS_FALLBACK The function call succeeded and the model
 *                                 graph was initialized for the backend through
 *                                 an emulation layer with substantial
 *                                 efficiency loss. If a backend decomposes an
 *                                 operator into multiple sub-operators, it
 *                                 MUST return this code. E.g. if a backend
 *                                 does not natively support grouped or
 *                                 depthwise convolution, but can execute it as
 *                                 multiple unit-group convolution operators, it
 *                                 should return this code.
 * \retval ONNXIFI_STATUS_INVALID_BACKEND The function call failed because
 *                                        backend is not an ONNXIFI backend
 *                                        handle.
 * \retval ONNXIFI_STATUS_INVALID_PROPERTY The function call failed because one
 *                                         of the graph initialization property
 *                                         values is invalid.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        onnxModel or graph pointer is NULL, or
 *                                        weightDescriptors pointer is NULL
 *                                        while weightsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_SIZE The function call failed because
 *                                     onnxModelSize is 0.
 * \retval ONNXIFI_STATUS_INVALID_PROTOBUF The function call failed because it
 *                                         couldn't parse the serialized
 *                                         protobuf as an ONNX ModelProto
 *                                         message.
 * \retval ONNXIFI_STATUS_INVALID_MODEL The function call failed because the
 *                                      parsed ModelProto message does not
 *                                      satisfy ONNX requirements and
 *                                      constraints.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions in
 *                                      weightDescriptors is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         weightDescriptors is unknown to the
 *                                         backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            weightDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in weightDescriptors
 *                                                is invalid (NULL pointer).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_PROPERTY The function call failed because
 *                                             backend does not recognize one
 *                                             of the graph initialization
 *                                             property IDs.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_VERSION The function call failed because
 *                                            the ONNX IR version or operator
 *                                            version is not supported by the
 *                                            backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_OPERATOR The function call failed because
 *                                             one of the operators in the model
 *                                             graph is not supported by the
 *                                             backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_ATTRIBUTE The function call failed because
 *                                              the backend does not support the
 *                                              particular AttributeProto
 *                                              values in one of the operators.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or
 *                                          output of one of the operators.
 *                                          The problematic tensor shapes could
 *                                          be directly specified through
 *                                          ValueInfoProto in GraphProto.input,
 *                                          GraphProto.output, or
 &                                          GraphProto.value_info, through
 *                                          TensorProto in
 *                                          GraphProto.initializer, through
 *                                          weightDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_DATATYPE The function call failed because
 *                                             the backend does not support the
 *                                             data types in an input or output
 *                                             of one of the operators. The
 *                                             problematic data types could be
 *                                             directly specified through
 *                                             ValueInfoProto in
 *                                             GraphProto.input,
 *                                             GraphProto.output, or
 *                                             GraphProto.value_info, through
 *                                             TensorProto in
 *                                             GraphProto.initializer, through
 *                                             weightDescriptors argument,
 *                                             or inferred from the inputs by
 *                                             the backend.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because one of the memory
 *                                                types in weightDescriptors is
 *                                                different from
 *                                                ONNXIFI_MEMORY_TYPE_CPU.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified in weight
 *                                          descriptors do not match the shapes
 *                                          specified in the ONNX model graph,
 *                                          or output or intermediate shapes
 *                                          specified in the ONNX model graph do
 *                                          not match the shapes inferred from
 *                                          input shapes.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified in weight
 *                                             descriptors do not match the data
 *                                             types specified in ONNX model
 *                                             graph, or output or intermediate
 *                                             data types specified in the ONNX
 *                                             model graph do not match the data
 *                                             types inferred from graph inputs.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the model graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       implementation experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntPointer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") IntBuffer auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);
public static native @Cast("onnxStatus") int onnxInitGraph(
    onnxBackend backend,
    @Cast("const uint64_t*") int[] auxPropertiesList,
    @Cast("size_t") long onnxModelSize,
    @Const Pointer onnxModel,
    @Cast("uint32_t") int weightsCount,
    @Const onnxTensorDescriptorV1 weightDescriptors,
    @ByPtrPtr onnxGraph graph);

/**
 * Set locations for inputs and outputs of an ONNXIFI graph.
 *
 * The caller MUST ensure that the memory buffers specified for input and output
 * tensors remain accessible until all in-flight graph executions which use
 * specified buffer locations complete AND
 * - Either a next call to onnxSetGraphIO specifies different buffer locations
 * - Or the graph is deinitialized via onnxReleaseGraph
 * The caller can invalidate other data in tensor descriptors, including shape,
 * once the function returns.
 *
 * Calls to onnxRunGraph WILL use input and output locations specified in the
 * preceding onnxSetGraphIO on the same graph. Asynchronous graph executions
 * that were in-flight before onnxSetGraphIO call will continue to use buffer
 * locations that were current when these graph executions started. An ONNXIFI
 * implementation MAY block inside onnxSetGraphIO until all in-flight graph
 * executions that started before the call complete.
 *
 * If a call to onnxSetGraphIO fails, it invalidates input and output locations
 * for the graph, and a subsequent call to onnxRunGraph will fail with
 * ONNXIFI_STATUS_UNIDENTIFIED_NAME.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param inputsCount - number of elements in the inputDescriptors array.
 * @param inputDescriptors [in] - descriptors of input tensors for the graph.
 *                               Elements of this array must provide a location
 *                               for each ValueInfoProto.name listed in
 *                               ModelProto.graph.input of the ONNX graph.
 *                               If inputsCount is non-zero, inputDescriptors
 *                               pointer must be non-NULL.
 * @param outputsCount - number of elements in the outputDescriptors array.
 *                       Must be greater than zero.
 * @param outputDescriptors [in] - descriptors of output tensors for the graph.
 *                                outputDescriptors pointer must be non-NULL.
 *                                Elements of this array must provide a location
 *                                for each ValueInfoProto.name listed in
 *                                ModelProto.graph.output of the ONNX graph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        outputDescriptors pointer is NULL or
 *                                        inputDescriptors pointer is NULL while
 *                                        inputsCount is non-zero.
 * \retval ONNXIFI_STATUS_INVALID_NAME The function call failed because one of
 *                                     the names in tensor descriptors doesn't
 *                                     match blob name in ModelProto.graph.input
 *                                     or ModelProto.graph.output, or the same
 *                                     name appears in more than one tensor
 *                                     descriptor.
 * \retval ONNXIFI_STATUS_INVALID_SHAPE The function call failed because one of
 *                                      the shape dimensions is 0.
 * \retval ONNXIFI_STATUS_INVALID_DATATYPE The function call failed because
 *                                         one of the data types in
 *                                         inputDescriptors or outputDescriptors
 *                                         is unknown to the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_TYPE The function call failed because
 *                                            one of the memory types in
 *                                            inputDescriptors or
 *                                            outputDescriptors is unknown to
 *                                            the backend.
 * \retval ONNXIFI_STATUS_INVALID_MEMORY_LOCATION The function call failed
 *                                                because one of the memory
 *                                                locations in inputDescriptors
 *                                                or outputDescriptors is not
 *                                                valid for the specified
 *                                                memory type (e.g. NULL pointer
 *                                                for ONNXIFI_MEMORY_TYPE_CPU).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because one
 *                                        of the tags in inputDescriptors or
 *                                        outputDescriptors is unknown to the
 *                                        backend (tag does not match
 *                                        ONNXIFI_TAG_TENSOR_DESCRIPTOR_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_SHAPE The function call failed because the
 *                                          backend does not support the
 *                                          tensor shapes in an input or output
 *                                          of one of the operators. The
 *                                          problematic tensor shapes could be
 *                                          directly specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument,
 *                                          or inferred from the inputs by the
 *                                          backend. This error code can be
 *                                          returned when the backend supports
 *                                          variable-size inputs and outputs,
 *                                          and the problematic tensor shape was
 *                                          provided in the ValueInfoProto as a
 *                                          symbolic variable.
 * \retval ONNXIFI_STATUS_UNSUPPORTED_MEMORY_TYPE The function call failed
 *                                                because the backend does not
 *                                                support one of the memory
 *                                                types in inputDescriptors or
 *                                                outputDescriptors.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because one
 *                                          of the ValueInfoProto.name value in
 *                                          ModelProto.graph.input or
 *                                          ModelProto.graph.output doesn't have
 *                                          a match in the inputDescriptors or
 *                                          outputDescriptors.
 * \retval ONNXIFI_STATUS_MISMATCHING_SHAPE The function call failed because
 *                                          the shapes specified through
 *                                          inputDescriptors or
 *                                          outputDescriptors argument are
 *                                          inconsistent with the shapes
 *                                          specified in the ONNX model graph.
 * \retval ONNXIFI_STATUS_MISMATCHING_DATATYPE The function call failed because
 *                                             data types specified through
 *                                             inputDescriptors or
 *                                             outputDescriptors argument are
 *                                             inconsistent with the data types
 *                                             specified in the ONNX model
 *                                             graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to parse, analyze, and
 *                                         initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to initialize the tensor locations.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to initialize the
 *                                            tensor locations.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxSetGraphIO(
    onnxGraph graph,
    @Cast("uint32_t") int inputsCount,
    @Const onnxTensorDescriptorV1 inputDescriptors,
    @Cast("uint32_t") int outputsCount,
    @Const onnxTensorDescriptorV1 outputDescriptors);

/**
 * Asynchronously execute operations in an ONNXIFI graph using pre-specified
 * locations for inputs and outputs.
 *
 * This function operates asynchronously: it doesn't require that the locations
 * for graph inputs graph inputs hold valid values before the function is
 * called, and doesn't guarantee that the locations for graph outputs hold
 * valid values when the function returns. Instead, two synchronization
 * primitives are used to signal to the backend when inputs are ready to use,
 * and to signal to the caller when outputs are ready to use. The only
 * synchronization primitive that is always available is onnxEvent
 * (ONNXIFI_SYNCHRONIZATION_EVENT memory fence type). If a backend supports
 * additional types of synchronization primitives, it must indicate them in
 * ONNXIFI_BACKEND_SYNCHRONIZATION_TYPES information query.
 *
 * The caller must successfully specify locations of input and output tensors
 * for the graph through onnxSetGraphIO before calling this function.
 *
 * @param graph - graph handle created by onnxInitGraph.
 * @param inputFence [in] - synchronization primitive that signals when graph
 *                         inputs are ready to use by the backend. The
 *                         synchronization primitive always must be initialized
 *                         by the caller.
 * @param outputFence [out] - synchronization primitive that signals when graph
 *                           outputs are ready to use by the caller. The type
 *                           of the synchronization primitive always must be
 *                           initialized by the caller. The type of the
 *                           synchronization primitive determines whether it
 *                           is initialized by the user before the call or by
 *                           the backend as a result of this call. Single-shot
 *                           synchronizatiom objects are initialized as a result
 *                           of the call. Reusable synchronization objects are
 *                           generally initialized by the user prior to the
 *                           call.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the all graph
 *                                inputs and outputs were matched to a memory
 *                                location.
 * \retval ONNXIFI_STATUS_INVALID_POINTER The function call failed because
 *                                        inputFence or outputFence pointer is
 *                                        NULL.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because
 *                                      graph is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INVALID_FENCE_TYPE The function call failed because
 *                                           the type of synchronization
 *                                           primitive specified in inputFence
 *                                           or outputFence is unknown to the
 *                                           backend.
 * \retval ONNXIFI_STATUS_INVALID_EVENT The function call failed because
 *                                      the memory synchronization primitive
 *                                      specified in inputFence or outputFence
 *                                      is not valid (e.g. NULL onnxEvent).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_TAG The function call failed because a tag
 *                                        in inputFence or outputFence is
 *                                        unknown to the backend (tag does not
 *                                        match ONNXIFI_TAG_MEMORY_FENCE_V1).
 * \retval ONNXIFI_STATUS_UNSUPPORTED_FENCE_TYPE The function call failed
 *                                               because the backend does not
 *                                               support the type of
 *                                               synchronization primitive
 *                                               specified in inputFence or
 *                                               outputFence.
 * \retval ONNXIFI_STATUS_UNIDENTIFIED_NAME The function call failed because
 *                                          some of the ValueInfoProto.name
 *                                          value in ModelProto.graph.input or
 *                                          ModelProto.graph.output were not
 *                                          specified in a call to
 *                                          onnxSetGraphIO.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_MEMORY The function call failed because the
 *                                         backend could not allocate enough
 *                                         system memory to execute the model
 *                                         graph.
 * \retval ONNXIFI_STATUS_NO_SYSTEM_RESOURCES The function call failed due to
 *                                            insufficient non-memory system
 *                                            resources (e.g. file handles) to
 *                                            execute the model graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_MEMORY The function call failed due to
 *                                         insufficient backend-specific memory
 *                                         to execute the graph.
 * \retval ONNXIFI_STATUS_NO_DEVICE_RESOURCES The function call failed due to
 *                                            insufficient non-memory
 *                                            backend-specific resources (e.g.
 *                                            command queues) to execute the
 *                                            graph.
 * \retval ONNXIFI_STATUS_BACKEND_UNAVAILABLE The function call failed because
 *                                            the backend was disconnected or
 *                                            uninstalled from the system.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       backend experienced an unrecovered
 *                                       internal error.
 */
public static native @Cast("onnxStatus") int onnxRunGraph(
    onnxGraph graph,
    @Const onnxMemoryFenceV1 inputFence,
    onnxMemoryFenceV1 outputFence);

/**
 * Deinitialize an ONNXIFI graph and release associated resources.
 *
 * If there are in-flight asynchronous inference operations on this graph,
 * the function MUST block until all outstanding operations complete.
 *
 * @param graph - graph handle created by onnxInitGraph.
 *
 * \retval ONNXIFI_STATUS_SUCCESS The function call succeeded and the graph
 *                                resources were released to the operating
 *                                system.
 * \retval ONNXIFI_STATUS_INVALID_GRAPH The function call failed because graph
 *                                      is not an ONNXIFI graph handle.
 * \retval ONNXIFI_STATUS_INTERNAL_ERROR The function call failed because the
 *                                       graph backend experienced an
 *                                       unrecovered internal error.
 */
public static native @Cast("onnxStatus") int onnxReleaseGraph(
    onnxGraph graph);

// #ifdef __cplusplus /* extern "C" */
// #endif

// #endif /* !defined(ONNXIFI_H) */


// Parsed from onnx/common/tensor.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include <cmath>
// #include <functional>
// #include <numeric>
// #include "onnx/common/assertions.h"
// #include "onnx/onnx_pb.h"
// Targeting ../Tensor.java



// #define define_data(type, field)
//   template <>
//   inline type* Tensor::data<type>() {
//     if (is_raw_data_) {
//       return (type*)&raw_data_.data()[0];
//     } else {
//       return field.data();
//     }
//   }
// 
//   template <>
//   inline const type* Tensor::data<type>() const {
//     if (is_raw_data_) {
//       return (type*)(raw_data_.data());
//     } else {
//       return field.data();
//     }
//   }



  


  


  


  


  


  
// #undef define_data







// #define APPLY_BINARY_FUNCTION(op_name, f)
//   inline void Tensor::op_name(const Tensor& other) {
//     TENSOR_ASSERTM(
//         other.eem_type() == elem_type_,
//         "Tensor types do not match: %s != %s",
//         to_string(elem_type_).c_str(),
//         " vs. ",
//         to_string(other.eem_type()).c_str());
//     TENSOR_ASSERTM(other.sizes() == sizes_, "Tensor sizes do not match.");
//     switch (elem_type_) {
//       case ONNX_NAMESPACE::TensorProto_DataType_FLOAT: {
//         bin_func(f<float>(), data<float>(), other.data<float>());
//         break;
//       }
//       case ONNX_NAMESPACE::TensorProto_DataType_BOOL:
//       case ONNX_NAMESPACE::TensorProto_DataType_INT8:
//       case ONNX_NAMESPACE::TensorProto_DataType_INT16:
//       case ONNX_NAMESPACE::TensorProto_DataType_INT32:
//       case ONNX_NAMESPACE::TensorProto_DataType_UINT8:
//       case ONNX_NAMESPACE::TensorProto_DataType_UINT16: {
//         bin_func(f<int32_t>(), data<int32_t>(), other.data<int32_t>());
//         break;
//       }
//       case ONNX_NAMESPACE::TensorProto_DataType_INT64: {
//         bin_func(f<int64_t>(), data<int64_t>(), other.data<int64_t>());
//         break;
//       }
//       case ONNX_NAMESPACE::TensorProto_DataType_UINT32:
//       case ONNX_NAMESPACE::TensorProto_DataType_UINT64: {
//         bin_func(f<uint64_t>(), data<uint64_t>(), other.data<uint64_t>());
//         break;
//       }
//       case ONNX_NAMESPACE::TensorProto_DataType_DOUBLE: {
//         bin_func(f<double>(), data<double>(), other.data<double>());
//         break;
//       }
//       default:
//         TENSOR_ASSERTM(
//             false,
//             "Operation %s not supported for data type %s",
//             #op_name,
//             " not supported for data type ",
//             to_string(elem_type_).c_str());
//     }
//   }






// #undef APPLY_BINARY_FUNCTION





 // namespace ONNX_NAMESPACE


// Parsed from onnx/common/array_ref.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

//===--- ArrayRef.h - Array Reference Wrapper -------------------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

// ONNX: modified from llvm::ArrayRef.
// removed llvm-specific functionality
// removed some implicit const -> non-const conversions that rely on
// complicated std::enable_if meta-programming
// removed a bunch of slice variants for simplicity...

// #pragma once
// #include <assert.h>
// #include <array>
// #include <vector>
  /** ArrayRef - Represent a constant reference to an array (0 or more elements
   *  consecutively in memory), i.e. a start pointer and a length.  It allows
   *  various APIs to take consecutive elements easily and conveniently.
   * 
   *  This class does not own the underlying data, it is expected to be used in
   *  situations where the data resides in some other buffer, whose lifetime
   *  extends past that of the ArrayRef. For this reason, it is not in general
   *  safe to store an ArrayRef.
   * 
   *  This is intended to be trivially copyable, so it should be passed by
   *  value. */

 // namespace ONNX_NAMESPACE


// Parsed from onnx/common/status.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #include <memory>
// #include <ostream>
// #include <string>

/** enum onnx::Common::StatusCategory */
public static final int
  NONE = 0,
  CHECKER = 1,
  OPTIMIZER = 2;

/** enum onnx::Common::StatusCode */
public static final int
  OK = 0,
  FAIL = 1,
  INVALID_ARGUMENT = 2,
  INVALID_PROTOBUF = 3;
// Targeting ../Status.java



@Namespace("onnx::Common") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Status status);

 // namespace Common
 // namespace ONNX_NAMESPACE


// Parsed from onnx/common/stl_backports.h

// #pragma once

// This file contains backports of STL features for newer C++.

// #include <memory>
// #include <type_traits>

/*
 * Use MOVE_CAPTURE_IF_CPP14 in a lambda capture so it gets
 * copied in C++11 and moved in C++14.
 * Example:
 *   std::string mystring;
 *   auto lambda = [MOVE_CAPTURE_IF_CPP14(mystring)] {
 *     std::cout << mystring;
 *   }
 */
// #ifdef __cpp_init_captures
//   #define MOVE_CAPTURE_IF_CPP14(variable) variable = std::move(variable)
// #else
//   #define MOVE_CAPTURE_IF_CPP14(variable) variable
// #endif


/**
 * For exception safety and consistency with make_shared. Erase me when
 * we have std::make_unique().
 *
 * @author Louis Brandy (ldbrandy\fb.com)
 * @author Xu Ning (xning\fb.com)
 * @author Sebastian Messmer (messmer\fb.com)
 */

// #if __cplusplus >= 201402L || __cpp_lib_make_unique >= 201304L ||
//     (__ANDROID__ && __cplusplus >= 201300L) || _MSC_VER >= 1900


// #else

// Allows 'make_unique<T[]>(10)'. (N3690 s20.9.1.4 p3-4)

// Disallows 'make_unique<T[10]>()'. (N3690 s20.9.1.4 p5)




// #endif


// Parsed from onnx/common/ir.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include <atomic>
// #include <algorithm>
// #include <cstdint>
// #include <functional>
// #include <iostream>
// #include <memory>
// #include <sstream>
// #include <stdint.h>
// #include <string>
// #include <unordered_set>
// #include <vector>

// #include "onnx/string_utils.h"
// #include "onnx/common/array_ref.h"
// #include "onnx/common/assertions.h"
// #include "onnx/common/interned_strings.h"
// #include "onnx/common/graph_node_list.h"
// #include "onnx/common/tensor.h"


// #define ONNX_DISALLOW_COPY_AND_ASSIGN(TypeName)
//   TypeName(const TypeName&) = delete;
//   TypeName& operator=(const TypeName&) = delete

// Graph represents one "function" of computation.
// It uses a simple ownership model where the graph owns all the nodes inside it.
// All references inside the graph are raw pointers.
// Destroying the Graph will invalidate any pointers to nodes in the graph.
// Targeting ../Node.java




// A Value represents an input or output to node that is either a
// Tensor or an opaque Handle object, as determined by type().
// Targeting ../DimensionIR.java




/** enum class onnx::AttributeKind */
public static final int
  // float, float list, int, int list, string, string list,
  // tensor, tensor list, subgraph, subgraph list
  f = 0, fs = 1, i = 2, is = 3, s = 4, ss = 5, t = 6, ts = 7, g = 8, gs = 9;



// Targeting ../AttributeValue.java




// CRTP so that Node which inherits Attributes can be return for
// method chaining e.g:
// Node * n = g->create(kSelect)->set_i(kOffset,3)->set_f(kValue,3.5);
// we return Derived* pointers because Nodes are normally held as pointers.
// Targeting ../Use.java



@Namespace("onnx") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Use a, @Const @ByRef Use b);


// the list types are intentionally simple, but we type-def
// them here so if we need to change them, refactoring will be easier
// Targeting ../Value.java


// Targeting ../OpSetID.java


// Targeting ../Graph.java



















/************* All nodes not required to be defined before Graph **************/







 // namespace ONNX_NAMESPACE


// Parsed from onnx/common/ir_pb_converter.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include "onnx/common/ir.h"
// #include "onnx/onnx_pb.h"
// Targeting ../ConvertError.java



// #define fail_convert(...)
//   throw ConvertError(MakeString(__VA_ARGS__));

@Namespace("onnx") public static native void ExportModelProto(ModelProto p_m, @SharedPtr Graph g);

@Namespace("onnx") public static native @UniquePtr Graph ImportModelProto(@Const @ByRef ModelProto mp);

@Namespace("onnx") public static native @ByVal ModelProto PrepareOutput(@Const @ByRef ModelProto mp_in);

@Namespace("onnx") public static native void assertNonNull(@SharedPtr Graph g);
 // namespace ONNX_NAMESPACE


// Parsed from onnx/version_converter/adapters/adapter.h

// Interface for Op Version Adapters

// #pragma once

// #include "onnx/onnx_pb.h"
// #include "onnx/version_converter/helper.h"
// Targeting ../Adapter.java



 // namespace ONNX_NAMESPACE::version_conversion


// Parsed from onnx/version_converter/helper.h

// Helper Methods for Adapters

// #pragma once

// #include "onnx/common/ir.h"
    @Namespace("onnx::version_conversion") public static native int check_numpy_unibroadcastable_and_require_broadcast(
            @StdVector DimensionIR input1_sizes,
            @StdVector DimensionIR input2_sizes);

    @Namespace("onnx::version_conversion") public static native void assert_numpy_multibroadcastable(@StdVector DimensionIR input1_sizes,
            @StdVector DimensionIR input2_sizes);

    @Namespace("onnx::version_conversion") public static native void assertNotParams(@StdVector DimensionIR sizes);



// Parsed from onnx/version_converter/BaseConverter.h

// Version converter interface for ONNX models between different opset versions.

// #pragma once

// #include "onnx/common/ir.h"
// #include "onnx/common/ir_pb_converter.h"
// #include "onnx/common/stl_backports.h"
// #include "onnx/proto_utils.h"
// #include "onnx/defs/schema.h"
// #include <utility>
// #include <iostream>
// #include <stdlib.h>
// #include "onnx/version_converter/adapters/adapter.h"
// Targeting ../BaseVersionConverter.java



 // namespace ONNX_NAMESPACE::version_conversion


// Parsed from onnx/version_converter/convert.h

// Default converter for ONNX models between different opset versions
// in the default domain ("" or "ai.onnx").

// #pragma once

// #include "onnx/version_converter/BaseConverter.h"
// #include "onnx/version_converter/adapters/argmax_argmin_12_11.h"
// #include "onnx/version_converter/adapters/averagepool_7_6.h"
// #include "onnx/version_converter/adapters/axes_attribute_to_input.h"
// #include "onnx/version_converter/adapters/axes_input_to_attribute.h"
// #include "onnx/version_converter/adapters/batch_normalization_6_5.h"
// #include "onnx/version_converter/adapters/batch_normalization_6_7.h"
// #include "onnx/version_converter/adapters/batch_normalization_8_9.h"
// #include "onnx/version_converter/adapters/broadcast_backward_compatibility.h"
// #include "onnx/version_converter/adapters/broadcast_forward_compatibility.h"
// #include "onnx/version_converter/adapters/cast_9_8.h"
// #include "onnx/version_converter/adapters/clip_10_11.h"
// #include "onnx/version_converter/adapters/compatible.h"
// #include "onnx/version_converter/adapters/concat_3_4.h"
// #include "onnx/version_converter/adapters/dropout_11_12.h"
// #include "onnx/version_converter/adapters/dropout_6_7.h"
// #include "onnx/version_converter/adapters/extend_supported_types.h"
// #include "onnx/version_converter/adapters/gemm_6_7.h"
// #include "onnx/version_converter/adapters/gemm_7_6.h"
// #include "onnx/version_converter/adapters/maxpool_8_7.h"
// #include "onnx/version_converter/adapters/no_previous_version.h"
// #include "onnx/version_converter/adapters/remove_consumed_inputs.h"
// #include "onnx/version_converter/adapters/reshape_4_5.h"
// #include "onnx/version_converter/adapters/reshape_5_4.h"
// #include "onnx/version_converter/adapters/scan_8_9.h"
// #include "onnx/version_converter/adapters/scan_9_8.h"
// #include "onnx/version_converter/adapters/set_is_test.h"
// #include "onnx/version_converter/adapters/split_12_13.h"
// #include "onnx/version_converter/adapters/split_13_12.h"
// #include "onnx/version_converter/adapters/sum_8_7.h"
// #include "onnx/version_converter/adapters/slice_9_10.h"
// #include "onnx/version_converter/adapters/type_restriction.h"
// #include "onnx/version_converter/adapters/upsample_8_9.h"
// #include "onnx/version_converter/adapters/upsample_9_8.h"
// Targeting ../DefaultVersionConverter.java



@Namespace("onnx::version_conversion") public static native @ByVal ModelProto ConvertVersion(
    @Const @ByRef ModelProto mp_in,
    int target_version);
 // namespace ONNX_NAMESPACE::version_conversion


// Parsed from onnx/optimizer/pass_registry.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include "onnx/common/ir.h"
// #include "onnx/common/ir_pb_converter.h"
// #include "onnx/common/stl_backports.h"
// #include "onnx/optimizer/passes/eliminate_deadend.h"
// #include "onnx/optimizer/passes/eliminate_identity.h"
// #include "onnx/optimizer/passes/eliminate_nop_dropout.h"
// #include "onnx/optimizer/passes/eliminate_nop_monotone_argmax.h"
// #include "onnx/optimizer/passes/eliminate_nop_pad.h"
// #include "onnx/optimizer/passes/eliminate_nop_transpose.h"
// #include "onnx/optimizer/passes/eliminate_unused_initializer.h"
// #include "onnx/optimizer/passes/extract_constant_to_initializer.h"
// #include "onnx/optimizer/passes/fuse_add_bias_into_conv.h"
// #include "onnx/optimizer/passes/fuse_bn_into_conv.h"
// #include "onnx/optimizer/passes/fuse_consecutive_concats.h"
// #include "onnx/optimizer/passes/fuse_consecutive_log_softmax.h"
// #include "onnx/optimizer/passes/fuse_consecutive_reduce_unsqueeze.h"
// #include "onnx/optimizer/passes/fuse_consecutive_squeezes.h"
// #include "onnx/optimizer/passes/fuse_consecutive_transposes.h"
// #include "onnx/optimizer/passes/fuse_matmul_add_bias_into_gemm.h"
// #include "onnx/optimizer/passes/fuse_pad_into_conv.h"
// #include "onnx/optimizer/passes/fuse_transpose_into_gemm.h"
// #include "onnx/optimizer/passes/lift_lexical_references.h"
// #include "onnx/optimizer/passes/nop.h"
// #include "onnx/optimizer/passes/split.h"
// #include "onnx/proto_utils.h"

// #include <unordered_set>
// #include <vector>
// Targeting ../GlobalPassRegistry.java


 // namespace optimization
 // namespace ONNX_NAMESPACE


// Parsed from onnx/optimizer/pass.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include <string>
// #include "onnx/common/ir.h"
// #include "onnx/onnx_pb.h"
// Targeting ../PostPassAnalysis.java



// Enum that represents the type of optimization it is.
/** enum onnx::optimization::PassType */
public static final int
  // Class of optimizations that fuses operations.
  Fuse = 0,
  // Class of optimizations that removes useless operations.
  Nop = 1,
  // Class of optimizations that includes some form of seperation.
  Separate = 2,
  // Immutable pass, also sometimes referred to as an analysis pass.
  Immutable = 3,
  // Other type of pass.
  Other = 4;

// Enum that represents the return type of the analysis.
/** enum onnx::optimization::PassAnalysisType */
public static final int
  // An empty analysis is returned. Most likely will return PostPassAnalysis.
  Empty = 0,
  // A count based analysis is returned. Most likely of type
  // CountBasedPassAnalysis
  CountBased = 1;

/** enum onnx::optimization::PassEfficiency */
public static final int
  // A partially efficient optimization pass cannot guarantee that running two
  // consecutive passes
  // will return the same result as running a single pass.
  Partial = 0,
  // A completely efficient optimization guarantees that running two consecutive
  // passes is equivalent
  // to running a single pass.
  Complete = 1;

// Describes what the optimization pass is attempting to optimize.
/** enum onnx::optimization::PassOptimizationType */
public static final int
  // Is not optimizing anything. Most likely will be used in an immutable pass.
  None = 0,
  // Optimizes for compute.
  Compute = 1,
  // Optimizes for memory.
  Memory = 2,
  // Optimizes for both compute and memory.
  ComputeMemory = 3,
  // Optimizes for stability (e.g. log-sum-exp trick).
  Stability = 4;

/** enum onnx::optimization::NodeDestroyType */
public static final int
  // Does not destroy node
  DestroyZero = 0,
  // Equivalent to calling it.destroyCurrent() once.
  DestroyOne = 1,
  // Equivalent to calling it.destroyCurrent() twice.
  DestroyTwo = 2;
// Targeting ../Pass.java


// Targeting ../CountBasedPassAnalysis.java


// Targeting ../PredicateBasedPass.java



// The most general pass which allows the user to run a pass given only a graph.

 // namespace optimization
 // namespace ONNX_NAMESPACE


// Parsed from onnx/defs/function.h

// Copyright (c) ONNX Project Contributors.
// Licensed under the MIT license.

// #pragma once

// #include <mutex>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include "attr_proto_util.h"
// #include "onnx/common/constants.h"
// #include "onnx/common/status.h"
// #include "onnx/onnx-operators_pb.h"
// #include "tensor_proto_util.h"
// Helper function to expand a function node given the function proto
@Namespace("onnx") public static native void FunctionExpandHelper(
    @Const @ByRef NodeProto node,
    @Const @ByRef FunctionProto func,
    @ByRef GraphProto g,
    @StdString BytePointer node_prefix/*=""*/);
@Namespace("onnx") public static native void FunctionExpandHelper(
    @Const @ByRef NodeProto node,
    @Const @ByRef FunctionProto func,
    @ByRef GraphProto g);
@Namespace("onnx") public static native void FunctionExpandHelper(
    @Const @ByRef NodeProto node,
    @Const @ByRef FunctionProto func,
    @ByRef GraphProto g,
    @StdString String node_prefix/*=""*/);
// Targeting ../FunctionBodyHelper.java



 // namespace ONNX_NAMESPACE


// Parsed from onnx/optimizer/optimize.h

// ATTENTION: The code in this file is highly EXPERIMENTAL.
// Adventurous users should note that the APIs will probably change.

// #pragma once

// #include "onnx/common/ir.h"
// #include "onnx/common/ir_pb_converter.h"
// #include "onnx/common/stl_backports.h"
// #include "onnx/optimizer/pass_manager.h"
// #include "onnx/optimizer/pass_registry.h"
// #include "onnx/proto_utils.h"

// #include "vector"
// Targeting ../Optimizer.java



@Namespace("onnx::optimization") public static native @Const @ByVal StringVector GetAvailablePasses();

@Namespace("onnx::optimization") public static native @ByVal ModelProto Optimize(
    @Const @ByRef ModelProto mp_in,
    @Const @ByRef StringVector names);

@Namespace("onnx::optimization") public static native @ByVal ModelProto OptimizeFixed(
    @Const @ByRef ModelProto mp_in,
    @Const @ByRef StringVector names);
 // namespace optimization
 // namespace ONNX_NAMESPACE


}

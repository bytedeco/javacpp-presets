// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflowlite;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.tensorflowlite.global.tensorflowlite.*;


/** Options class for {@code Interpreter}.
 *  WARNING: This is an experimental API and subject to change. */
@Namespace("tflite") @NoOffset @Properties(inherit = org.bytedeco.tensorflowlite.presets.tensorflowlite.class)
public class InterpreterOptions extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public InterpreterOptions(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public InterpreterOptions(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public InterpreterOptions position(long position) {
        return (InterpreterOptions)super.position(position);
    }
    @Override public InterpreterOptions getPointer(long i) {
        return new InterpreterOptions((Pointer)this).offsetAddress(i);
    }

  public InterpreterOptions() { super((Pointer)null); allocate(); }
  private native void allocate();

  /** Preserving all intermediates tensors for debugging.
   *  WARNING: This is an experimental API and subject to change. */
  public native void SetPreserveAllTensors(@Cast("bool") boolean value/*=true*/);
  public native void SetPreserveAllTensors();

  /** Returns if the {@code experimental_preserve_all_tensors_} feature is enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native @Cast("bool") boolean GetPreserveAllTensors();

  /** Force all intermediate dynamic tensors to be released once they are not
   *  used by the model. Please use this configuration with caution, since it
   *  might reduce the peak memory usage of the model at the cost of a slower
   *  inference speed.
   *  WARNING: This is an experimental API and subject to change. */
  public native void SetEnsureDynamicTensorsAreReleased(@Cast("bool") boolean value/*=true*/);
  public native void SetEnsureDynamicTensorsAreReleased();

  /** Returns if the {@code experimental_ensure_dynamic_tensors_are_released_} feature
   *  is enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native @Cast("bool") boolean GetEnsureDynamicTensorsAreReleased();

  /** Use dynamic tensor allocation and deallocation method for large tensors
   *  instead of static memory planner. Dynamic tensors are allocated just
   *  before when they're needed and released when they're not needed anymore.
   *  It improves peak memory usage but there could be some latency impact. The
   *  value (in bytes, and default is 1024 * 1024) is used to determine large
   *  tensors.
   *  WARNING: This is an experimental API and subject to change. */
  public native void OptimizeMemoryForLargeTensors(int value/*=(1 << 20)*/);
  public native void OptimizeMemoryForLargeTensors();

  /** Returns the size (in bytes) threshold for dynamic tensor allocation
   *  method. It returns zero if the feature is not enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native int GetDynamicAllocationForLargeTensors();

  // Returns true iff delegate clustering (i.e., reordering execution such that
  // the number of switches between non-delegated and delegated execution of
  // nodes is minimized) is disabled.
  // WARNING: This is an experimental API and subject to change.
  public native @Cast("bool") boolean GetDisableDelegateClustering();

  // If value == true, disable delegate clustering (see above), otherwise,
  // enable it.
  // WARNING: This is an experimental API and subject to change.
  public native void SetDisableDelegateClustering(@Cast("bool") boolean value/*=true*/);
  public native void SetDisableDelegateClustering();
}

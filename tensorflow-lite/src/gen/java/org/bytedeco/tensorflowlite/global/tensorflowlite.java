// Targeted by JavaCPP version 1.5.10: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflowlite.global;

import org.bytedeco.tensorflowlite.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class tensorflowlite extends org.bytedeco.tensorflowlite.presets.tensorflowlite {
    static { Loader.load(); }

// Targeting ../StringIntMap.java


// Targeting ../StringStringMap.java


// Targeting ../TfLiteDelegatePtrVector.java


// Targeting ../StringVector.java


// Targeting ../NodeSubsetVector.java


// Targeting ../SubgraphVector.java


// Targeting ../IntIntPairVector.java


// Targeting ../RegistrationNodePairVector.java


// Targeting ../IntIntPair.java


// Targeting ../RegistrationNodePair.java


// Targeting ../IntResourceBaseMap.java


// Parsed from tensorflow/lite/builtin_ops.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_BUILTIN_OPS_H_
// #define TENSORFLOW_LITE_BUILTIN_OPS_H_

// DO NOT EDIT MANUALLY: This file is automatically generated by
// `schema/builtin_ops_header/generator.cc`.

// #ifdef __cplusplus
// #endif  // __cplusplus

// The enum for builtin operators.
// Note: CUSTOM, DELEGATE, and PLACEHOLDER_FOR_GREATER_OP_CODES are 3 special
// ops which are not real built-in ops.
/** enum TfLiteBuiltinOperator */
public static final int
  kTfLiteBuiltinAdd = 0,
  kTfLiteBuiltinAveragePool2d = 1,
  kTfLiteBuiltinConcatenation = 2,
  kTfLiteBuiltinConv2d = 3,
  kTfLiteBuiltinDepthwiseConv2d = 4,
  kTfLiteBuiltinDepthToSpace = 5,
  kTfLiteBuiltinDequantize = 6,
  kTfLiteBuiltinEmbeddingLookup = 7,
  kTfLiteBuiltinFloor = 8,
  kTfLiteBuiltinFullyConnected = 9,
  kTfLiteBuiltinHashtableLookup = 10,
  kTfLiteBuiltinL2Normalization = 11,
  kTfLiteBuiltinL2Pool2d = 12,
  kTfLiteBuiltinLocalResponseNormalization = 13,
  kTfLiteBuiltinLogistic = 14,
  kTfLiteBuiltinLshProjection = 15,
  kTfLiteBuiltinLstm = 16,
  kTfLiteBuiltinMaxPool2d = 17,
  kTfLiteBuiltinMul = 18,
  kTfLiteBuiltinRelu = 19,
  kTfLiteBuiltinReluN1To1 = 20,
  kTfLiteBuiltinRelu6 = 21,
  kTfLiteBuiltinReshape = 22,
  kTfLiteBuiltinResizeBilinear = 23,
  kTfLiteBuiltinRnn = 24,
  kTfLiteBuiltinSoftmax = 25,
  kTfLiteBuiltinSpaceToDepth = 26,
  kTfLiteBuiltinSvdf = 27,
  kTfLiteBuiltinTanh = 28,
  kTfLiteBuiltinConcatEmbeddings = 29,
  kTfLiteBuiltinSkipGram = 30,
  kTfLiteBuiltinCall = 31,
  kTfLiteBuiltinCustom = 32,
  kTfLiteBuiltinEmbeddingLookupSparse = 33,
  kTfLiteBuiltinPad = 34,
  kTfLiteBuiltinUnidirectionalSequenceRnn = 35,
  kTfLiteBuiltinGather = 36,
  kTfLiteBuiltinBatchToSpaceNd = 37,
  kTfLiteBuiltinSpaceToBatchNd = 38,
  kTfLiteBuiltinTranspose = 39,
  kTfLiteBuiltinMean = 40,
  kTfLiteBuiltinSub = 41,
  kTfLiteBuiltinDiv = 42,
  kTfLiteBuiltinSqueeze = 43,
  kTfLiteBuiltinUnidirectionalSequenceLstm = 44,
  kTfLiteBuiltinStridedSlice = 45,
  kTfLiteBuiltinBidirectionalSequenceRnn = 46,
  kTfLiteBuiltinExp = 47,
  kTfLiteBuiltinTopkV2 = 48,
  kTfLiteBuiltinSplit = 49,
  kTfLiteBuiltinLogSoftmax = 50,
  kTfLiteBuiltinDelegate = 51,
  kTfLiteBuiltinBidirectionalSequenceLstm = 52,
  kTfLiteBuiltinCast = 53,
  kTfLiteBuiltinPrelu = 54,
  kTfLiteBuiltinMaximum = 55,
  kTfLiteBuiltinArgMax = 56,
  kTfLiteBuiltinMinimum = 57,
  kTfLiteBuiltinLess = 58,
  kTfLiteBuiltinNeg = 59,
  kTfLiteBuiltinPadv2 = 60,
  kTfLiteBuiltinGreater = 61,
  kTfLiteBuiltinGreaterEqual = 62,
  kTfLiteBuiltinLessEqual = 63,
  kTfLiteBuiltinSelect = 64,
  kTfLiteBuiltinSlice = 65,
  kTfLiteBuiltinSin = 66,
  kTfLiteBuiltinTransposeConv = 67,
  kTfLiteBuiltinSparseToDense = 68,
  kTfLiteBuiltinTile = 69,
  kTfLiteBuiltinExpandDims = 70,
  kTfLiteBuiltinEqual = 71,
  kTfLiteBuiltinNotEqual = 72,
  kTfLiteBuiltinLog = 73,
  kTfLiteBuiltinSum = 74,
  kTfLiteBuiltinSqrt = 75,
  kTfLiteBuiltinRsqrt = 76,
  kTfLiteBuiltinShape = 77,
  kTfLiteBuiltinPow = 78,
  kTfLiteBuiltinArgMin = 79,
  kTfLiteBuiltinFakeQuant = 80,
  kTfLiteBuiltinReduceProd = 81,
  kTfLiteBuiltinReduceMax = 82,
  kTfLiteBuiltinPack = 83,
  kTfLiteBuiltinLogicalOr = 84,
  kTfLiteBuiltinOneHot = 85,
  kTfLiteBuiltinLogicalAnd = 86,
  kTfLiteBuiltinLogicalNot = 87,
  kTfLiteBuiltinUnpack = 88,
  kTfLiteBuiltinReduceMin = 89,
  kTfLiteBuiltinFloorDiv = 90,
  kTfLiteBuiltinReduceAny = 91,
  kTfLiteBuiltinSquare = 92,
  kTfLiteBuiltinZerosLike = 93,
  kTfLiteBuiltinFill = 94,
  kTfLiteBuiltinFloorMod = 95,
  kTfLiteBuiltinRange = 96,
  kTfLiteBuiltinResizeNearestNeighbor = 97,
  kTfLiteBuiltinLeakyRelu = 98,
  kTfLiteBuiltinSquaredDifference = 99,
  kTfLiteBuiltinMirrorPad = 100,
  kTfLiteBuiltinAbs = 101,
  kTfLiteBuiltinSplitV = 102,
  kTfLiteBuiltinUnique = 103,
  kTfLiteBuiltinCeil = 104,
  kTfLiteBuiltinReverseV2 = 105,
  kTfLiteBuiltinAddN = 106,
  kTfLiteBuiltinGatherNd = 107,
  kTfLiteBuiltinCos = 108,
  kTfLiteBuiltinWhere = 109,
  kTfLiteBuiltinRank = 110,
  kTfLiteBuiltinElu = 111,
  kTfLiteBuiltinReverseSequence = 112,
  kTfLiteBuiltinMatrixDiag = 113,
  kTfLiteBuiltinQuantize = 114,
  kTfLiteBuiltinMatrixSetDiag = 115,
  kTfLiteBuiltinRound = 116,
  kTfLiteBuiltinHardSwish = 117,
  kTfLiteBuiltinIf = 118,
  kTfLiteBuiltinWhile = 119,
  kTfLiteBuiltinNonMaxSuppressionV4 = 120,
  kTfLiteBuiltinNonMaxSuppressionV5 = 121,
  kTfLiteBuiltinScatterNd = 122,
  kTfLiteBuiltinSelectV2 = 123,
  kTfLiteBuiltinDensify = 124,
  kTfLiteBuiltinSegmentSum = 125,
  kTfLiteBuiltinBatchMatmul = 126,
  kTfLiteBuiltinPlaceholderForGreaterOpCodes = 127,
  kTfLiteBuiltinCumsum = 128,
  kTfLiteBuiltinCallOnce = 129,
  kTfLiteBuiltinBroadcastTo = 130,
  kTfLiteBuiltinRfft2d = 131,
  kTfLiteBuiltinConv3d = 132,
  kTfLiteBuiltinImag = 133,
  kTfLiteBuiltinReal = 134,
  kTfLiteBuiltinComplexAbs = 135,
  kTfLiteBuiltinHashtable = 136,
  kTfLiteBuiltinHashtableFind = 137,
  kTfLiteBuiltinHashtableImport = 138,
  kTfLiteBuiltinHashtableSize = 139,
  kTfLiteBuiltinReduceAll = 140,
  kTfLiteBuiltinConv3dTranspose = 141,
  kTfLiteBuiltinVarHandle = 142,
  kTfLiteBuiltinReadVariable = 143,
  kTfLiteBuiltinAssignVariable = 144,
  kTfLiteBuiltinBroadcastArgs = 145,
  kTfLiteBuiltinRandomStandardNormal = 146,
  kTfLiteBuiltinBucketize = 147,
  kTfLiteBuiltinRandomUniform = 148,
  kTfLiteBuiltinMultinomial = 149,
  kTfLiteBuiltinGelu = 150,
  kTfLiteBuiltinDynamicUpdateSlice = 151,
  kTfLiteBuiltinRelu0To1 = 152,
  kTfLiteBuiltinUnsortedSegmentProd = 153,
  kTfLiteBuiltinUnsortedSegmentMax = 154,
  kTfLiteBuiltinUnsortedSegmentSum = 155,
  kTfLiteBuiltinAtan2 = 156,
  kTfLiteBuiltinUnsortedSegmentMin = 157,
  kTfLiteBuiltinSign = 158,
  kTfLiteBuiltinBitcast = 159,
  kTfLiteBuiltinBitwiseXor = 160,
  kTfLiteBuiltinRightShift = 161,
  kTfLiteBuiltinStablehloLogistic = 162,
  kTfLiteBuiltinStablehloAdd = 163,
  kTfLiteBuiltinStablehloDivide = 164,
  kTfLiteBuiltinStablehloMultiply = 165,
  kTfLiteBuiltinStablehloMaximum = 166,
  kTfLiteBuiltinStablehloReshape = 167,
  kTfLiteBuiltinStablehloClamp = 168,
  kTfLiteBuiltinStablehloConcatenate = 169,
  kTfLiteBuiltinStablehloBroadcastInDim = 170,
  kTfLiteBuiltinStablehloConvolution = 171,
  kTfLiteBuiltinStablehloSlice = 172,
  kTfLiteBuiltinStablehloCustomCall = 173,
  kTfLiteBuiltinStablehloReduce = 174,
  kTfLiteBuiltinStablehloAbs = 175,
  kTfLiteBuiltinStablehloAnd = 176,
  kTfLiteBuiltinStablehloCosine = 177,
  kTfLiteBuiltinStablehloExponential = 178,
  kTfLiteBuiltinStablehloFloor = 179,
  kTfLiteBuiltinStablehloLog = 180,
  kTfLiteBuiltinStablehloMinimum = 181,
  kTfLiteBuiltinStablehloNegate = 182,
  kTfLiteBuiltinStablehloOr = 183,
  kTfLiteBuiltinStablehloPower = 184,
  kTfLiteBuiltinStablehloRemainder = 185,
  kTfLiteBuiltinStablehloRsqrt = 186,
  kTfLiteBuiltinStablehloSelect = 187,
  kTfLiteBuiltinStablehloSubtract = 188,
  kTfLiteBuiltinStablehloTanh = 189,
  kTfLiteBuiltinStablehloScatter = 190,
  kTfLiteBuiltinStablehloCompare = 191,
  kTfLiteBuiltinStablehloConvert = 192,
  kTfLiteBuiltinStablehloDynamicSlice = 193,
  kTfLiteBuiltinStablehloDynamicUpdateSlice = 194,
  kTfLiteBuiltinStablehloPad = 195,
  kTfLiteBuiltinStablehloIota = 196,
  kTfLiteBuiltinStablehloDotGeneral = 197,
  kTfLiteBuiltinStablehloReduceWindow = 198,
  kTfLiteBuiltinStablehloSort = 199,
  kTfLiteBuiltinStablehloWhile = 200,
  kTfLiteBuiltinStablehloGather = 201,
  kTfLiteBuiltinStablehloTranspose = 202,
  kTfLiteBuiltinDilate = 203,
  kTfLiteBuiltinStablehloRngBitGenerator = 204,
  kTfLiteBuiltinReduceWindow = 205;

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_BUILTIN_OPS_H_


// Parsed from tensorflow/lite/c/c_api_types.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_TYPES_H_

///
///
// #define TENSORFLOW_LITE_C_C_API_TYPES_H_

/** \file
 * 
 *  C API types for TensorFlow Lite.
 * 
 *  For documentation, see tensorflow/lite/core/c/c_api_types.h */

// #include "tensorflow/lite/core/c/c_api_types.h"

// #endif  // TENSORFLOW_LITE_C_C_API_TYPES_H_


// Parsed from tensorflow/lite/core/c/c_api_types.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

/** This file declares types used by the pure C inference API defined in
/** c_api.h, some of which are also used in the C++ and C kernel and interpreter
/** APIs. */

// WARNING: Users of TensorFlow Lite should not include this file directly,
// but should instead include
// "third_party/tensorflow/lite/c/c_api_types.h".
// Only the TensorFlow Lite implementation itself should include this
// file directly.

// IWYU pragma: private, include "third_party/tensorflow/lite/c/c_api_types.h"

// #ifndef TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_

// #include <stdint.h>

// #ifdef __cplusplus
// #endif

/** \addtogroup c_api_types tensorflow/lite/c/c_api_types.h
 *  \{
 */

// Define TFL_CAPI_EXPORT macro to export a function properly with a shared
// library.
// #ifdef SWIG
// #define TFL_CAPI_EXPORT
// #elif defined(TFL_STATIC_LIBRARY_BUILD)
// #define TFL_CAPI_EXPORT
// #else  // not definded TFL_STATIC_LIBRARY_BUILD
// #if defined(_WIN32)
// #ifdef TFL_COMPILE_LIBRARY
// #define TFL_CAPI_EXPORT __declspec(dllexport)
// #else
// #define TFL_CAPI_EXPORT __declspec(dllimport)
// #endif  // TFL_COMPILE_LIBRARY
// #else
// #define TFL_CAPI_EXPORT __attribute__((visibility("default")))
// #endif  // _WIN32
// #endif  // SWIG

/** Note that new error status values may be added in future in order to
 *  indicate more fine-grained internal states, therefore, applications should
 *  not rely on status values being members of the enum. */
/** enum TfLiteStatus */
public static final int
  /** Success */
  kTfLiteOk = 0,

  /** Generally referring to an error in the runtime (i.e. interpreter) */
  kTfLiteError = 1,

  /** Generally referring to an error from a TfLiteDelegate itself. */
  kTfLiteDelegateError = 2,

  /** Generally referring to an error in applying a delegate due to
   *  incompatibility between runtime and delegate, e.g., this error is returned
   *  when trying to apply a TF Lite delegate onto a model graph that's already
   *  immutable. */
  kTfLiteApplicationError = 3,

  /** Generally referring to serialized delegate data not being found.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataNotFound = 4,

  /** Generally referring to data-writing issues in delegate serialization.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataWriteError = 5,

  /** Generally referring to data-reading issues in delegate serialization.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataReadError = 6,

  /** Generally referring to issues when the TF Lite model has ops that cannot
   *  be resolved at runtime. This could happen when the specific op is not
   *  registered or built with the TF Lite framework. */
  kTfLiteUnresolvedOps = 7,

  /** Generally referring to invocation cancelled by the user.
   *  See {@code interpreter::Cancel}. */
  // TODO(b/194915839): Implement `interpreter::Cancel`.
  // TODO(b/250636993): Cancellation triggered by `SetCancellationFunction`
  // should also return this status code.
  kTfLiteCancelled = 8;

/** Types supported by tensor */
/** enum TfLiteType */
public static final int
  kTfLiteNoType = 0,
  kTfLiteFloat32 = 1,
  kTfLiteInt32 = 2,
  kTfLiteUInt8 = 3,
  kTfLiteInt64 = 4,
  kTfLiteString = 5,
  kTfLiteBool = 6,
  kTfLiteInt16 = 7,
  kTfLiteComplex64 = 8,
  kTfLiteInt8 = 9,
  kTfLiteFloat16 = 10,
  kTfLiteFloat64 = 11,
  kTfLiteComplex128 = 12,
  kTfLiteUInt64 = 13,
  kTfLiteResource = 14,
  kTfLiteVariant = 15,
  kTfLiteUInt32 = 16,
  kTfLiteUInt16 = 17,
  kTfLiteInt4 = 18;
// Targeting ../TfLiteQuantizationParams.java


// Targeting ../TfLiteOpaqueContext.java


// Targeting ../TfLiteOpaqueNode.java


// Targeting ../TfLiteOpaqueTensor.java



/** TfLiteDelegate: allows delegation of nodes to alternative backends.
 *  Forward declaration of concrete type declared in common.h. */
// Targeting ../TfLiteOpaqueDelegateStruct.java



/** TfLiteOpaqueDelegate: conditionally opaque version of
 *  TfLiteDelegate; allows delegation of nodes to alternative backends.
 *  For TF Lite in Play Services, this is an opaque type,
 *  but for regular TF Lite, this is just a typedef for TfLiteDelegate.
 *  WARNING: This is an experimental type and subject to change. */
// #if TFLITE_WITH_STABLE_ABI || TFLITE_USE_OPAQUE_DELEGATE
// #else
// #endif

/** \} */

// #ifdef __cplusplus  // extern C
// #endif
// #endif  // TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_


// Parsed from tensorflow/lite/c/c_api.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_H_

///
///
// #define TENSORFLOW_LITE_C_C_API_H_

/** \file
 * 
 *  C API for TensorFlow Lite.
 * 
 *  For documentation, see tensorflow/lite/core/c/c_api.h */

// #include "tensorflow/lite/core/c/c_api.h"

// #endif  // TENSORFLOW_LITE_C_C_API_H_


// Parsed from tensorflow/lite/core/c/c_api.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// \warning Note: Users of TensorFlow Lite should not include this file
// directly, but should instead include
// "third_party/tensorflow/lite/c/c_api.h". Only the TensorFlow Lite
// implementation itself should include this
// file directly.

// #ifndef TENSORFLOW_LITE_CORE_C_C_API_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_H_

// #include <stdarg.h>
// #include <stdbool.h>
// #include <stdint.h>
// #include <stdlib.h>

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/core/async/c/types.h"
// #include "tensorflow/lite/core/c/c_api_types.h"  // IWYU pragma: export

///
///
///
///
///
///
///
///
// #include "tensorflow/lite/core/c/registration_external.h"  // IWYU pragma: export

/** C API for TensorFlow Lite:
 * 
 *  The API leans towards simplicity and uniformity instead of convenience, as
 *  most usage will be by language-specific wrappers. It provides largely the
 *  same set of functionality as that of the C++ TensorFlow Lite {@code Interpreter}
 *  API, but is useful for shared libraries where having a stable ABI boundary
 *  is important.
 * 
 *  Conventions:
 *  * We use the prefix TfLite for everything in the API.
 *  * size_t is used to represent byte sizes of objects that are
 *    materialized in the address space of the calling process.
 *  * int is used as an index into arrays.
 * 
 *  Usage:
 *  <pre><code>
 *  // Create the model and interpreter options.
 *  TfLiteModel* model = TfLiteModelCreateFromFile("/path/to/model.tflite");
 *  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
 *  TfLiteInterpreterOptionsSetNumThreads(options, 2);
 * 
 *  // Create the interpreter.
 *  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
 * 
 *  // Allocate tensors and populate the input tensor data.
 *  TfLiteInterpreterAllocateTensors(interpreter);
 *  TfLiteTensor* input_tensor =
 *      TfLiteInterpreterGetInputTensor(interpreter, 0);
 *  TfLiteTensorCopyFromBuffer(input_tensor, input.data(),
 *                             input.size() * sizeof(float));
 * 
 *  // Execute inference.
 *  TfLiteInterpreterInvoke(interpreter);
 * 
 *  // Extract the output tensor data.
 *  const TfLiteTensor* output_tensor =
 *       TfLiteInterpreterGetOutputTensor(interpreter, 0);
 *  TfLiteTensorCopyToBuffer(output_tensor, output.data(),
 *                           output.size() * sizeof(float));
 * 
 *  // Dispose of the model and interpreter objects.
 *  TfLiteInterpreterDelete(interpreter);
 *  TfLiteInterpreterOptionsDelete(options);
 *  TfLiteModelDelete(model);
 *  </code></pre> */

// #ifdef __cplusplus
// Targeting ../TfLiteModel.java


// Targeting ../TfLiteInterpreterOptions.java


// Targeting ../TfLiteInterpreter.java



/** A tensor in the interpreter system which is a wrapper around a buffer of
 *  data including a dimensionality (or NULL if not currently defined). */
// Targeting ../TfLiteSignatureRunner.java



// --------------------------------------------------------------------------
/** The TensorFlow Lite Runtime version.
 * 
 *  Returns a pointer to a statically allocated string that is the version
 *  number of the (potentially dynamically loaded) TF Lite Runtime library.
 *  TensorFlow Lite uses semantic versioning, and the return value should be
 *  in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
 *  e.g. "2.12.0" or "2.13.0-rc2". */

///
///
///
public static native @Cast("const char*") BytePointer TfLiteVersion();

// --------------------------------------------------------------------------
/** The TensorFlow Lite Extension APIs version.
 * 
 *  Returns a pointer to a statically allocated string that is the version
 *  number of the TF Lite Extension APIs supported by the (potentially
 *  dynamically loaded) TF Lite Runtime library.  The TF Lite "Extension APIs"
 *  are the APIs for extending TF Lite with custom ops and delegates.
 *  More specifically, this version number covers the (non-experimental)
 *  functionality documented in the following header files:
 * 
 *    * lite/c/c_api_opaque.h
 *    * lite/c/common.h
 *    * lite/c/builtin_op_data.h
 *    * lite/builtin_ops.h
 * 
 *  This version number uses semantic versioning, and the return value should
 *  be in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
 *  e.g. "2.14.0" or "2.15.0-rc2". */

///
///
public static native @Cast("const char*") BytePointer TfLiteExtensionApisVersion();

/** The supported TensorFlow Lite model file Schema version.
 * 
 *  Returns the (major) version number of the Schema used for model
 *  files that is supported by the (potentially dynamically loaded)
 *  TensorFlow Lite Runtime.
 * 
 *  Model files using schema versions different to this may not be supported by
 *  the current version of the TF Lite Runtime. */

///
public static native int TfLiteSchemaVersion();

/** Returns a model from the provided buffer, or null on failure.
 * 
 *  \note The caller retains ownership of the {@code model_data} buffer and should
 *  ensure that the lifetime of the {@code model_data} buffer must be at least as long
 *  as the lifetime of the {@code TfLiteModel} and of any {@code TfLiteInterpreter} objects
 *  created from that {@code TfLiteModel}, and furthermore the contents of the
 *  {@code model_data} buffer must not be modified during that time." */
public static native TfLiteModel TfLiteModelCreate(@Const Pointer model_data,
                                                      @Cast("size_t") long model_size);
// Targeting ../Reporter_Pointer_BytePointer_Pointer.java



///
public static native TfLiteModel TfLiteModelCreateWithErrorReporter(
    @Const Pointer model_data, @Cast("size_t") long model_size,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
// Targeting ../Reporter_Pointer_String_Pointer.java


public static native TfLiteModel TfLiteModelCreateWithErrorReporter(
    @Const Pointer model_data, @Cast("size_t") long model_size,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Returns a model from the provided file, or null on failure.
 * 
 *  \note The file's contents must not be modified during the lifetime of the
 *  {@code TfLiteModel} or of any {@code TfLiteInterpreter} objects created from that
 *  {@code TfLiteModel}. */
public static native TfLiteModel TfLiteModelCreateFromFile(
    @Cast("const char*") BytePointer model_path);
public static native TfLiteModel TfLiteModelCreateFromFile(
    String model_path);

/** Same as {@code TfLiteModelCreateFromFile} with customizble error reporter.
 *  * {@code reporter} takes the provided {@code user_data} object, as well as a C-style
 *    format string and arg list (see also vprintf).
 *  * {@code user_data} is optional. If non-null, it is owned by the client and must
 *    remain valid for the duration of the interpreter lifetime. */
public static native TfLiteModel TfLiteModelCreateFromFileWithErrorReporter(
    @Cast("const char*") BytePointer model_path,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
public static native TfLiteModel TfLiteModelCreateFromFileWithErrorReporter(
    String model_path,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Destroys the model instance. */
public static native void TfLiteModelDelete(TfLiteModel model);

/** Returns a new interpreter options instances. */

///
public static native TfLiteInterpreterOptions TfLiteInterpreterOptionsCreate();

/** Creates and returns a shallow copy of an options object.
 * 
 *  The caller is responsible for calling {@code TfLiteInterpreterOptionsDelete} to
 *  deallocate the object pointed to by the returned pointer. */
public static native TfLiteInterpreterOptions TfLiteInterpreterOptionsCopy(
    @Const TfLiteInterpreterOptions from);

/** Destroys the interpreter options instance. */
public static native void TfLiteInterpreterOptionsDelete(
    TfLiteInterpreterOptions options);

/** Sets the number of CPU threads to use for the interpreter. */

///
///
///
public static native void TfLiteInterpreterOptionsSetNumThreads(
    TfLiteInterpreterOptions options, int num_threads);

/** Adds a delegate to be applied during {@code TfLiteInterpreter} creation.
 * 
 *  If delegate application fails, interpreter creation will also fail with an
 *  associated error logged.
 * 
 *  \note The caller retains ownership of the delegate and should ensure that it
 *  remains valid for the duration of any created interpreter's lifetime.
 * 
 *  If you are NOT using "TensorFlow Lite in Play Services", and NOT building
 *  with {@code TFLITE_WITH_STABLE_ABI} or {@code TFLITE_USE_OPAQUE_DELEGATE} macros
 *  enabled, it is possible to pass a {@code TfLiteDelegate*} rather than a
 *  {@code TfLiteOpaqueDelegate*} to this function, since in those cases,
 *  {@code TfLiteOpaqueDelegate} is just a typedef alias for {@code TfLiteDelegate}.
 *  This is for compatibility with existing source code
 *  and existing delegates.  For new delegates, it is recommended to
 *  use {@code TfLiteOpaqueDelegate} rather than {@code TfLiteDelegate}.  (See
 *  {@code TfLiteOpaqueDelegate} in tensorflow/lite/core/c/c_api_types.h.) */

///
public static native void TfLiteInterpreterOptionsAddDelegate(
    TfLiteInterpreterOptions options, @Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);

/** Sets a custom error reporter for interpreter execution.
 * 
 *  * {@code reporter} takes the provided {@code user_data} object, as well as a C-style
 *    format string and arg list (see also vprintf).
 *  * {@code user_data} is optional. If non-null, it is owned by the client and must
 *    remain valid for the duration of the interpreter lifetime. */

///
public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Adds an op registration to be applied during {@code TfLiteInterpreter} creation.
 * 
 *  The {@code TfLiteRegistrationExternal} object is needed to implement custom op of
 *  TFLite Interpreter via C API. Calling this function ensures that any
 *  {@code TfLiteInterpreter} created with the specified {@code options} can execute models
 *  that use the custom operator specified in {@code registration}.
 *  Please refer https://www.tensorflow.org/lite/guide/ops_custom for custom op
 *  support.
 *  \note The caller retains ownership of the TfLiteRegistrationExternal object
 *  and should ensure that it remains valid for the duration of any created
 *  interpreter's lifetime.
 *  \warning This is an experimental API and subject to change. */

///
///
public static native void TfLiteInterpreterOptionsAddRegistrationExternal(
    TfLiteInterpreterOptions options,
    TfLiteRegistrationExternal registration);

/** Enables users to cancel in-flight invocations with
 *  {@code TfLiteInterpreterCancel}.
 * 
 *  By default it is disabled and calling to {@code TfLiteInterpreterCancel} will
 *  return kTfLiteError. See {@code TfLiteInterpreterCancel}.
 * 
 *  \warning This is an experimental API and subject to change. */

///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterOptionsEnableCancellation(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Returns a new interpreter using the provided model and options, or null on
 *  failure.
 * 
 *  * {@code model} must be a valid model instance. The caller retains ownership of
 *    the object, and may destroy it (via TfLiteModelDelete) immediately after
 *    creating the interpreter.  However, if the TfLiteModel was allocated with
 *    TfLiteModelCreate, then the {@code model_data} buffer that was passed to
 *    TfLiteModelCreate must outlive the lifetime of the TfLiteInterpreter
 *    object that this function returns, and must not be modified during that
 *    time; and if the TfLiteModel was allocated with TfLiteModelCreateFromFile,
 *    then the contents of the model file must not be modified during the
 *    lifetime of the TfLiteInterpreter object that this function returns.
 *  * {@code optional_options} may be null. The caller retains ownership of the
 *    object, and can safely destroy it (via TfLiteInterpreterOptionsDelete)
 *    immediately after creating the interpreter.
 * 
 *  \note The client *must* explicitly allocate tensors before attempting to
 *  access input tensor data or invoke the interpreter. */
public static native TfLiteInterpreter TfLiteInterpreterCreate(
    @Const TfLiteModel model, @Const TfLiteInterpreterOptions optional_options);

/** Destroys the interpreter. */
public static native void TfLiteInterpreterDelete(
    TfLiteInterpreter interpreter);

/** Returns the number of input tensors associated with the model. */

///
///
public static native int TfLiteInterpreterGetInputTensorCount(
    @Const TfLiteInterpreter interpreter);

/** Returns a pointer to an array of input tensor indices.  The length of the
 *  array can be obtained via a call to {@code TfLiteInterpreterGetInputTensorCount}.
 * 
 *  Typically the input tensors associated with an {@code interpreter} would be set
 *  during the initialization of the {@code interpreter}, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code SetInputs} on the interpreter invalidate the returned pointer.
 * 
 *  The ownership of the array remains with the TFLite runtime. */
public static native @Const IntPointer TfLiteInterpreterInputTensorIndices(
    @Const TfLiteInterpreter interpreter);

/** Returns the tensor associated with the input index.
 *  REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor) */

///
///
///
public static native TfLiteTensor TfLiteInterpreterGetInputTensor(
    @Const TfLiteInterpreter interpreter, int input_index);

/** Resizes the specified input tensor.
 * 
 *  \note After a resize, the client *must* explicitly allocate tensors before
 *  attempting to access the resized tensor data or invoke the interpreter.
 * 
 *  REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
 * 
 *  This function makes a copy of the input dimensions, so the client can safely
 *  deallocate {@code input_dims} immediately after this function returns. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntPointer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntBuffer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const int[] input_dims,
    int input_dims_size);

/** Updates allocations for all tensors, resizing dependent tensors using the
 *  specified input tensor dimensionality.
 * 
 *  This is a relatively expensive operation, and need only be called after
 *  creating the graph and/or resizing any inputs. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterAllocateTensors(
    TfLiteInterpreter interpreter);

/** Runs inference for the loaded graph.
 * 
 *  Before calling this function, the caller should first invoke
 *  TfLiteInterpreterAllocateTensors() and should also set the values for the
 *  input tensors.  After successfully calling this function, the values for the
 *  output tensors will be set.
 * 
 *  \note It is possible that the interpreter is not in a ready state to
 *  evaluate (e.g., if AllocateTensors() hasn't been called, or if a
 *  ResizeInputTensor() has been performed without a subsequent call to
 *  AllocateTensors()).
 * 
 *    If the (experimental!) delegate fallback option was enabled in the
 *    interpreter options, then the interpreter will automatically fall back to
 *    not using any delegates if execution with delegates fails. For details,
 *    see TfLiteInterpreterOptionsSetEnableDelegateFallback in
 *    c_api_experimental.h.
 * 
 *  Returns one of the following status codes:
 *   - kTfLiteOk: Success. Output is valid.
 *   - kTfLiteDelegateError: Execution with delegates failed, due to a problem
 *     with the delegate(s). If fallback was not enabled, output is invalid.
 *     If fallback was enabled, this return value indicates that fallback
 *     succeeded, the output is valid, and all delegates previously applied to
 *     the interpreter have been undone.
 *   - kTfLiteApplicationError: Same as for kTfLiteDelegateError, except that
 *     the problem was not with the delegate itself, but rather was
 *     due to an incompatibility between the delegate(s) and the
 *     interpreter or model.
 *   - kTfLiteError: Unexpected/runtime failure. Output is invalid. */
public static native @Cast("TfLiteStatus") int TfLiteInterpreterInvoke(
    TfLiteInterpreter interpreter);

/** Returns the number of output tensors associated with the model. */

///
///
public static native int TfLiteInterpreterGetOutputTensorCount(
    @Const TfLiteInterpreter interpreter);

/** Returns a pointer to an array of output tensor indices.  The length of the
 *  array can be obtained via a call to {@code TfLiteInterpreterGetOutputTensorCount}.
 * 
 *  Typically the output tensors associated with an {@code interpreter} would be set
 *  during the initialization of the {@code interpreter}, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code SetOutputs} on the interpreter invalidate the returned pointer.
 * 
 *  The ownership of the array remains with the TFLite runtime. */

///
public static native @Const IntPointer TfLiteInterpreterOutputTensorIndices(
    @Const TfLiteInterpreter interpreter);

/** Returns the tensor associated with the output index.
 *  REQUIRES: 0 <= output_index < TfLiteInterpreterGetOutputTensorCount(tensor)
 * 
 *  \note The shape and underlying data buffer for output tensors may be not
 *  be available until after the output tensor has been both sized and
 *  allocated.
 *  In general, best practice is to interact with the output tensor *after*
 *  calling TfLiteInterpreterInvoke(). */

///
///
///
///
public static native @Const TfLiteTensor TfLiteInterpreterGetOutputTensor(
    @Const TfLiteInterpreter interpreter, int output_index);

/** Returns modifiable access to the tensor that corresponds to the
 *  specified {@code index} and is associated with the provided {@code interpreter}.
 * 
 *  This requires the {@code index} to be between 0 and N - 1, where N is the
 *  number of tensors in the model.
 * 
 *  Typically the tensors associated with the {@code interpreter} would be set during
 *  the {@code interpreter} initialization, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code AddTensors} on the interpreter invalidate the returned pointer.
 * 
 *  Note the difference between this function and
 *  {@code TfLiteInterpreterGetInputTensor} (or {@code TfLiteInterpreterGetOutputTensor} for
 *  that matter): {@code TfLiteInterpreterGetTensor} takes an index into the array of
 *  all tensors associated with the {@code interpreter}'s model, whereas
 *  {@code TfLiteInterpreterGetInputTensor} takes an index into the array of input
 *  tensors.
 * 
 *  The ownership of the tensor remains with the TFLite runtime, meaning the
 *  caller should not deallocate the pointer. */

///
///
///
public static native TfLiteTensor TfLiteInterpreterGetTensor(@Const TfLiteInterpreter interpreter,
                                         int index);

/** Tries to cancel any in-flight invocation.
 * 
 *  \note This only cancels {@code TfLiteInterpreterInvoke} calls that happen before
 *  calling this and it does not cancel subsequent invocations.
 *  \note Calling this function will also cancel any in-flight invocations of
 *  SignatureRunners constructed from this interpreter.
 *  Non-blocking and thread safe.
 * 
 *  Returns kTfLiteError if cancellation is not enabled via
 *  {@code TfLiteInterpreterOptionsEnableCancellation}.
 * 
 *  \warning This is an experimental API and subject to change. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterCancel(
    @Const TfLiteInterpreter interpreter);

/** --------------------------------------------------------------------------
 *  SignatureRunner APIs
 * 
 *  You can run inference by either:
 * 
 *  (i) (recommended) using the Interpreter to initialize SignatureRunner(s) and
 *      then only using SignatureRunner APIs.
 * 
 *  (ii) only using Interpreter APIs.
 * 
 *  NOTE:
 *  * Only use one of the above options to run inference, i.e. avoid mixing both
 *    SignatureRunner APIs and Interpreter APIs to run inference as they share
 *    the same underlying data (e.g. updating an input tensor “A” retrieved
 *    using the Interpreter APIs will update the state of the input tensor “B”
 *    retrieved using SignatureRunner APIs, if they point to the same underlying
 *    tensor in the model; as it is not possible for a user to debug this by
 *    analyzing the code, it can lead to undesirable behavior).
 *  * The TfLiteSignatureRunner type is conditionally thread-safe, provided that
 *    no two threads attempt to simultaneously access two TfLiteSignatureRunner
 *    instances that point to the same underlying signature, or access a
 *    TfLiteSignatureRunner and its underlying TfLiteInterpreter, unless all
 *    such simultaneous accesses are reads (rather than writes).
 *  * The lifetime of a TfLiteSignatureRunner object ends when
 *    TfLiteSignatureRunnerDelete() is called on it (or when the lifetime of the
 *    underlying TfLiteInterpreter ends -- but you should call
 *    TfLiteSignatureRunnerDelete() before that happens in order to avoid
 *    resource leaks).
 *  * You can only apply delegates to the interpreter (via
 *    TfLiteInterpreterOptions) and not to a signature.
 <p>
 *  Returns the number of signatures defined in the model. */

///
public static native int TfLiteInterpreterGetSignatureCount(
    @Const TfLiteInterpreter interpreter);

/** Returns the key of the Nth signature in the model, where N is specified as
 *  {@code signature_index}.
 * 
 *  NOTE: The lifetime of the returned key is the same as (and depends on) the
 *  lifetime of {@code interpreter}. */

///
///
public static native @Cast("const char*") BytePointer TfLiteInterpreterGetSignatureKey(
    @Const TfLiteInterpreter interpreter, int signature_index);

/** Returns a new signature runner using the provided interpreter and signature
 *  key, or nullptr on failure.
 * 
 *  NOTE: {@code signature_key} is a null-terminated C string that must match the
 *  key of a signature in the interpreter's model.
 * 
 *  NOTE: The returned signature runner should be destroyed, by calling
 *  TfLiteSignatureRunnerDelete(), before the interpreter is destroyed. */
public static native TfLiteSignatureRunner TfLiteInterpreterGetSignatureRunner(@Const TfLiteInterpreter interpreter,
                                    @Cast("const char*") BytePointer signature_key);
public static native TfLiteSignatureRunner TfLiteInterpreterGetSignatureRunner(@Const TfLiteInterpreter interpreter,
                                    String signature_key);

/** Returns the number of inputs associated with a signature. */

///
public static native @Cast("size_t") long TfLiteSignatureRunnerGetInputCount(
    @Const TfLiteSignatureRunner signature_runner);

/** Returns the (null-terminated) name of the Nth input in a signature, where N
 *  is specified as {@code input_index}.
 * 
 *  NOTE: The lifetime of the returned name is the same as (and depends on) the
 *  lifetime of {@code signature_runner}. */

///
///
///
///
public static native @Cast("const char*") BytePointer TfLiteSignatureRunnerGetInputName(
    @Const TfLiteSignatureRunner signature_runner, int input_index);

/** Resizes the input tensor identified as {@code input_name} to be the dimensions
 *  specified by {@code input_dims} and {@code input_dims_size}. Only unknown dimensions can
 *  be resized with this function. Unknown dimensions are indicated as {@code -1} in
 *  the {@code dims_signature} attribute of a TfLiteTensor.
 * 
 *  Returns status of failure or success. Note that this doesn't actually resize
 *  any existing buffers. A call to TfLiteSignatureRunnerAllocateTensors() is
 *  required to change the tensor input buffer.
 * 
 *  NOTE: This function is similar to TfLiteInterpreterResizeInputTensorStrict()
 *  and not TfLiteInterpreterResizeInputTensor().
 * 
 *  NOTE: {@code input_name} must match the name of an input in the signature.
 * 
 *  NOTE: This function makes a copy of the input dimensions, so the caller can
 *  safely deallocate {@code input_dims} immediately after this function returns. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const IntPointer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const IntBuffer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const int[] input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const IntPointer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const IntBuffer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const int[] input_dims, int input_dims_size);

/** Updates allocations for tensors associated with a signature and resizes
 *  dependent tensors using the specified input tensor dimensionality.
 *  This is a relatively expensive operation and hence should only be called
 *  after initializing the signature runner object and/or resizing any inputs. */

///
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerAllocateTensors(
    TfLiteSignatureRunner signature_runner);

/** Returns the input tensor identified by {@code input_name} in the given signature.
 *  Returns nullptr if the given name is not valid.
 * 
 *  NOTE: The lifetime of the returned tensor is the same as (and depends on)
 *  the lifetime of {@code signature_runner}. */

///
public static native TfLiteTensor TfLiteSignatureRunnerGetInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name);
public static native TfLiteTensor TfLiteSignatureRunnerGetInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name);

/** Runs inference on a given signature.
 * 
 *  Before calling this function, the caller should first invoke
 *  TfLiteSignatureRunnerAllocateTensors() and should also set the values for
 *  the input tensors. After successfully calling this function, the values for
 *  the output tensors will be set. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerInvoke(
    TfLiteSignatureRunner signature_runner);

/** Returns the number of output tensors associated with the signature. */

///
public static native @Cast("size_t") long TfLiteSignatureRunnerGetOutputCount(
    @Const TfLiteSignatureRunner signature_runner);

/** Returns the (null-terminated) name of the Nth output in a signature, where
 *  N is specified as {@code output_index}.
 * 
 *  NOTE: The lifetime of the returned name is the same as (and depends on) the
 *  lifetime of {@code signature_runner}. */

///
public static native @Cast("const char*") BytePointer TfLiteSignatureRunnerGetOutputName(
    @Const TfLiteSignatureRunner signature_runner, int output_index);

/** Returns the output tensor identified by {@code output_name} in the given
 *  signature. Returns nullptr if the given name is not valid.
 * 
 *  NOTE: The lifetime of the returned tensor is the same as (and depends on)
 *  the lifetime of {@code signature_runner}. */
public static native @Const TfLiteTensor TfLiteSignatureRunnerGetOutputTensor(
    @Const TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer output_name);
public static native @Const TfLiteTensor TfLiteSignatureRunnerGetOutputTensor(
    @Const TfLiteSignatureRunner signature_runner, String output_name);

// --------------------------------------------------------------------------
// TfLiteTensor wraps data associated with a graph tensor.
//
// Note that, while the TfLiteTensor struct is not currently opaque, and its
// fields can be accessed directly, these methods are still convenient for
// language bindings. In the future the tensor struct will likely be made opaque
// in the public API.

/** Returns the type of a tensor element. */
public static native @Cast("TfLiteType") int TfLiteTensorType(@Const TfLiteTensor tensor);

/** Returns the number of dimensions that the tensor has.  Returns -1 in case
 *  the 'opaque_tensor' does not have its dimensions property set. */
public static native int TfLiteTensorNumDims(@Const TfLiteTensor tensor);

/** Returns the length of the tensor in the "dim_index" dimension.
 *  REQUIRES: 0 <= dim_index < TFLiteTensorNumDims(tensor) */
public static native int TfLiteTensorDim(@Const TfLiteTensor tensor,
                                               int dim_index);

/** Returns the size of the underlying data in bytes. */

///
public static native @Cast("size_t") long TfLiteTensorByteSize(@Const TfLiteTensor tensor);

/** Returns a pointer to the underlying data buffer.
 * 
 *  \note The result may be null if tensors have not yet been allocated, e.g.,
 *  if the Tensor has just been created or resized and {@code TfLiteAllocateTensors()}
 *  has yet to be called, or if the output tensor is dynamically sized and the
 *  interpreter hasn't been invoked. */
public static native Pointer TfLiteTensorData(@Const TfLiteTensor tensor);

/** Returns the (null-terminated) name of the tensor. */
public static native @Cast("const char*") BytePointer TfLiteTensorName(@Const TfLiteTensor tensor);

/** Returns the parameters for asymmetric quantization. The quantization
 *  parameters are only valid when the tensor type is {@code kTfLiteUInt8} and the
 *  {@code scale != 0}. Quantized values can be converted back to float using:
 *     real_value = scale * (quantized_value - zero_point); */
public static native @ByVal TfLiteQuantizationParams TfLiteTensorQuantizationParams(
    @Const TfLiteTensor tensor);

/** Copies from the provided input buffer into the tensor's buffer.
 *  REQUIRES: input_data_size == TfLiteTensorByteSize(tensor) */
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyFromBuffer(
    TfLiteTensor tensor, @Const Pointer input_data, @Cast("size_t") long input_data_size);

/** Copies to the provided output buffer from the tensor's buffer.
 *  REQUIRES: output_data_size == TfLiteTensorByteSize(tensor) */
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyToBuffer(
    @Const TfLiteTensor output_tensor, Pointer output_data,
    @Cast("size_t") long output_data_size);

/** Destroys the signature runner. */
public static native void TfLiteSignatureRunnerDelete(
    TfLiteSignatureRunner signature_runner);

// NOLINTEND(modernize-redundant-void-arg)

/** \} */

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_C_API_H_


// Parsed from tensorflow/lite/core/c/registration_external.h

/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \warning Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/c/c_api.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_C_REGISTRATION_EXTERNAL_H_
// #define TENSORFLOW_LITE_CORE_C_REGISTRATION_EXTERNAL_H_

// #include <stdlib.h>

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/core/async/c/types.h"
// #include "tensorflow/lite/core/c/c_api_types.h"

// #ifdef __cplusplus
// Targeting ../TfLiteRegistrationExternal.java



// Returns a new TfLiteRegistrationExternal instance.
//
// \note The caller retains ownership and should ensure that
// the lifetime of the `TfLiteRegistrationExternal` must be at least as long as
// the lifetime of the `TfLiteInterpreter`.
//
// \warning This is an experimental API and subject to change.
public static native TfLiteRegistrationExternal TfLiteRegistrationExternalCreate(@Cast("TfLiteBuiltinOperator") int builtin_code,
                                 @Cast("const char*") BytePointer custom_name, int version);
public static native TfLiteRegistrationExternal TfLiteRegistrationExternalCreate(@Cast("TfLiteBuiltinOperator") int builtin_code,
                                 String custom_name, int version);

// Destroys the TfLiteRegistrationExternal instance.
//
// \warning This is an experimental API and subject to change.
public static native void TfLiteRegistrationExternalDelete(
    TfLiteRegistrationExternal registration);

// Return the builtin op code of the provided external 'registration'.
//
// \warning This is an experimental API and subject to change.

///
public static native @Cast("TfLiteBuiltinOperator") int TfLiteRegistrationExternalGetBuiltInCode(
    @Const TfLiteRegistrationExternal registration);

/** Returns the custom name of the provided 'registration'. The returned pointer
 *  will be non-null iff the op is a custom op.
 * 
 *  \warning This is an experimental API and subject to change. */

///
public static native @Cast("const char*") BytePointer TfLiteRegistrationExternalGetCustomName(
    @Const TfLiteRegistrationExternal registration);

/** Return the OP version of the provided external 'registration'.  Return -1
 *  in case of error, or if the provided address is null.
 * 
 *  \warning This is an experimental API and subject to change. */
public static native int TfLiteRegistrationExternalGetVersion(
    @Const TfLiteRegistrationExternal registration);
// Targeting ../Init_TfLiteOpaqueContext_BytePointer_long.java


public static native void TfLiteRegistrationExternalSetInit(
    TfLiteRegistrationExternal registration,
    Init_TfLiteOpaqueContext_BytePointer_long init);
// Targeting ../Init_TfLiteOpaqueContext_String_long.java


public static native void TfLiteRegistrationExternalSetInit(
    TfLiteRegistrationExternal registration,
    Init_TfLiteOpaqueContext_String_long init);
// Targeting ../Free_TfLiteOpaqueContext_Pointer.java


public static native void TfLiteRegistrationExternalSetFree(
    TfLiteRegistrationExternal registration,
    Free_TfLiteOpaqueContext_Pointer _free);
// Targeting ../Prepare_TfLiteOpaqueContext_TfLiteOpaqueNode.java


public static native void TfLiteRegistrationExternalSetPrepare(
    TfLiteRegistrationExternal registration,
    Prepare_TfLiteOpaqueContext_TfLiteOpaqueNode prepare);
// Targeting ../Invoke_TfLiteOpaqueContext_TfLiteOpaqueNode.java



///
public static native void TfLiteRegistrationExternalSetInvoke(
    TfLiteRegistrationExternal registration,
    Invoke_TfLiteOpaqueContext_TfLiteOpaqueNode invoke);

/** Sets the async kernel accessor callback for the registration.
 * 
 *  The callback is called to retrieve the async kernel if the delegate supports
 *  it. If the delegate does not support async execution, either this function
 *  should not be called, or {@code async_kernel} needs to be nullptr.
 *  {@code node} is the delegate TfLiteNode created by {@code ModifyGraphWithDelegate}.
 *  Please refer {@code async_kernel} of {@code TfLiteRegistration} for the detail.
 *  \warning This is an experimental API and subject to change. */

/** Sets the inplace_operator field of the external registration.
 * 
 *  This is a bitmask. Please refer to {@code inplace_operator} field of
 *  {@code TfLiteRegistration} for details.
 * 
 *  \warning This is an experimental API and subject to change. */
public static native void TfLiteRegistrationExternalSetInplaceOperator(
    TfLiteRegistrationExternal registration, @Cast("uint64_t") long inplace_operator);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_REGISTRATION_EXTERNAL_H_


// Parsed from tensorflow/lite/c/c_api_experimental.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_
// #define TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_

// #include "tensorflow/lite/core/c/c_api_experimental.h"

// #endif  // TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_


// Parsed from tensorflow/lite/core/c/c_api_experimental.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include
/** "third_party/tensorflow/lite/c/c_api_experimental.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/core/c/c_api.h"
// #include "tensorflow/lite/core/c/common.h"

// #ifdef __cplusplus
// #endif  // __cplusplus

// --------------------------------------------------------------------------
/** Resets all variable tensors to zero.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResetVariableTensors(
    TfLiteInterpreter interpreter);

/** Adds an op registration for a builtin operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of the interpreter's lifetime. A common
 *  practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} (or related functions) on the same
 *  options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
///
public static native void TfLiteInterpreterOptionsAddBuiltinOp(
    TfLiteInterpreterOptions options, @Cast("TfLiteBuiltinOperator") int op,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);

/** Adds an op registration for a custom operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of any created interpreter's lifetime. A
 *  common practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  The lifetime of the string pointed to by {@code name} must be at least as long
 *  as the lifetime of the {@code TfLiteInterpreterOptions}.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} (or related functions) on the same
 *  options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
///
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, @Cast("const char*") BytePointer name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, String name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
// Targeting ../Find_builtin_op_external_Pointer_int_int.java


// Targeting ../Find_custom_op_external_Pointer_String_int.java



///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverExternal(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op,
    Find_custom_op_external_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_Pointer_int_int.java


// Targeting ../Find_custom_op_Pointer_BytePointer_int.java



///
///
///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverExternalWithFallback(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op_external,
    Find_custom_op_external_Pointer_String_int find_custom_op_external,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_BytePointer_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverExternalWithFallback(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op_external,
    Find_custom_op_external_Pointer_String_int find_custom_op_external,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);

/** Registers callbacks for resolving builtin or custom operators.
 * 
 *  The {@code TfLiteInterpreterOptionsSetOpResolver} function provides an alternative
 *  method for registering builtin ops and/or custom ops, by providing operator
 *  resolver callbacks.  Unlike using {@code TfLiteInterpreterOptionsAddBuiltinOp}
 *  and/or {@code TfLiteInterpreterOptionsAddAddCustomOp}, these let you register all
 *  the operators in a single call.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsAddBuiltin} or
 *  {@code TfLiteInterpreterOptionsAddCustomOp} on the same options object.
 * 
 *  If {@code op_resolver_user_data} is non-null, its lifetime must be at least as
 *  long as the lifetime of the {@code TfLiteInterpreterOptions}.
 * 
 *  WARNING: This is an experimental API and subject to change.
 * 
 *  DEPRECATED: use TfLiteInterpreterOptionsSetOpResolverExternal instead. */

///
public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_BytePointer_int find_custom_op,
    Pointer op_resolver_user_data);
public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v3_Pointer_int_int.java


// Targeting ../Find_custom_op_v3_Pointer_BytePointer_int.java



///
public static native void TfLiteInterpreterOptionsSetOpResolverV3(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v3_Pointer_int_int find_builtin_op_v3,
    Find_custom_op_v3_Pointer_BytePointer_int find_custom_op_v3,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v3_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV3(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v3_Pointer_int_int find_builtin_op_v3,
    Find_custom_op_v3_Pointer_String_int find_custom_op_v3,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v2_Pointer_int_int.java


// Targeting ../Find_custom_op_v2_Pointer_BytePointer_int.java



///
public static native void TfLiteInterpreterOptionsSetOpResolverV2(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v2_Pointer_int_int find_builtin_op_v2,
    Find_custom_op_v2_Pointer_BytePointer_int find_custom_op_v2,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v2_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV2(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v2_Pointer_int_int find_builtin_op_v2,
    Find_custom_op_v2_Pointer_String_int find_custom_op_v2,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v1_Pointer_int_int.java


// Targeting ../Find_custom_op_v1_Pointer_BytePointer_int.java



///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverV1(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v1_Pointer_int_int find_builtin_op_v1,
    Find_custom_op_v1_Pointer_BytePointer_int find_custom_op_v1,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v1_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV1(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v1_Pointer_int_int find_builtin_op_v1,
    Find_custom_op_v1_Pointer_String_int find_custom_op_v1,
    Pointer op_resolver_user_data);

/** Returns a new interpreter using the provided model and options, or null on
 *  failure, where the model uses only the operators explicitly added to the
 *  options.  This is the same as {@code TFLiteInterpreterCreate} from {@code c_api.h},
 *  except that the only operators that are supported are the ones registered
 *  in {@code options} via calls to {@code TfLiteInterpreterOptionsSetOpResolver},
 *  {@code TfLiteInterpreterOptionsAddBuiltinOp}, and/or
 *  {@code TfLiteInterpreterOptionsAddCustomOp}.
 * 
 *  * {@code model} must be a valid model instance. The caller retains ownership of
 *    the object, and can destroy it immediately after creating the interpreter;
 *    the interpreter will maintain its own reference to the underlying model
 *    data.
 *  * {@code options} should not be null. The caller retains ownership of the object,
 *    and can safely destroy it immediately after creating the interpreter.
 * 
 *  NOTE: The client *must* explicitly allocate tensors before attempting to
 *  access input tensor data or invoke the interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native TfLiteInterpreter TfLiteInterpreterCreateWithSelectedOps(@Const TfLiteModel model,
                                       @Const TfLiteInterpreterOptions options);

/** Enable or disable the NN API delegate for the interpreter (true to enable).
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
public static native void TfLiteInterpreterOptionsSetUseNNAPI(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Enable or disable CPU fallback for the interpreter (true to enable).
 *  If enabled, TfLiteInterpreterInvoke will do automatic fallback from
 *  executing with delegate(s) to regular execution without delegates
 *  (i.e. on CPU).
 * 
 *  Allowing the fallback is suitable only if both of the following hold:
 *  - The caller is known not to cache pointers to tensor data across
 *    TfLiteInterpreterInvoke calls.
 *  - The model is not stateful (no variables, no LSTMs) or the state isn't
 *    needed between batches.
 * 
 *  When delegate fallback is enabled, TfLiteInterpreterInvoke will
 *  behave as follows:
 *    If one or more delegates were set in the interpreter options
 *    (see TfLiteInterpreterOptionsAddDelegate),
 *    AND inference fails,
 *    then the interpreter will fall back to not using any delegates.
 *    In that case, the previously applied delegate(s) will be automatically
 *    undone, and an attempt will be made to return the interpreter to an
 *    invokable state, which may invalidate previous tensor addresses,
 *    and the inference will be attempted again, using input tensors with
 *    the same value as previously set.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native void TfLiteInterpreterOptionsSetEnableDelegateFallback(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

// Set if buffer handle output is allowed.
//
/** When using hardware delegation, Interpreter will make the data of output
 *  tensors available in {@code tensor->data} by default. If the application can
 *  consume the buffer handle directly (e.g. reading output from OpenGL
 *  texture), it can set this flag to false, so Interpreter won't copy the
 *  data from buffer handle to CPU memory. WARNING: This is an experimental
 *  API and subject to change. */
public static native void TfLiteSetAllowBufferHandleOutput(
    @Const TfLiteInterpreter interpreter, @Cast("bool") boolean allow_buffer_handle_output);

/** Allow a delegate to look at the graph and modify the graph to handle
 *  parts of the graph themselves. After this is called, the graph may
 *  contain new nodes that replace 1 more nodes.
 *  'delegate' must outlive the interpreter.
 *  Use {@code TfLiteInterpreterOptionsAddDelegate} instead of this unless
 *  absolutely required.
 *  Returns one of the following three status codes:
 *  1. kTfLiteOk: Success.
 *  2. kTfLiteDelegateError: Delegation failed due to an error in the
 *  delegate. The Interpreter has been restored to its pre-delegation state.
 *  NOTE: This undoes all delegates previously applied to the Interpreter.
 *  3. kTfLiteError: Unexpected/runtime failure.
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterModifyGraphWithDelegate(
    @Const TfLiteInterpreter interpreter, TfLiteDelegate delegate);

/** Returns the tensor index corresponding to the input tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native int TfLiteInterpreterGetInputTensorIndex(
    @Const TfLiteInterpreter interpreter, int input_index);

/** Returns the tensor index corresponding to the output tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
public static native int TfLiteInterpreterGetOutputTensorIndex(
    @Const TfLiteInterpreter interpreter, int output_index);

/** Assigns (or reassigns) a custom memory allocation for the given
 *  tensor. {@code flags} is a bitmask, see TfLiteCustomAllocationFlags.
 *  The runtime does NOT take ownership of the underlying memory.
 * 
 *  NOTE: User needs to call TfLiteInterpreterAllocateTensors() after this.
 *  Invalid/insufficient buffers will cause an error during
 *  TfLiteInterpreterAllocateTensors or TfLiteInterpreterInvoke (in case of
 *  dynamic shapes in the graph).
 * 
 *  Parameters should satisfy the following conditions:
 *  1. tensor->allocation_type == kTfLiteArenaRw or kTfLiteArenaRwPersistent
 *     In general, this is true for I/O tensors & variable tensors.
 *  2. allocation->data has the appropriate permissions for runtime access
 *     (Read-only for inputs, Read-Write for others), and outlives
 *     TfLiteInterpreter.
 *  3. allocation->bytes >= tensor->bytes.
 *     This condition is checked again if any tensors are resized.
 *  4. allocation->data should be aligned to kDefaultTensorAlignment
 *     defined in lite/util.h. (Currently 64 bytes)
 *     This check is skipped if kTfLiteCustomAllocationFlagsSkipAlignCheck is
 *     set through {@code flags}.
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterSetCustomAllocationForTensor(
    TfLiteInterpreter interpreter, int tensor_index,
    @Const TfLiteCustomAllocation allocation, @Cast("int64_t") long flags);

/** --------------------------------------------------------------------------
 *  SignatureRunner APIs
 <p>
 *  Attempts to cancel in flight invocation if any.
 *  This will not affect calls to {@code Invoke} that happend after this.
 *  Non blocking and thread safe.
 *  Returns kTfLiteError if cancellation is not enabled, otherwise returns
 *  kTfLiteOk.
 *  NOTE: Calling this function will cancel in-flight invocations
 *  in all SignatureRunners built from the same interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerCancel(
    TfLiteSignatureRunner signature_runner);

// Forward declaration, to avoid need for dependency on
// tensorflow/lite/profiling/telemetry/profiler.h.

/** Registers the telemetry profiler to the interpreter.
 *  Note: The interpreter does not take the ownership of profiler, but callers
 *  must ensure profiler->data outlives the lifespan of the interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native void TfLiteInterpreterOptionsSetTelemetryProfiler(
    TfLiteInterpreterOptions options,
    TfLiteTelemetryProfilerStruct profiler);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_


// Parsed from tensorflow/lite/c/common.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

/** \file
/**
/** This file defines common C types and APIs for implementing operations,
/** delegates and other constructs in TensorFlow Lite. The actual operations and
/** delegates can be defined using C++, but the interface between the
/** interpreter and the operations are C.
/**
/** For documentation, see tensorflow/lite/core/c/common.h. */

// #ifndef TENSORFLOW_LITE_C_COMMON_H_
// #define TENSORFLOW_LITE_C_COMMON_H_

// #include "tensorflow/lite/core/c/common.h"

// #endif  // TENSORFLOW_LITE_C_COMMON_H_


// Parsed from tensorflow/lite/core/c/common.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file defines common C types and APIs for implementing operations,
// delegates and other constructs in TensorFlow Lite. The actual operations and
// delegates can be defined using C++, but the interface between the interpreter
// and the operations are C.
//
// Summary of abstractions
// TF_LITE_ENSURE - Self-sufficient error checking
// TfLiteStatus - Status reporting
// TfLiteIntArray - stores tensor shapes (dims),
// TfLiteContext - allows an op to access the tensors
// TfLiteTensor - tensor (a multidimensional array)
// TfLiteNode - a single node or operation
// TfLiteRegistration - the implementation of a conceptual operation.
// TfLiteDelegate - allows delegation of nodes to alternative backends.
//
// Some abstractions in this file are created and managed by Interpreter.
//
// NOTE: The order of values in these structs are "semi-ABI stable". New values
// should be added only to the end of structs and never reordered.

/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include
/** "third_party/tensorflow/lite/c/common.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// IWYU pragma: private, include "third_party/tensorflow/lite/c/common.h"

// #ifndef TENSORFLOW_LITE_CORE_C_COMMON_H_
// #define TENSORFLOW_LITE_CORE_C_COMMON_H_

// #include <stdarg.h>
// #include <stdbool.h>
// #include <stddef.h>
// #include <stdint.h>

// #include "tensorflow/lite/core/c/c_api_types.h"  // IWYU pragma: export

// #ifdef __cplusplus
// #endif  // __cplusplus

// The list of external context types known to TF Lite. This list exists solely
// to avoid conflicts and to ensure ops can share the external contexts they
// need. Access to the external contexts is controlled by one of the
// corresponding support files.
/** enum TfLiteExternalContextType */
public static final int
  kTfLiteEigenContext = 0,       // include eigen_support.h to use.
  kTfLiteGemmLowpContext = 1,    // include gemm_support.h to use.
  kTfLiteEdgeTpuContext = 2,     // Placeholder for Edge TPU support.
  kTfLiteCpuBackendContext = 3,  // include cpu_backend_context.h to use.
  kTfLiteMaxExternalContexts = 4;

// Forward declare so dependent structs and methods can reference these types
// prior to the struct definitions.
// Targeting ../TfLiteExternalContext.java



public static final int kTfLiteOptionalTensor = (-1);
// Targeting ../TfLiteIntArray.java



// Given the size (number of elements) in a TfLiteIntArray, calculate its size
// in bytes.
public static native @Cast("size_t") long TfLiteIntArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a array of a given `size` (uninitialized entries).
// This returns a pointer, that you must free using TfLiteIntArrayFree().
public static native TfLiteIntArray TfLiteIntArrayCreate(int size);
// #endif

// Check if two intarrays are equal. Returns 1 if they are equal, 0 otherwise.
public static native int TfLiteIntArrayEqual(@Const TfLiteIntArray a, @Const TfLiteIntArray b);

// Check if an intarray equals an array. Returns 1 if equals, 0 otherwise.
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntPointer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntBuffer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const int[] b_data);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a copy of an array passed as `src`.
// You are expected to free memory with TfLiteIntArrayFree
public static native TfLiteIntArray TfLiteIntArrayCopy(@Const TfLiteIntArray src);

// Free memory of array `a`.
public static native void TfLiteIntArrayFree(TfLiteIntArray a);
// Targeting ../TfLiteFloatArray.java



// Given the size (number of elements) in a TfLiteFloatArray, calculate its size
// in bytes.
public static native int TfLiteFloatArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a array of a given `size` (uninitialized entries).
// This returns a pointer, that you must free using TfLiteFloatArrayFree().
public static native TfLiteFloatArray TfLiteFloatArrayCreate(int size);

// Create a copy of an array passed as `src`.
// You are expected to free memory with TfLiteFloatArrayFree.
public static native TfLiteFloatArray TfLiteFloatArrayCopy(@Const TfLiteFloatArray src);

// Free memory of array `a`.
public static native void TfLiteFloatArrayFree(TfLiteFloatArray a);
// #endif  // TF_LITE_STATIC_MEMORY

// Since we must not depend on any libraries, define a minimal subset of
// error macros while avoiding names that have pre-conceived meanings like
// assert and check.

// Try to make all reporting calls through TF_LITE_KERNEL_LOG rather than
// calling the context->ReportError function directly, so that message strings
// can be stripped out if the binary size needs to be severely optimized.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_KERNEL_LOG(context, ...)
//   do {
//     (context)->ReportError((context), __VA_ARGS__);
//   } while (false)

// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...)
//   do {
//     if ((context) != nullptr) {
//       (context)->ReportError((context), __VA_ARGS__);
//     }
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define ARGS_UNUSED(...) (void)sizeof(#__VA_ARGS__)
// #define TF_LITE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)
// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

// Check whether value is true, and if not return kTfLiteError from
// the current function (and report the error string msg).
// #define TF_LITE_ENSURE_MSG(context, value, msg)
//   do {
//     if (!(value)) {
//       TF_LITE_KERNEL_LOG((context), __FILE__ " " msg);
//       return kTfLiteError;
//     }
//   } while (0)

// Check whether the value `a` is true, and if not return kTfLiteError from
// the current function, while also reporting the location of the error.
// #define TF_LITE_ENSURE(context, a)
//   do {
//     if (!(a)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s was not true.", __FILE__,
//                          __LINE__, #a);
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_STATUS(a)
//   do {
//     const TfLiteStatus s = (a);
//     if (s != kTfLiteOk) {
//       return s;
//     }
//   } while (0)

// Check whether the value `a == b` is true, and if not return kTfLiteError from
// the current function, while also reporting the location of the error.
// `a` and `b` may be evaluated more than once, so no side effects or
// extremely expensive computations should be done.
// NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes.
// #define TF_LITE_ENSURE_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%d != %d)", __FILE__,
//                          __LINE__, #a, #b, (a), (b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_TYPES_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%s != %s)", __FILE__,
//                          __LINE__, #a, #b, TfLiteTypeGetName(a),
//                          TfLiteTypeGetName(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_NEAR(context, a, b, epsilon)
//   do {
//     auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));
//     if (delta > epsilon) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s not near %s (%f != %f)",
//                          __FILE__, __LINE__, #a, #b, static_cast<double>(a),
//                          static_cast<double>(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_OK(context, status)
//   do {
//     const TfLiteStatus s = (status);
//     if ((s) != kTfLiteOk) {
//       return s;
//     }
//   } while (0)
// Targeting ../TfLiteComplex64.java


// Targeting ../TfLiteComplex128.java


// Targeting ../TfLiteFloat16.java



// Return the name of a given type, for error reporting purposes.
public static native @Cast("const char*") BytePointer TfLiteTypeGetName(@Cast("TfLiteType") int type);

// SupportedQuantizationTypes.
/** enum TfLiteQuantizationType */
public static final int
  // No quantization.
  kTfLiteNoQuantization = 0,
  // Affine quantization (with support for per-channel quantization).
  // Corresponds to TfLiteAffineQuantization.
  kTfLiteAffineQuantization = 1;
// Targeting ../TfLiteQuantization.java


// Targeting ../TfLiteAffineQuantization.java


// Targeting ../TfLitePtrUnion.java



// Memory allocation strategies.
//  * kTfLiteMmapRo: Read-only memory-mapped data, or data externally allocated.
//  * kTfLiteArenaRw: Arena allocated with no guarantees about persistence,
//        and available during eval.
//  * kTfLiteArenaRwPersistent: Arena allocated but persistent across eval, and
//        only available during eval.
//  * kTfLiteDynamic: Allocated during eval, or for string tensors.
//  * kTfLitePersistentRo: Allocated and populated during prepare. This is
//        useful for tensors that can be computed during prepare and treated
//        as constant inputs for downstream ops (also in prepare).
//  * kTfLiteCustom: Custom memory allocation provided by the user. See
//        TfLiteCustomAllocation below.
// * kTfLiteVariantObject: Allocation is an arbitrary type-erased C++ object.
//        Allocation and deallocation are done through `new` and `delete`.
/** enum TfLiteAllocationType */
public static final int
  kTfLiteMemNone = 0,
  kTfLiteMmapRo = 1,
  kTfLiteArenaRw = 2,
  kTfLiteArenaRwPersistent = 3,
  kTfLiteDynamic = 4,
  kTfLitePersistentRo = 5,
  kTfLiteCustom = 6,
  kTfLiteVariantObject = 7;

// Memory allocation strategies.
//
// TfLiteAllocationType values have been overloaded to mean more than their
// original intent. This enum should only be used to document the allocation
// strategy used by a tensor for it data.
/** enum TfLiteAllocationStrategy */
public static final int
  kTfLiteAllocationStrategyUnknown = 0,
  kTfLiteAllocationStrategyNone = 1,    // No data is allocated.
  kTfLiteAllocationStrategyMMap = 2,    // Data is mmaped.
  kTfLiteAllocationStrategyArena = 3,   // Handled by the arena.
  kTfLiteAllocationStrategyMalloc = 4,  // Uses `malloc`/`free`.
  kTfLiteAllocationStrategyNew = 5;      // Uses `new[]`/`delete[]`.

// Describes how stable a tensor attribute is with regards to an interpreter
// runs.
/** enum TfLiteRunStability */
public static final int
  kTfLiteRunStabilityUnknown = 0,
  kTfLiteRunStabilityUnstable = 1,   // May change at any time.
  kTfLiteRunStabilitySingleRun = 2,  // Will stay the same for one run.
  kTfLiteRunStabilityAcrossRuns = 3;  // Will stay the same across all runs.

// Describes the steps of a TFLite operation life cycle.
/** enum TfLiteRunStep */
public static final int
  kTfLiteRunStepUnknown = 0,
  kTfLiteRunStepInit = 1,
  kTfLiteRunStepPrepare = 2,
  kTfLiteRunStepEval = 3;

// The delegates should use zero or positive integers to represent handles.
// -1 is reserved from unallocated status.
/** enum  */
public static final int
  kTfLiteNullBufferHandle = -1;

// Storage format of each dimension in a sparse tensor.
/** enum TfLiteDimensionType */
public static final int
  kTfLiteDimDense = 0,
  kTfLiteDimSparseCSR = 1;
// Targeting ../TfLiteDimensionMetadata.java


// Targeting ../TfLiteSparsity.java


// Targeting ../TfLiteCustomAllocation.java



// The flags used in `Interpreter::SetCustomAllocationForTensor`.
// Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc.
/** enum TfLiteCustomAllocationFlags */
public static final int
  kTfLiteCustomAllocationFlagsNone = 0,
  // Skips checking whether allocation.data points to an aligned buffer as
  // expected by the TFLite runtime.
  // NOTE: Setting this flag can cause crashes when calling Invoke().
  // Use with caution.
  kTfLiteCustomAllocationFlagsSkipAlignCheck = 1;
// Targeting ../TfLiteTensor.java


// Targeting ../TfLiteNode.java


// #else   // defined(TF_LITE_STATIC_MEMORY)?
// NOTE: This flag is opt-in only at compile time.
//
// Specific reduced TfLiteTensor struct for TF Micro runtime. This struct
// contains only the minimum fields required to initialize and prepare a micro
// inference graph. The fields in this struct have been ordered from
// largest-to-smallest for optimal struct sizeof.
//
// This struct does not use:
// - allocation
// - buffer_handle
// - data_is_stale
// - delegate
// - dims_signature
// - name
// - sparsity

// Specific reduced TfLiteNode struct for TF Micro runtime. This struct contains
// only the minimum fields required to represent a node.
//
// This struct does not use:
// - delegate
// - intermediates
// - temporaries
// Targeting ../TfLiteEvalTensor.java



// #ifndef TF_LITE_STATIC_MEMORY
// Free data memory of tensor `t`.
public static native void TfLiteTensorDataFree(TfLiteTensor t);

// Free quantization data.
public static native void TfLiteQuantizationFree(TfLiteQuantization quantization);

// Free sparsity parameters.
public static native void TfLiteSparsityFree(TfLiteSparsity sparsity);

// Free memory of tensor `t`.
public static native void TfLiteTensorFree(TfLiteTensor t);

// Set all of a tensor's fields (and free any previously allocated data).
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);

// Copies the contents of 'src' in 'dst'.
// Function does nothing if either 'src' or 'dst' is passed as nullptr and
// return kTfLiteOk.
// Returns kTfLiteError if 'src' and 'dst' doesn't have matching data size.
// Note function copies contents, so it won't create new data pointer
// or change allocation type.
// All Tensor related properties will be copied from 'src' to 'dst' like
// quantization, sparsity, ...
public static native @Cast("TfLiteStatus") int TfLiteTensorCopy(@Const TfLiteTensor src, TfLiteTensor dst);

// Change the size of the memory block owned by `tensor` to `num_bytes`.
// Tensors with allocation types other than `kTfLiteDynamic` will be ignored and
// a kTfLiteOk will be returned.
// `tensor`'s internal data buffer will be assigned a pointer
// which can safely be passed to free or realloc if `num_bytes` is zero.
// If `preserve_data` is true, tensor data will be unchanged in the range from
// the start of the region up to the minimum of the old and new sizes. In the
// case of NULL tensor, or an error allocating new memory, returns
// `kTfLiteError`.
public static native @Cast("TfLiteStatus") int TfLiteTensorResizeMaybeCopy(@Cast("size_t") long num_bytes, TfLiteTensor tensor,
                                         @Cast("bool") boolean preserve_data);

// Change the size of the memory block owned by `tensor` to `num_bytes`.
// Tensors with allocation types other than kTfLiteDynamic will be ignored and
// a kTfLiteOk will be returned.
// `tensor`'s internal data buffer will be assigned a pointer
// which can safely be passed to free or realloc if `num_bytes` is zero.
// Tensor data will be unchanged in the range from the start of the region up to
// the minimum of the old and new sizes. In the case
// of NULL tensor, or an error allocating new memory, returns `kTfLiteError`.
public static native @Cast("TfLiteStatus") int TfLiteTensorRealloc(@Cast("size_t") long num_bytes, TfLiteTensor tensor);
// Targeting ../TfLiteDelegateParams.java


// Targeting ../TfLiteOpaqueDelegateParams.java


// Targeting ../TfLiteContext.java



// `TfLiteRegistrationExternal` is an external version of `TfLiteRegistration`
// for C API which doesn't use internal types (such as `TfLiteContext`) but only
// uses stable API types (such as `TfLiteOpaqueContext`). The purpose of each
// field is the exactly the same as with `TfLiteRegistration`.

// The valid values of the `inplace_operator` field in `TfLiteRegistration`.
// This allow an op to signal to the runtime that the same data pointer
// may be passed as an input and output without impacting the result.
// This does not mean that the memory can safely be reused, it is up to the
// runtime to determine this, e.g. if another op consumes the same input or not
// or if an input tensor has sufficient memory allocated to store the output
// data.
//
// Setting these flags authorizes the runtime to set the data pointers of an
// input and output tensor to the same value. In such cases, the memory required
// by the output must be less than or equal to that required by the shared
// input, never greater. If kTfLiteInplaceOpDataUnmodified is set, then the
// runtime can share the same input tensor with multiple operator's outputs,
// provided that kTfLiteInplaceOpDataUnmodified is set for all of them.
// Otherwise, if an input tensor is consumed by multiple operators, it may only
// be shared with the operator which is the last to consume it.
//
// Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc.
/** enum TfLiteInPlaceOp */
public static final int
  // The default value. This indicates that the same data pointer cannot safely
  // be passed as an op's input and output.
  kTfLiteInplaceOpNone = 0,
  // This indicates that an op's first output's data is identical to its first
  // input's data, for example Reshape.
  kTfLiteInplaceOpDataUnmodified = 1,
  // Setting kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput means
  // that InputN may be shared with OutputN instead of with the first output.
  // This flag requires one or more of kTfLiteInplaceOpInputNShared to be set.
  kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput = 2,
  // kTfLiteInplaceOpInputNShared indicates that it is safe for an op to share
  // InputN's data pointer with an output tensor. If
  // kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is set then
  // kTfLiteInplaceOpInputNShared indicates that InputN may be shared
  // with OutputN, otherwise kTfLiteInplaceOpInputNShared indicates that InputN
  // may be shared with the first output.
  //
  // Indicates that an op's first input may be shared with the first output
  // tensor. kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput has
  // no impact on the behavior allowed by this flag.
  kTfLiteInplaceOpInput0Shared = 4,
  // Indicates that an op's second input may be shared with the first output
  // if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
  // or second output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput
  // is set.
  kTfLiteInplaceOpInput1Shared = 8,
  // Indicates that an op's third input may be shared with the first output
  // if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
  // or third output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is
  // set.
  kTfLiteInplaceOpInput2Shared = 16;
public static native @MemberGetter int kTfLiteInplaceOpMaxValue();
public static final int
  // Placeholder to ensure that enum can hold 64 bit values to accommodate
  // future fields.
  kTfLiteInplaceOpMaxValue = kTfLiteInplaceOpMaxValue();

// The number of shareable inputs supported.
@MemberGetter public static native int kTfLiteMaxSharableOpInputs();
public static final int kTfLiteMaxSharableOpInputs = kTfLiteMaxSharableOpInputs();
// Targeting ../TfLiteRegistration.java


// Targeting ../TfLiteRegistration_V3.java


// Targeting ../TfLiteRegistration_V2.java


// Targeting ../TfLiteRegistration_V1.java



// The flags used in `TfLiteDelegate`. Note that this is a bitmask, so the
// values should be 1, 2, 4, 8, ...etc.
/** enum TfLiteDelegateFlags */
public static final int
  kTfLiteDelegateFlagsNone = 0,
  // The flag is set if the delegate can handle dynamic sized tensors.
  // For example, the output shape of a `Resize` op with non-constant shape
  // can only be inferred when the op is invoked.
  // In this case, the Delegate is responsible for calling
  // `SetTensorToDynamic` to mark the tensor as a dynamic tensor, and calling
  // `ResizeTensor` when invoking the op.
  //
  // If the delegate isn't capable to handle dynamic tensors, this flag need
  // to be set to false.
  kTfLiteDelegateFlagsAllowDynamicTensors = 1,

  // This flag can be used by delegates (that allow dynamic tensors) to ensure
  // applicable tensor shapes are automatically propagated in the case of tensor
  // resizing.
  // This means that non-dynamic (allocation_type != kTfLiteDynamic) I/O tensors
  // of a delegate kernel will have correct shapes before its Prepare() method
  // is called. The runtime leverages TFLite builtin ops in the original
  // execution plan to propagate shapes.
  //
  // A few points to note:
  // 1. This requires kTfLiteDelegateFlagsAllowDynamicTensors. If that flag is
  // false, this one is redundant since the delegate kernels are re-initialized
  // every time tensors are resized.
  // 2. Enabling this flag adds some overhead to AllocateTensors(), since extra
  // work is required to prepare the original execution plan.
  // 3. This flag requires that the original execution plan only have ops with
  // valid registrations (and not 'dummy' custom ops like with Flex).
  // WARNING: This feature is experimental and subject to change.
  kTfLiteDelegateFlagsRequirePropagatedShapes = 2,

  // This flag can be used by delegates to request per-operator profiling. If a
  // node is a delegate node, this flag will be checked before profiling. If
  // set, then the node will not be profiled. The delegate will then add per
  // operator information using Profiler::EventType::OPERATOR_INVOKE_EVENT and
  // the results will appear in the operator-wise Profiling section and not in
  // the Delegate internal section.
  kTfLiteDelegateFlagsPerOperatorProfiling = 4;
// Targeting ../TfLiteDelegate.java



// Build a 'null' delegate, with all the fields properly set to their default
// values.
public static native @ByVal TfLiteDelegate TfLiteDelegateCreate();
// Targeting ../TfLiteOpaqueDelegateBuilder.java



// #ifndef TF_LITE_STATIC_MEMORY
// Creates an opaque delegate and returns its address.  The opaque delegate will
// behave according to the provided 'opaque_delegate_builder'.  The lifetime of
// the objects pointed to by any of the fields within the
// 'opaque_delegate_builder' must outlive the returned
// 'TfLiteOpaqueDelegate' and any 'TfLiteInterpreter',
// 'TfLiteInterpreterOptions', 'tflite::Interpreter', or
// 'tflite::InterpreterBuilder' that the delegate is added to.  The returned
// address should be passed to 'TfLiteOpaqueDelegateDelete' for deletion.  If
// 'opaque_delegate_builder' is a null pointer, then a null pointer will be
// returned.
public static native @Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct TfLiteOpaqueDelegateCreate(
    @Const TfLiteOpaqueDelegateBuilder opaque_delegate_builder);

// Deletes the provided opaque 'delegate'.  This function has no effect if the
// 'delegate' is a null pointer.
public static native void TfLiteOpaqueDelegateDelete(@Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);
// #endif  // TF_LITE_STATIC_MEMORY

// Returns a pointer to the data associated with the provided opaque 'delegate'.
//
// A null pointer will be returned when:
// - The 'delegate' is null.
// - The 'data' field of the 'TfLiteOpaqueDelegateBuilder' used to construct the
//   'delegate' was null.
// - Or in case of any other error.
// - The 'delegate' has been constructed via a 'TfLiteOpaqueDelegateBuilder',
//   but the 'data' field of the 'TfLiteOpaqueDelegateBuilder' is null.
//
//  The data_ field of 'delegate' will be returned if the
//  'opaque_delegate_builder' field is null.
public static native Pointer TfLiteOpaqueDelegateGetData(@Cast("const TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);

// Returns a tensor data allocation strategy.
public static native @Cast("TfLiteAllocationStrategy") int TfLiteTensorGetAllocationStrategy(
    @Const TfLiteTensor t);

// Returns how stable a tensor data buffer address is across runs.
public static native @Cast("TfLiteRunStability") int TfLiteTensorGetBufferAddressStability(@Const TfLiteTensor t);

// Returns how stable a tensor data values are across runs.
public static native @Cast("TfLiteRunStability") int TfLiteTensorGetDataStability(@Const TfLiteTensor t);

// Returns the operation step when the data of a tensor is populated.
//
// Some operations can precompute their results before the evaluation step. This
// makes the data available earlier for subsequent operations.
public static native @Cast("TfLiteRunStep") int TfLiteTensorGetDataKnownStep(@Const TfLiteTensor t);

// Returns the operation steop when the shape of a tensor is computed.
//
// Some operations can precompute the shape of their results before the
// evaluation step. This makes the shape available earlier for subsequent
// operations.
public static native @Cast("TfLiteRunStep") int TfLiteTensorGetShapeKnownStep(@Const TfLiteTensor t);

// #ifdef __cplusplus  // extern "C"

// #include <utility>
// Targeting ../VariantData.java



// Concrete implementations extend `AbstractVariantData` with CRPT.

// Analogous to `TfLiteTensorRealloc` for allocation of tensors whose
// data member points to an arbitrary C++ object. `VariantType` refers
// to the erased type of said object and `VariantArgs` refers to
// a list of argument types with which to construct a new `VariantType`.
// `VariantArgs` must match a constructor of `VariantType`.

// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_CORE_C_COMMON_H_


// Parsed from tensorflow/lite/core/api/error_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_
// #define TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_

// #include <cstdarg>
// Targeting ../ErrorReporter.java



  // namespace tflite

// You should not make bare calls to the error reporter, instead use the
// TF_LITE_REPORT_ERROR macro, since this allows message strings to be
// stripped when the binary size has to be optimized. If you are looking to
// reduce binary size, define TF_LITE_STRIP_ERROR_STRINGS when compiling and
// every call will be stubbed out, taking no memory.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
//   do {
//     static_cast<::tflite::ErrorReporter*>(reporter)->Report(__VA_ARGS__);
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

// #endif  // TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_


// Parsed from tensorflow/lite/core/api/op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_

// #include <functional>
// #include <limits>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// Targeting ../OpResolverInternal.java

  // For friend declaration below.
// Targeting ../CommonOpaqueConversionUtil.java

  // For friend declaration below.  // Forward decl.

// Targeting ../OpResolver.java


// Targeting ../RegistrationExternalsCache.java


  // namespace internal
// #endif

// Handles the logic for converting between an OperatorCode structure extracted
// from a flatbuffer and information about a registered operator
// implementation.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Cast("const TfLiteRegistration**") PointerPointer registration);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Const @ByPtrPtr TfLiteRegistration registration);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_


// Parsed from tensorflow/lite/core/api/profiler.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_PROFILER_H_
// #define TENSORFLOW_LITE_CORE_API_PROFILER_H_

// #include <cstdint>
// Targeting ../Profiler.java


// Targeting ../ScopedProfile.java


// Targeting ../ScopedOperatorProfile.java


// Targeting ../ScopedDelegateOperatorProfile.java


// Targeting ../ScopedDelegateProfiledOperatorProfile.java


// Targeting ../ScopedRuntimeInstrumentationProfile.java



  // namespace tflite

// #define TFLITE_VARNAME_UNIQ_IMPL(name, ctr) name##ctr
// #define TFLITE_VARNAME_UNIQ(name, ctr) TFLITE_VARNAME_UNIQ_IMPL(name, ctr)

// #define TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler, tag)
//   tflite::ScopedProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag))

// #define TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedOperatorProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag), (node_index))

// #define TFLITE_SCOPED_DELEGATE_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedDelegateOperatorProfile TFLITE_VARNAME_UNIQ(
//       _profile_, __COUNTER__)((profiler), (tag), (node_index))

// #define TFLITE_SCOPED_DELEGATE_PROFILED_OPERATOR_PROFILE(profiler, tag,
//                                                          node_index)
//   tflite::ScopedDelegateProfiledOperatorProfile TFLITE_VARNAME_UNIQ(
//       _profile_, __COUNTER__)((profiler), (tag), (node_index))

// #define TFLITE_ADD_RUNTIME_INSTRUMENTATION_EVENT(
//     profiler, tag, event_metadata1, event_metadata2)
//   do {
//     if (profiler) {
//       const auto handle = profiler->BeginEvent(
//           tag, Profiler::EventType::GENERAL_RUNTIME_INSTRUMENTATION_EVENT,
//           event_metadata1, event_metadata2);
//       profiler->EndEvent(handle);
//     }
//   } while (false);

// #endif  // TENSORFLOW_LITE_CORE_API_PROFILER_H_


// Parsed from tensorflow/lite/core/api/verifier.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Abstract interface for verifying a model. */
// #ifndef TENSORFLOW_LITE_CORE_API_VERIFIER_H_
// #define TENSORFLOW_LITE_CORE_API_VERIFIER_H_

// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../TfLiteVerifier.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_API_VERIFIER_H_


// Parsed from tensorflow/lite/experimental/resource/initialization_status.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_

// #include <memory>
// #include <unordered_map>

// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// Targeting ../InitializationStatus.java



/** WARNING: Experimental interface, subject to change. */

@Namespace("tflite::resource") public static native InitializationStatus GetInitializationStatus(@Cast("tflite::resource::InitializationStatusMap*") IntResourceBaseMap map,
                                              int subgraph_id);

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_


// Parsed from tensorflow/lite/experimental/resource/resource_base.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_

// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <utility>
// Targeting ../ResourceBase.java



/** WARNING: Experimental interface, subject to change. */

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_


// Parsed from tensorflow/lite/allocation.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Memory management for TF Lite. */
// #ifndef TENSORFLOW_LITE_ALLOCATION_H_
// #define TENSORFLOW_LITE_ALLOCATION_H_

// #include <stddef.h>

// #include <cstdio>
// #include <cstdlib>
// #include <memory>

// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../Allocation.java



/** Note that not all platforms support MMAP-based allocation.
 *  Use {@code IsSupported()} to check. */
// Targeting ../FileCopyAllocation.java


// Targeting ../MemoryAllocation.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_ALLOCATION_H_


// Parsed from tensorflow/lite/stderr_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_STDERR_REPORTER_H_
// #define TENSORFLOW_LITE_STDERR_REPORTER_H_

// #include <cstdarg>

// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/c/common.h"
// Targeting ../StderrReporter.java



// Return the default error reporter (output to stderr).
@Namespace("tflite") public static native ErrorReporter DefaultErrorReporter();

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STDERR_REPORTER_H_


// Parsed from tensorflow/lite/graph_info.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_GRAPH_INFO_H_
// #define TENSORFLOW_LITE_GRAPH_INFO_H_

// #include <stddef.h>

// #include <cstdint>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../GraphInfo.java


// Targeting ../NodeSubset.java



// Node edge.second depends on node edge.first.

// Partitions a list of node indices `nodes_to_partition` into node subsets.
// Each node subset is in dependency order internally (i.e. all members of the
// node subsets can be executed in the order they occur) and externally (i.e.,
// node subsets are executable in the order they occur.) The function assumes
// that the nodes of the graph represented in *info are in dependency order.
//
// Depending on the value of `greedily`, the function behaves
//
// - greedily: while a node_set is generated whose members are (aren't) members
// of
//   `*nodes_to_partition`, it will add nodes to this subset, as long as they
//   are (aren't) members of *nodes_to_partition and they are schedulable (i.e.,
//   all nodes they depend have already be added to `*node_subsets`.)
//
// - non-greedily: this preserves the original execution order, i.e. the node
//   subsets generated will be of the form [ [0..i_1), [i1..i2), ... ].
//
// `control_edges` specifies a control dependency DAG on the nodes contained in
// `info`. The resulting partitioning will respect these control
// dependencies. This way, restrictions (in addition to the nodes' data
// dependencies) can be imposed on the ultimate execution order of the graph
// (naturally, this is relevant only if ordering greedily.)
//
// (Example: with `greedily`, `control_edges.empty()`, and `nodes_to_partition
// == {2, 3}`, the graph
//
//                    /------------\
//                    |            v
// 0 --> 1 --> 2* --> 3*     4 --> 5
//       |                   ^
//       \-------------------/
//
// will be partitioned as {{0, 1, 4}, {2, 3}, {5}}, since data dependencies
// (notated '-->') allow for execution of 4 immediately after 1.
//
// With an additional control dependency `control_edges == {{3, 4}}` (notated
// '==>'), execution of node 4 requires prior execution of node 3:
//
//                    /------------\
//                    |            v
// 0 --> 1 --> 2* --> 3* ==> 4 --> 5
//       |                   ^
//       \-------------------/
//
// and the partitioning will be {{0, 1}, {2, 3}, {4, 5}}.)
//
// If control_edges == nullptr, the algorithm preserves the relative ordering of
// nodes that have their `might_have_side_effects` attribute set, i.e., it
// behaves as if `*control_dependencies` of the form `{ {n_1, n_2}, {n_2, n_3},
// ... }` had been handed in, where the n_i are the (sorted) indices of nodes
// with `might_have_side_effects` attribute set.
//
// The function assumes that `*node_subsets` is initially empty.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int PartitionGraphIntoIndependentNodeSubsets(
    @Const GraphInfo info, @Const TfLiteIntArray nodes_to_partition,
    NodeSubsetVector node_subsets, @Cast("bool") boolean greedily,
    @Cast("const tflite::ControlEdges*") IntIntPairVector control_edges/*=nullptr*/);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int PartitionGraphIntoIndependentNodeSubsets(
    @Const GraphInfo info, @Const TfLiteIntArray nodes_to_partition,
    NodeSubsetVector node_subsets, @Cast("bool") boolean greedily);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_GRAPH_INFO_H_


// Parsed from tensorflow/lite/interpreter_options.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Provides options to an interpreter.
/** */
// #ifndef TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_
// #define TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_
// Targeting ../InterpreterOptions.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_


// Parsed from tensorflow/lite/memory_planner.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MEMORY_PLANNER_H_
// #define TENSORFLOW_LITE_MEMORY_PLANNER_H_

// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../MemoryPlanner.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MEMORY_PLANNER_H_


// Parsed from tensorflow/lite/util.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file provides general C++ utility functions in TFLite.
// For example: Converting between `TfLiteIntArray`, `std::vector` and
// Flatbuffer vectors. These functions can't live in `context.h` since it's pure
// C.

// #ifndef TENSORFLOW_LITE_UTIL_H_
// #define TENSORFLOW_LITE_UTIL_H_

// #include <stddef.h>
// #include <stdlib.h>

// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <vector>

// #include "tensorflow/lite/array.h"
// #include "tensorflow/lite/core/c/common.h"

// Memory allocation parameter used by ArenaPlanner.
// Clients (such as delegates) might look at this to ensure interop between
// TFLite memory & hardware buffers.
// NOTE: This only holds for tensors allocated on the arena.
@Namespace("tflite") @MemberGetter public static native int kDefaultTensorAlignment();

// The prefix of Flex op custom code.
// This will be matched agains the `custom_code` field in `OperatorCode`
// Flatbuffer Table.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kFlexCustomCodePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kFlexCustomCodePrefix();

// Checks whether the prefix of the custom name indicates the operation is an
// Flex operation.
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(@Cast("const char*") BytePointer custom_name);
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(String custom_name);

// Converts a `std::vector` to a `TfLiteIntArray`. The caller takes ownership
// of the returned pointer.
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntPointer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntBuffer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector int[] input);

// Converts an array (of the given size) to a `TfLiteIntArray`. The caller
// takes ownership of the returned pointer, and must make sure 'dims' has at
// least 'ndims' elements.
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const IntPointer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const IntBuffer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const int[] dims);

// Checks whether a `TfLiteIntArray` and an int array have matching elements.
// The caller must guarantee that 'b' has at least 'b_size' elements.
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntPointer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntBuffer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const int[] b);

// Populates the size in bytes of a type into `bytes`. Returns kTfLiteOk for
// valid types, and kTfLiteError otherwise.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetSizeOfType(TfLiteContext context, @Cast("const TfLiteType") int type,
                           @Cast("size_t*") SizeTPointer bytes);

// Creates a stub TfLiteRegistration instance with the provided
// `custom_op_name`. The op will fail if invoked, and is useful as a
// placeholder to defer op resolution.
// Note that `custom_op_name` must remain valid for the returned op's lifetime..
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(@Cast("const char*") BytePointer custom_op_name);
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(String custom_op_name);

// Checks whether the provided op is an unresolved custom op.
@Namespace("tflite") public static native @Cast("bool") boolean IsUnresolvedCustomOp(@Const @ByRef TfLiteRegistration registration);

// Returns a descriptive name with the given op TfLiteRegistration.
@Namespace("tflite") public static native @StdString String GetOpNameByRegistration(@Const @ByRef TfLiteRegistration registration);

// The prefix of a validation subgraph name.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kValidationSubgraphNamePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kValidationSubgraphNamePrefix();

// Checks whether the prefix of the subgraph name indicates the subgraph is a
// validation subgraph.
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(@Cast("const char*") BytePointer name);
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(String name);

// Multiply two sizes and return true if overflow occurred;
// This is based off tensorflow/overflow.h but is simpler as we already
// have unsigned numbers. It is also generalized to work where sizeof(size_t)
// is not 8.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int MultiplyAndCheckOverflow(@Cast("size_t") long a, @Cast("size_t") long b, @Cast("size_t*") SizeTPointer product);

// Returns whether the TfLiteTensor is a resource or variant tensor.
@Namespace("tflite") public static native @Cast("bool") boolean IsResourceOrVariant(@Const TfLiteTensor tensor);

// Compute the number of bytes required to represent a tensor with dimensions
// specified by the array dims (of length dims_size). Returns the status code
// and bytes.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const IntPointer dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const IntBuffer dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const int[] dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
// Targeting ../TfLiteTensorDeleter.java





  // namespace tflite

// #endif  // TENSORFLOW_LITE_UTIL_H_


// Parsed from tensorflow/lite/core/macros.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// This provides utility macros and functions that are inherently platform
// specific or shared across runtime & converter.
// #ifndef TENSORFLOW_LITE_CORE_MACROS_H_
// #define TENSORFLOW_LITE_CORE_MACROS_H_

// #ifdef __has_builtin
// #define TFLITE_HAS_BUILTIN(x) __has_builtin(x)
// #else
// #define TFLITE_HAS_BUILTIN(x) 0
// #endif

// #if (!defined(__NVCC__)) && (TFLITE_HAS_BUILTIN(__builtin_expect) ||
//                              (defined(__GNUC__) && __GNUC__ >= 3))
// #define TFLITE_EXPECT_FALSE(cond) __builtin_expect(cond, false)
// #define TFLITE_EXPECT_TRUE(cond) __builtin_expect(!!(cond), true)
// #else
// #define TFLITE_EXPECT_FALSE(cond) (cond)
// #define TFLITE_EXPECT_TRUE(cond) (cond)
// #endif

// #ifdef _WIN32
// #define TFLITE_NOINLINE __declspec(noinline)
// #else
// #ifdef __has_attribute
// #if __has_attribute(noinline)
// #define TFLITE_NOINLINE __attribute__((noinline))
// #else
// #define TFLITE_NOINLINE
// #endif  // __has_attribute(noinline)
// #else
// #define TFLITE_NOINLINE
// #endif  // __has_attribute
// #endif  // _WIN32

// Normally we'd use ABSL_HAVE_ATTRIBUTE_WEAK and ABSL_ATTRIBUTE_WEAK, but
// we avoid the absl dependency for binary size reasons.
// #ifdef __has_attribute
// #define TFLITE_HAS_ATTRIBUTE(x) __has_attribute(x)
// #else
// #define TFLITE_HAS_ATTRIBUTE(x) 0
// #endif

// #if (TFLITE_HAS_ATTRIBUTE(weak) ||
//      (defined(__GNUC__) && !defined(__clang__))) &&
//     !(defined(__llvm__) && defined(_WIN32)) && !defined(__MINGW32__)
// #undef TFLITE_ATTRIBUTE_WEAK
// #define TFLITE_ATTRIBUTE_WEAK __attribute__((weak))
public static final int TFLITE_HAS_ATTRIBUTE_WEAK = 1;
// #else
// #define TFLITE_ATTRIBUTE_WEAK
// #endif

// #ifndef TF_LITE_STATIC_MEMORY
// maximum size of a valid flatbuffer
@MemberGetter public static native @Cast("const unsigned int") int flatbuffer_size_max();
// If none zero then the buffer is stored outside of the flatbuffers, string
@MemberGetter public static native @Cast("const char") byte tflite_metadata_buffer_location(int i);
@MemberGetter public static native @Cast("const char*") BytePointer tflite_metadata_buffer_location();
// field for minimum runtime version, string
@MemberGetter public static native @Cast("const char") byte tflite_metadata_min_runtime_version(int i);
@MemberGetter public static native @Cast("const char*") BytePointer tflite_metadata_min_runtime_version();
// #endif

// #endif  // TENSORFLOW_LITE_CORE_MACROS_H_


// Parsed from tensorflow/lite/core/subgraph.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SUBGRAPH_H_
// #define TENSORFLOW_LITE_CORE_SUBGRAPH_H_

// #include <stdarg.h>
// #include <stddef.h>

// #include <atomic>
// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_set>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/c/common_internal.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/macros.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/graph_info.h"
// #include "tensorflow/lite/interpreter_options.h"
// #include "tensorflow/lite/memory_planner.h"
// #include "tensorflow/lite/util.h"
// Targeting ../SingleOpModel.java

  // Class for friend declarations.  // Class for friend declarations.

// Targeting ../AsyncSubgraph.java

  // Class for friend declarations.
         // Class for friend declarations.  // Class for friend declarations.     // Class for friend declarations.

// Targeting ../TestDelegate.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../Subgraph.java



  // namespace tflite
// #endif  // TENSORFLOW_LITE_CORE_SUBGRAPH_H_


// Parsed from tensorflow/lite/external_cpu_backend_context.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_
// #define TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_

// #include <memory>
// #include <utility>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../TfLiteInternalBackendContext.java


// Targeting ../ExternalCpuBackendContext.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_


// Parsed from tensorflow/lite/portable_type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.

// Arduino build defines abs as a macro here. That is invalid C++, and breaks
// libc++'s <complex> header, undefine it.
// #ifdef abs
// #undef abs
// #endif

// #include <stdint.h>

// #include "tensorflow/lite/core/c/common.h"

// Map statically from a C++ type to a TfLiteType. Used in interpreter for
// safe casts.
// Example:
//  typeToTfLiteType<bool>() -> kTfLiteBool

// Map from TfLiteType to the corresponding C++ type.
// Example:
//   TfLiteTypeToType<kTfLiteBool>::Type -> bool  // Specializations below

// Template specialization for both typeToTfLiteType and TfLiteTypeToType.
// #define MATCH_TYPE_AND_TFLITE_TYPE(CPP_TYPE, TFLITE_TYPE_ENUM)
//   template <>
//   constexpr TfLiteType typeToTfLiteType<CPP_TYPE>() {
//     return TFLITE_TYPE_ENUM;
//   }
//   template <>
//   struct TfLiteTypeToType<TFLITE_TYPE_ENUM> {
//     using Type = CPP_TYPE;
//   }

// No string mapping is included here, since the TF Lite packed representation
// doesn't correspond to a C++ type well.

// Targeting ../TfLiteTypeToType.java














  // namespace tflite
// #endif  // TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/profiling/root_profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_

// #include <cstdint>
// #include <map>
// #include <memory>
// #include <vector>

// #include "tensorflow/lite/core/api/profiler.h"
// Targeting ../RootProfiler.java



  // namespace profiling
  // namespace tflite

// #endif  // TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_


// Parsed from tensorflow/lite/signature_runner.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_SIGNATURE_RUNNER_H_

///
// #define TENSORFLOW_LITE_SIGNATURE_RUNNER_H_
/** \file
 * 
 *  An abstraction for invoking the TF Lite interpreter.
 *  Provides support for named parameters, and for including multiple
 *  named computations in a single model, each with its own inputs/outputs.
 <p>
 *  For documentation, see
 *  third_party/tensorflow/lite/core/signature_runner.h. */

// #include "tensorflow/lite/core/signature_runner.h"  // IWYU pragma: export
  // namespace tflite

// #endif  // TENSORFLOW_LITE_SIGNATURE_RUNNER_H_


// Parsed from tensorflow/lite/core/signature_runner.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_

///
///
// #define TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_
/** \file
 * 
 *  An abstraction for invoking the TF Lite interpreter.
 *  Provides support for named parameters, and for including multiple
 *  named computations in a single model, each with its own inputs/outputs.
 * 
 *  Do NOT include this file directly,
 *  instead include third_party/tensorflow/lite/signature_riunner.h
 *  See third_party/tensorflow/lite/c/common.h for the API for defining
 *  operations (TfLiteRegistration). */

// #include <cstddef>
// #include <cstdint>
// #include <string>
// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/internal/signature_def.h"  // Class for friend declarations.

// Targeting ../SignatureRunnerHelper.java


// Targeting ../SignatureRunnerJNIHelper.java


// Targeting ../TensorHandle.java

              // Class for friend declarations.
// Targeting ../SignatureRunner.java



  // namespace impl
  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_


// Parsed from tensorflow/lite/type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_

// #include <complex>
// #include <string>

// #include "tensorflow/lite/core/c/common.h"

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.
// #include "tensorflow/lite/portable_type_to_tflitetype.h"

// TODO(b/163167649): This string conversion means that only the first entry
// in a string tensor will be returned as a std::string, so it's deprecated.





  // namespace tflite
// #endif  // TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/string_type.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// Abstract string. We don't want even absl at this level.
// #ifndef TENSORFLOW_LITE_STRING_TYPE_H_
// #define TENSORFLOW_LITE_STRING_TYPE_H_

// #include <string>

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STRING_TYPE_H_


// Parsed from tensorflow/lite/mutable_op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_

// #include <stddef.h>

// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/util.h"
// Targeting ../ValueHasher.java



// Targeting ../MutableOpResolver.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_


// Parsed from tensorflow/lite/interpreter.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_INTERPRETER_H_
// #define TENSORFLOW_LITE_INTERPRETER_H_

/** For documentation, see
 *  tensorflow/lite/core/interpreter.h. */

// #include "tensorflow/lite/core/interpreter.h"
  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_H_


// Parsed from tensorflow/lite/core/interpreter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Main abstraction controlling the tflite interpreter.
/** Do NOT include this file directly,
/** instead include third_party/tensorflow/lite/interpreter.h
/** See third_party/tensorflow/lite/c/common.h for the API for defining
/** operations (TfLiteRegistration). */
// #ifndef TENSORFLOW_LITE_CORE_INTERPRETER_H_
// #define TENSORFLOW_LITE_CORE_INTERPRETER_H_

// IWYU pragma: private, include "third_party/tensorflow/lite/interpreter.h"
// IWYU pragma: friend third_party/tensorflow/lite/interpreter.h

// #include <stddef.h>
// #include <stdint.h>

// #include <atomic>
// #include <complex>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <map>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/async/async_signature_runner.h"
// #include "tensorflow/lite/core/c/common.h"  // IWYU pragma: export
// #include "tensorflow/lite/core/signature_runner.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/experimental/remat/metadata_util.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/external_cpu_backend_context.h"
// #include "tensorflow/lite/internal/signature_def.h"
// #include "tensorflow/lite/interpreter_options.h"
// #include "tensorflow/lite/portable_type_to_tflitetype.h"
// #include "tensorflow/lite/profiling/root_profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h"
// #include "tensorflow/lite/stderr_reporter.h"
// #include "tensorflow/lite/string_type.h"
// #include "tensorflow/lite/type_to_tflitetype.h"
// Targeting ../InterpreterTest.java


// Targeting ../InterpreterUtils.java


// Targeting ../TestDelegation.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../InterpreterWrapper.java

  // Class for friend declarations.
  // namespace interpreter_wrapper
// #endif  // DOXYGEN_SKIP

/** An interpreter for a graph of nodes that input and output from tensors.
 *  Each node of the graph processes a set of input tensors and produces a
 *  set of output Tensors. All inputs/output tensors are referenced by index.
 * 
 *  Usage:
 * 
 *  <pre><code>
 *  // Create model from file. Note that the model instance must outlive the
 *  // interpreter instance.
 *  auto model = tflite::FlatBufferModel::BuildFromFile(...);
 *  if (model == nullptr) {
 *    // Return error.
 *  }
 *  // Create an Interpreter with an InterpreterBuilder.
 *  std::unique_ptr<tflite::Interpreter> interpreter;
 *  tflite::ops::builtin::BuiltinOpResolver resolver;
 *  if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {
 *    // Return failure.
 *  }
 *  if (interpreter->AllocateTensors() != kTfLiteOk) {
 *    // Return failure.
 *  }
 * 
 *  auto input = interpreter->typed_tensor<float>(0);
 *  for (int i = 0; i < input_size; i++) {
 *    input[i] = ...; */
//  }
/** interpreter->Invoke();
/** </code></pre>
/**
/** Note: For nearly all practical use cases, one should not directly construct
/** an Interpreter object, but rather use the InterpreterBuilder.
/**
/** \warning This class is *not* thread-safe. The client is responsible for
/** ensuring serialized interaction to avoid data races and undefined behavior. */
// Targeting ../Interpreter.java



  // namespace impl

  // namespace tflite
// #endif  // TENSORFLOW_LITE_CORE_INTERPRETER_H_


// Parsed from tensorflow/lite/model_builder.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MODEL_BUILDER_H_
// #define TENSORFLOW_LITE_MODEL_BUILDER_H_

/** For documentation, see third_party/tensorflow/lite/core/model_builder.h. */

// #include "tensorflow/lite/core/model_builder.h"
  // namespace tflite

// #endif  // TENSORFLOW_LITE_MODEL_BUILDER_H_


// Parsed from tensorflow/lite/core/model_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Deserialization infrastructure for tflite. Provides functionality
/** to go from a serialized tflite model in flatbuffer format to an
/** in-memory representation of the model.
/**
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/model_builder.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_
// #define TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_

// #include <stddef.h>

// #include <map>
// #include <memory>
// #include <string>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/api/verifier.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/mutable_op_resolver.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/stderr_reporter.h"
// #include "tensorflow/lite/string_type.h"
// Targeting ../FlatBufferModel.java



  // namespace impl

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_


// Parsed from tensorflow/lite/interpreter_builder.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_INTERPRETER_BUILDER_H_
// #define TENSORFLOW_LITE_INTERPRETER_BUILDER_H_

/** For documentation, see third_party/tensorflow/lite/core/interpreter_builder.h. */

// #include "tensorflow/lite/core/interpreter_builder.h"
  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_BUILDER_H_


// Parsed from tensorflow/lite/core/interpreter_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Provides functionality to construct an interpreter for a model.
/**
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include
/** "third_party/tensorflow/lite/interpreter_builder.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_
// #define TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_

// #include <map>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #include "flatbuffers/flatbuffers.h"
// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/interpreter.h"
// #include "tensorflow/lite/core/model_builder.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/mutable_op_resolver.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h"
// #include "tensorflow/lite/profiling/telemetry/profiler.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/stderr_reporter.h"

/** Build an interpreter capable of interpreting {@code model}.
 * 
 *  * {@code model}: A model whose lifetime must be at least as long as any
 *    interpreter(s) created by the builder. In principle multiple interpreters
 *    can be made from a single model.
 *  * {@code op_resolver}: An instance that implements the {@code OpResolver} interface,
 *    which maps custom op names and builtin op codes to op registrations. The
 *    lifetime of the provided {@code op_resolver} object must be at least as long as
 *    the {@code InterpreterBuilder}; unlike {@code model} and {@code error_reporter}, the
 *    {@code op_resolver} does not need to exist for the duration of any created
 *    {@code Interpreter} objects.
 *  * {@code error_reporter}: a functor that is called to report errors that handles
 *    printf var arg semantics. The lifetime of the {@code error_reporter} object must
 *    be greater than or equal to the {@code Interpreter} created by {@code operator()}.
 *  * {@code options_experimental}: Options that can change behavior of interpreter.
 *    WARNING: this parameter is an experimental API and is subject to change.
 * 
 *  Returns a kTfLiteOk when successful and sets interpreter to a valid
 *  Interpreter. Note: The user must ensure the lifetime of the model (and error
 *  reporter, if provided) is at least as long as interpreter's lifetime, and
 *  a single model instance may safely be used with multiple interpreters. */
// Targeting ../InterpreterBuilder.java



  // namespace impl

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_


// Parsed from tensorflow/lite/model.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_
// #define TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_

/** For documentation, see third_party/tensorflow/lite/core/model.h. */

// #include "tensorflow/lite/interpreter_builder.h"
// #include "tensorflow/lite/model_builder.h"

// #endif  // TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_


// Parsed from tensorflow/lite/kernels/register.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_KERNELS_REGISTER_H_
// #define TENSORFLOW_LITE_KERNELS_REGISTER_H_

// #include "tensorflow/lite/core/kernels/register.h"

  // namespace builtin
  // namespace ops
  // namespace tflite

// #endif  // TENSORFLOW_LITE_KERNELS_REGISTER_H_


// Parsed from tensorflow/lite/core/kernels/register.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/kernels/register.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_
// #define TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_

// #include "tensorflow/lite/core/model.h"  // Legacy.
// #include "tensorflow/lite/mutable_op_resolver.h"
// Targeting ../BuiltinOpResolver.java


// Targeting ../BuiltinOpResolverWithXNNPACK.java


// Targeting ../BuiltinOpResolverWithoutDefaultDelegates.java



  // namespace builtin
  // namespace ops
  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_


// Parsed from tensorflow/lite/optional_debug_tools.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Optional debugging functionality.
/** For small sized binaries, these are not needed. */
// #ifndef TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_
// #define TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_

// #include "tensorflow/lite/core/interpreter.h"

// Prints a dump of what tensors and what nodes are in the interpreter.
@Namespace("tflite") public static native void PrintInterpreterState(@Const Interpreter interpreter,
                           int tensor_name_display_length/*=25*/,
                           int tensor_type_display_length/*=15*/,
                           int alloc_type_display_length/*=18*/);
@Namespace("tflite") public static native void PrintInterpreterState(@Const Interpreter interpreter);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_


// Parsed from tensorflow/lite/profiling/telemetry/c/profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_

// #include <stdint.h>

// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting.h"

// #ifdef __cplusplus
// Targeting ../TfLiteTelemetryProfilerStruct.java



// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_


// Parsed from tensorflow/lite/profiling/telemetry/c/telemetry_setting.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_

// #include <stddef.h>
// #include <stdint.h>

// #include "tensorflow/lite/core/c/common.h"

// #ifdef __cplusplus
// Targeting ../TfLiteTelemetrySettings.java


// Targeting ../TfLiteTelemetryConversionMetadata.java



public static native @Const IntPointer TfLiteTelemetryConversionMetadataGetModelOptimizationModes(
    @Const TfLiteTelemetryConversionMetadata metadata);

public static native @Cast("size_t") long TfLiteTelemetryConversionMetadataGetNumModelOptimizationModes(
    @Const TfLiteTelemetryConversionMetadata metadata);
// Targeting ../TfLiteTelemetryInterpreterSettings.java



public static native @Const TfLiteTelemetryConversionMetadata TfLiteTelemetryInterpreterSettingsGetConversionMetadata(
    @Const TfLiteTelemetryInterpreterSettings settings);
// Targeting ../TfLiteTelemetrySubgraphInfo.java



public static native @Cast("size_t") long TfLiteTelemetryInterpreterSettingsGetNumSubgraphInfo(
    @Const TfLiteTelemetryInterpreterSettings settings);

public static native @Const TfLiteTelemetrySubgraphInfo TfLiteTelemetryInterpreterSettingsGetSubgraphInfo(
    @Const TfLiteTelemetryInterpreterSettings settings);

public static native @Cast("size_t") long TfLiteTelemetrySubgraphInfoGetNumQuantizations(
    TfLiteTelemetrySubgraphInfo subgraph_info);

public static native @Const TfLiteQuantization TfLiteTelemetrySubgraphInfoGetQuantizations(
    TfLiteTelemetrySubgraphInfo subgraph_info);
// Targeting ../TfLiteTelemetryGpuDelegateSettings.java



public static native @Cast("size_t") long TfLiteTelemetryGpuDelegateSettingsGetNumNodesDelegated(
    @Const TfLiteTelemetryGpuDelegateSettings settings);

public static native int TfLiteTelemetryGpuDelegateSettingsGetBackend(
    @Const TfLiteTelemetryGpuDelegateSettings settings);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_


// Parsed from tensorflow/lite/profiling/telemetry/telemetry_status.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_

// #include <cstdint>

// #include "tensorflow/lite/core/c/c_api_types.h"

// The source of a telemetry event. Enum values intentionally follow proto
// guidelines as they are used for Clearcut logging.
/** enum class tflite::telemetry::TelemetrySource */
public static final int
  UNKNOWN = 0,
  TFLITE_INTERPRETER = 1,

  // For external delegate.
  // External delegate should identify themselves in telemetry event names by
  // prefixing the delegame name to it.
  TFLITE_CUSTOM_DELEGATE = 2,

  TFLITE_GPU = 3,
  TFLITE_NNAPI = 4,
  TFLITE_HEXAGON = 5,
  TFLITE_XNNPACK = 6,
  TFLITE_COREML = 7;
// Targeting ../TelemetryStatusCode.java



  // namespace tflite::telemetry

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_


// Parsed from tensorflow/lite/profiling/telemetry/profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_

// #include <cstdint>

// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting.h"
// #include "tensorflow/lite/profiling/telemetry/telemetry_status.h"
// Targeting ../TelemetryProfiler.java



// Creates a concrete TelemetryProfiler that wraps the
// `TfLiteTelemetryProfilerStruct` C API.
@Namespace("tflite::telemetry") public static native TelemetryProfiler MakeTfLiteTelemetryProfiler(
    TfLiteTelemetryProfilerStruct profiler);

  // namespace tflite::telemetry

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_


}

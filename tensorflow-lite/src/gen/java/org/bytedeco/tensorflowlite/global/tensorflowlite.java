// Targeted by JavaCPP version 1.5.12-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflowlite.global;

import org.bytedeco.tensorflowlite.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class tensorflowlite extends org.bytedeco.tensorflowlite.presets.tensorflowlite {
    static { Loader.load(); }

// Targeting ../StringIntMap.java


// Targeting ../StringStringMap.java


// Targeting ../TfLiteDelegatePtrVector.java


// Targeting ../StringVector.java


// Targeting ../NodeSubsetVector.java


// Targeting ../SubgraphVector.java


// Targeting ../IntIntPairVector.java


// Targeting ../IntIntPairVectorVector.java


// Targeting ../RegistrationNodePairVector.java


// Targeting ../IntIntPair.java


// Targeting ../RegistrationNodePair.java


// Targeting ../SizeTSizeTMap.java


// Targeting ../IntResourceBaseMap.java


// Parsed from tensorflow/compiler/mlir/lite/allocation.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Memory management for TF Lite. */
// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_ALLOCATION_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_ALLOCATION_H_

// #include <stddef.h>

// #include <cstdio>
// #include <cstdlib>
// #include <memory>

// #include "tensorflow/compiler/mlir/lite/core/api/error_reporter.h"
// Targeting ../Allocation.java



/** Note that not all platforms support MMAP-based allocation.
 *  Use {@code IsSupported()} to check. */
// Targeting ../FileCopyAllocation.java


// Targeting ../MemoryAllocation.java



  // namespace tflite

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_ALLOCATION_H_


// Parsed from tensorflow/compiler/mlir/lite/core/api/verifier.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Abstract interface for verifying a model. */
// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_VERIFIER_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_VERIFIER_H_

// #include "tensorflow/compiler/mlir/lite/core/api/error_reporter.h"
// Targeting ../TfLiteVerifier.java



  // namespace tflite

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_VERIFIER_H_


// Parsed from tensorflow/compiler/mlir/lite/core/api/error_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_ERROR_REPORTER_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_ERROR_REPORTER_H_

// #include <cstdarg>
// Targeting ../ErrorReporter.java



  // namespace tflite

// You should not make bare calls to the error reporter, instead use the
// TF_LITE_REPORT_ERROR macro, since this allows message strings to be
// stripped when the binary size has to be optimized. If you are looking to
// reduce binary size, define TF_LITE_STRIP_ERROR_STRINGS when compiling and
// every call will be stubbed out, taking no memory.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
//   do {
//     static_cast<::tflite::ErrorReporter*>(reporter)->Report(__VA_ARGS__);
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_CORE_API_ERROR_REPORTER_H_


// Parsed from tensorflow/compiler/mlir/lite/core/model_builder_base.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Deserialization infrastructure for tflite. Provides functionality
/** to go from a serialized tflite model in flatbuffer format to an
/** in-memory representation of the model.
/**
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/model_builder.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// IWYU pragma: private, include "third_party/tensorflow/lite/model_builder.h"

// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_CORE_MODEL_BUILDER_BASE_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_CORE_MODEL_BUILDER_BASE_H_

// #include <stddef.h>

// #include <algorithm>
// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <utility>

// #include "flatbuffers/base.h"  // from @flatbuffers
// #include "flatbuffers/buffer.h"  // from @flatbuffers
// #include "flatbuffers/vector.h"  // from @flatbuffers
// #include "flatbuffers/verifier.h"  // from @flatbuffers
// #include "tensorflow/compiler/mlir/lite/allocation.h"
// #include "tensorflow/compiler/mlir/lite/core/api/error_reporter.h"
// #include "tensorflow/compiler/mlir/lite/core/api/verifier.h"
// #include "tensorflow/compiler/mlir/lite/core/macros.h"
// #include "tensorflow/compiler/mlir/lite/schema/schema_generated.h"

@Namespace("tflite") public static native @UniquePtr Allocation GetAllocationFromFile(
    @Cast("const char*") BytePointer filename, ErrorReporter error_reporter);
@Namespace("tflite") public static native @UniquePtr Allocation GetAllocationFromFile(
    String filename, ErrorReporter error_reporter);

@Namespace("tflite") public static native @UniquePtr Allocation GetAllocationFromFile(
    int fd, ErrorReporter error_reporter);
// Targeting ../FlatBufferModelBase.java



  // namespace impl

  // namespace tflite

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_CORE_MODEL_BUILDER_BASE_H_


// Parsed from tensorflow/compiler/mlir/lite/utils/control_edges.h

/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_UTILS_CONTROL_EDGES_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_UTILS_CONTROL_EDGES_H_

// #include <cstdint>
// #include <utility>
// #include <vector>

// LINT.IfChange

// LINT.ThenChange(//tensorflow/lite/graph_info.h)

  // namespace tflite

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_UTILS_CONTROL_EDGES_H_


// Parsed from tensorflow/compiler/mlir/lite/experimental/remat/metadata_util.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Functions for serializiation/deserialization of control dependency
/** information to/from model metadata.
/** */

// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_REMAT_METADATA_UTIL_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_REMAT_METADATA_UTIL_H_

// #include <cstddef>
// #include <cstdint>
// #include <string>
// #include <vector>

// #include "tensorflow/compiler/mlir/lite/utils/control_edges.h"

/** Control dependencies for the model is the collection of control dependencies
 *  for its subgraphs. */

/** Serializes {@code in} into the returned string. The result is parseable with
 *  ParseModelControlDependencies. */
@Namespace("tflite") public static native @StdString String SerializeModelControlDependencies(
    @Cast("const tflite::ModelControlDependencies*") @ByRef IntIntPairVectorVector in);

/** Deserializes {@code *out} from a character buffer of size {@code size} at {@code data}.
 *  Returns true iff successful. {@code *out} needn't be empty before invocation.
 *  When returning false, {@code *out}'s state is undefined. */
@Namespace("tflite") public static native @Cast("bool") boolean ParseModelControlDependencies(@Cast("const char*") BytePointer data, @Cast("size_t") long size,
                                   @Cast("tflite::ModelControlDependencies*") IntIntPairVectorVector out);
@Namespace("tflite") public static native @Cast("bool") boolean ParseModelControlDependencies(String data, @Cast("size_t") long size,
                                   @Cast("tflite::ModelControlDependencies*") IntIntPairVectorVector out);

/** The key under which to store the serialized control dependencies in the
 *  model's metadata. */
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kModelControlDependenciesMetadataKey(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kModelControlDependenciesMetadataKey();

/** To allow future changes to the format, serialized control dependency data
 *  will contain a version; this constant is the version that will be used for
 *  serialization.  For deserialization, past versions should remain parseable. */
@Namespace("tflite") @MemberGetter public static native @Cast("const uint32_t") int kModelControlDependenciesMetadataVersion();

@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kModelUseStablehloTensorKey(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kModelUseStablehloTensorKey();

  // namespace tflite

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_REMAT_METADATA_UTIL_H_


// Parsed from tensorflow/compiler/mlir/lite/core/c/tflite_types.h

/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// This file hosts data structures that are needed both for LiteRT and
// Compiler.

// WARNING: Users of TensorFlow Lite should not include this file directly, but
// should instead include "third_party/tensorflow/lite/c/c_api_types.h".
// Only the TensorFlow Lite implementation itself should include this file
// directly.

// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \note Users of TensorFlow Lite should use
/** <pre>{@code
/** #include "tensorflow/lite/c/c_api_types.h"
/** }</pre>
/** to access the APIs documented on this page. */
// NOLINTEND(whitespace/line_length)
// clang-format on

// IWYU pragma: private, include "third_party/tensorflow/lite/c/c_api_types.h"

// #ifndef TENSORFLOW_COMPILER_MLIR_LITE_CORE_C_TFLITE_TYPES_H_
// #define TENSORFLOW_COMPILER_MLIR_LITE_CORE_C_TFLITE_TYPES_H_

// #include <stdint.h>

// #ifdef __cplusplus
// #endif

/** Types supported by tensor */
// LINT.IfChange
/** enum TfLiteType */
public static final int
  kTfLiteNoType = 0,
  kTfLiteFloat32 = 1,
  kTfLiteInt32 = 2,
  kTfLiteUInt8 = 3,
  kTfLiteInt64 = 4,
  kTfLiteString = 5,
  kTfLiteBool = 6,
  kTfLiteInt16 = 7,
  kTfLiteComplex64 = 8,
  kTfLiteInt8 = 9,
  kTfLiteFloat16 = 10,
  kTfLiteFloat64 = 11,
  kTfLiteComplex128 = 12,
  kTfLiteUInt64 = 13,
  kTfLiteResource = 14,
  kTfLiteVariant = 15,
  kTfLiteUInt32 = 16,
  kTfLiteUInt16 = 17,
  kTfLiteInt4 = 18,
  kTfLiteBFloat16 = 19;
// Targeting ../TfLiteQuantizationParams.java



/** Storage format of each dimension in a sparse tensor. */
/** enum TfLiteDimensionType */
public static final int
  kTfLiteDimDense = 0,
  kTfLiteDimSparseCSR = 1;

// #ifdef __cplusplus  // extern C
// #endif

// #endif  // TENSORFLOW_COMPILER_MLIR_LITE_CORE_C_TFLITE_TYPES_H_


// Parsed from tensorflow/lite/builtin_ops.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_BUILTIN_OPS_H_
// #define TENSORFLOW_LITE_BUILTIN_OPS_H_

// DO NOT EDIT MANUALLY: This file is automatically generated by
// `schema/builtin_ops_header/generator.cc`.

// #ifdef __cplusplus
// #endif  // __cplusplus

// The enum for builtin operators.
// Note: CUSTOM, DELEGATE, and PLACEHOLDER_FOR_GREATER_OP_CODES are 3 special
// ops which are not real built-in ops.
/** enum TfLiteBuiltinOperator */
public static final int
  kTfLiteBuiltinAdd = 0,
  kTfLiteBuiltinAveragePool2d = 1,
  kTfLiteBuiltinConcatenation = 2,
  kTfLiteBuiltinConv2d = 3,
  kTfLiteBuiltinDepthwiseConv2d = 4,
  kTfLiteBuiltinDepthToSpace = 5,
  kTfLiteBuiltinDequantize = 6,
  kTfLiteBuiltinEmbeddingLookup = 7,
  kTfLiteBuiltinFloor = 8,
  kTfLiteBuiltinFullyConnected = 9,
  kTfLiteBuiltinHashtableLookup = 10,
  kTfLiteBuiltinL2Normalization = 11,
  kTfLiteBuiltinL2Pool2d = 12,
  kTfLiteBuiltinLocalResponseNormalization = 13,
  kTfLiteBuiltinLogistic = 14,
  kTfLiteBuiltinLshProjection = 15,
  kTfLiteBuiltinLstm = 16,
  kTfLiteBuiltinMaxPool2d = 17,
  kTfLiteBuiltinMul = 18,
  kTfLiteBuiltinRelu = 19,
  kTfLiteBuiltinReluN1To1 = 20,
  kTfLiteBuiltinRelu6 = 21,
  kTfLiteBuiltinReshape = 22,
  kTfLiteBuiltinResizeBilinear = 23,
  kTfLiteBuiltinRnn = 24,
  kTfLiteBuiltinSoftmax = 25,
  kTfLiteBuiltinSpaceToDepth = 26,
  kTfLiteBuiltinSvdf = 27,
  kTfLiteBuiltinTanh = 28,
  kTfLiteBuiltinConcatEmbeddings = 29,
  kTfLiteBuiltinSkipGram = 30,
  kTfLiteBuiltinCall = 31,
  kTfLiteBuiltinCustom = 32,
  kTfLiteBuiltinEmbeddingLookupSparse = 33,
  kTfLiteBuiltinPad = 34,
  kTfLiteBuiltinUnidirectionalSequenceRnn = 35,
  kTfLiteBuiltinGather = 36,
  kTfLiteBuiltinBatchToSpaceNd = 37,
  kTfLiteBuiltinSpaceToBatchNd = 38,
  kTfLiteBuiltinTranspose = 39,
  kTfLiteBuiltinMean = 40,
  kTfLiteBuiltinSub = 41,
  kTfLiteBuiltinDiv = 42,
  kTfLiteBuiltinSqueeze = 43,
  kTfLiteBuiltinUnidirectionalSequenceLstm = 44,
  kTfLiteBuiltinStridedSlice = 45,
  kTfLiteBuiltinBidirectionalSequenceRnn = 46,
  kTfLiteBuiltinExp = 47,
  kTfLiteBuiltinTopkV2 = 48,
  kTfLiteBuiltinSplit = 49,
  kTfLiteBuiltinLogSoftmax = 50,
  kTfLiteBuiltinDelegate = 51,
  kTfLiteBuiltinBidirectionalSequenceLstm = 52,
  kTfLiteBuiltinCast = 53,
  kTfLiteBuiltinPrelu = 54,
  kTfLiteBuiltinMaximum = 55,
  kTfLiteBuiltinArgMax = 56,
  kTfLiteBuiltinMinimum = 57,
  kTfLiteBuiltinLess = 58,
  kTfLiteBuiltinNeg = 59,
  kTfLiteBuiltinPadv2 = 60,
  kTfLiteBuiltinGreater = 61,
  kTfLiteBuiltinGreaterEqual = 62,
  kTfLiteBuiltinLessEqual = 63,
  kTfLiteBuiltinSelect = 64,
  kTfLiteBuiltinSlice = 65,
  kTfLiteBuiltinSin = 66,
  kTfLiteBuiltinTransposeConv = 67,
  kTfLiteBuiltinSparseToDense = 68,
  kTfLiteBuiltinTile = 69,
  kTfLiteBuiltinExpandDims = 70,
  kTfLiteBuiltinEqual = 71,
  kTfLiteBuiltinNotEqual = 72,
  kTfLiteBuiltinLog = 73,
  kTfLiteBuiltinSum = 74,
  kTfLiteBuiltinSqrt = 75,
  kTfLiteBuiltinRsqrt = 76,
  kTfLiteBuiltinShape = 77,
  kTfLiteBuiltinPow = 78,
  kTfLiteBuiltinArgMin = 79,
  kTfLiteBuiltinFakeQuant = 80,
  kTfLiteBuiltinReduceProd = 81,
  kTfLiteBuiltinReduceMax = 82,
  kTfLiteBuiltinPack = 83,
  kTfLiteBuiltinLogicalOr = 84,
  kTfLiteBuiltinOneHot = 85,
  kTfLiteBuiltinLogicalAnd = 86,
  kTfLiteBuiltinLogicalNot = 87,
  kTfLiteBuiltinUnpack = 88,
  kTfLiteBuiltinReduceMin = 89,
  kTfLiteBuiltinFloorDiv = 90,
  kTfLiteBuiltinReduceAny = 91,
  kTfLiteBuiltinSquare = 92,
  kTfLiteBuiltinZerosLike = 93,
  kTfLiteBuiltinFill = 94,
  kTfLiteBuiltinFloorMod = 95,
  kTfLiteBuiltinRange = 96,
  kTfLiteBuiltinResizeNearestNeighbor = 97,
  kTfLiteBuiltinLeakyRelu = 98,
  kTfLiteBuiltinSquaredDifference = 99,
  kTfLiteBuiltinMirrorPad = 100,
  kTfLiteBuiltinAbs = 101,
  kTfLiteBuiltinSplitV = 102,
  kTfLiteBuiltinUnique = 103,
  kTfLiteBuiltinCeil = 104,
  kTfLiteBuiltinReverseV2 = 105,
  kTfLiteBuiltinAddN = 106,
  kTfLiteBuiltinGatherNd = 107,
  kTfLiteBuiltinCos = 108,
  kTfLiteBuiltinWhere = 109,
  kTfLiteBuiltinRank = 110,
  kTfLiteBuiltinElu = 111,
  kTfLiteBuiltinReverseSequence = 112,
  kTfLiteBuiltinMatrixDiag = 113,
  kTfLiteBuiltinQuantize = 114,
  kTfLiteBuiltinMatrixSetDiag = 115,
  kTfLiteBuiltinRound = 116,
  kTfLiteBuiltinHardSwish = 117,
  kTfLiteBuiltinIf = 118,
  kTfLiteBuiltinWhile = 119,
  kTfLiteBuiltinNonMaxSuppressionV4 = 120,
  kTfLiteBuiltinNonMaxSuppressionV5 = 121,
  kTfLiteBuiltinScatterNd = 122,
  kTfLiteBuiltinSelectV2 = 123,
  kTfLiteBuiltinDensify = 124,
  kTfLiteBuiltinSegmentSum = 125,
  kTfLiteBuiltinBatchMatmul = 126,
  kTfLiteBuiltinPlaceholderForGreaterOpCodes = 127,
  kTfLiteBuiltinCumsum = 128,
  kTfLiteBuiltinCallOnce = 129,
  kTfLiteBuiltinBroadcastTo = 130,
  kTfLiteBuiltinRfft2d = 131,
  kTfLiteBuiltinConv3d = 132,
  kTfLiteBuiltinImag = 133,
  kTfLiteBuiltinReal = 134,
  kTfLiteBuiltinComplexAbs = 135,
  kTfLiteBuiltinHashtable = 136,
  kTfLiteBuiltinHashtableFind = 137,
  kTfLiteBuiltinHashtableImport = 138,
  kTfLiteBuiltinHashtableSize = 139,
  kTfLiteBuiltinReduceAll = 140,
  kTfLiteBuiltinConv3dTranspose = 141,
  kTfLiteBuiltinVarHandle = 142,
  kTfLiteBuiltinReadVariable = 143,
  kTfLiteBuiltinAssignVariable = 144,
  kTfLiteBuiltinBroadcastArgs = 145,
  kTfLiteBuiltinRandomStandardNormal = 146,
  kTfLiteBuiltinBucketize = 147,
  kTfLiteBuiltinRandomUniform = 148,
  kTfLiteBuiltinMultinomial = 149,
  kTfLiteBuiltinGelu = 150,
  kTfLiteBuiltinDynamicUpdateSlice = 151,
  kTfLiteBuiltinRelu0To1 = 152,
  kTfLiteBuiltinUnsortedSegmentProd = 153,
  kTfLiteBuiltinUnsortedSegmentMax = 154,
  kTfLiteBuiltinUnsortedSegmentSum = 155,
  kTfLiteBuiltinAtan2 = 156,
  kTfLiteBuiltinUnsortedSegmentMin = 157,
  kTfLiteBuiltinSign = 158,
  kTfLiteBuiltinBitcast = 159,
  kTfLiteBuiltinBitwiseXor = 160,
  kTfLiteBuiltinRightShift = 161,
  kTfLiteBuiltinStablehloLogistic = 162,
  kTfLiteBuiltinStablehloAdd = 163,
  kTfLiteBuiltinStablehloDivide = 164,
  kTfLiteBuiltinStablehloMultiply = 165,
  kTfLiteBuiltinStablehloMaximum = 166,
  kTfLiteBuiltinStablehloReshape = 167,
  kTfLiteBuiltinStablehloClamp = 168,
  kTfLiteBuiltinStablehloConcatenate = 169,
  kTfLiteBuiltinStablehloBroadcastInDim = 170,
  kTfLiteBuiltinStablehloConvolution = 171,
  kTfLiteBuiltinStablehloSlice = 172,
  kTfLiteBuiltinStablehloCustomCall = 173,
  kTfLiteBuiltinStablehloReduce = 174,
  kTfLiteBuiltinStablehloAbs = 175,
  kTfLiteBuiltinStablehloAnd = 176,
  kTfLiteBuiltinStablehloCosine = 177,
  kTfLiteBuiltinStablehloExponential = 178,
  kTfLiteBuiltinStablehloFloor = 179,
  kTfLiteBuiltinStablehloLog = 180,
  kTfLiteBuiltinStablehloMinimum = 181,
  kTfLiteBuiltinStablehloNegate = 182,
  kTfLiteBuiltinStablehloOr = 183,
  kTfLiteBuiltinStablehloPower = 184,
  kTfLiteBuiltinStablehloRemainder = 185,
  kTfLiteBuiltinStablehloRsqrt = 186,
  kTfLiteBuiltinStablehloSelect = 187,
  kTfLiteBuiltinStablehloSubtract = 188,
  kTfLiteBuiltinStablehloTanh = 189,
  kTfLiteBuiltinStablehloScatter = 190,
  kTfLiteBuiltinStablehloCompare = 191,
  kTfLiteBuiltinStablehloConvert = 192,
  kTfLiteBuiltinStablehloDynamicSlice = 193,
  kTfLiteBuiltinStablehloDynamicUpdateSlice = 194,
  kTfLiteBuiltinStablehloPad = 195,
  kTfLiteBuiltinStablehloIota = 196,
  kTfLiteBuiltinStablehloDotGeneral = 197,
  kTfLiteBuiltinStablehloReduceWindow = 198,
  kTfLiteBuiltinStablehloSort = 199,
  kTfLiteBuiltinStablehloWhile = 200,
  kTfLiteBuiltinStablehloGather = 201,
  kTfLiteBuiltinStablehloTranspose = 202,
  kTfLiteBuiltinDilate = 203,
  kTfLiteBuiltinStablehloRngBitGenerator = 204,
  kTfLiteBuiltinReduceWindow = 205,
  kTfLiteBuiltinStablehloComposite = 206,
  kTfLiteBuiltinStablehloShiftLeft = 207,
  kTfLiteBuiltinStablehloCbrt = 208,
  kTfLiteBuiltinStablehloCase = 209;

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_BUILTIN_OPS_H_


// Parsed from tensorflow/lite/c/c_api_types.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_TYPES_H_

///
///
// #define TENSORFLOW_LITE_C_C_API_TYPES_H_

/** \file
 * 
 *  C API types for TensorFlow Lite.
 * 
 *  For documentation, see tensorflow/lite/core/c/c_api_types.h */

// #include "tensorflow/lite/core/c/c_api_types.h"

// #endif  // TENSORFLOW_LITE_C_C_API_TYPES_H_


// Parsed from tensorflow/lite/core/c/c_api_types.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// WARNING: Users of TensorFlow Lite should not include this file directly, but
// should instead include "third_party/tensorflow/lite/c/c_api_types.h".
// Only the TensorFlow Lite implementation itself should include this file
// directly.

/** This file declares types used by the pure C inference API defined in
/** c_api.h, some of which are also used in the C++ and C kernel and interpreter
/** APIs.
/** */
// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \note Users of TensorFlow Lite should use
/** <pre>{@code
/** #include "tensorflow/lite/c/c_api_types.h"
/** }</pre>
/** to access the APIs documented on this page. */
// NOLINTEND(whitespace/line_length)
// clang-format on

// IWYU pragma: private, include "third_party/tensorflow/lite/c/c_api_types.h"

// #ifndef TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_

// #ifdef __cplusplus
// #endif

// #include "tensorflow/compiler/mlir/lite/core/c/tflite_types.h"  // IWYU pragma: export

// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \defgroup c_api_types lite/c/c_api_types.h
 *  \{
 */
// NOLINTEND(whitespace/line_length)
// clang-format on

// Define TFL_CAPI_EXPORT macro to export a function properly with a shared
// library.
// #ifdef SWIG
// #define TFL_CAPI_EXPORT
// #elif defined(TFL_STATIC_LIBRARY_BUILD)
// #define TFL_CAPI_EXPORT
// #else  // not defined TFL_STATIC_LIBRARY_BUILD
// #if defined(_WIN32)
// #ifdef TFL_COMPILE_LIBRARY
// #define TFL_CAPI_EXPORT __declspec(dllexport)
// #else
// #define TFL_CAPI_EXPORT
// #endif  // TFL_COMPILE_LIBRARY
// #else
// #define TFL_CAPI_EXPORT __attribute__((visibility("default")))
// #endif  // _WIN32
// #endif  // SWIG

/** Note that new error status values may be added in future in order to
 *  indicate more fine-grained internal states, therefore, applications should
 *  not rely on status values being members of the enum. */
/** enum TfLiteStatus */
public static final int
  /** Success */
  kTfLiteOk = 0,

  /** Generally referring to an error in the runtime (i.e. interpreter) */
  kTfLiteError = 1,

  /** Generally referring to an error from a TfLiteDelegate itself. */
  kTfLiteDelegateError = 2,

  /** Generally referring to an error in applying a delegate due to
   *  incompatibility between runtime and delegate, e.g., this error is returned
   *  when trying to apply a TF Lite delegate onto a model graph that's already
   *  immutable. */
  kTfLiteApplicationError = 3,

  /** Generally referring to serialized delegate data not being found.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataNotFound = 4,

  /** Generally referring to data-writing issues in delegate serialization.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataWriteError = 5,

  /** Generally referring to data-reading issues in delegate serialization.
   *  See tflite::delegates::Serialization. */
  kTfLiteDelegateDataReadError = 6,

  /** Generally referring to issues when the TF Lite model has ops that cannot
   *  be resolved at runtime. This could happen when the specific op is not
   *  registered or built with the TF Lite framework. */
  kTfLiteUnresolvedOps = 7,

  /** Generally referring to invocation cancelled by the user.
   *  See {@code interpreter::Cancel}. */
  // TODO(b/194915839): Implement `interpreter::Cancel`.
  // TODO(b/250636993): Cancellation triggered by `SetCancellationFunction`
  // should also return this status code.
  kTfLiteCancelled = 8,

  // This status is returned by Prepare when the output shape cannot be
  // determined but the size of the output tensor is known. For example, the
  // output of reshape is always the same size as the input. This means that
  // such ops may be
  // done in place.
  kTfLiteOutputShapeNotKnown = 9;
// Targeting ../TfLiteOpaqueContext.java


// Targeting ../TfLiteOpaqueNode.java


// Targeting ../TfLiteOpaqueTensor.java



/** TfLiteDelegate: allows delegation of nodes to alternative backends.
 *  Forward declaration of concrete type declared in common.h. */
// Targeting ../TfLiteOpaqueDelegateStruct.java



/** TfLiteOpaqueDelegate: conditionally opaque version of
 *  TfLiteDelegate; allows delegation of nodes to alternative backends.
 *  For TF Lite in Play Services, this is an opaque type,
 *  but for regular TF Lite, this is just a typedef for TfLiteDelegate.
 * 
 *  WARNING: This is an experimental type and subject to change. */
// #if TFLITE_WITH_STABLE_ABI || TFLITE_USE_OPAQUE_DELEGATE
// #else
// #endif

/** \} */

// #ifdef __cplusplus  // extern C
// #endif
// #endif  // TENSORFLOW_LITE_CORE_C_C_API_TYPES_H_


// Parsed from tensorflow/lite/c/c_api.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_H_

///
///
// #define TENSORFLOW_LITE_C_C_API_H_

/** \file
 * 
 *  C API for TensorFlow Lite.
 * 
 *  For documentation, see tensorflow/lite/core/c/c_api.h */

// #include "tensorflow/lite/core/c/c_api.h"

// #ifndef DOYXGEN_SKIP
// #endif  // DOYXGEN_SKIP

// #endif  // TENSORFLOW_LITE_C_C_API_H_


// Parsed from tensorflow/lite/core/c/c_api.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// WARNING: Users of TensorFlow Lite should not include this file directly, but
// should instead include "third_party/tensorflow/lite/c/c_api.h".
// Only the TensorFlow Lite implementation itself should include this file
// directly.

// #ifndef TENSORFLOW_LITE_CORE_C_C_API_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_H_

// #include <stdarg.h>
// #include <stdbool.h>
// #include <stdint.h>
// #include <stdlib.h>

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/core/async/c/types.h"
// #include "tensorflow/lite/core/c/c_api_types.h"  // IWYU pragma: export

///
///
///
///
///
///
///
///
///
// #include "tensorflow/lite/core/c/operator.h"  // IWYU pragma: export

/** C API for TensorFlow Lite.
 * 
 *  The API leans towards simplicity and uniformity instead of convenience, as
 *  most usage will be by language-specific wrappers. It provides largely the
 *  same set of functionality as that of the C++ TensorFlow Lite {@code Interpreter}
 *  API, but is useful for shared libraries where having a stable ABI boundary
 *  is important.
 * 
 *  Conventions:
 *  * We use the prefix TfLite for everything in the API.
 *  * size_t is used to represent byte sizes of objects that are
 *    materialized in the address space of the calling process.
 *  * int is used as an index into arrays.
 * 
 *  Usage:
 *  <pre><code>
 *  // Create the model and interpreter options.
 *  TfLiteModel* model = TfLiteModelCreateFromFile("/path/to/model.tflite");
 *  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
 *  TfLiteInterpreterOptionsSetNumThreads(options, 2);
 * 
 *  // Create the interpreter.
 *  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
 * 
 *  // Allocate tensors and populate the input tensor data.
 *  TfLiteInterpreterAllocateTensors(interpreter);
 *  TfLiteTensor* input_tensor =
 *      TfLiteInterpreterGetInputTensor(interpreter, 0);
 *  TfLiteTensorCopyFromBuffer(input_tensor, input.data(),
 *                             input.size() * sizeof(float));
 * 
 *  // Execute inference.
 *  TfLiteInterpreterInvoke(interpreter);
 * 
 *  // Extract the output tensor data.
 *  const TfLiteTensor* output_tensor =
 *       TfLiteInterpreterGetOutputTensor(interpreter, 0);
 *  TfLiteTensorCopyToBuffer(output_tensor, output.data(),
 *                           output.size() * sizeof(float));
 * 
 *  // Dispose of the model and interpreter objects.
 *  TfLiteInterpreterDelete(interpreter);
 *  TfLiteInterpreterOptionsDelete(options);
 *  TfLiteModelDelete(model);
 *  </code></pre>
 *  */
// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \note Users of TensorFlow Lite should use
/** <pre>{@code
/** #include "tensorflow/lite/c/c_api.h"
/** }</pre>
/** to access the APIs documented on this page. */
// NOLINTEND(whitespace/line_length)
// clang-format on

// #ifdef __cplusplus
// Targeting ../TfLiteModel.java


// Targeting ../TfLiteInterpreterOptions.java


// Targeting ../TfLiteInterpreter.java



/** A tensor in the interpreter system which is a wrapper around a buffer of
 *  data including a dimensionality (or NULL if not currently defined). */
// Targeting ../TfLiteSignatureRunner.java



// --------------------------------------------------------------------------
/** The TensorFlow Lite Runtime version.
 * 
 *  Returns a pointer to a statically allocated string that is the version
 *  number of the (potentially dynamically loaded) TF Lite Runtime library.
 *  TensorFlow Lite uses semantic versioning, and the return value should be
 *  in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
 *  e.g. "2.12.0" or "2.13.0-rc2". */

///
///
///
public static native @Cast("const char*") BytePointer TfLiteVersion();

// --------------------------------------------------------------------------
/** The TensorFlow Lite Extension APIs version.
 * 
 *  Returns a pointer to a statically allocated string that is the version
 *  number of the TF Lite Extension APIs supported by the (potentially
 *  dynamically loaded) TF Lite Runtime library.  The TF Lite "Extension APIs"
 *  are the APIs for extending TF Lite with custom ops and delegates.
 *  More specifically, this version number covers the (non-experimental)
 *  functionality documented in the following header files:
 * 
 *    * lite/c/c_api_opaque.h
 *    * lite/c/common.h
 *    * lite/c/builtin_op_data.h
 *    * lite/builtin_ops.h
 * 
 *  This version number uses semantic versioning, and the return value should
 *  be in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
 *  e.g. "2.14.0" or "2.15.0-rc2". */

///
///
public static native @Cast("const char*") BytePointer TfLiteExtensionApisVersion();

/** The supported TensorFlow Lite model file Schema version.
 * 
 *  Returns the (major) version number of the Schema used for model
 *  files that is supported by the (potentially dynamically loaded)
 *  TensorFlow Lite Runtime.
 * 
 *  Model files using schema versions different to this may not be supported by
 *  the current version of the TF Lite Runtime. */

///
public static native int TfLiteSchemaVersion();

/** Returns a model from the provided buffer, or null on failure.
 * 
 *  \note The caller retains ownership of the {@code model_data} buffer and should
 *  ensure that the lifetime of the {@code model_data} buffer must be at least as long
 *  as the lifetime of the {@code TfLiteModel} and of any {@code TfLiteInterpreter} objects
 *  created from that {@code TfLiteModel}, and furthermore the contents of the
 *  {@code model_data} buffer must not be modified during that time." */
public static native TfLiteModel TfLiteModelCreate(@Const Pointer model_data,
                                                      @Cast("size_t") long model_size);
// Targeting ../Reporter_Pointer_BytePointer_Pointer.java



///
public static native TfLiteModel TfLiteModelCreateWithErrorReporter(
    @Const Pointer model_data, @Cast("size_t") long model_size,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
// Targeting ../Reporter_Pointer_String_Pointer.java


public static native TfLiteModel TfLiteModelCreateWithErrorReporter(
    @Const Pointer model_data, @Cast("size_t") long model_size,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Returns a model from the provided file, or null on failure.
 * 
 *  \note The file's contents must not be modified during the lifetime of the
 *  {@code TfLiteModel} or of any {@code TfLiteInterpreter} objects created from that
 *  {@code TfLiteModel}. */
public static native TfLiteModel TfLiteModelCreateFromFile(
    @Cast("const char*") BytePointer model_path);
public static native TfLiteModel TfLiteModelCreateFromFile(
    String model_path);

/** Same as {@code TfLiteModelCreateFromFile} with customizble error reporter.
 *  * {@code reporter} takes the provided {@code user_data} object, as well as a C-style
 *    format string and arg list (see also vprintf).
 *  * {@code user_data} is optional. If non-null, it is owned by the client and must
 *    remain valid for the duration of the interpreter lifetime. */
public static native TfLiteModel TfLiteModelCreateFromFileWithErrorReporter(
    @Cast("const char*") BytePointer model_path,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
public static native TfLiteModel TfLiteModelCreateFromFileWithErrorReporter(
    String model_path,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Destroys the model instance. */
public static native void TfLiteModelDelete(TfLiteModel model);

/** Returns a new interpreter options instances. */

///
public static native TfLiteInterpreterOptions TfLiteInterpreterOptionsCreate();

/** Creates and returns a shallow copy of an options object.
 * 
 *  The caller is responsible for calling {@code TfLiteInterpreterOptionsDelete} to
 *  deallocate the object pointed to by the returned pointer. */
public static native TfLiteInterpreterOptions TfLiteInterpreterOptionsCopy(
    @Const TfLiteInterpreterOptions from);

/** Destroys the interpreter options instance. */
public static native void TfLiteInterpreterOptionsDelete(
    TfLiteInterpreterOptions options);

/** Sets the number of CPU threads to use for the interpreter. */

///
///
///
public static native void TfLiteInterpreterOptionsSetNumThreads(
    TfLiteInterpreterOptions options, int num_threads);

/** Adds a delegate to be applied during {@code TfLiteInterpreter} creation.
 * 
 *  If delegate application fails, interpreter creation will also fail with an
 *  associated error logged.
 * 
 *  \note The caller retains ownership of the delegate and should ensure that it
 *  remains valid for the duration of any created interpreter's lifetime.
 * 
 *  If you are NOT using "TensorFlow Lite in Play Services", and NOT building
 *  with {@code TFLITE_WITH_STABLE_ABI} or {@code TFLITE_USE_OPAQUE_DELEGATE} macros
 *  enabled, it is possible to pass a {@code TfLiteDelegate*} rather than a
 *  {@code TfLiteOpaqueDelegate*} to this function, since in those cases,
 *  {@code TfLiteOpaqueDelegate} is just a typedef alias for {@code TfLiteDelegate}.
 *  This is for compatibility with existing source code
 *  and existing delegates.  For new delegates, it is recommended to
 *  use {@code TfLiteOpaqueDelegate} rather than {@code TfLiteDelegate}.  (See
 *  {@code TfLiteOpaqueDelegate} in tensorflow/lite/core/c/c_api_types.h.) */

///
public static native void TfLiteInterpreterOptionsAddDelegate(
    TfLiteInterpreterOptions options, @Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);

/** Sets a custom error reporter for interpreter execution.
 * 
 *  * {@code reporter} takes the provided {@code user_data} object, as well as a C-style
 *    format string and arg list (see also vprintf).
 *  * {@code user_data} is optional. If non-null, it is owned by the client and must
 *    remain valid for the duration of the interpreter lifetime. */

///
public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

/** Adds an op registration to be applied during {@code TfLiteInterpreter} creation.
 * 
 *  The {@code TfLiteOperator} object is needed to implement custom op of
 *  TFLite Interpreter via C API. Calling this function ensures that any
 *  {@code TfLiteInterpreter} created with the specified {@code options} can execute models
 *  that use the custom operator specified in {@code registration}.
 *  Please refer https://www.tensorflow.org/lite/guide/ops_custom for custom op
 *  support.
 *  \note The caller retains ownership of the TfLiteOperator object
 *  and should ensure that it remains valid for the duration of any created
 *  interpreter's lifetime.
 *  \warning This is an experimental API and subject to change. */

///
public static native void TfLiteInterpreterOptionsAddOperator(
    TfLiteInterpreterOptions options, TfLiteOperator registration);

/** Enables users to cancel in-flight invocations with
 *  {@code TfLiteInterpreterCancel}.
 * 
 *  By default it is disabled and calling to {@code TfLiteInterpreterCancel} will
 *  return kTfLiteError. See {@code TfLiteInterpreterCancel}. */

///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterOptionsEnableCancellation(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Returns a new interpreter using the provided model and options, or null on
 *  failure.
 * 
 *  * {@code model} must be a valid model instance. The caller retains ownership of
 *    the object, and may destroy it (via TfLiteModelDelete) immediately after
 *    creating the interpreter.  However, if the TfLiteModel was allocated with
 *    TfLiteModelCreate, then the {@code model_data} buffer that was passed to
 *    TfLiteModelCreate must outlive the lifetime of the TfLiteInterpreter
 *    object that this function returns, and must not be modified during that
 *    time; and if the TfLiteModel was allocated with TfLiteModelCreateFromFile,
 *    then the contents of the model file must not be modified during the
 *    lifetime of the TfLiteInterpreter object that this function returns.
 *  * {@code optional_options} may be null. The caller retains ownership of the
 *    object, and can safely destroy it (via TfLiteInterpreterOptionsDelete)
 *    immediately after creating the interpreter.
 * 
 *  \note The client *must* explicitly allocate tensors before attempting to
 *  access input tensor data or invoke the interpreter. */
public static native TfLiteInterpreter TfLiteInterpreterCreate(
    @Const TfLiteModel model, @Const TfLiteInterpreterOptions optional_options);

/** Destroys the interpreter. */
public static native void TfLiteInterpreterDelete(
    TfLiteInterpreter interpreter);

/** Returns the number of input tensors associated with the model. */

///
///
public static native int TfLiteInterpreterGetInputTensorCount(
    @Const TfLiteInterpreter interpreter);

/** Returns a pointer to an array of input tensor indices.  The length of the
 *  array can be obtained via a call to {@code TfLiteInterpreterGetInputTensorCount}.
 * 
 *  Typically the input tensors associated with an {@code interpreter} would be set
 *  during the initialization of the {@code interpreter}, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code SetInputs} on the interpreter invalidate the returned pointer.
 * 
 *  The ownership of the array remains with the TFLite runtime. */
public static native @Const IntPointer TfLiteInterpreterInputTensorIndices(
    @Const TfLiteInterpreter interpreter);

/** Returns the tensor associated with the input index.
 *  REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor) */

///
///
///
public static native TfLiteTensor TfLiteInterpreterGetInputTensor(
    @Const TfLiteInterpreter interpreter, int input_index);

/** Resizes the specified input tensor.
 * 
 *  \note After a resize, the client *must* explicitly allocate tensors before
 *  attempting to access the resized tensor data or invoke the interpreter.
 * 
 *  REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
 * 
 *  This function makes a copy of the input dimensions, so the client can safely
 *  deallocate {@code input_dims} immediately after this function returns. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntPointer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntBuffer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const int[] input_dims,
    int input_dims_size);

/** Updates allocations for all tensors, resizing dependent tensors using the
 *  specified input tensor dimensionality.
 * 
 *  This is a relatively expensive operation, and need only be called after
 *  creating the graph and/or resizing any inputs. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterAllocateTensors(
    TfLiteInterpreter interpreter);

/** Runs inference for the loaded graph.
 * 
 *  Before calling this function, the caller should first invoke
 *  TfLiteInterpreterAllocateTensors() and should also set the values for the
 *  input tensors.  After successfully calling this function, the values for the
 *  output tensors will be set.
 * 
 *  \note It is possible that the interpreter is not in a ready state to
 *  evaluate (e.g., if AllocateTensors() hasn't been called, or if a
 *  ResizeInputTensor() has been performed without a subsequent call to
 *  AllocateTensors()).
 * 
 *    If the (experimental!) delegate fallback option was enabled in the
 *    interpreter options, then the interpreter will automatically fall back to
 *    not using any delegates if execution with delegates fails. For details,
 *    see TfLiteInterpreterOptionsSetEnableDelegateFallback in
 *    c_api_experimental.h.
 * 
 *  Returns one of the following status codes:
 *   - kTfLiteOk: Success. Output is valid.
 *   - kTfLiteDelegateError: Execution with delegates failed, due to a problem
 *     with the delegate(s). If fallback was not enabled, output is invalid.
 *     If fallback was enabled, this return value indicates that fallback
 *     succeeded, the output is valid, and all delegates previously applied to
 *     the interpreter have been undone.
 *   - kTfLiteApplicationError: Same as for kTfLiteDelegateError, except that
 *     the problem was not with the delegate itself, but rather was
 *     due to an incompatibility between the delegate(s) and the
 *     interpreter or model.
 *   - kTfLiteError: Unexpected/runtime failure. Output is invalid. */
public static native @Cast("TfLiteStatus") int TfLiteInterpreterInvoke(
    TfLiteInterpreter interpreter);

/** Returns the number of output tensors associated with the model. */

///
///
public static native int TfLiteInterpreterGetOutputTensorCount(
    @Const TfLiteInterpreter interpreter);

/** Returns a pointer to an array of output tensor indices.  The length of the
 *  array can be obtained via a call to {@code TfLiteInterpreterGetOutputTensorCount}.
 * 
 *  Typically the output tensors associated with an {@code interpreter} would be set
 *  during the initialization of the {@code interpreter}, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code SetOutputs} on the interpreter invalidate the returned pointer.
 * 
 *  The ownership of the array remains with the TFLite runtime. */

///
public static native @Const IntPointer TfLiteInterpreterOutputTensorIndices(
    @Const TfLiteInterpreter interpreter);

/** Returns the tensor associated with the output index.
 *  REQUIRES: 0 <= output_index < TfLiteInterpreterGetOutputTensorCount(tensor)
 * 
 *  \note The shape and underlying data buffer for output tensors may be not
 *  be available until after the output tensor has been both sized and
 *  allocated.
 *  In general, best practice is to interact with the output tensor *after*
 *  calling TfLiteInterpreterInvoke(). */

///
///
///
///
public static native @Const TfLiteTensor TfLiteInterpreterGetOutputTensor(
    @Const TfLiteInterpreter interpreter, int output_index);

/** Returns modifiable access to the tensor that corresponds to the
 *  specified {@code index} and is associated with the provided {@code interpreter}.
 * 
 *  This requires the {@code index} to be between 0 and N - 1, where N is the
 *  number of tensors in the model.
 * 
 *  Typically the tensors associated with the {@code interpreter} would be set during
 *  the {@code interpreter} initialization, through a mechanism like the
 *  {@code InterpreterBuilder}, and remain unchanged throughout the lifetime of the
 *  interpreter.  However, there are some circumstances in which the pointer may
 *  not remain valid throughout the lifetime of the interpreter, because calls
 *  to {@code AddTensors} on the interpreter invalidate the returned pointer.
 * 
 *  Note the difference between this function and
 *  {@code TfLiteInterpreterGetInputTensor} (or {@code TfLiteInterpreterGetOutputTensor} for
 *  that matter): {@code TfLiteInterpreterGetTensor} takes an index into the array of
 *  all tensors associated with the {@code interpreter}'s model, whereas
 *  {@code TfLiteInterpreterGetInputTensor} takes an index into the array of input
 *  tensors.
 * 
 *  The ownership of the tensor remains with the TFLite runtime, meaning the
 *  caller should not deallocate the pointer. */

///
///
public static native TfLiteTensor TfLiteInterpreterGetTensor(@Const TfLiteInterpreter interpreter,
                                         int index);

/** Tries to cancel any in-flight invocation.
 * 
 *  \note This only cancels {@code TfLiteInterpreterInvoke} calls that happen before
 *  calling this and it does not cancel subsequent invocations.
 *  \note Calling this function will also cancel any in-flight invocations of
 *  SignatureRunners constructed from this interpreter.
 *  Non-blocking and thread safe.
 * 
 *  Returns kTfLiteError if cancellation is not enabled via
 *  {@code TfLiteInterpreterOptionsEnableCancellation}. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterCancel(
    @Const TfLiteInterpreter interpreter);

/** --------------------------------------------------------------------------
 *  SignatureRunner APIs
 * 
 *  You can run inference by either:
 * 
 *  (i) (recommended) using the Interpreter to initialize SignatureRunner(s) and
 *      then only using SignatureRunner APIs.
 * 
 *  (ii) only using Interpreter APIs.
 * 
 *  NOTE:
 *  * Only use one of the above options to run inference, i.e. avoid mixing both
 *    SignatureRunner APIs and Interpreter APIs to run inference as they share
 *    the same underlying data (e.g. updating an input tensor A retrieved
 *    using the Interpreter APIs will update the state of the input tensor B
 *    retrieved using SignatureRunner APIs, if they point to the same underlying
 *    tensor in the model; as it is not possible for a user to debug this by
 *    analyzing the code, it can lead to undesirable behavior).
 *  * The TfLiteSignatureRunner type is conditionally thread-safe, provided that
 *    no two threads attempt to simultaneously access two TfLiteSignatureRunner
 *    instances that point to the same underlying signature, or access a
 *    TfLiteSignatureRunner and its underlying TfLiteInterpreter, unless all
 *    such simultaneous accesses are reads (rather than writes).
 *  * The lifetime of a TfLiteSignatureRunner object ends when
 *    TfLiteSignatureRunnerDelete() is called on it (or when the lifetime of the
 *    underlying TfLiteInterpreter ends -- but you should call
 *    TfLiteSignatureRunnerDelete() before that happens in order to avoid
 *    resource leaks).
 *  * You can only apply delegates to the interpreter (via
 *    TfLiteInterpreterOptions) and not to a signature.
 <p>
 *  Returns the number of signatures defined in the model. */

///
public static native int TfLiteInterpreterGetSignatureCount(
    @Const TfLiteInterpreter interpreter);

/** Returns the key of the Nth signature in the model, where N is specified as
 *  {@code signature_index}.
 * 
 *  NOTE: The lifetime of the returned key is the same as (and depends on) the
 *  lifetime of {@code interpreter}. */

///
///
public static native @Cast("const char*") BytePointer TfLiteInterpreterGetSignatureKey(
    @Const TfLiteInterpreter interpreter, int signature_index);

/** Returns a new signature runner using the provided interpreter and signature
 *  key, or nullptr on failure.
 * 
 *  NOTE: {@code signature_key} is a null-terminated C string that must match the
 *  key of a signature in the interpreter's model.
 * 
 *  NOTE: The returned signature runner should be destroyed, by calling
 *  TfLiteSignatureRunnerDelete(), before the interpreter is destroyed. */
public static native TfLiteSignatureRunner TfLiteInterpreterGetSignatureRunner(@Const TfLiteInterpreter interpreter,
                                    @Cast("const char*") BytePointer signature_key);
public static native TfLiteSignatureRunner TfLiteInterpreterGetSignatureRunner(@Const TfLiteInterpreter interpreter,
                                    String signature_key);

/** Returns the number of inputs associated with a signature. */

///
public static native @Cast("size_t") long TfLiteSignatureRunnerGetInputCount(
    @Const TfLiteSignatureRunner signature_runner);

/** Returns the (null-terminated) name of the Nth input in a signature, where N
 *  is specified as {@code input_index}.
 * 
 *  NOTE: The lifetime of the returned name is the same as (and depends on) the
 *  lifetime of {@code signature_runner}. */

///
///
///
///
public static native @Cast("const char*") BytePointer TfLiteSignatureRunnerGetInputName(
    @Const TfLiteSignatureRunner signature_runner, int input_index);

/** Resizes the input tensor identified as {@code input_name} to be the dimensions
 *  specified by {@code input_dims} and {@code input_dims_size}. Only unknown dimensions can
 *  be resized with this function. Unknown dimensions are indicated as {@code -1} in
 *  the {@code dims_signature} attribute of a TfLiteTensor.
 * 
 *  Returns status of failure or success. Note that this doesn't actually resize
 *  any existing buffers. A call to TfLiteSignatureRunnerAllocateTensors() is
 *  required to change the tensor input buffer.
 * 
 *  NOTE: This function is similar to TfLiteInterpreterResizeInputTensorStrict()
 *  and not TfLiteInterpreterResizeInputTensor().
 * 
 *  NOTE: {@code input_name} must match the name of an input in the signature.
 * 
 *  NOTE: This function makes a copy of the input dimensions, so the caller can
 *  safely deallocate {@code input_dims} immediately after this function returns. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const IntPointer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const IntBuffer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const int[] input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const IntPointer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name,
    @Const IntBuffer input_dims, int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerResizeInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name,
    @Const int[] input_dims, int input_dims_size);

/** Updates allocations for tensors associated with a signature and resizes
 *  dependent tensors using the specified input tensor dimensionality.
 *  This is a relatively expensive operation and hence should only be called
 *  after initializing the signature runner object and/or resizing any inputs. */

///
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerAllocateTensors(
    TfLiteSignatureRunner signature_runner);

/** Returns the input tensor identified by {@code input_name} in the given signature.
 *  Returns nullptr if the given name is not valid.
 * 
 *  NOTE: The lifetime of the returned tensor is the same as (and depends on)
 *  the lifetime of {@code signature_runner}. */

///
public static native TfLiteTensor TfLiteSignatureRunnerGetInputTensor(
    TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer input_name);
public static native TfLiteTensor TfLiteSignatureRunnerGetInputTensor(
    TfLiteSignatureRunner signature_runner, String input_name);

/** Runs inference on a given signature.
 * 
 *  Before calling this function, the caller should first invoke
 *  TfLiteSignatureRunnerAllocateTensors() and should also set the values for
 *  the input tensors. After successfully calling this function, the values for
 *  the output tensors will be set. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerInvoke(
    TfLiteSignatureRunner signature_runner);

/** Returns the number of output tensors associated with the signature. */

///
public static native @Cast("size_t") long TfLiteSignatureRunnerGetOutputCount(
    @Const TfLiteSignatureRunner signature_runner);

/** Returns the (null-terminated) name of the Nth output in a signature, where
 *  N is specified as {@code output_index}.
 * 
 *  NOTE: The lifetime of the returned name is the same as (and depends on) the
 *  lifetime of {@code signature_runner}. */

///
public static native @Cast("const char*") BytePointer TfLiteSignatureRunnerGetOutputName(
    @Const TfLiteSignatureRunner signature_runner, int output_index);

/** Returns the output tensor identified by {@code output_name} in the given
 *  signature. Returns nullptr if the given name is not valid.
 * 
 *  NOTE: The lifetime of the returned tensor is the same as (and depends on)
 *  the lifetime of {@code signature_runner}. */
public static native @Const TfLiteTensor TfLiteSignatureRunnerGetOutputTensor(
    @Const TfLiteSignatureRunner signature_runner, @Cast("const char*") BytePointer output_name);
public static native @Const TfLiteTensor TfLiteSignatureRunnerGetOutputTensor(
    @Const TfLiteSignatureRunner signature_runner, String output_name);

// --------------------------------------------------------------------------
// TfLiteTensor wraps data associated with a graph tensor.
//
// Note that, while the TfLiteTensor struct is not currently opaque, and its
// fields can be accessed directly, these methods are still convenient for
// language bindings. In the future the tensor struct will likely be made opaque
// in the public API.

/** Returns the type of a tensor element. */
public static native @Cast("TfLiteType") int TfLiteTensorType(@Const TfLiteTensor tensor);

/** Returns the number of dimensions that the tensor has.  Returns -1 in case
 *  the 'opaque_tensor' does not have its dimensions property set. */
public static native int TfLiteTensorNumDims(@Const TfLiteTensor tensor);

/** Returns the length of the tensor in the "dim_index" dimension.
 *  REQUIRES: 0 <= dim_index < TFLiteTensorNumDims(tensor) */
public static native int TfLiteTensorDim(@Const TfLiteTensor tensor,
                                               int dim_index);

/** Returns the size of the underlying data in bytes. */

///
public static native @Cast("size_t") long TfLiteTensorByteSize(@Const TfLiteTensor tensor);

/** Returns a pointer to the underlying data buffer.
 * 
 *  \note The result may be null if tensors have not yet been allocated, e.g.,
 *  if the Tensor has just been created or resized and {@code TfLiteAllocateTensors()}
 *  has yet to be called, or if the output tensor is dynamically sized and the
 *  interpreter hasn't been invoked. */
public static native Pointer TfLiteTensorData(@Const TfLiteTensor tensor);

/** Returns the (null-terminated) name of the tensor. */
public static native @Cast("const char*") BytePointer TfLiteTensorName(@Const TfLiteTensor tensor);

/** Returns the parameters for asymmetric quantization. The quantization
 *  parameters are only valid when the tensor type is {@code kTfLiteUInt8} and the
 *  {@code scale != 0}. Quantized values can be converted back to float using:
 *     real_value = scale * (quantized_value - zero_point); */
public static native @ByVal TfLiteQuantizationParams TfLiteTensorQuantizationParams(
    @Const TfLiteTensor tensor);

/** Copies from the provided input buffer into the tensor's buffer.
 *  REQUIRES: input_data_size == TfLiteTensorByteSize(tensor) */
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyFromBuffer(
    TfLiteTensor tensor, @Const Pointer input_data, @Cast("size_t") long input_data_size);

/** Copies to the provided output buffer from the tensor's buffer.
 *  REQUIRES: output_data_size == TfLiteTensorByteSize(tensor) */
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyToBuffer(
    @Const TfLiteTensor output_tensor, Pointer output_data,
    @Cast("size_t") long output_data_size);

/** Destroys the signature runner. */
public static native void TfLiteSignatureRunnerDelete(
    TfLiteSignatureRunner signature_runner);

// NOLINTEND(modernize-redundant-void-arg)

/** \} */

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_C_API_H_


// Parsed from tensorflow/lite/core/c/operator.h

/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \warning Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/c/c_api.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly.
/**
/** The types and functions declared in operator.h are
/** part of the TensorFlow Lite Extension APIs.
/** We reserve the right to make changes to this API in future releases,
/** potentially including non-backwards-compatible changes, on a different
/** schedule than for the other TensorFlow Lite APIs. See
/** https://www.tensorflow.org/guide/versions#separate_version_number_for_tensorflow_lite_extension_apis. */
// #ifndef TENSORFLOW_LITE_CORE_C_OPERATOR_H_
// #define TENSORFLOW_LITE_CORE_C_OPERATOR_H_

// #include <stdint.h>
// #include <stdlib.h>

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/core/async/c/types.h"
// #include "tensorflow/lite/core/c/c_api_types.h"

// #ifdef __cplusplus
// Targeting ../TfLiteOperator.java



/** Returns a new TfLiteOperator instance.
 * 
 *  The returned TfLiteOperator instance represents a definition
 *  of an operator with the identity (builtin_code/custom_name and
 *  version) specified by the parameters, but with all callbacks initially
 *  unset.
 * 
 *  Evaluation of any operation using this operator will be done using
 *  the "prepare" and "invoke" callbacks, which can be set using
 *  {@code TfLiteOperatorSetPrepare} and
 *  {@code TfLiteOperatorSetInvoke}, or for async execution
 *  the "prepare", "eval", and "wait" callbacks of the {@code TfLiteAsyncKernel},
 *  which can be set using {@code TfLiteOperatorSetAsyncKernel}.
 *  If the relevant callbacks are not set, then such evaluation will result
 *  in an error status.  So normally any use of this function should be followed
 *  by appropriate calls to set those callbacks.
 * 
 *  \note The caller retains ownership and should ensure that
 *  the lifetime of the {@code TfLiteOperator} must be at least as long as
 *  the lifetime of any {@code TfLiteInterpreter} or {@code tflite::Interpreter} that it is
 *  used in.
 * 
 *  @param builtin_code Enumeration code specifying which builtin operator this
 *                      defines, or {@code TfLiteBuiltinCustom} to define a custom op.
 *  @param custom_name  Name of the custom op, or {@code nullptr} for a builtin op.
 *                      If {@code custom_name} is non-null, then {@code builtin_code} should
 *                      be {@code TfLiteBuiltinCustom}.
 *  @param version      Version of the op.  See
 *                      https://www.tensorflow.org/lite/guide/ops_version
 *  @param user_data    Opaque pointer passed to the operator's callbacks set
 *                      with functions such as {@code TfLiteOperatorSetXXXWithData}.
 *                      The user is expected to manage the memory pointed by
 *                      this field and the lifetime of that memory should extend
 *                      at least from the call to {@code TfLiteOperatorCreate}
 *                      to the invocation of the callback set with
 *                      {@code TfLiteOperatorSetFreeWithData}.
 * 
 *  @return a newly created TfLiteOperator on success, or a nullptr on failure */

///
public static native TfLiteOperator TfLiteOperatorCreate(
    @Cast("TfLiteBuiltinOperator") int builtin_code, @Cast("const char*") BytePointer custom_name, int version,
    Pointer user_data);
public static native TfLiteOperator TfLiteOperatorCreate(
    @Cast("TfLiteBuiltinOperator") int builtin_code, String custom_name, int version,
    Pointer user_data);

/** Destroys the TfLiteOperator instance.
 *  */

///
public static native void TfLiteOperatorDelete(TfLiteOperator registration);

/** Return the builtin op code of the provided external 'registration'.
 *  */

///
public static native @Cast("TfLiteBuiltinOperator") int TfLiteOperatorGetBuiltInCode(
    @Const TfLiteOperator registration);

/** Returns the custom name of the provided 'registration'. The returned pointer
 *  will be non-null iff the op is a custom op.
 *  */

///
public static native @Cast("const char*") BytePointer TfLiteOperatorGetCustomName(
    @Const TfLiteOperator registration);

/** Return the OP version of the provided external 'registration'.  Return -1
 *  in case of error, or if the provided address is null.
 *  */

///
public static native int TfLiteOperatorGetVersion(
    @Const TfLiteOperator registration);

/** Return the user data field of the provided external 'registration', or
 *  nullptr if none was set.
 *  */

///
///
public static native Pointer TfLiteOperatorGetUserData(
    @Const TfLiteOperator registration);
// Targeting ../Init_TfLiteOpaqueContext_BytePointer_long.java



///
///
public static native void TfLiteOperatorSetInit(
    TfLiteOperator registration,
    Init_TfLiteOpaqueContext_BytePointer_long init);
// Targeting ../Init_TfLiteOpaqueContext_String_long.java


public static native void TfLiteOperatorSetInit(
    TfLiteOperator registration,
    Init_TfLiteOpaqueContext_String_long init);
// Targeting ../Init_Pointer_TfLiteOpaqueContext_BytePointer_long.java



///
///
public static native @Cast("TfLiteStatus") int TfLiteOperatorSetInitWithData(
    TfLiteOperator registration,
    Init_Pointer_TfLiteOpaqueContext_BytePointer_long init);
// Targeting ../Init_Pointer_TfLiteOpaqueContext_String_long.java


public static native @Cast("TfLiteStatus") int TfLiteOperatorSetInitWithData(
    TfLiteOperator registration,
    Init_Pointer_TfLiteOpaqueContext_String_long init);
// Targeting ../Free_TfLiteOpaqueContext_Pointer.java



///
///
public static native void TfLiteOperatorSetFree(
    TfLiteOperator registration,
    Free_TfLiteOpaqueContext_Pointer _free);
// Targeting ../Free_Pointer_TfLiteOpaqueContext_Pointer.java



///
///
public static native @Cast("TfLiteStatus") int TfLiteOperatorSetFreeWithData(
    TfLiteOperator registration,
    Free_Pointer_TfLiteOpaqueContext_Pointer _free);
// Targeting ../Prepare_TfLiteOpaqueContext_TfLiteOpaqueNode.java



///
///
public static native void TfLiteOperatorSetPrepare(
    TfLiteOperator registration,
    Prepare_TfLiteOpaqueContext_TfLiteOpaqueNode prepare);
// Targeting ../Prepare_Pointer_TfLiteOpaqueContext_TfLiteOpaqueNode.java



///
///
public static native @Cast("TfLiteStatus") int TfLiteOperatorSetPrepareWithData(
    TfLiteOperator registration,
    Prepare_Pointer_TfLiteOpaqueContext_TfLiteOpaqueNode prepare);
// Targeting ../Invoke_TfLiteOpaqueContext_TfLiteOpaqueNode.java



///
///
public static native void TfLiteOperatorSetInvoke(
    TfLiteOperator registration,
    Invoke_TfLiteOpaqueContext_TfLiteOpaqueNode invoke);
// Targeting ../Invoke_Pointer_TfLiteOpaqueContext_TfLiteOpaqueNode.java



///
///
public static native @Cast("TfLiteStatus") int TfLiteOperatorSetInvokeWithData(
    TfLiteOperator registration,
    Invoke_Pointer_TfLiteOpaqueContext_TfLiteOpaqueNode invoke);

/** Sets the async kernel accessor callback for the registration.
 * 
 *  The callback is called to retrieve the async kernel if the delegate supports
 *  it. If the delegate does not support async execution, either this function
 *  should not be called, or {@code async_kernel} needs to be nullptr.
 *  {@code node} is the delegate TfLiteNode created by {@code ModifyGraphWithDelegate}.
 *  Please refer {@code async_kernel} of {@code TfLiteRegistration} for the detail.
 * 
 *  \warning This is an experimental API and subject to change.
 *  Deprecated: Use {@code TfLiteOperatorSetAsyncKernelWithData} */

/** Sets the async kernel accessor callback for the registration. The function
 *  returns an error upon failure.
 * 
 *  The callback is called to retrieve the async kernel if the delegate supports
 *  it. If the delegate does not support async execution, either this function
 *  should not be called, or {@code async_kernel} needs to be nullptr.  {@code node} is the
 *  delegate TfLiteNode created by {@code ModifyGraphWithDelegate}.  The value passed
 *  in the {@code user_data} parameter is the value that was passed to
 *  {@code TfLiteOperatorCreate}.  Please refer {@code async_kernel} of {@code TfLiteRegistration}
 *  for the detail.
 * 
 *  \warning This is an experimental API and subject to change. */

/** Sets the inplace_operator field of the external registration.
 * 
 *  This is a bitmask. Please refer to {@code inplace_operator} field of
 *  {@code TfLiteRegistration} for details.
 *  */
public static native void TfLiteOperatorSetInplaceOperator(
    TfLiteOperator registration, @Cast("uint64_t") long inplace_operator);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_OPERATOR_H_


// Parsed from tensorflow/lite/c/common.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

/** \file
/**
/** This file defines common C types and APIs for implementing operations,
/** delegates and other constructs in TensorFlow Lite. The actual operations and
/** delegates can be defined using C++, but the interface between the
/** interpreter and the operations are C.
/**
/** For documentation, see tensorflow/lite/core/c/common.h.
/**
/** See also c_api_opaque.h which has more ABI-stable variants of some of these
/** APIs. */

// #ifndef TENSORFLOW_LITE_C_COMMON_H_
// #define TENSORFLOW_LITE_C_COMMON_H_

// #include "tensorflow/lite/core/c/common.h"

// #endif  // TENSORFLOW_LITE_C_COMMON_H_


// Parsed from tensorflow/lite/core/c/common.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// WARNING: Users of TensorFlow Lite should not include this file directly, but
// should instead include "third_party/tensorflow/lite/c/common.h".
// Only the TensorFlow Lite implementation itself should include this file
// directly.

/** This file defines common C types and APIs for implementing operations,
/** delegates and other constructs in TensorFlow Lite. The actual operations and
/** delegates can be defined using C++, but the interface between the
/** interpreter and the operations are C.
/**
/** Summary of abstractions:
/** * {@code TF_LITE_ENSURE} - self-sufficient error checking
/** * {@code TfLiteStatus} - status reporting
/** * {@code TfLiteIntArray} - stores tensor shapes (dims),
/** * {@code TfLiteContext} - allows an op to access the tensors
/** * {@code TfLiteTensor} - tensor (a multidimensional array)
/** * {@code TfLiteNode} - a single node or operation
/** * {@code TfLiteRegistration} - the implementation of a conceptual operation.
/** * {@code TfLiteDelegate} - allows delegation of nodes to alternative backends.
/**
/** Some abstractions in this file are created and managed by Interpreter.
/**
/** NOTE: The order of values in these structs are "semi-ABI stable". New values
/** should be added only to the end of structs and never reordered.
/** */
// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \note Users of TensorFlow Lite should use
/** <pre>{@code
/** #include "tensorflow/lite/c/common.h"
/** }</pre>
/** to access the APIs documented on this page. */
// NOLINTEND(whitespace/line_length)
// clang-format on

// IWYU pragma: private, include "third_party/tensorflow/lite/c/common.h"

// #ifndef TENSORFLOW_LITE_CORE_C_COMMON_H_
// #define TENSORFLOW_LITE_CORE_C_COMMON_H_

// #include <stdarg.h>
// #include <stdbool.h>
// #include <stddef.h>
// #include <stdint.h>

// #include "tensorflow/lite/core/c/c_api_types.h"  // IWYU pragma: export

// #ifdef __cplusplus
// #endif  // __cplusplus

// clang-format off
// NOLINTBEGIN(whitespace/line_length)
/** \defgroup common lite/c/common.h
 *  \{
 */
// NOLINTEND(whitespace/line_length)
// clang-format on

/** The list of external context types known to TF Lite. This list exists solely
/** to avoid conflicts and to ensure ops can share the external contexts they
/** need. Access to the external contexts is controlled by one of the
/** corresponding support files. */
/** enum TfLiteExternalContextType */
public static final int
  kTfLiteEigenContext = 0,       /** include eigen_support.h to use. */
  kTfLiteGemmLowpContext = 1,    /** include gemm_support.h to use. */
  kTfLiteEdgeTpuContext = 2,     /** Placeholder for Edge TPU support. */
  kTfLiteCpuBackendContext = 3,  /** include cpu_backend_context.h to use. */
  kTfLiteLiteRtBufferContext = 4,  /** include external_litert_buffer_context.h to use. */
  kTfLiteMaxExternalContexts = 5;

// Forward declare so dependent structs and methods can reference these types
// prior to the struct definitions.
// Targeting ../TfLiteExternalContext.java



// LINT.IfChange(optional_tensor)
public static final int kTfLiteOptionalTensor = (-1);
// Targeting ../TfLiteIntArray.java



/** Given the size (number of elements) in a TfLiteIntArray, calculate its size
 *  in bytes. */
public static native @Cast("size_t") long TfLiteIntArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
/** Create a array of a given {@code size} (uninitialized entries).
 *  This returns a pointer, that you must free using TfLiteIntArrayFree(). */
public static native TfLiteIntArray TfLiteIntArrayCreate(int size);
// #endif

/** Check if two intarrays are equal. Returns 1 if they are equal, 0 otherwise. */
public static native int TfLiteIntArrayEqual(@Const TfLiteIntArray a, @Const TfLiteIntArray b);

/** Check if an intarray equals an array. Returns 1 if equals, 0 otherwise. */
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntPointer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntBuffer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const int[] b_data);

// #ifndef TF_LITE_STATIC_MEMORY
/** Create a copy of an array passed as {@code src}.
 *  You are expected to free memory with TfLiteIntArrayFree */
public static native TfLiteIntArray TfLiteIntArrayCopy(@Const TfLiteIntArray src);

/** Free memory of array {@code a}. */
public static native void TfLiteIntArrayFree(TfLiteIntArray a);
// Targeting ../TfLiteFloatArray.java



/** Given the size (number of elements) in a TfLiteFloatArray, calculate its
 *  size in bytes. */
public static native int TfLiteFloatArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
/** Create a array of a given {@code size} (uninitialized entries).
 *  This returns a pointer, that you must free using TfLiteFloatArrayFree(). */
public static native TfLiteFloatArray TfLiteFloatArrayCreate(int size);

/** Create a copy of an array passed as {@code src}.
 *  You are expected to free memory with TfLiteFloatArrayFree. */
public static native TfLiteFloatArray TfLiteFloatArrayCopy(@Const TfLiteFloatArray src);

/** Free memory of array {@code a}. */
public static native void TfLiteFloatArrayFree(TfLiteFloatArray a);
// #endif  // TF_LITE_STATIC_MEMORY

// Since we must not depend on any libraries, define a minimal subset of
// error macros while avoiding names that have pre-conceived meanings like
// assert and check.

// Try to make all reporting calls through TF_LITE_KERNEL_LOG rather than
// calling the context->ReportError function directly, so that message strings
// can be stripped out if the binary size needs to be severely optimized.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_KERNEL_LOG(context, ...)
//   do {
//     (context)->ReportError((context), __VA_ARGS__);
//   } while (false)

// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...)
//   do {
//     if ((context) != nullptr) {
//       (context)->ReportError((context), __VA_ARGS__);
//     }
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define ARGS_UNUSED(...) (void)sizeof(#__VA_ARGS__)
// #define TF_LITE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)
// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

/** Check whether value is true, and if not return kTfLiteError from
 *  the current function (and report the error string msg). */
// #define TF_LITE_ENSURE_MSG(context, value, ...)
//   do {
//     if (!(value)) {
//       TF_LITE_KERNEL_LOG((context), __FILE__ " " __VA_ARGS__);
//       return kTfLiteError;
//     }
//   } while (0)

/** Check whether the value {@code a} is true, and if not return kTfLiteError from
 *  the current function, while also reporting the location of the error. */
// #define TF_LITE_ENSURE(context, a)
//   do {
//     if (!(a)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s was not true.", __FILE__,
//                          __LINE__, #a);
//       return kTfLiteError;
//     }
//   } while (0)


///
// #define TF_LITE_ENSURE_STATUS(a)
//   do {
//     const TfLiteStatus s = (a);
//     if (s != kTfLiteOk) {
//       return s;
//     }
//   } while (0)

/** Check whether the value {@code a == b} is true, and if not return kTfLiteError
 *  from the current function, while also reporting the location of the error.
 *  {@code a} and {@code b} may be evaluated more than once, so no side effects or
 *  extremely expensive computations should be done.
 * 
 *  NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes. */
// #define TF_LITE_ENSURE_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%d != %d)", __FILE__,
//                          __LINE__, #a, #b, (a), (b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_TYPES_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%s != %s)", __FILE__,
//                          __LINE__, #a, #b, TfLiteTypeGetName(a),
//                          TfLiteTypeGetName(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_NEAR(context, a, b, epsilon)
//   do {
//     auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));
//     if (delta > epsilon) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s not near %s (%f != %f)",
//                          __FILE__, __LINE__, #a, #b, static_cast<double>(a),
//                          static_cast<double>(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_OK(context, status)
//   do {
//     const TfLiteStatus s = (status);
//     if ((s) != kTfLiteOk) {
//       return s;
//     }
//   } while (0)

// `std::unreachable` not available until CC23.
// #ifdef __GNUC__  // GCC, Clang, ICC

// #define TFL_UNREACHABLE() (__builtin_unreachable())

// #elif defined(_MSC_VER)  // MSVC

// #define TFL_UNREACHABLE() (__assume(false))
// Targeting ../TfLiteComplex64.java


// Targeting ../TfLiteComplex128.java


// Targeting ../TfLiteFloat16.java


// Targeting ../TfLiteBFloat16.java



/** Return the name of a given type, for error reporting purposes. */
public static native @Cast("const char*") BytePointer TfLiteTypeGetName(@Cast("TfLiteType") int type);

/** SupportedQuantizationTypes. */
/** enum TfLiteQuantizationType */
public static final int
  /** No quantization. */
  kTfLiteNoQuantization = 0,
  /** Affine quantization (with support for per-channel quantization).
   *  Corresponds to TfLiteAffineQuantization. */
  kTfLiteAffineQuantization = 1;
// Targeting ../TfLiteQuantization.java


// Targeting ../TfLiteAffineQuantization.java


// Targeting ../TfLitePtrUnion.java



/** Memory allocation strategies.
 *   * {@code kTfLiteMmapRo}: Read-only memory-mapped data, or data externally
 *         allocated.
 *   * {@code kTfLiteArenaRw}: Arena allocated with no guarantees about persistence,
 *         and available during eval.
 *   * {@code kTfLiteArenaRwPersistent}: Arena allocated but persistent across eval,
 *   and only available during eval.
 *   * {@code kTfLiteDynamic}: Allocated during eval, or for string tensors.
 *   * {@code kTfLitePersistentRo}: Allocated and populated during prepare. This is
 *         useful for tensors that can be computed during prepare and treated
 *         as constant inputs for downstream ops (also in prepare).
 *   * {@code kTfLiteCustom}: Custom memory allocation provided by the user. See
 *         TfLiteCustomAllocation below.
 *   * {@code kTfLiteVariantObject}: Allocation is an arbitrary type-erased C++
 *   object.
 *         Allocation and deallocation are done through {@code new} and {@code delete}. */
/** enum TfLiteAllocationType */
public static final int
  kTfLiteMemNone = 0,
  kTfLiteMmapRo = 1,
  kTfLiteArenaRw = 2,
  kTfLiteArenaRwPersistent = 3,
  kTfLiteDynamic = 4,
  kTfLitePersistentRo = 5,
  kTfLiteCustom = 6,
  kTfLiteVariantObject = 7;

/** Memory allocation strategies.
 * 
 *  TfLiteAllocationType values have been overloaded to mean more than their
 *  original intent. This enum should only be used to document the allocation
 *  strategy used by a tensor for it data. */
/** enum TfLiteAllocationStrategy */
public static final int
  kTfLiteAllocationStrategyUnknown = 0,
  kTfLiteAllocationStrategyNone = 1,    /** No data is allocated. */
  kTfLiteAllocationStrategyMMap = 2,    /** Data is mmaped. */
  kTfLiteAllocationStrategyArena = 3,   /** Handled by the arena. */
  kTfLiteAllocationStrategyMalloc = 4,  /** Uses {@code malloc}/{@code free}. */
  kTfLiteAllocationStrategyNew = 5;      /** Uses {@code new[]}/{@code delete[]}. */

/** Describes how stable a tensor attribute is with regards to an interpreter
 *  runs. */
/** enum TfLiteRunStability */
public static final int
  kTfLiteRunStabilityUnknown = 0,
  kTfLiteRunStabilityUnstable = 1,   /** May change at any time. */
  kTfLiteRunStabilitySingleRun = 2,  /** Will stay the same for one run. */
  kTfLiteRunStabilityAcrossRuns = 3;  /** Will stay the same across all runs. */

/** Describes the steps of a TFLite operation life cycle. */
/** enum TfLiteRunStep */
public static final int
  kTfLiteRunStepUnknown = 0,
  kTfLiteRunStepInit = 1,
  kTfLiteRunStepPrepare = 2,
  kTfLiteRunStepEval = 3;

/** The delegates should use zero or positive integers to represent handles.
 *  -1 is reserved from unallocated status. */
/** enum  */
public static final int
  kTfLiteNullBufferHandle = -1;
// Targeting ../TfLiteDimensionMetadata.java


// Targeting ../TfLiteSparsity.java


// Targeting ../TfLiteCustomAllocation.java



/** The flags used in {@code Interpreter::SetCustomAllocationForTensor}.
 *  Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc. */
/** enum TfLiteCustomAllocationFlags */
public static final int
  kTfLiteCustomAllocationFlagsNone = 0,
  /** Skips checking whether allocation.data points to an aligned buffer as
   *  expected by the TFLite runtime.
   *  NOTE: Setting this flag can cause crashes when calling Invoke().
   *  Use with caution. */
  kTfLiteCustomAllocationFlagsSkipAlignCheck = 1;

/** enum  */

public static native @MemberGetter int kTfLiteNoBufferIdentifier();
public static final int kTfLiteNoBufferIdentifier = kTfLiteNoBufferIdentifier();
// Targeting ../TfLiteTensor.java


// Targeting ../TfLiteNode.java


// #else   // defined(TF_LITE_STATIC_MEMORY)?
// NOTE: This flag is opt-in only at compile time.
//
// Specific reduced TfLiteTensor struct for TF Micro runtime. This struct
// contains only the minimum fields required to initialize and prepare a micro
// inference graph. The fields in this struct have been ordered from
// largest-to-smallest for optimal struct sizeof.
//
// This struct does not use:
// - allocation
// - buffer_handle
// - data_is_stale
// - delegate
// - dims_signature
// - name
// - sparsity

// Specific reduced TfLiteNode struct for TF Micro runtime. This struct contains
// only the minimum fields required to represent a node.
//
// This struct does not use:
// - delegate
// - intermediates
// - temporaries
// Targeting ../TfLiteEvalTensor.java



// #ifndef TF_LITE_STATIC_MEMORY
/** Free data memory of tensor {@code t}. */
public static native void TfLiteTensorDataFree(TfLiteTensor t);

/** Free quantization data. */
public static native void TfLiteQuantizationFree(TfLiteQuantization quantization);

/** Free sparsity parameters. */
public static native void TfLiteSparsityFree(TfLiteSparsity sparsity);

/** Free memory of tensor {@code t}. */
public static native void TfLiteTensorFree(TfLiteTensor t);

/** Set all of a tensor's fields (and free any previously allocated data). */
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);

/** Copies the contents of {@code src} in {@code dst}.
 *  Function does nothing if either {@code src} or {@code dst} is passed as nullptr and
 *  return {@code kTfLiteOk}.
 *  Returns {@code kTfLiteError} if {@code src} and {@code dst} doesn't have matching data size.
 *  Note function copies contents, so it won't create new data pointer
 *  or change allocation type.
 *  All Tensor related properties will be copied from {@code src} to {@code dst} like
 *  quantization, sparsity, ... */
public static native @Cast("TfLiteStatus") int TfLiteTensorCopy(@Const TfLiteTensor src, TfLiteTensor dst);

/** Change the size of the memory block owned by {@code tensor} to {@code num_bytes}.
 *  Tensors with allocation types other than {@code kTfLiteDynamic} will be ignored
 *  and a {@code kTfLiteOk} will be returned. {@code tensor}'s internal data buffer will be
 *  assigned a pointer which can safely be passed to free or realloc if
 *  {@code num_bytes} is zero. If {@code preserve_data} is true, tensor data will be
 *  unchanged in the range from the start of the region up to the minimum of the
 *  old and new sizes. In the case of NULL tensor, or an error allocating new
 *  memory, returns {@code kTfLiteError}. */
public static native @Cast("TfLiteStatus") int TfLiteTensorResizeMaybeCopy(@Cast("size_t") long num_bytes, TfLiteTensor tensor,
                                         @Cast("bool") boolean preserve_data);

/** Change the size of the memory block owned by {@code tensor} to {@code num_bytes}.
 *  Tensors with allocation types other than {@code kTfLiteDynamic} will be ignored
 *  and a {@code kTfLiteOk} will be returned. {@code tensor}'s internal data buffer will be
 *  assigned a pointer which can safely be passed to free or realloc if
 *  {@code num_bytes} is zero. Tensor data will be unchanged in the range from the
 *  start of the region up to the minimum of the old and new sizes. In the case
 *  of NULL tensor, or an error allocating new memory, returns {@code kTfLiteError}. */

///
///
public static native @Cast("TfLiteStatus") int TfLiteTensorRealloc(@Cast("size_t") long num_bytes, TfLiteTensor tensor);
// Targeting ../TfLiteDelegateParams.java


// Targeting ../TfLiteOpaqueDelegateParams.java


// Targeting ../TfLiteContext.java



/** {@code TfLiteOperator} is an external version of {@code TfLiteRegistration}
 *  for C API which doesn't use internal types (such as {@code TfLiteContext}) but
 *  only uses stable API types (such as {@code TfLiteOpaqueContext}). The purpose of
 *  each field is the exactly the same as with {@code TfLiteRegistration}. */

// #ifndef DOXYGEN_SKIP
// For backwards compatibility.
// Deprecated. Use TfLiteOperator instead.

///
///
// #endif

/** The valid values of the {@code inplace_operator} field in {@code TfLiteRegistration}.
 *  This allow an op to signal to the runtime that the same data pointer
 *  may be passed as an input and output without impacting the result.
 *  This does not mean that the memory can safely be reused, it is up to the
 *  runtime to determine this, e.g. if another op consumes the same input or not
 *  or if an input tensor has sufficient memory allocated to store the output
 *  data.
 * 
 *  Setting these flags authorizes the runtime to set the data pointers of an
 *  input and output tensor to the same value. In such cases, the memory
 *  required by the output must be less than or equal to that required by the
 *  shared input, never greater. If kTfLiteInplaceOpDataUnmodified is set, then
 *  the runtime can share the same input tensor with multiple operator's
 *  outputs, provided that kTfLiteInplaceOpDataUnmodified is set for all of
 *  them. Otherwise, if an input tensor is consumed by multiple operators, it
 *  may only be shared with the operator which is the last to consume it.
 * 
 *  Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc. */
/** enum TfLiteInPlaceOp */
public static final int
  /** The default value. This indicates that the same data pointer cannot safely
   *  be passed as an op's input and output. */
  kTfLiteInplaceOpNone = 0,
  /** This indicates that an op's first output's data is identical to its first
   *  input's data, for example Reshape. */
  kTfLiteInplaceOpDataUnmodified = 1,
  /** Setting kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput means
   *  that InputN may be shared with OutputN instead of with the first output.
   *  This flag requires one or more of kTfLiteInplaceOpInputNShared to be set. */
  
///
  kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput = 2,
  /** kTfLiteInplaceOpInputNShared indicates that it is safe for an op to share
   *  InputN's data pointer with an output tensor. If
   *  kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is set then
   *  kTfLiteInplaceOpInputNShared indicates that InputN may be shared
   *  with OutputN, otherwise kTfLiteInplaceOpInputNShared indicates that InputN
   *  may be shared with the first output.
   * 
   *  Indicates that an op's first input may be shared with the first output
   *  tensor. kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput has
   *  no impact on the behavior allowed by this flag. */
  kTfLiteInplaceOpInput0Shared = 4,
  /** Indicates that an op's second input may be shared with the first output
   *  if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
   *  or second output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput
   *  is set. */
  kTfLiteInplaceOpInput1Shared = 8,
  /** Indicates that an op's third input may be shared with the first output
   *  if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
   *  or third output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput
   *  is
   *  set. */
  kTfLiteInplaceOpInput2Shared = 16;
public static native @MemberGetter int kTfLiteInplaceOpMaxValue();
public static final int
  /** Placeholder to ensure that enum can hold 64 bit values to accommodate
   *  future fields. */
  kTfLiteInplaceOpMaxValue = kTfLiteInplaceOpMaxValue();

/** The number of shareable inputs supported. */

///
///
@MemberGetter public static native int kTfLiteMaxSharableOpInputs();
public static final int kTfLiteMaxSharableOpInputs = kTfLiteMaxSharableOpInputs();
// Targeting ../TfLiteRegistration.java


// Targeting ../TfLiteRegistration_V3.java


// Targeting ../TfLiteRegistration_V2.java


// Targeting ../TfLiteRegistration_V1.java



/** The flags used in {@code TfLiteDelegate}. Note that this is a bitmask, so the
 *  values should be 1, 2, 4, 8, ...etc. */
/** enum TfLiteDelegateFlags */
public static final int
  
///
  kTfLiteDelegateFlagsNone = 0,
  /** The flag is set if the delegate can handle dynamic sized tensors.
   *  For example, the output shape of a {@code Resize} op with non-constant shape
   *  can only be inferred when the op is invoked.
   *  In this case, the Delegate is responsible for calling
   *  {@code SetTensorToDynamic} to mark the tensor as a dynamic tensor, and calling
   *  {@code ResizeTensor} when invoking the op.
   * 
   *  If the delegate isn't capable to handle dynamic tensors, this flag need
   *  to be set to false. */
  
///
///
  kTfLiteDelegateFlagsAllowDynamicTensors = 1,

  /** This flag can be used by delegates (that allow dynamic tensors) to ensure
   *  applicable tensor shapes are automatically propagated in the case of
   *  tensor resizing. This means that non-dynamic (allocation_type !=
   *  kTfLiteDynamic) I/O tensors of a delegate kernel will have correct shapes
   *  before its Prepare() method is called. The runtime leverages TFLite
   *  builtin ops in the original execution plan to propagate shapes.
   * 
   *  A few points to note:
   *  1. This requires kTfLiteDelegateFlagsAllowDynamicTensors. If that flag is
   *  false, this one is redundant since the delegate kernels are re-initialized
   *  every time tensors are resized.
   *  2. Enabling this flag adds some overhead to AllocateTensors(), since extra
   *  work is required to prepare the original execution plan.
   *  3. This flag requires that the original execution plan only have ops with
   *  valid registrations (and not 'dummy' custom ops like with Flex).
   * 
   *  WARNING: This feature is experimental and subject to change. */
  kTfLiteDelegateFlagsRequirePropagatedShapes = 2,

  /** This flag can be used by delegates to request per-operator profiling. If a
   *  node is a delegate node, this flag will be checked before profiling. If
   *  set, then the node will not be profiled. The delegate will then add per
   *  operator information using {@code Profiler::EventType::OPERATOR_INVOKE_EVENT}
   *  and the results will appear in the operator-wise Profiling section and not
   *  in the Delegate internal section. */
  kTfLiteDelegateFlagsPerOperatorProfiling = 4;
// Targeting ../TfLiteDelegate.java



/** Build a {@code null} delegate, with all the fields properly set to their default
 *  values. */

///
///
public static native @ByVal TfLiteDelegate TfLiteDelegateCreate();
// Targeting ../TfLiteOpaqueDelegateBuilder.java



// #ifndef TF_LITE_STATIC_MEMORY
// See c_api_opaque.h.
// This declaration in common.h is only for backwards compatibility.
// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
public static native @Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct TfLiteOpaqueDelegateCreate(
    @Const TfLiteOpaqueDelegateBuilder opaque_delegate_builder);

// See c_api_opaque.h.
// This declaration in common.h is only for backwards compatibility.
// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
public static native void TfLiteOpaqueDelegateDelete(@Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);
// #endif  // TF_LITE_STATIC_MEMORY

// See c_api_opaque.h.
// This declaration in common.h is only for backwards compatibility.
// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
public static native Pointer TfLiteOpaqueDelegateGetData(@Cast("const TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);

/** Returns a tensor data allocation strategy. */
public static native @Cast("TfLiteAllocationStrategy") int TfLiteTensorGetAllocationStrategy(
    @Const TfLiteTensor t);

/** Returns how stable a tensor data buffer address is across runs. */
public static native @Cast("TfLiteRunStability") int TfLiteTensorGetBufferAddressStability(@Const TfLiteTensor t);

/** Returns how stable a tensor data values are across runs. */

///
public static native @Cast("TfLiteRunStability") int TfLiteTensorGetDataStability(@Const TfLiteTensor t);

/** Returns the operation step when the data of a tensor is populated.
 * 
 *  Some operations can precompute their results before the evaluation step.
 *  This makes the data available earlier for subsequent operations. */

///
public static native @Cast("TfLiteRunStep") int TfLiteTensorGetDataKnownStep(@Const TfLiteTensor t);

/** Returns the operation steop when the shape of a tensor is computed.
 * 
 *  Some operations can precompute the shape of their results before the
 *  evaluation step. This makes the shape available earlier for subsequent
 *  operations. */
public static native @Cast("TfLiteRunStep") int TfLiteTensorGetShapeKnownStep(@Const TfLiteTensor t);

/** \} */
// Ends `\addtogroup`, it's important for the doc generator that this doesn't
// include the CC code below.

// #ifdef __cplusplus  // extern "C"

// #include <utility>
// Targeting ../VariantData.java



// Concrete implementations extend `AbstractVariantData` with CRPT.

// Analogous to `TfLiteTensorRealloc` for allocation of tensors whose
// data member points to an arbitrary C++ object. `VariantType` refers
// to the erased type of said object and `VariantArgs` refers to
// a list of argument types with which to construct a new `VariantType`.
// `VariantArgs` must match a constructor of `VariantType`.

// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_CORE_C_COMMON_H_


// Parsed from tensorflow/lite/c/c_api_experimental.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_
// #define TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_

/** For documentation, see
 *  third_party/tensorflow/lite/core/c/c_api_experimental.h */

// #include "tensorflow/lite/core/c/c_api_experimental.h"

// #endif  // TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_


// Parsed from tensorflow/lite/core/c/c_api_experimental.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include
/** "third_party/tensorflow/lite/c/c_api_experimental.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_
// #define TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_

// #include <stdint.h>

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/c/c_api_types.h"
// #include "tensorflow/lite/core/c/c_api.h"
// #include "tensorflow/lite/core/c/common.h"

// #ifdef __cplusplus
// #endif  // __cplusplus

// --------------------------------------------------------------------------
/** Resets all variable tensors to zero.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResetVariableTensors(
    TfLiteInterpreter interpreter);

// Returns the number of variable tensors associated with the model.
public static native int TfLiteInterpreterGetVariableTensorCount(
    @Const TfLiteInterpreter interpreter);

// Returns the tensor associated with the variable tensor index.
// REQUIRES: 0 <= input_index <
// TfLiteInterpreterGetVariableTensorCount(interpreter)

///
///
///
///
public static native TfLiteTensor TfLiteInterpreterGetVariableTensor(
    @Const TfLiteInterpreter interpreter, int variable_index);

/** Adds an op registration for a builtin operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of the interpreter's lifetime. A common
 *  practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} (or related functions) on the same
 *  options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
///
public static native void TfLiteInterpreterOptionsAddBuiltinOp(
    TfLiteInterpreterOptions options, @Cast("TfLiteBuiltinOperator") int op,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);

/** Adds an op registration for a custom operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of any created interpreter's lifetime. A
 *  common practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  The lifetime of the string pointed to by {@code name} must be at least as long
 *  as the lifetime of the {@code TfLiteInterpreterOptions}.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} (or related functions) on the same
 *  options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
///
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, @Cast("const char*") BytePointer name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, String name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
// Targeting ../Find_builtin_op_external_Pointer_int_int.java


// Targeting ../Find_custom_op_external_Pointer_String_int.java



///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverExternal(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op,
    Find_custom_op_external_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_Pointer_int_int.java


// Targeting ../Find_custom_op_Pointer_BytePointer_int.java



///
///
///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverExternalWithFallback(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op_external,
    Find_custom_op_external_Pointer_String_int find_custom_op_external,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_BytePointer_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverExternalWithFallback(
    TfLiteInterpreterOptions options,
    Find_builtin_op_external_Pointer_int_int find_builtin_op_external,
    Find_custom_op_external_Pointer_String_int find_custom_op_external,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);

/** Registers callbacks for resolving builtin or custom operators.
 * 
 *  The {@code TfLiteInterpreterOptionsSetOpResolver} function provides an alternative
 *  method for registering builtin ops and/or custom ops, by providing operator
 *  resolver callbacks.  Unlike using {@code TfLiteInterpreterOptionsAddBuiltinOp}
 *  and/or {@code TfLiteInterpreterOptionsAddAddCustomOp}, these let you register all
 *  the operators in a single call.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsAddBuiltin} or
 *  {@code TfLiteInterpreterOptionsAddCustomOp} on the same options object.
 * 
 *  If {@code op_resolver_user_data} is non-null, its lifetime must be at least as
 *  long as the lifetime of the {@code TfLiteInterpreterOptions}.
 * 
 *  WARNING: This is an experimental API and subject to change.
 * 
 *  DEPRECATED: use TfLiteInterpreterOptionsSetOpResolverExternal instead. */

///
public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_BytePointer_int find_custom_op,
    Pointer op_resolver_user_data);
public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v3_Pointer_int_int.java


// Targeting ../Find_custom_op_v3_Pointer_BytePointer_int.java



///
public static native void TfLiteInterpreterOptionsSetOpResolverV3(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v3_Pointer_int_int find_builtin_op_v3,
    Find_custom_op_v3_Pointer_BytePointer_int find_custom_op_v3,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v3_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV3(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v3_Pointer_int_int find_builtin_op_v3,
    Find_custom_op_v3_Pointer_String_int find_custom_op_v3,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v2_Pointer_int_int.java


// Targeting ../Find_custom_op_v2_Pointer_BytePointer_int.java



///
public static native void TfLiteInterpreterOptionsSetOpResolverV2(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v2_Pointer_int_int find_builtin_op_v2,
    Find_custom_op_v2_Pointer_BytePointer_int find_custom_op_v2,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v2_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV2(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v2_Pointer_int_int find_builtin_op_v2,
    Find_custom_op_v2_Pointer_String_int find_custom_op_v2,
    Pointer op_resolver_user_data);
// Targeting ../Find_builtin_op_v1_Pointer_int_int.java


// Targeting ../Find_custom_op_v1_Pointer_BytePointer_int.java



///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolverV1(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v1_Pointer_int_int find_builtin_op_v1,
    Find_custom_op_v1_Pointer_BytePointer_int find_custom_op_v1,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_v1_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolverV1(
    TfLiteInterpreterOptions options,
    Find_builtin_op_v1_Pointer_int_int find_builtin_op_v1,
    Find_custom_op_v1_Pointer_String_int find_custom_op_v1,
    Pointer op_resolver_user_data);

/** Returns a new interpreter using the provided model and options, or null on
 *  failure, where the model uses only the operators explicitly added to the
 *  options.  This is the same as {@code TFLiteInterpreterCreate} from {@code c_api.h},
 *  except that the only operators that are supported are the ones registered
 *  in {@code options} via calls to {@code TfLiteInterpreterOptionsSetOpResolver},
 *  {@code TfLiteInterpreterOptionsAddBuiltinOp}, and/or
 *  {@code TfLiteInterpreterOptionsAddCustomOp}.
 * 
 *  * {@code model} must be a valid model instance. The caller retains ownership of
 *    the object, and can destroy it immediately after creating the interpreter;
 *    the interpreter will maintain its own reference to the underlying model
 *    data.
 *  * {@code options} should not be null. The caller retains ownership of the object,
 *    and can safely destroy it immediately after creating the interpreter.
 * 
 *  NOTE: The client *must* explicitly allocate tensors before attempting to
 *  access input tensor data or invoke the interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native TfLiteInterpreter TfLiteInterpreterCreateWithSelectedOps(@Const TfLiteModel model,
                                       @Const TfLiteInterpreterOptions options);

/** Enable or disable the NN API delegate for the interpreter (true to enable).
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
public static native void TfLiteInterpreterOptionsSetUseNNAPI(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Enable or disable CPU fallback for the interpreter (true to enable).
 *  If enabled, TfLiteInterpreterInvoke will do automatic fallback from
 *  executing with delegate(s) to regular execution without delegates
 *  (i.e. on CPU).
 * 
 *  Allowing the fallback is suitable only if both of the following hold:
 *  - The caller is known not to cache pointers to tensor data across
 *    TfLiteInterpreterInvoke calls.
 *  - The model is not stateful (no variables, no LSTMs) or the state isn't
 *    needed between batches.
 * 
 *  When delegate fallback is enabled, TfLiteInterpreterInvoke will
 *  behave as follows:
 *    If one or more delegates were set in the interpreter options
 *    (see TfLiteInterpreterOptionsAddDelegate),
 *    AND inference fails,
 *    then the interpreter will fall back to not using any delegates.
 *    In that case, the previously applied delegate(s) will be automatically
 *    undone, and an attempt will be made to return the interpreter to an
 *    invokable state, which may invalidate previous tensor addresses,
 *    and the inference will be attempted again, using input tensors with
 *    the same value as previously set.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native void TfLiteInterpreterOptionsSetEnableDelegateFallback(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Allow a delegate to look at the graph and modify the graph to handle
 *  parts of the graph themselves. After this is called, the graph may
 *  contain new nodes that replace 1 more nodes.
 *  'delegate' must outlive the interpreter.
 *  Use {@code TfLiteInterpreterOptionsAddDelegate} instead of this unless
 *  absolutely required.
 *  Returns one of the following three status codes:
 *  1. kTfLiteOk: Success.
 *  2. kTfLiteDelegateError: Delegation failed due to an error in the
 *  delegate. The Interpreter has been restored to its pre-delegation state.
 *  NOTE: This undoes all delegates previously applied to the Interpreter.
 *  3. kTfLiteError: Unexpected/runtime failure.
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterModifyGraphWithDelegate(
    @Const TfLiteInterpreter interpreter, TfLiteDelegate delegate);

/** Returns the tensor index corresponding to the input tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native int TfLiteInterpreterGetInputTensorIndex(
    @Const TfLiteInterpreter interpreter, int input_index);

/** Returns the tensor index corresponding to the output tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
public static native int TfLiteInterpreterGetOutputTensorIndex(
    @Const TfLiteInterpreter interpreter, int output_index);

/** Assigns (or reassigns) a custom memory allocation for the given
 *  tensor. {@code flags} is a bitmask, see TfLiteCustomAllocationFlags.
 *  The runtime does NOT take ownership of the underlying memory.
 * 
 *  NOTE: User needs to call TfLiteInterpreterAllocateTensors() after this.
 *  Invalid/insufficient buffers will cause an error during
 *  TfLiteInterpreterAllocateTensors or TfLiteInterpreterInvoke (in case of
 *  dynamic shapes in the graph).
 * 
 *  Parameters should satisfy the following conditions:
 *  1. tensor->allocation_type == kTfLiteArenaRw or kTfLiteArenaRwPersistent
 *     In general, this is true for I/O tensors & variable tensors.
 *  2. allocation->data has the appropriate permissions for runtime access
 *     (Read-only for inputs, Read-Write for others), and outlives
 *     TfLiteInterpreter.
 *  3. allocation->bytes >= tensor->bytes.
 *     This condition is checked again if any tensors are resized.
 *  4. allocation->data should be aligned to kDefaultTensorAlignment
 *     defined in lite/util.h. (Currently 64 bytes)
 *     This check is skipped if kTfLiteCustomAllocationFlagsSkipAlignCheck is
 *     set through {@code flags}.
 *  WARNING: This is an experimental API and subject to change. */

///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterSetCustomAllocationForTensor(
    TfLiteInterpreter interpreter, int tensor_index,
    @Const TfLiteCustomAllocation allocation, @Cast("int64_t") long flags);

/** --------------------------------------------------------------------------
 *  BufferHandle APIs
 <p>
 *  Sets the delegate buffer handle for the given tensor.
 * 
 *  This function sets the buffer handle for a tensor that is used by other
 *  computing hardware such as EdgeTpu. For example, EdgeTpu delegate imports a
 *  tensor's memory into EdgeTpu's virtual address and returns a buffer handle.
 *  Then EdgeTpu delegate calls this API to associate the tensor with the buffer
 *  handle.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterSetBufferHandle(
    TfLiteInterpreter interpreter, TfLiteTensor tensor,
    @Cast("TfLiteBufferHandle") int buffer_handle, @Cast("TfLiteOpaqueDelegate*") TfLiteOpaqueDelegateStruct delegate);

/** Gets the delegate buffer handle, and the delegate which can process
 *  the buffer handle.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterGetBufferHandle(
    TfLiteInterpreter interpreter, int tensor_index,
    @Cast("TfLiteBufferHandle*") IntPointer buffer_handle, @Cast("TfLiteOpaqueDelegate**") PointerPointer delegate);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterGetBufferHandle(
    TfLiteInterpreter interpreter, int tensor_index,
    @Cast("TfLiteBufferHandle*") IntPointer buffer_handle, @Cast("TfLiteOpaqueDelegate**") @ByPtrPtr TfLiteOpaqueDelegateStruct delegate);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterGetBufferHandle(
    TfLiteInterpreter interpreter, int tensor_index,
    @Cast("TfLiteBufferHandle*") IntBuffer buffer_handle, @Cast("TfLiteOpaqueDelegate**") @ByPtrPtr TfLiteOpaqueDelegateStruct delegate);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterGetBufferHandle(
    TfLiteInterpreter interpreter, int tensor_index,
    @Cast("TfLiteBufferHandle*") int[] buffer_handle, @Cast("TfLiteOpaqueDelegate**") @ByPtrPtr TfLiteOpaqueDelegateStruct delegate);

/** Sets whether buffer handle output is allowed.
 *  When using hardware delegation, Interpreter will make the data of output
 *  tensors available in {@code tensor->data} by default. If the application can
 *  consume the buffer handle directly (e.g. reading output from OpenGL
 *  texture), it can set this flag to false, so Interpreter won't copy the
 *  data from buffer handle to CPU memory.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native void TfLiteSetAllowBufferHandleOutput(
    @Const TfLiteInterpreter interpreter, @Cast("bool") boolean allow_buffer_handle_output);

/** --------------------------------------------------------------------------
 *  SignatureRunner APIs
 <p>
 *  Attempts to cancel in flight invocation if any.
 *  This will not affect calls to {@code Invoke} that happen after this.
 *  Non blocking and thread safe.
 *  Returns kTfLiteError if cancellation is not enabled, otherwise returns
 *  kTfLiteOk.
 *  NOTE: Calling this function will cancel in-flight invocations
 *  in all SignatureRunners built from the same interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native @Cast("TfLiteStatus") int TfLiteSignatureRunnerCancel(
    TfLiteSignatureRunner signature_runner);

// Forward declaration, to avoid need for dependency on
// tensorflow/lite/profiling/telemetry/profiler.h.

/** Registers the telemetry profiler to the interpreter.
 *  Note: The interpreter does not take the ownership of profiler, but callers
 *  must ensure profiler->data outlives the lifespan of the interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native void TfLiteInterpreterOptionsSetTelemetryProfiler(
    TfLiteInterpreterOptions options,
    TfLiteTelemetryProfilerStruct profiler);

/** Ensures the data of the tensor at the given index is readable.
 *  Note: If a delegate has been used, and {@code SetAllowBufferHandleOutput(true)}
 *  has been called, tensor outputs may be stored as delegate buffer handles
 *  whose data is not directly readable until this method has been called. In
 *  such cases, this method will copy the data from the delegate buffer handle
 *  to CPU memory.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native @Cast("TfLiteStatus") int TfLiteInterpreterEnsureTensorDataIsReadable(
    TfLiteInterpreter interpreter, int tensor_index);
// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_CORE_C_C_API_EXPERIMENTAL_H_


// Parsed from tensorflow/lite/core/api/error_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_
// #define TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_

// #include "tensorflow/compiler/mlir/lite/core/api/error_reporter.h"  // IWYU pragma: export

// #endif  // TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_


// Parsed from tensorflow/lite/core/api/op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_

// #include <cstddef>
// #include <functional>
// #include <limits>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include "tensorflow/compiler/mlir/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// Targeting ../OpResolverInternal.java

  // For friend declaration below.
// Targeting ../CommonOpaqueConversionUtil.java

  // For friend declaration below.              // Forward decl.

// Targeting ../OpResolver.java


// Targeting ../OperatorsCache.java


  // namespace internal
// #endif

// Handles the logic for converting between an OperatorCode structure extracted
// from a flatbuffer and information about a registered operator
// implementation.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Cast("const TfLiteRegistration**") PointerPointer registration);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Const @ByPtrPtr TfLiteRegistration registration);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_


// Parsed from tensorflow/lite/core/api/profiler.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_PROFILER_H_
// #define TENSORFLOW_LITE_CORE_API_PROFILER_H_

// #include <cstdint>
// Targeting ../Profiler.java


// Targeting ../ScopedProfile.java


// Targeting ../ScopedOperatorProfile.java


// Targeting ../ScopedDelegateOperatorProfile.java


// Targeting ../ScopedDelegateProfiledOperatorProfile.java


// Targeting ../ScopedRuntimeInstrumentationProfile.java



  // namespace tflite

// #define TFLITE_VARNAME_UNIQ_IMPL(name, ctr) name##ctr
// #define TFLITE_VARNAME_UNIQ(name, ctr) TFLITE_VARNAME_UNIQ_IMPL(name, ctr)

// #define TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler, tag)
//   tflite::ScopedProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag))

// #define TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedOperatorProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag), (node_index))

// #define TFLITE_SCOPED_DELEGATE_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedDelegateOperatorProfile TFLITE_VARNAME_UNIQ(
//       _profile_, __COUNTER__)((profiler), (tag), (node_index))

// #define TFLITE_SCOPED_DELEGATE_PROFILED_OPERATOR_PROFILE(profiler, tag,
//                                                          node_index)
//   tflite::ScopedDelegateProfiledOperatorProfile TFLITE_VARNAME_UNIQ(
//       _profile_, __COUNTER__)((profiler), (tag), (node_index))

// #define TFLITE_ADD_RUNTIME_INSTRUMENTATION_EVENT(
//     profiler, tag, event_metadata1, event_metadata2)
//   do {
//     if (profiler) {
//       const auto handle = profiler->BeginEvent(
//           tag, Profiler::EventType::GENERAL_RUNTIME_INSTRUMENTATION_EVENT,
//           event_metadata1, event_metadata2);
//       profiler->EndEvent(handle);
//     }
//   } while (false);

// #endif  // TENSORFLOW_LITE_CORE_API_PROFILER_H_


// Parsed from tensorflow/lite/core/api/verifier.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Abstract interface for verifying a model. */
// #ifndef TENSORFLOW_LITE_CORE_API_VERIFIER_H_
// #define TENSORFLOW_LITE_CORE_API_VERIFIER_H_

// #include "tensorflow/compiler/mlir/lite/core/api/verifier.h"  // IWYU pragma: export

// #endif  // TENSORFLOW_LITE_CORE_API_VERIFIER_H_


// Parsed from tensorflow/lite/experimental/resource/initialization_status.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_

// #include <cstddef>
// #include <cstdint>
// #include <memory>
// #include <unordered_map>

// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// Targeting ../InitializationStatus.java



/** WARNING: Experimental interface, subject to change. */

@Namespace("tflite::resource") public static native InitializationStatus GetInitializationStatus(@Cast("tflite::resource::InitializationStatusMap*") SizeTSizeTMap map,
                                              int subgraph_id);

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_


// Parsed from tensorflow/lite/experimental/resource/resource_base.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_

// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <utility>
// Targeting ../ResourceBase.java



/** WARNING: Experimental interface, subject to change. */

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_


// Parsed from tensorflow/lite/allocation.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Memory management for TF Lite. */
// #ifndef TENSORFLOW_LITE_ALLOCATION_H_
// #define TENSORFLOW_LITE_ALLOCATION_H_

// #include "tensorflow/compiler/mlir/lite/allocation.h"

// #endif  // TENSORFLOW_LITE_ALLOCATION_H_


// Parsed from tensorflow/lite/stderr_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_STDERR_REPORTER_H_
// #define TENSORFLOW_LITE_STDERR_REPORTER_H_

// #include <cstdarg>

// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../StderrReporter.java



// Return the default error reporter (output to stderr).
@Namespace("tflite") public static native ErrorReporter DefaultErrorReporter();

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STDERR_REPORTER_H_


// Parsed from tensorflow/lite/graph_info.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_GRAPH_INFO_H_
// #define TENSORFLOW_LITE_GRAPH_INFO_H_

// #include <stddef.h>

// #include <cstdint>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../GraphInfo.java


// Targeting ../NodeSubset.java



// LINT.IfChange
// Node edge.second depends on node edge.first.
// LINT.ThenChange(//tensorflow/compiler/mlir/lite/utils/control_edges.h)

// Partitions a list of node indices `nodes_to_partition` into node subsets.
// Each node subset is in dependency order internally (i.e. all members of the
// node subsets can be executed in the order they occur) and externally (i.e.,
// node subsets are executable in the order they occur.) The function assumes
// that the nodes of the graph represented in *info are in dependency order.
//
// Depending on the value of `greedily`, the function behaves
//
// - greedily: while a node_set is generated whose members are (aren't) members
// of
//   `*nodes_to_partition`, it will add nodes to this subset, as long as they
//   are (aren't) members of *nodes_to_partition and they are schedulable (i.e.,
//   all nodes they depend have already be added to `*node_subsets`.)
//
// - non-greedily: this preserves the original execution order, i.e. the node
//   subsets generated will be of the form [ [0..i_1), [i1..i2), ... ].
//
// `control_edges` specifies a control dependency DAG on the nodes contained in
// `info`. The resulting partitioning will respect these control
// dependencies. This way, restrictions (in addition to the nodes' data
// dependencies) can be imposed on the ultimate execution order of the graph
// (naturally, this is relevant only if ordering greedily.)
//
// (Example: with `greedily`, `control_edges.empty()`, and `nodes_to_partition
// == {2, 3}`, the graph
//
//                    ------------
//                    |            v
// 0 --> 1 --> 2* --> 3*     4 --> 5
//       |                   ^
//       -------------------
//
// will be partitioned as {{0, 1, 4}, {2, 3}, {5}}, since data dependencies
// (notated '-->') allow for execution of 4 immediately after 1.
//
// With an additional control dependency `control_edges == {{3, 4}}` (notated
// '==>'), execution of node 4 requires prior execution of node 3:
//
//                    ------------
//                    |            v
// 0 --> 1 --> 2* --> 3* ==> 4 --> 5
//       |                   ^
//       -------------------
//
// and the partitioning will be {{0, 1}, {2, 3}, {4, 5}}.)
//
// If control_edges == nullptr, the algorithm preserves the relative ordering of
// nodes that have their `might_have_side_effects` attribute set, i.e., it
// behaves as if `*control_dependencies` of the form `{ {n_1, n_2}, {n_2, n_3},
// ... }` had been handed in, where the n_i are the (sorted) indices of nodes
// with `might_have_side_effects` attribute set.
//
// The function assumes that `*node_subsets` is initially empty.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int PartitionGraphIntoIndependentNodeSubsets(
    @Const GraphInfo info, @Const TfLiteIntArray nodes_to_partition,
    NodeSubsetVector node_subsets, @Cast("bool") boolean greedily,
    @Cast("const tflite::ControlEdges*") IntIntPairVector control_edges/*=nullptr*/,
    @Cast("bool") boolean disable_node_fusion/*=false*/);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int PartitionGraphIntoIndependentNodeSubsets(
    @Const GraphInfo info, @Const TfLiteIntArray nodes_to_partition,
    NodeSubsetVector node_subsets, @Cast("bool") boolean greedily);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_GRAPH_INFO_H_


// Parsed from tensorflow/lite/interpreter_options.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Provides options to an interpreter.
/** */
// #ifndef TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_
// #define TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_
// Targeting ../InterpreterOptions.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_OPTIONS_H_


// Parsed from tensorflow/lite/memory_planner.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MEMORY_PLANNER_H_
// #define TENSORFLOW_LITE_MEMORY_PLANNER_H_

// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../MemoryPlanner.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MEMORY_PLANNER_H_


// Parsed from tensorflow/lite/util.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file provides general C++ utility functions in TFLite.
// For example: Converting between `TfLiteIntArray`, `std::vector` and
// Flatbuffer vectors. These functions can't live in `context.h` since it's pure
// C.

// #ifndef TENSORFLOW_LITE_UTIL_H_
// #define TENSORFLOW_LITE_UTIL_H_

// #include <stddef.h>
// #include <stdlib.h>

// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <vector>

// #include "tensorflow/lite/array.h"
// #include "tensorflow/lite/core/c/common.h"

// Memory allocation parameter used by ArenaPlanner.
// Clients (such as delegates) might look at this to ensure interop between
// TFLite memory & hardware buffers.
// NOTE: This only holds for tensors allocated on the arena.
@Namespace("tflite") @MemberGetter public static native int kDefaultTensorAlignment();

// The prefix of Flex op custom code.
// This will be matched agains the `custom_code` field in `OperatorCode`
// Flatbuffer Table.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kFlexCustomCodePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kFlexCustomCodePrefix();

// Checks whether the prefix of the custom name indicates the operation is an
// Flex operation.
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(@Cast("const char*") BytePointer custom_name);
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(String custom_name);

// Converts a `std::vector` to a `TfLiteIntArray`. The caller takes ownership
// of the returned pointer.
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntPointer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntBuffer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector int[] input);

// Converts an array (of the given size) to a `TfLiteIntArray`. The caller
// takes ownership of the returned pointer, and must make sure 'dims' has at
// least 'ndims' elements.
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const IntPointer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const IntBuffer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int ndims, @Const int[] dims);

// Checks whether a `TfLiteIntArray` and an int array have matching elements.
// The caller must guarantee that 'b' has at least 'b_size' elements.
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntPointer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntBuffer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const int[] b);

// Populates the size in bytes of a type into `bytes`. Returns kTfLiteOk for
// valid types, and kTfLiteError otherwise.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetSizeOfType(TfLiteContext context, @Cast("const TfLiteType") int type,
                           @Cast("size_t*") SizeTPointer bytes);

// Creates a stub TfLiteRegistration instance with the provided
// `custom_op_name`. The op will fail if invoked, and is useful as a
// placeholder to defer op resolution.
// Note that `custom_op_name` must remain valid for the returned op's lifetime..
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(@Cast("const char*") BytePointer custom_op_name);
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(String custom_op_name);

// Checks whether the provided op is an unresolved custom op.
@Namespace("tflite") public static native @Cast("bool") boolean IsUnresolvedCustomOp(@Const @ByRef TfLiteRegistration registration);

// Returns a descriptive name with the given op TfLiteRegistration.
@Namespace("tflite") public static native @StdString String GetOpNameByRegistration(@Const @ByRef TfLiteRegistration registration);

// The prefix of a validation subgraph name.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kValidationSubgraphNamePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kValidationSubgraphNamePrefix();

// Checks whether the prefix of the subgraph name indicates the subgraph is a
// validation subgraph.
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(@Cast("const char*") BytePointer name);
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(String name);

// Multiply two sizes and return true if overflow occurred;
// This is based off tensorflow/overflow.h but is simpler as we already
// have unsigned numbers. It is also generalized to work where sizeof(size_t)
// is not 8.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int MultiplyAndCheckOverflow(@Cast("size_t") long a, @Cast("size_t") long b, @Cast("size_t*") SizeTPointer product);

// Returns whether the TfLiteTensor is a resource or variant tensor.
@Namespace("tflite") public static native @Cast("bool") boolean IsResourceOrVariant(@Const TfLiteTensor tensor);

// Compute the number of bytes required to represent a tensor with dimensions
// specified by the array dims (of length dims_size). Returns the status code
// and bytes.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const IntPointer dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const IntBuffer dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int BytesRequired(@Cast("TfLiteType") int type, @Const int[] dims, @Cast("size_t") long dims_size,
                           @Cast("size_t*") SizeTPointer bytes, TfLiteContext context);
// Targeting ../TfLiteTensorDeleter.java





  // namespace tflite

// #endif  // TENSORFLOW_LITE_UTIL_H_


// Parsed from tensorflow/lite/core/macros.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// This provides utility macros and functions that are inherently platform
// specific or shared across runtime & converter.
// #ifndef TENSORFLOW_LITE_CORE_MACROS_H_
// #define TENSORFLOW_LITE_CORE_MACROS_H_

// #ifdef __has_builtin
// #define TFLITE_HAS_BUILTIN(x) __has_builtin(x)
// #else
// #define TFLITE_HAS_BUILTIN(x) 0
// #endif

// #if (!defined(__NVCC__)) && (TFLITE_HAS_BUILTIN(__builtin_expect) ||
//                              (defined(__GNUC__) && __GNUC__ >= 3))
// #define TFLITE_EXPECT_FALSE(cond) __builtin_expect(cond, false)
// #define TFLITE_EXPECT_TRUE(cond) __builtin_expect(!!(cond), true)
// #else
// #define TFLITE_EXPECT_FALSE(cond) (cond)
// #define TFLITE_EXPECT_TRUE(cond) (cond)
// #endif

// #ifdef _WIN32
// #define TFLITE_NOINLINE __declspec(noinline)
// #else
// #ifdef __has_attribute
// #if __has_attribute(noinline)
// #define TFLITE_NOINLINE __attribute__((noinline))
// #else
// #define TFLITE_NOINLINE
// #endif  // __has_attribute(noinline)
// #else
// #define TFLITE_NOINLINE
// #endif  // __has_attribute
// #endif  // _WIN32

// Normally we'd use ABSL_HAVE_ATTRIBUTE_WEAK and ABSL_ATTRIBUTE_WEAK, but
// we avoid the absl dependency for binary size reasons.
// #ifdef __has_attribute
// #define TFLITE_HAS_ATTRIBUTE(x) __has_attribute(x)
// #else
// #define TFLITE_HAS_ATTRIBUTE(x) 0
// #endif

// #if (TFLITE_HAS_ATTRIBUTE(weak) ||
//      (defined(__GNUC__) && !defined(__clang__))) &&
//     !(defined(__llvm__) && defined(_WIN32)) && !defined(__MINGW32__)
// #undef TFLITE_ATTRIBUTE_WEAK
// #define TFLITE_ATTRIBUTE_WEAK __attribute__((weak))
public static final int TFLITE_HAS_ATTRIBUTE_WEAK = 1;
// #else
// #define TFLITE_ATTRIBUTE_WEAK
// #endif

// #endif  // TENSORFLOW_LITE_CORE_MACROS_H_


// Parsed from tensorflow/lite/core/subgraph.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SUBGRAPH_H_
// #define TENSORFLOW_LITE_CORE_SUBGRAPH_H_

// #include <stdarg.h>
// #include <stddef.h>

// #include <atomic>
// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <utility>
// #include <vector>

// #include "tensorflow/compiler/mlir/lite/allocation.h"
// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/array.h"
// #include "tensorflow/lite/c/common_internal.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/macros.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/graph_info.h"
// #include "tensorflow/lite/interpreter_options.h"
// #include "tensorflow/lite/memory_planner.h"
// #include "tensorflow/lite/util.h"
// Targeting ../SingleOpModel.java

  // Class for friend declarations.  // Class for friend declarations.

// Targeting ../AsyncSubgraph.java

  // Class for friend declarations.
         // Class for friend declarations.  // Class for friend declarations.     // Class for friend declarations.

// Targeting ../TestDelegate.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../Subgraph.java



  // namespace tflite
// #endif  // TENSORFLOW_LITE_CORE_SUBGRAPH_H_


// Parsed from tensorflow/lite/external_cpu_backend_context.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_
// #define TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_

// #include <memory>
// #include <utility>

// #include "tensorflow/lite/core/c/common.h"
// Targeting ../TfLiteInternalBackendContext.java


// Targeting ../ExternalCpuBackendContext.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_


// Parsed from tensorflow/lite/portable_type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.

// Arduino build defines abs as a macro here. That is invalid C++, and breaks
// libc++'s <complex> header, undefine it.
// #ifdef abs
// #undef abs
// #endif

// #include <stdint.h>

// #include "tensorflow/lite/core/c/common.h"

// Map statically from a C++ type to a TfLiteType. Used in interpreter for
// safe casts.
// Example:
//  typeToTfLiteType<bool>() -> kTfLiteBool

// Map from TfLiteType to the corresponding C++ type.
// Example:
//   TfLiteTypeToType<kTfLiteBool>::Type -> bool  // Specializations below

// Template specialization for both typeToTfLiteType and TfLiteTypeToType.
// #define MATCH_TYPE_AND_TFLITE_TYPE(CPP_TYPE, TFLITE_TYPE_ENUM)
//   template <>
//   constexpr TfLiteType typeToTfLiteType<CPP_TYPE>() {
//     return TFLITE_TYPE_ENUM;
//   }
//   template <>
//   constexpr TfLiteType typeToTfLiteType<const CPP_TYPE>() {
//     return TFLITE_TYPE_ENUM;
//   }
//   template <>
//   struct TfLiteTypeToType<TFLITE_TYPE_ENUM> {
//     using Type = CPP_TYPE;
//   }

// No string mapping is included here, since the TF Lite packed representation
// doesn't correspond to a C++ type well.

  

  

  

  

  

  

  

  

  

  

  

  

  

  // namespace tflite
// #endif  // TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/profiling/root_profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_

// #include <cstdint>
// #include <map>
// #include <memory>
// #include <vector>

// #include "tensorflow/lite/core/api/profiler.h"
// Targeting ../RootProfiler.java



  // namespace profiling
  // namespace tflite

// #endif  // TENSORFLOW_LITE_PROFILING_ROOT_PROFILER_H_


// Parsed from tensorflow/lite/signature_runner.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_SIGNATURE_RUNNER_H_

///
// #define TENSORFLOW_LITE_SIGNATURE_RUNNER_H_
/** \file
 * 
 *  An abstraction for invoking the TF Lite interpreter.
 *  Provides support for named parameters, and for including multiple
 *  named computations in a single model, each with its own inputs/outputs.
 <p>
 *  For documentation, see
 *  third_party/tensorflow/lite/core/signature_runner.h. */

// #include "tensorflow/lite/core/signature_runner.h"  // IWYU pragma: export
  // namespace tflite

// #endif  // TENSORFLOW_LITE_SIGNATURE_RUNNER_H_


// Parsed from tensorflow/lite/core/signature_runner.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_

///
///
// #define TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_
/** \file
 * 
 *  An abstraction for invoking the TF Lite interpreter.
 *  Provides support for named parameters, and for including multiple
 *  named computations in a single model, each with its own inputs/outputs.
 * 
 *  Do NOT include this file directly,
 *  instead include third_party/tensorflow/lite/signature_riunner.h
 *  See third_party/tensorflow/lite/c/common.h for the API for defining
 *  operations (TfLiteRegistration). */

// #include <cstddef>
// #include <cstdint>
// #include <string>
// #include <vector>

// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/internal/signature_def.h"  // Class for friend declarations.

// Targeting ../SignatureRunnerHelper.java


// Targeting ../SignatureRunnerJNIHelper.java


// Targeting ../TensorHandle.java

              // Class for friend declarations.
// Targeting ../SignatureRunner.java



  // namespace impl
  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_


// Parsed from tensorflow/lite/type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_

// #include <complex>
// #include <string>

// #include "tensorflow/lite/core/c/common.h"

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.
// #include "tensorflow/lite/portable_type_to_tflitetype.h"

// TODO(b/163167649): This string conversion means that only the first entry
// in a string tensor will be returned as a std::string, so it's deprecated.

  


  

  

  // namespace tflite
// #endif  // TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/string_type.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// Abstract string. We don't want even absl at this level.
// #ifndef TENSORFLOW_LITE_STRING_TYPE_H_
// #define TENSORFLOW_LITE_STRING_TYPE_H_

// #include <string>

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STRING_TYPE_H_


// Parsed from tensorflow/lite/mutable_op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_

// #include <stddef.h>

// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/util.h"

// Some versions of gcc don't support partial specialization in class scope,
// so these are defined in a namescope.

// Targeting ../MutableOpResolver.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_


// Parsed from tensorflow/lite/interpreter.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_INTERPRETER_H_
// #define TENSORFLOW_LITE_INTERPRETER_H_

/** For documentation, see
 *  tensorflow/lite/core/interpreter.h. */

// #include "tensorflow/lite/core/interpreter.h"
  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_H_


// Parsed from tensorflow/lite/core/interpreter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Main abstraction controlling the tflite interpreter.
/** Do NOT include this file directly,
/** instead include third_party/tensorflow/lite/interpreter.h
/** See third_party/tensorflow/lite/c/common.h for the API for defining
/** operations (TfLiteRegistration). */
// #ifndef TENSORFLOW_LITE_CORE_INTERPRETER_H_
// #define TENSORFLOW_LITE_CORE_INTERPRETER_H_

// IWYU pragma: private, include "third_party/tensorflow/lite/interpreter.h"
// IWYU pragma: friend third_party/tensorflow/lite/interpreter.h

// #include <stddef.h>
// #include <stdint.h>

// #include <atomic>
// #include <complex>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <map>
// #include <memory>
// #include <string>
// #include <type_traits>
// #include <utility>
// #include <vector>

// #include "tensorflow/compiler/mlir/lite/allocation.h"
// #include "tensorflow/compiler/mlir/lite/experimental/remat/metadata_util.h"
// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/async/async_signature_runner.h"
// #include "tensorflow/lite/core/c/common.h"  // IWYU pragma: export
// #include "tensorflow/lite/core/signature_runner.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/external_cpu_backend_context.h"
// #include "tensorflow/lite/internal/signature_def.h"
// #include "tensorflow/lite/interpreter_options.h"
// #include "tensorflow/lite/portable_type_to_tflitetype.h"
// #include "tensorflow/lite/profiling/root_profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h"
// #include "tensorflow/lite/stderr_reporter.h"
// #include "tensorflow/lite/string_type.h"
// #include "tensorflow/lite/type_to_tflitetype.h"
// Targeting ../InterpreterTest.java


// Targeting ../InterpreterUtils.java


// Targeting ../TestDelegation.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../InterpreterWrapper.java

  // Class for friend declarations.

// Targeting ../ModelBuilder.java


  // namespace model_builder
// #endif  // DOXYGEN_SKIP

/** An interpreter for a graph of nodes that input and output from tensors.
 *  Each node of the graph processes a set of input tensors and produces a
 *  set of output Tensors. All inputs/output tensors are referenced by index.
 * 
 *  Usage:
 * 
 *  <pre><code>
 *  // Create model from file. Note that the model instance must outlive the
 *  // interpreter instance.
 *  auto model = tflite::FlatBufferModel::BuildFromFile(...);
 *  if (model == nullptr) {
 *    // Return error.
 *  }
 *  // Create an Interpreter with an InterpreterBuilder.
 *  std::unique_ptr<tflite::Interpreter> interpreter;
 *  tflite::ops::builtin::BuiltinOpResolver resolver;
 *  if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {
 *    // Return failure.
 *  }
 *  if (interpreter->AllocateTensors() != kTfLiteOk) {
 *    // Return failure.
 *  }
 * 
 *  auto input = interpreter->typed_tensor<float>(0);
 *  for (int i = 0; i < input_size; i++) {
 *    input[i] = ...; */
//  }
/** interpreter->Invoke();
/** </code></pre>
/**
/** Note: For nearly all practical use cases, one should not directly construct
/** an Interpreter object, but rather use the InterpreterBuilder.
/**
/** \warning This class is *not* thread-safe. The client is responsible for
/** ensuring serialized interaction to avoid data races and undefined behavior. */
// Targeting ../Interpreter.java



  // namespace impl

  // namespace tflite
// #endif  // TENSORFLOW_LITE_CORE_INTERPRETER_H_


// Parsed from tensorflow/lite/model_builder.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MODEL_BUILDER_H_
// #define TENSORFLOW_LITE_MODEL_BUILDER_H_

/** For documentation, see third_party/tensorflow/lite/core/model_builder.h. */

// #include "tensorflow/lite/core/model_builder.h"
  // namespace tflite

// #endif  // TENSORFLOW_LITE_MODEL_BUILDER_H_


// Parsed from tensorflow/lite/core/model_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Deserialization infrastructure for tflite. Provides functionality
/** to go from a serialized tflite model in flatbuffer format to an
/** in-memory representation of the model.
/**
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/model_builder.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */

// #ifndef TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_
// #define TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_

// #include <stddef.h>

// #include <memory>

// #include "tensorflow/compiler/mlir/lite/core/model_builder_base.h"  // IWYU pragma: export
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/stderr_reporter.h"
// Targeting ../FlatBufferModel.java



  // namespace impl

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_MODEL_BUILDER_H_


// Parsed from tensorflow/lite/interpreter_builder.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_INTERPRETER_BUILDER_H_
// #define TENSORFLOW_LITE_INTERPRETER_BUILDER_H_

/** For documentation, see third_party/tensorflow/lite/core/interpreter_builder.h. */

// #include "tensorflow/lite/core/interpreter_builder.h"  // IWYU pragma: export
  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_BUILDER_H_


// Parsed from tensorflow/lite/core/interpreter_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Provides functionality to construct an interpreter for a model.
/**
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include
/** "third_party/tensorflow/lite/interpreter_builder.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_
// #define TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_

// #include <map>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #include "flatbuffers/flatbuffers.h"
// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/c/common.h"
// #include "tensorflow/lite/core/interpreter.h"
// #include "tensorflow/lite/core/model_builder.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/mutable_op_resolver.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h"
// #include "tensorflow/lite/profiling/telemetry/profiler.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/stderr_reporter.h"

/** Build an interpreter capable of interpreting {@code model}.
 * 
 *  * {@code model}: A model whose lifetime must be at least as long as any
 *    interpreter(s) created by the builder. In principle multiple interpreters
 *    can be made from a single model.
 *  * {@code op_resolver}: An instance that implements the {@code OpResolver} interface,
 *    which maps custom op names and builtin op codes to op registrations. The
 *    lifetime of the provided {@code op_resolver} object must be at least as long as
 *    the {@code InterpreterBuilder}; unlike {@code model} and {@code error_reporter}, the
 *    {@code op_resolver} does not need to exist for the duration of any created
 *    {@code Interpreter} objects.
 *  * {@code error_reporter}: a functor that is called to report errors that handles
 *    printf var arg semantics. The lifetime of the {@code error_reporter} object must
 *    be greater than or equal to the {@code Interpreter} created by {@code operator()}.
 *  * {@code options_experimental}: Options that can change behavior of interpreter.
 *    WARNING: this parameter is an experimental API and is subject to change.
 * 
 *  Returns a kTfLiteOk when successful and sets interpreter to a valid
 *  Interpreter. Note: The user must ensure the lifetime of the model (and error
 *  reporter, if provided) is at least as long as interpreter's lifetime, and
 *  a single model instance may safely be used with multiple interpreters. */
// Targeting ../InterpreterBuilder.java



  // namespace impl

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_INTERPRETER_BUILDER_H_


// Parsed from tensorflow/lite/model.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_
// #define TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_

/** For documentation, see third_party/tensorflow/lite/core/model.h. */

// #include "tensorflow/lite/interpreter_builder.h"
// #include "tensorflow/lite/model_builder.h"

// #endif  // TENSORFLOW_LITE_CORE_SHIMS_CC_MODEL_H_


// Parsed from tensorflow/lite/kernels/register.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_KERNELS_REGISTER_H_
// #define TENSORFLOW_LITE_KERNELS_REGISTER_H_

/** For documentation, see third_party/tensorflow/lite/core/kernels/register.h */

// #include "tensorflow/lite/core/kernels/register.h"  // IWYU pragma: export

  // namespace builtin
  // namespace ops
  // namespace tflite

// #endif  // TENSORFLOW_LITE_KERNELS_REGISTER_H_


// Parsed from tensorflow/lite/core/kernels/register.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** WARNING: Users of TensorFlow Lite should not include this file directly,
/** but should instead include "third_party/tensorflow/lite/kernels/register.h".
/** Only the TensorFlow Lite implementation itself should include this
/** file directly. */
// #ifndef TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_
// #define TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_

// #include "tensorflow/lite/core/model.h"  // Legacy.
// #include "tensorflow/lite/mutable_op_resolver.h"
// Targeting ../BuiltinOpResolver.java


// Targeting ../BuiltinOpResolverWithXNNPACK.java


// Targeting ../BuiltinOpResolverWithoutDefaultDelegates.java



  // namespace builtin
  // namespace ops
  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_KERNELS_REGISTER_H_


// Parsed from tensorflow/lite/optional_debug_tools.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/**
/** Optional debugging functionality.
/** For small sized binaries, these are not needed. */
// #ifndef TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_
// #define TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_

// #include <cstdint>
// #include <vector>

// #include "tensorflow/lite/core/interpreter.h"
// #include "tensorflow/lite/core/subgraph.h"
// Returns the name of the allocation type.
@Namespace("tflite") public static native @Cast("const char*") BytePointer AllocTypeName(@Cast("TfLiteAllocationType") int type);

// Prints a dump of what tensors and what nodes are in the interpreter.
@Namespace("tflite") public static native void PrintInterpreterState(@Const Interpreter interpreter,
                           int tensor_name_display_length/*=25*/,
                           int tensor_type_display_length/*=15*/,
                           int alloc_type_display_length/*=18*/);
@Namespace("tflite") public static native void PrintInterpreterState(@Const Interpreter interpreter);
// Targeting ../SubgraphDelegationMetadata.java



// Returns the metadata of the delegation of the subgraph.
@Namespace("tflite") public static native @ByVal SubgraphDelegationMetadata GetNodeDelegationMetadata(@StdMove Subgraph subgraph);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_


// Parsed from tensorflow/lite/profiling/telemetry/c/profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_

// #include <stdint.h>

// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting.h"

// #ifdef __cplusplus
// Targeting ../TfLiteTelemetryProfilerStruct.java



// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_C_PROFILER_H_


// Parsed from tensorflow/lite/profiling/telemetry/c/telemetry_setting.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_

// #include <stddef.h>
// #include <stdint.h>

// #include "tensorflow/lite/core/c/common.h"

// #ifdef __cplusplus
// Targeting ../TfLiteTelemetrySettings.java


// Targeting ../TfLiteTelemetryConversionMetadata.java



public static native @Const IntPointer TfLiteTelemetryConversionMetadataGetModelOptimizationModes(
    @Const TfLiteTelemetryConversionMetadata metadata);

public static native @Cast("size_t") long TfLiteTelemetryConversionMetadataGetNumModelOptimizationModes(
    @Const TfLiteTelemetryConversionMetadata metadata);
// Targeting ../TfLiteTelemetryInterpreterSettings.java



public static native @Const TfLiteTelemetryConversionMetadata TfLiteTelemetryInterpreterSettingsGetConversionMetadata(
    @Const TfLiteTelemetryInterpreterSettings settings);
// Targeting ../TfLiteTelemetrySubgraphInfo.java



public static native @Cast("size_t") long TfLiteTelemetryInterpreterSettingsGetNumSubgraphInfo(
    @Const TfLiteTelemetryInterpreterSettings settings);

public static native @Const TfLiteTelemetrySubgraphInfo TfLiteTelemetryInterpreterSettingsGetSubgraphInfo(
    @Const TfLiteTelemetryInterpreterSettings settings);

public static native @Cast("size_t") long TfLiteTelemetrySubgraphInfoGetNumQuantizations(
    TfLiteTelemetrySubgraphInfo subgraph_info);

public static native @Const TfLiteQuantization TfLiteTelemetrySubgraphInfoGetQuantizations(
    TfLiteTelemetrySubgraphInfo subgraph_info);
// Targeting ../TfLiteTelemetryGpuDelegateSettings.java



public static native @Cast("size_t") long TfLiteTelemetryGpuDelegateSettingsGetNumNodesDelegated(
    @Const TfLiteTelemetryGpuDelegateSettings settings);

public static native int TfLiteTelemetryGpuDelegateSettingsGetBackend(
    @Const TfLiteTelemetryGpuDelegateSettings settings);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_C_TELEMETRY_SETTING_H_


// Parsed from tensorflow/lite/profiling/telemetry/telemetry_status.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_

// #include <cstdint>

// #include "tensorflow/lite/core/c/c_api_types.h"

// The source of a telemetry event. Enum values intentionally follow proto
// guidelines as they are used for Clearcut logging.
/** enum class tflite::telemetry::TelemetrySource */
public static final int
  UNKNOWN = 0,
  TFLITE_INTERPRETER = 1,

  // For external delegate.
  // External delegate should identify themselves in telemetry event names by
  // prefixing the delegame name to it.
  TFLITE_CUSTOM_DELEGATE = 2,

  TFLITE_GPU = 3,
  TFLITE_NNAPI = 4,
  TFLITE_HEXAGON = 5,
  TFLITE_XNNPACK = 6,
  TFLITE_COREML = 7;
// Targeting ../TelemetryStatusCode.java



  // namespace tflite::telemetry

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_TELEMETRY_STATUS_H_


// Parsed from tensorflow/lite/profiling/telemetry/profiler.h

/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_
// #define TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_

// #include <cstdint>

// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/profiler.h"
// #include "tensorflow/lite/profiling/telemetry/c/telemetry_setting.h"
// #include "tensorflow/lite/profiling/telemetry/telemetry_status.h"
// Targeting ../TelemetryProfiler.java



// Creates a concrete TelemetryProfiler that wraps the
// `TfLiteTelemetryProfilerStruct` C API.
@Namespace("tflite::telemetry") public static native TelemetryProfiler MakeTfLiteTelemetryProfiler(
    TfLiteTelemetryProfilerStruct profiler);

  // namespace tflite::telemetry

// #endif  // TENSORFLOW_LITE_PROFILING_TELEMETRY_PROFILER_H_


}

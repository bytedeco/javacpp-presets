// Targeted by JavaCPP version 1.5.7: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflowlite.global;

import org.bytedeco.tensorflowlite.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class tensorflowlite extends org.bytedeco.tensorflowlite.presets.tensorflowlite {
    static { Loader.load(); }

// Targeting ../StringIntMap.java


// Targeting ../StringStringMap.java


// Targeting ../TfLiteDelegatePtrVector.java


// Targeting ../StringVector.java


// Targeting ../SubgraphVector.java


// Targeting ../RegistrationNodePairVector.java


// Targeting ../RegistrationNodePair.java


// Targeting ../IntResourceBaseMap.java


// Parsed from tensorflow/lite/builtin_ops.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// #ifndef TENSORFLOW_LITE_BUILTIN_OPS_H_
// #define TENSORFLOW_LITE_BUILTIN_OPS_H_

// DO NOT EDIT MANUALLY: This file is automatically generated by
// `schema/builtin_ops_header/generator.cc`.

// #ifdef __cplusplus
// #endif  // __cplusplus

// The enum for builtin operators.
// Note: CUSTOM, DELEGATE, and PLACEHOLDER_FOR_GREATER_OP_CODES are 3 special
// ops which are not real built-in ops.
/** enum TfLiteBuiltinOperator */
public static final int
  kTfLiteBuiltinAdd = 0,
  kTfLiteBuiltinAveragePool2d = 1,
  kTfLiteBuiltinConcatenation = 2,
  kTfLiteBuiltinConv2d = 3,
  kTfLiteBuiltinDepthwiseConv2d = 4,
  kTfLiteBuiltinDepthToSpace = 5,
  kTfLiteBuiltinDequantize = 6,
  kTfLiteBuiltinEmbeddingLookup = 7,
  kTfLiteBuiltinFloor = 8,
  kTfLiteBuiltinFullyConnected = 9,
  kTfLiteBuiltinHashtableLookup = 10,
  kTfLiteBuiltinL2Normalization = 11,
  kTfLiteBuiltinL2Pool2d = 12,
  kTfLiteBuiltinLocalResponseNormalization = 13,
  kTfLiteBuiltinLogistic = 14,
  kTfLiteBuiltinLshProjection = 15,
  kTfLiteBuiltinLstm = 16,
  kTfLiteBuiltinMaxPool2d = 17,
  kTfLiteBuiltinMul = 18,
  kTfLiteBuiltinRelu = 19,
  kTfLiteBuiltinReluN1To1 = 20,
  kTfLiteBuiltinRelu6 = 21,
  kTfLiteBuiltinReshape = 22,
  kTfLiteBuiltinResizeBilinear = 23,
  kTfLiteBuiltinRnn = 24,
  kTfLiteBuiltinSoftmax = 25,
  kTfLiteBuiltinSpaceToDepth = 26,
  kTfLiteBuiltinSvdf = 27,
  kTfLiteBuiltinTanh = 28,
  kTfLiteBuiltinConcatEmbeddings = 29,
  kTfLiteBuiltinSkipGram = 30,
  kTfLiteBuiltinCall = 31,
  kTfLiteBuiltinCustom = 32,
  kTfLiteBuiltinEmbeddingLookupSparse = 33,
  kTfLiteBuiltinPad = 34,
  kTfLiteBuiltinUnidirectionalSequenceRnn = 35,
  kTfLiteBuiltinGather = 36,
  kTfLiteBuiltinBatchToSpaceNd = 37,
  kTfLiteBuiltinSpaceToBatchNd = 38,
  kTfLiteBuiltinTranspose = 39,
  kTfLiteBuiltinMean = 40,
  kTfLiteBuiltinSub = 41,
  kTfLiteBuiltinDiv = 42,
  kTfLiteBuiltinSqueeze = 43,
  kTfLiteBuiltinUnidirectionalSequenceLstm = 44,
  kTfLiteBuiltinStridedSlice = 45,
  kTfLiteBuiltinBidirectionalSequenceRnn = 46,
  kTfLiteBuiltinExp = 47,
  kTfLiteBuiltinTopkV2 = 48,
  kTfLiteBuiltinSplit = 49,
  kTfLiteBuiltinLogSoftmax = 50,
  kTfLiteBuiltinDelegate = 51,
  kTfLiteBuiltinBidirectionalSequenceLstm = 52,
  kTfLiteBuiltinCast = 53,
  kTfLiteBuiltinPrelu = 54,
  kTfLiteBuiltinMaximum = 55,
  kTfLiteBuiltinArgMax = 56,
  kTfLiteBuiltinMinimum = 57,
  kTfLiteBuiltinLess = 58,
  kTfLiteBuiltinNeg = 59,
  kTfLiteBuiltinPadv2 = 60,
  kTfLiteBuiltinGreater = 61,
  kTfLiteBuiltinGreaterEqual = 62,
  kTfLiteBuiltinLessEqual = 63,
  kTfLiteBuiltinSelect = 64,
  kTfLiteBuiltinSlice = 65,
  kTfLiteBuiltinSin = 66,
  kTfLiteBuiltinTransposeConv = 67,
  kTfLiteBuiltinSparseToDense = 68,
  kTfLiteBuiltinTile = 69,
  kTfLiteBuiltinExpandDims = 70,
  kTfLiteBuiltinEqual = 71,
  kTfLiteBuiltinNotEqual = 72,
  kTfLiteBuiltinLog = 73,
  kTfLiteBuiltinSum = 74,
  kTfLiteBuiltinSqrt = 75,
  kTfLiteBuiltinRsqrt = 76,
  kTfLiteBuiltinShape = 77,
  kTfLiteBuiltinPow = 78,
  kTfLiteBuiltinArgMin = 79,
  kTfLiteBuiltinFakeQuant = 80,
  kTfLiteBuiltinReduceProd = 81,
  kTfLiteBuiltinReduceMax = 82,
  kTfLiteBuiltinPack = 83,
  kTfLiteBuiltinLogicalOr = 84,
  kTfLiteBuiltinOneHot = 85,
  kTfLiteBuiltinLogicalAnd = 86,
  kTfLiteBuiltinLogicalNot = 87,
  kTfLiteBuiltinUnpack = 88,
  kTfLiteBuiltinReduceMin = 89,
  kTfLiteBuiltinFloorDiv = 90,
  kTfLiteBuiltinReduceAny = 91,
  kTfLiteBuiltinSquare = 92,
  kTfLiteBuiltinZerosLike = 93,
  kTfLiteBuiltinFill = 94,
  kTfLiteBuiltinFloorMod = 95,
  kTfLiteBuiltinRange = 96,
  kTfLiteBuiltinResizeNearestNeighbor = 97,
  kTfLiteBuiltinLeakyRelu = 98,
  kTfLiteBuiltinSquaredDifference = 99,
  kTfLiteBuiltinMirrorPad = 100,
  kTfLiteBuiltinAbs = 101,
  kTfLiteBuiltinSplitV = 102,
  kTfLiteBuiltinUnique = 103,
  kTfLiteBuiltinCeil = 104,
  kTfLiteBuiltinReverseV2 = 105,
  kTfLiteBuiltinAddN = 106,
  kTfLiteBuiltinGatherNd = 107,
  kTfLiteBuiltinCos = 108,
  kTfLiteBuiltinWhere = 109,
  kTfLiteBuiltinRank = 110,
  kTfLiteBuiltinElu = 111,
  kTfLiteBuiltinReverseSequence = 112,
  kTfLiteBuiltinMatrixDiag = 113,
  kTfLiteBuiltinQuantize = 114,
  kTfLiteBuiltinMatrixSetDiag = 115,
  kTfLiteBuiltinRound = 116,
  kTfLiteBuiltinHardSwish = 117,
  kTfLiteBuiltinIf = 118,
  kTfLiteBuiltinWhile = 119,
  kTfLiteBuiltinNonMaxSuppressionV4 = 120,
  kTfLiteBuiltinNonMaxSuppressionV5 = 121,
  kTfLiteBuiltinScatterNd = 122,
  kTfLiteBuiltinSelectV2 = 123,
  kTfLiteBuiltinDensify = 124,
  kTfLiteBuiltinSegmentSum = 125,
  kTfLiteBuiltinBatchMatmul = 126,
  kTfLiteBuiltinPlaceholderForGreaterOpCodes = 127,
  kTfLiteBuiltinCumsum = 128,
  kTfLiteBuiltinCallOnce = 129,
  kTfLiteBuiltinBroadcastTo = 130,
  kTfLiteBuiltinRfft2d = 131,
  kTfLiteBuiltinConv3d = 132,
  kTfLiteBuiltinImag = 133,
  kTfLiteBuiltinReal = 134,
  kTfLiteBuiltinComplexAbs = 135,
  kTfLiteBuiltinHashtable = 136,
  kTfLiteBuiltinHashtableFind = 137,
  kTfLiteBuiltinHashtableImport = 138,
  kTfLiteBuiltinHashtableSize = 139,
  kTfLiteBuiltinReduceAll = 140,
  kTfLiteBuiltinConv3dTranspose = 141,
  kTfLiteBuiltinVarHandle = 142,
  kTfLiteBuiltinReadVariable = 143,
  kTfLiteBuiltinAssignVariable = 144,
  kTfLiteBuiltinBroadcastArgs = 145,
  kTfLiteBuiltinRandomStandardNormal = 146,
  kTfLiteBuiltinBucketize = 147,
  kTfLiteBuiltinRandomUniform = 148,
  kTfLiteBuiltinMultinomial = 149,
  kTfLiteBuiltinGelu = 150;

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_BUILTIN_OPS_H_


// Parsed from tensorflow/lite/c/c_api_types.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file declares types used by the pure C inference API defined in c_api.h,
// some of which are also used in the C++ and C kernel and interpreter APIs.

// #ifndef TENSORFLOW_LITE_C_C_API_TYPES_H_
// #define TENSORFLOW_LITE_C_C_API_TYPES_H_

// #include <stdint.h>

// #ifdef __cplusplus
// #endif

// Define TFL_CAPI_EXPORT macro to export a function properly with a shared
// library.
// #ifdef SWIG
// #define TFL_CAPI_EXPORT
// #elif defined(TFL_STATIC_LIBRARY_BUILD)
// #define TFL_CAPI_EXPORT
// #else  // not definded TFL_STATIC_LIBRARY_BUILD
// #if defined(_WIN32)
// #ifdef TFL_COMPILE_LIBRARY
// #define TFL_CAPI_EXPORT __declspec(dllexport)
// #else
// #define TFL_CAPI_EXPORT __declspec(dllimport)
// #endif  // TFL_COMPILE_LIBRARY
// #else
// #define TFL_CAPI_EXPORT __attribute__((visibility("default")))
// #endif  // _WIN32
// #endif  // SWIG

// Note that new error status values may be added in future in order to
// indicate more fine-grained internal states, therefore, applications should
// not rely on status values being members of the enum.
/** enum TfLiteStatus */
public static final int
  kTfLiteOk = 0,

  // Generally referring to an error in the runtime (i.e. interpreter)
  kTfLiteError = 1,

  // Generally referring to an error from a TfLiteDelegate itself.
  kTfLiteDelegateError = 2,

  // Generally referring to an error in applying a delegate due to
  // incompatibility between runtime and delegate, e.g., this error is returned
  // when trying to apply a TF Lite delegate onto a model graph that's already
  // immutable.
  kTfLiteApplicationError = 3,

  // Generally referring to serialized delegate data not being found.
  // See tflite::delegates::Serialization.
  kTfLiteDelegateDataNotFound = 4,

  // Generally referring to data-writing issues in delegate serialization.
  // See tflite::delegates::Serialization.
  kTfLiteDelegateDataWriteError = 5,

  // Generally referring to data-reading issues in delegate serialization.
  // See tflite::delegates::Serialization.
  kTfLiteDelegateDataReadError = 6,

  // Generally referring to issues when the TF Lite model has ops that cannot be
  // resolved at runtime. This could happen when the specific op is not
  // registered or built with the TF Lite framework.
  kTfLiteUnresolvedOps = 7;

// Types supported by tensor
/** enum TfLiteType */
public static final int
  kTfLiteNoType = 0,
  kTfLiteFloat32 = 1,
  kTfLiteInt32 = 2,
  kTfLiteUInt8 = 3,
  kTfLiteInt64 = 4,
  kTfLiteString = 5,
  kTfLiteBool = 6,
  kTfLiteInt16 = 7,
  kTfLiteComplex64 = 8,
  kTfLiteInt8 = 9,
  kTfLiteFloat16 = 10,
  kTfLiteFloat64 = 11,
  kTfLiteComplex128 = 12,
  kTfLiteUInt64 = 13,
  kTfLiteResource = 14,
  kTfLiteVariant = 15,
  kTfLiteUInt32 = 16;
// Targeting ../TfLiteQuantizationParams.java



// #ifdef __cplusplus  // extern C
// #endif
// #endif  // TENSORFLOW_LITE_C_C_API_TYPES_H_


// Parsed from tensorflow/lite/c/c_api.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_H_
// #define TENSORFLOW_LITE_C_C_API_H_

// #include <stdarg.h>
// #include <stdint.h>
// #include <stdlib.h>


///
///
///
///
///
///
///
///
// #include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export

// --------------------------------------------------------------------------
/** C API for TensorFlow Lite.
 * 
 *  The API leans towards simplicity and uniformity instead of convenience, as
 *  most usage will be by language-specific wrappers. It provides largely the
 *  same set of functionality as that of the C++ TensorFlow Lite {@code Interpreter}
 *  API, but is useful for shared libraries where having a stable ABI boundary
 *  is important.
 * 
 *  Conventions:
 *  * We use the prefix TfLite for everything in the API.
 *  * size_t is used to represent byte sizes of objects that are
 *    materialized in the address space of the calling process.
 *  * int is used as an index into arrays.
 * 
 *  Usage:
 *  <pre><code>
 *  // Create the model and interpreter options.
 *  TfLiteModel* model = TfLiteModelCreateFromFile("/path/to/model.tflite");
 *  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
 *  TfLiteInterpreterOptionsSetNumThreads(options, 2);
 * 
 *  // Create the interpreter.
 *  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
 * 
 *  // Allocate tensors and populate the input tensor data.
 *  TfLiteInterpreterAllocateTensors(interpreter);
 *  TfLiteTensor* input_tensor =
 *      TfLiteInterpreterGetInputTensor(interpreter, 0);
 *  TfLiteTensorCopyFromBuffer(input_tensor, input.data(),
 *                             input.size() * sizeof(float));
 * 
 *  // Execute inference.
 *  TfLiteInterpreterInvoke(interpreter);
 * 
 *  // Extract the output tensor data.
 *  const TfLiteTensor* output_tensor = */
//      TfLiteInterpreterGetOutputTensor(interpreter, 0);
/** TfLiteTensorCopyToBuffer(output_tensor, output.data(),
/**                          output.size() * sizeof(float));
/**
/** // Dispose of the model and interpreter objects.
/** TfLiteInterpreterDelete(interpreter);
/** TfLiteInterpreterOptionsDelete(options);
/** TfLiteModelDelete(model); */

// #ifdef __cplusplus
// Targeting ../TfLiteModel.java


// Targeting ../TfLiteInterpreterOptions.java



// Allows delegation of nodes to alternative backends.
// Targeting ../TfLiteInterpreter.java



// A tensor in the interpreter system which is a wrapper around a buffer of
// data including a dimensionality (or NULL if not currently defined).

// --------------------------------------------------------------------------
// TfLiteVersion returns a string describing version information of the
// TensorFlow Lite library. TensorFlow Lite uses semantic versioning.
public static native @Cast("const char*") BytePointer TfLiteVersion();

// Returns a model from the provided buffer, or null on failure.
//
// NOTE: The caller retains ownership of the `model_data` and should ensure that
// the lifetime of the `model_data` must be at least as long as the lifetime
// of the `TfLiteModel`.
public static native TfLiteModel TfLiteModelCreate(@Const Pointer model_data,
                                                      @Cast("size_t") long model_size);

// Returns a model from the provided file, or null on failure.
public static native TfLiteModel TfLiteModelCreateFromFile(
    @Cast("const char*") BytePointer model_path);
public static native TfLiteModel TfLiteModelCreateFromFile(
    String model_path);

// Destroys the model instance.
public static native void TfLiteModelDelete(TfLiteModel model);

// Returns a new interpreter options instances.
public static native TfLiteInterpreterOptions TfLiteInterpreterOptionsCreate();

// Destroys the interpreter options instance.
public static native void TfLiteInterpreterOptionsDelete(
    TfLiteInterpreterOptions options);

// Sets the number of CPU threads to use for the interpreter.
public static native void TfLiteInterpreterOptionsSetNumThreads(
    TfLiteInterpreterOptions options, int num_threads);

// Adds a delegate to be applied during `TfLiteInterpreter` creation.
//
// If delegate application fails, interpreter creation will also fail with an
// associated error logged.
//
// NOTE: The caller retains ownership of the delegate and should ensure that it
// remains valid for the duration of any created interpreter's lifetime.
public static native void TfLiteInterpreterOptionsAddDelegate(
    TfLiteInterpreterOptions options, TfLiteDelegate delegate);
// Targeting ../Reporter_Pointer_BytePointer_Pointer.java


public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_BytePointer_Pointer reporter,
    Pointer user_data);
// Targeting ../Reporter_Pointer_String_Pointer.java


public static native void TfLiteInterpreterOptionsSetErrorReporter(
    TfLiteInterpreterOptions options,
    Reporter_Pointer_String_Pointer reporter,
    Pointer user_data);

// Returns a new interpreter using the provided model and options, or null on
// failure.
//
// * `model` must be a valid model instance. The caller retains ownership of the
//   object, and can destroy it immediately after creating the interpreter; the
//   interpreter will maintain its own reference to the underlying model data.
// * `optional_options` may be null. The caller retains ownership of the object,
//   and can safely destroy it immediately after creating the interpreter.
//
// NOTE: The client *must* explicitly allocate tensors before attempting to
// access input tensor data or invoke the interpreter.
public static native TfLiteInterpreter TfLiteInterpreterCreate(
    @Const TfLiteModel model, @Const TfLiteInterpreterOptions optional_options);

// Destroys the interpreter.
public static native void TfLiteInterpreterDelete(
    TfLiteInterpreter interpreter);

// Returns the number of input tensors associated with the model.
public static native int TfLiteInterpreterGetInputTensorCount(
    @Const TfLiteInterpreter interpreter);

// Returns the tensor associated with the input index.
// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
public static native TfLiteTensor TfLiteInterpreterGetInputTensor(
    @Const TfLiteInterpreter interpreter, int input_index);

// Resizes the specified input tensor.
//
// NOTE: After a resize, the client *must* explicitly allocate tensors before
// attempting to access the resized tensor data or invoke the interpreter.
//
// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
//
// This function makes a copy of the input dimensions, so the client can safely
// deallocate `input_dims` immediately after this function returns.
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntPointer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const IntBuffer input_dims,
    int input_dims_size);
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResizeInputTensor(
    TfLiteInterpreter interpreter, int input_index, @Const int[] input_dims,
    int input_dims_size);

// Updates allocations for all tensors, resizing dependent tensors using the
// specified input tensor dimensionality.
//
// This is a relatively expensive operation, and need only be called after
// creating the graph and/or resizing any inputs.
public static native @Cast("TfLiteStatus") int TfLiteInterpreterAllocateTensors(
    TfLiteInterpreter interpreter);

// Runs inference for the loaded graph.
//
// Before calling this function, the caller should first invoke
// TfLiteInterpreterAllocateTensors() and should also set the values for the
// input tensors.  After successfully calling this function, the values for the
// output tensors will be set.
//
// NOTE: It is possible that the interpreter is not in a ready state to
// evaluate (e.g., if AllocateTensors() hasn't been called, or if a
// ResizeInputTensor() has been performed without a subsequent call to
// AllocateTensors()).
//
//   If the (experimental!) delegate fallback option was enabled in the
//   interpreter options, then the interpreter will automatically fall back to
//   not using any delegates if execution with delegates fails. For details, see
//   TfLiteInterpreterOptionsSetEnableDelegateFallback in c_api_experimental.h.
//
// Returns one of the following status codes:
//  - kTfLiteOk: Success. Output is valid.
//  - kTfLiteDelegateError: Execution with delegates failed, due to a problem
//    with the delegate(s). If fallback was not enabled, output is invalid.
//    If fallback was enabled, this return value indicates that fallback
//    succeeded, the output is valid, and all delegates previously applied to
//    the interpreter have been undone.
//  - kTfLiteApplicationError: Same as for kTfLiteDelegateError, except that
//    the problem was not with the delegate itself, but rather was
//    due to an incompatibility between the delegate(s) and the
//    interpreter or model.
//  - kTfLiteError: Unexpected/runtime failure. Output is invalid.

public static native @Cast("TfLiteStatus") int TfLiteInterpreterInvoke(
    TfLiteInterpreter interpreter);

// Returns the number of output tensors associated with the model.
public static native int TfLiteInterpreterGetOutputTensorCount(
    @Const TfLiteInterpreter interpreter);

// Returns the tensor associated with the output index.
// REQUIRES: 0 <= output_index < TfLiteInterpreterGetOutputTensorCount(tensor)
//
// NOTE: The shape and underlying data buffer for output tensors may be not
// be available until after the output tensor has been both sized and allocated.
// In general, best practice is to interact with the output tensor *after*
// calling TfLiteInterpreterInvoke().
public static native @Const TfLiteTensor TfLiteInterpreterGetOutputTensor(
    @Const TfLiteInterpreter interpreter, int output_index);

// --------------------------------------------------------------------------
// TfLiteTensor wraps data associated with a graph tensor.
//
// Note that, while the TfLiteTensor struct is not currently opaque, and its
// fields can be accessed directly, these methods are still convenient for
// language bindings. In the future the tensor struct will likely be made opaque
// in the public API.

// Returns the type of a tensor element.
public static native @Cast("TfLiteType") int TfLiteTensorType(@Const TfLiteTensor tensor);

// Returns the number of dimensions that the tensor has.
public static native int TfLiteTensorNumDims(@Const TfLiteTensor tensor);

// Returns the length of the tensor in the "dim_index" dimension.
// REQUIRES: 0 <= dim_index < TFLiteTensorNumDims(tensor)
public static native int TfLiteTensorDim(@Const TfLiteTensor tensor,
                                               int dim_index);

// Returns the size of the underlying data in bytes.
public static native @Cast("size_t") long TfLiteTensorByteSize(@Const TfLiteTensor tensor);

// Returns a pointer to the underlying data buffer.
//
// NOTE: The result may be null if tensors have not yet been allocated, e.g.,
// if the Tensor has just been created or resized and `TfLiteAllocateTensors()`
// has yet to be called, or if the output tensor is dynamically sized and the
// interpreter hasn't been invoked.
public static native Pointer TfLiteTensorData(@Const TfLiteTensor tensor);

// Returns the (null-terminated) name of the tensor.
public static native @Cast("const char*") BytePointer TfLiteTensorName(@Const TfLiteTensor tensor);

// Returns the parameters for asymmetric quantization. The quantization
// parameters are only valid when the tensor type is `kTfLiteUInt8` and the
// `scale != 0`. Quantized values can be converted back to float using:
//    real_value = scale * (quantized_value - zero_point);
public static native @ByVal TfLiteQuantizationParams TfLiteTensorQuantizationParams(
    @Const TfLiteTensor tensor);

// Copies from the provided input buffer into the tensor's buffer.
// REQUIRES: input_data_size == TfLiteTensorByteSize(tensor)
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyFromBuffer(
    TfLiteTensor tensor, @Const Pointer input_data, @Cast("size_t") long input_data_size);

// Copies to the provided output buffer from the tensor's buffer.
// REQUIRES: output_data_size == TfLiteTensorByteSize(tensor)
public static native @Cast("TfLiteStatus") int TfLiteTensorCopyToBuffer(
    @Const TfLiteTensor output_tensor, Pointer output_data,
    @Cast("size_t") long output_data_size);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_C_C_API_H_


// Parsed from tensorflow/lite/c/c_api_experimental.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_
// #define TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_

// #include "tensorflow/lite/builtin_ops.h"
// #include "tensorflow/lite/c/c_api.h"
// #include "tensorflow/lite/c/common.h"

// #ifdef __cplusplus
// #endif  // __cplusplus

/** Resets all variable tensors to zero.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterResetVariableTensors(
    TfLiteInterpreter interpreter);

/** Adds an op registration for a builtin operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of the interpreter's lifetime. A common
 *  practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} on the same options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
///
public static native void TfLiteInterpreterOptionsAddBuiltinOp(
    TfLiteInterpreterOptions options, @Cast("TfLiteBuiltinOperator") int op,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);

/** Adds an op registration for a custom operator.
 * 
 *  Op registrations are used to map ops referenced in the flatbuffer model
 *  to executable function pointers ({@code TfLiteRegistration}s).
 * 
 *  NOTE: The interpreter will make a shallow copy of {@code registration} internally,
 *  so the caller should ensure that its contents (function pointers, etc...)
 *  remain valid for the duration of any created interpreter's lifetime. A
 *  common practice is making the provided {@code TfLiteRegistration} instance static.
 * 
 *  The lifetime of the string pointed to by {@code name} must be at least as long
 *  as the lifetime of the {@code TfLiteInterpreterOptions}.
 * 
 *  Code that uses this function should NOT call
 *  {@code TfLiteInterpreterOptionsSetOpResolver} on the same options object.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
///
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, @Cast("const char*") BytePointer name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
public static native void TfLiteInterpreterOptionsAddCustomOp(
    TfLiteInterpreterOptions options, String name,
    @Const TfLiteRegistration registration, int min_version,
    int max_version);
// Targeting ../Find_builtin_op_Pointer_int_int.java


// Targeting ../Find_custom_op_Pointer_BytePointer_int.java



///
///
///
public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_BytePointer_int find_custom_op,
    Pointer op_resolver_user_data);
// Targeting ../Find_custom_op_Pointer_String_int.java


public static native void TfLiteInterpreterOptionsSetOpResolver(
    TfLiteInterpreterOptions options,
    Find_builtin_op_Pointer_int_int find_builtin_op,
    Find_custom_op_Pointer_String_int find_custom_op,
    Pointer op_resolver_user_data);

/** Returns a new interpreter using the provided model and options, or null on
 *  failure, where the model uses only the operators explicitly added to the
 *  options.  This is the same as {@code TFLiteInterpreterCreate} from {@code c_api.h},
 *  except that the only operators that are supported are the ones registered
 *  in {@code options} via calls to {@code TfLiteInterpreterOptionsSetOpResolver},
 *  {@code TfLiteInterpreterOptionsAddBuiltinOp}, and/or
 *  {@code TfLiteInterpreterOptionsAddCustomOp}.
 * 
 *  * {@code model} must be a valid model instance. The caller retains ownership of
 *    the object, and can destroy it immediately after creating the interpreter;
 *    the interpreter will maintain its own reference to the underlying model
 *    data.
 *  * {@code options} should not be null. The caller retains ownership of the object,
 *    and can safely destroy it immediately after creating the interpreter.
 * 
 *  NOTE: The client *must* explicitly allocate tensors before attempting to
 *  access input tensor data or invoke the interpreter.
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native TfLiteInterpreter TfLiteInterpreterCreateWithSelectedOps(@Const TfLiteModel model,
                                       @Const TfLiteInterpreterOptions options);

/** Enable or disable the NN API delegate for the interpreter (true to enable).
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
///
///
public static native void TfLiteInterpreterOptionsSetUseNNAPI(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

/** Enable or disable CPU fallback for the interpreter (true to enable).
 *  If enabled, TfLiteInterpreterInvoke will do automatic fallback from
 *  executing with delegate(s) to regular execution without delegates
 *  (i.e. on CPU).
 * 
 *  Allowing the fallback is suitable only if both of the following hold:
 *  - The caller is known not to cache pointers to tensor data across
 *    TfLiteInterpreterInvoke calls.
 *  - The model is not stateful (no variables, no LSTMs) or the state isn't
 *    needed between batches.
 * 
 *  When delegate fallback is enabled, TfLiteInterpreterInvoke will
 *  behave as follows:
 *    If one or more delegates were set in the interpreter options
 *    (see TfLiteInterpreterOptionsAddDelegate),
 *    AND inference fails,
 *    then the interpreter will fall back to not using any delegates.
 *    In that case, the previously applied delegate(s) will be automatically
 *    undone, and an attempt will be made to return the interpreter to an
 *    invokable state, which may invalidate previous tensor addresses,
 *    and the inference will be attempted again, using input tensors with
 *    the same value as previously set.
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native void TfLiteInterpreterOptionsSetEnableDelegateFallback(
    TfLiteInterpreterOptions options, @Cast("bool") boolean enable);

// Set if buffer handle output is allowed.
//
/** When using hardware delegation, Interpreter will make the data of output
 *  tensors available in {@code tensor->data} by default. If the application can
 *  consume the buffer handle directly (e.g. reading output from OpenGL
 *  texture), it can set this flag to false, so Interpreter won't copy the
 *  data from buffer handle to CPU memory. WARNING: This is an experimental
 *  API and subject to change. */
public static native void TfLiteSetAllowBufferHandleOutput(
    @Const TfLiteInterpreter interpreter, @Cast("bool") boolean allow_buffer_handle_output);

/** Allow a delegate to look at the graph and modify the graph to handle
 *  parts of the graph themselves. After this is called, the graph may
 *  contain new nodes that replace 1 more nodes.
 *  'delegate' must outlive the interpreter.
 *  Use {@code TfLiteInterpreterOptionsAddDelegate} instead of this unless
 *  absolutely required.
 *  Returns one of the following three status codes:
 *  1. kTfLiteOk: Success.
 *  2. kTfLiteDelegateError: Delegation failed due to an error in the
 *  delegate. The Interpreter has been restored to its pre-delegation state.
 *  NOTE: This undoes all delegates previously applied to the Interpreter.
 *  3. kTfLiteError: Unexpected/runtime failure.
 *  WARNING: This is an experimental API and subject to change. */

///
public static native @Cast("TfLiteStatus") int TfLiteInterpreterModifyGraphWithDelegate(
    @Const TfLiteInterpreter interpreter, TfLiteDelegate delegate);

/** Returns the tensor index corresponding to the input tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */

///
public static native int TfLiteInterpreterGetInputTensorIndex(
    @Const TfLiteInterpreter interpreter, int input_index);

/** Returns the tensor index corresponding to the output tensor
 * 
 *  WARNING: This is an experimental API and subject to change. */
public static native int TfLiteInterpreterGetOutputTensorIndex(
    @Const TfLiteInterpreter interpreter, int output_index);

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus

// #endif  // TENSORFLOW_LITE_C_C_API_EXPERIMENTAL_H_


// Parsed from tensorflow/lite/c/common.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file defines common C types and APIs for implementing operations,
// delegates and other constructs in TensorFlow Lite. The actual operations and
// delegates can be defined using C++, but the interface between the interpreter
// and the operations are C.
//
// Summary of abstractions
// TF_LITE_ENSURE - Self-sufficient error checking
// TfLiteStatus - Status reporting
// TfLiteIntArray - stores tensor shapes (dims),
// TfLiteContext - allows an op to access the tensors
// TfLiteTensor - tensor (a multidimensional array)
// TfLiteNode - a single node or operation
// TfLiteRegistration - the implementation of a conceptual operation.
// TfLiteDelegate - allows delegation of nodes to alternative backends.
//
// Some abstractions in this file are created and managed by Interpreter.
//
// NOTE: The order of values in these structs are "semi-ABI stable". New values
// should be added only to the end of structs and never reordered.

// #ifndef TENSORFLOW_LITE_C_COMMON_H_
// #define TENSORFLOW_LITE_C_COMMON_H_

// #include <stdbool.h>
// #include <stddef.h>
// #include <stdint.h>

// #include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export

// #ifdef __cplusplus
// #endif  // __cplusplus

// The list of external context types known to TF Lite. This list exists solely
// to avoid conflicts and to ensure ops can share the external contexts they
// need. Access to the external contexts is controlled by one of the
// corresponding support files.
/** enum TfLiteExternalContextType */
public static final int
  kTfLiteEigenContext = 0,       // include eigen_support.h to use.
  kTfLiteGemmLowpContext = 1,    // include gemm_support.h to use.
  kTfLiteEdgeTpuContext = 2,     // Placeholder for Edge TPU support.
  kTfLiteCpuBackendContext = 3,  // include cpu_backend_context.h to use.
  kTfLiteMaxExternalContexts = 4;

// Forward declare so dependent structs and methods can reference these types
// prior to the struct definitions.
// Targeting ../TfLiteExternalContext.java



public static final int kTfLiteOptionalTensor = (-1);
// Targeting ../TfLiteIntArray.java



// Given the size (number of elements) in a TfLiteIntArray, calculate its size
// in bytes.
public static native @Cast("size_t") long TfLiteIntArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a array of a given `size` (uninitialized entries).
// This returns a pointer, that you must free using TfLiteIntArrayFree().
public static native TfLiteIntArray TfLiteIntArrayCreate(int size);
// #endif

// Check if two intarrays are equal. Returns 1 if they are equal, 0 otherwise.
public static native int TfLiteIntArrayEqual(@Const TfLiteIntArray a, @Const TfLiteIntArray b);

// Check if an intarray equals an array. Returns 1 if equals, 0 otherwise.
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntPointer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const IntBuffer b_data);
public static native int TfLiteIntArrayEqualsArray(@Const TfLiteIntArray a, int b_size,
                              @Const int[] b_data);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a copy of an array passed as `src`.
// You are expected to free memory with TfLiteIntArrayFree
public static native TfLiteIntArray TfLiteIntArrayCopy(@Const TfLiteIntArray src);

// Free memory of array `a`.
public static native void TfLiteIntArrayFree(TfLiteIntArray a);
// Targeting ../TfLiteFloatArray.java



// Given the size (number of elements) in a TfLiteFloatArray, calculate its size
// in bytes.
public static native int TfLiteFloatArrayGetSizeInBytes(int size);

// #ifndef TF_LITE_STATIC_MEMORY
// Create a array of a given `size` (uninitialized entries).
// This returns a pointer, that you must free using TfLiteFloatArrayFree().
public static native TfLiteFloatArray TfLiteFloatArrayCreate(int size);

// Free memory of array `a`.
public static native void TfLiteFloatArrayFree(TfLiteFloatArray a);
// #endif  // TF_LITE_STATIC_MEMORY

// Since we must not depend on any libraries, define a minimal subset of
// error macros while avoiding names that have pre-conceived meanings like
// assert and check.

// Try to make all reporting calls through TF_LITE_KERNEL_LOG rather than
// calling the context->ReportError function directly, so that message strings
// can be stripped out if the binary size needs to be severely optimized.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_KERNEL_LOG(context, ...)
//   do {
//     (context)->ReportError((context), __VA_ARGS__);
//   } while (false)

// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...)
//   do {
//     if ((context) != nullptr) {
//       (context)->ReportError((context), __VA_ARGS__);
//     }
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_KERNEL_LOG(context, ...)
// #define TF_LITE_MAYBE_KERNEL_LOG(context, ...)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

// Check whether value is true, and if not return kTfLiteError from
// the current function (and report the error string msg).
// #define TF_LITE_ENSURE_MSG(context, value, msg)
//   do {
//     if (!(value)) {
//       TF_LITE_KERNEL_LOG((context), __FILE__ " " msg);
//       return kTfLiteError;
//     }
//   } while (0)

// Check whether the value `a` is true, and if not return kTfLiteError from
// the current function, while also reporting the location of the error.
// #define TF_LITE_ENSURE(context, a)
//   do {
//     if (!(a)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s was not true.", __FILE__,
//                          __LINE__, #a);
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_STATUS(a)
//   do {
//     const TfLiteStatus s = (a);
//     if (s != kTfLiteOk) {
//       return s;
//     }
//   } while (0)

// Check whether the value `a == b` is true, and if not return kTfLiteError from
// the current function, while also reporting the location of the error.
// `a` and `b` may be evaluated more than once, so no side effects or
// extremely expensive computations should be done.
// NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes.
// #define TF_LITE_ENSURE_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%d != %d)", __FILE__,
//                          __LINE__, #a, #b, (a), (b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_TYPES_EQ(context, a, b)
//   do {
//     if ((a) != (b)) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s != %s (%s != %s)", __FILE__,
//                          __LINE__, #a, #b, TfLiteTypeGetName(a),
//                          TfLiteTypeGetName(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_NEAR(context, a, b, epsilon)
//   do {
//     auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));
//     if (delta > epsilon) {
//       TF_LITE_KERNEL_LOG((context), "%s:%d %s not near %s (%f != %f)",
//                          __FILE__, __LINE__, #a, #b, static_cast<double>(a),
//                          static_cast<double>(b));
//       return kTfLiteError;
//     }
//   } while (0)

// #define TF_LITE_ENSURE_OK(context, status)
//   do {
//     const TfLiteStatus s = (status);
//     if ((s) != kTfLiteOk) {
//       return s;
//     }
//   } while (0)
// Targeting ../TfLiteComplex64.java


// Targeting ../TfLiteComplex128.java


// Targeting ../TfLiteFloat16.java



// Return the name of a given type, for error reporting purposes.
public static native @Cast("const char*") BytePointer TfLiteTypeGetName(@Cast("TfLiteType") int type);

// SupportedQuantizationTypes.
/** enum TfLiteQuantizationType */
public static final int
  // No quantization.
  kTfLiteNoQuantization = 0,
  // Affine quantization (with support for per-channel quantization).
  // Corresponds to TfLiteAffineQuantization.
  kTfLiteAffineQuantization = 1;
// Targeting ../TfLiteQuantization.java


// Targeting ../TfLiteAffineQuantization.java


// Targeting ../TfLitePtrUnion.java



// Memory allocation strategies.
//  * kTfLiteMmapRo: Read-only memory-mapped data, or data externally allocated.
//  * kTfLiteArenaRw: Arena allocated with no guarantees about persistence,
//        and available during eval.
//  * kTfLiteArenaRwPersistent: Arena allocated but persistent across eval, and
//        only available during eval.
//  * kTfLiteDynamic: Allocated during eval, or for string tensors.
//  * kTfLitePersistentRo: Allocated and populated during prepare. This is
//        useful for tensors that can be computed during prepare and treated
//        as constant inputs for downstream ops (also in prepare).
//  * kTfLiteCustom: Custom memory allocation provided by the user. See
//        TfLiteCustomAllocation below.
/** enum TfLiteAllocationType */
public static final int
  kTfLiteMemNone = 0,
  kTfLiteMmapRo = 1,
  kTfLiteArenaRw = 2,
  kTfLiteArenaRwPersistent = 3,
  kTfLiteDynamic = 4,
  kTfLitePersistentRo = 5,
  kTfLiteCustom = 6;

// The delegates should use zero or positive integers to represent handles.
// -1 is reserved from unallocated status.
/** enum  */
public static final int
  kTfLiteNullBufferHandle = -1;

// Storage format of each dimension in a sparse tensor.
/** enum TfLiteDimensionType */
public static final int
  kTfLiteDimDense = 0,
  kTfLiteDimSparseCSR = 1;
// Targeting ../TfLiteDimensionMetadata.java


// Targeting ../TfLiteSparsity.java


// Targeting ../TfLiteCustomAllocation.java



// The flags used in `Interpreter::SetCustomAllocationForTensor`.
// Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc.
/** enum TfLiteCustomAllocationFlags */
public static final int
  kTfLiteCustomAllocationFlagsNone = 0,
  // Skips checking whether allocation.data points to an aligned buffer as
  // expected by the TFLite runtime.
  // NOTE: Setting this flag can cause crashes when calling Invoke().
  // Use with caution.
  kTfLiteCustomAllocationFlagsSkipAlignCheck = 1;
// Targeting ../TfLiteTensor.java


// Targeting ../TfLiteNode.java


// #else   // defined(TF_LITE_STATIC_MEMORY)?
// NOTE: This flag is opt-in only at compile time.
//
// Specific reduced TfLiteTensor struct for TF Micro runtime. This struct
// contains only the minimum fields required to initialize and prepare a micro
// inference graph. The fields in this struct have been ordered from
// largest-to-smallest for optimal struct sizeof.
//
// This struct does not use:
// - allocation
// - buffer_handle
// - data_is_stale
// - delegate
// - dims_signature
// - name
// - sparsity

// Specific reduced TfLiteNode struct for TF Micro runtime. This struct contains
// only the minimum fields required to represent a node.
//
// This struct does not use:
// - delegate
// - intermediates
// - temporaries
// Targeting ../TfLiteEvalTensor.java



// #ifndef TF_LITE_STATIC_MEMORY
// Free data memory of tensor `t`.
public static native void TfLiteTensorDataFree(TfLiteTensor t);

// Free quantization data.
public static native void TfLiteQuantizationFree(TfLiteQuantization quantization);

// Free sparsity parameters.
public static native void TfLiteSparsityFree(TfLiteSparsity sparsity);

// Free memory of tensor `t`.
public static native void TfLiteTensorFree(TfLiteTensor t);

// Set all of a tensor's fields (and free any previously allocated data).
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") BytePointer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, @Cast("const char*") BytePointer name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") ByteBuffer buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);
public static native void TfLiteTensorReset(@Cast("TfLiteType") int type, String name, TfLiteIntArray dims,
                       @ByVal TfLiteQuantizationParams quantization, @Cast("char*") byte[] buffer,
                       @Cast("size_t") long size, @Cast("TfLiteAllocationType") int allocation_type,
                       @Const Pointer allocation, @Cast("bool") boolean is_variable,
                       TfLiteTensor tensor);

// Copies the contents of 'src' in 'dst'.
// Function does nothing if either 'src' or 'dst' is passed as nullptr and
// return kTfLiteOk.
// Returns kTfLiteError if 'src' and 'dst' doesn't have matching data size.
// Note function copies contents, so it won't create new data pointer
// or change allocation type.
// All Tensor related properties will be copied from 'src' to 'dst' like
// quantization, sparsity, ...
public static native @Cast("TfLiteStatus") int TfLiteTensorCopy(@Const TfLiteTensor src, TfLiteTensor dst);

// Resize the allocated data of a (dynamic) tensor. Tensors with allocation
// types other than kTfLiteDynamic will be ignored.
public static native void TfLiteTensorRealloc(@Cast("size_t") long num_bytes, TfLiteTensor tensor);
// Targeting ../TfLiteDelegateParams.java


// Targeting ../TfLiteContext.java


// Targeting ../TfLiteRegistration.java



// The flags used in `TfLiteDelegate`. Note that this is a bitmask, so the
// values should be 1, 2, 4, 8, ...etc.
/** enum TfLiteDelegateFlags */
public static final int
  kTfLiteDelegateFlagsNone = 0,
  // The flag is set if the delegate can handle dynamic sized tensors.
  // For example, the output shape of a `Resize` op with non-constant shape
  // can only be inferred when the op is invoked.
  // In this case, the Delegate is responsible for calling
  // `SetTensorToDynamic` to mark the tensor as a dynamic tensor, and calling
  // `ResizeTensor` when invoking the op.
  //
  // If the delegate isn't capable to handle dynamic tensors, this flag need
  // to be set to false.
  kTfLiteDelegateFlagsAllowDynamicTensors = 1,

  // This flag can be used by delegates (that allow dynamic tensors) to ensure
  // applicable tensor shapes are automatically propagated in the case of tensor
  // resizing.
  // This means that non-dynamic (allocation_type != kTfLiteDynamic) I/O tensors
  // of a delegate kernel will have correct shapes before its Prepare() method
  // is called. The runtime leverages TFLite builtin ops in the original
  // execution plan to propagate shapes.
  //
  // A few points to note:
  // 1. This requires kTfLiteDelegateFlagsAllowDynamicTensors. If that flag is
  // false, this one is redundant since the delegate kernels are re-initialized
  // every time tensors are resized.
  // 2. Enabling this flag adds some overhead to AllocateTensors(), since extra
  // work is required to prepare the original execution plan.
  // 3. This flag requires that the original execution plan only have ops with
  // valid registrations (and not 'dummy' custom ops like with Flex).
  // WARNING: This feature is experimental and subject to change.
  kTfLiteDelegateFlagsRequirePropagatedShapes = 2;
// Targeting ../TfLiteDelegate.java



// Build a 'null' delegate, with all the fields properly set to their default
// values.
public static native @ByVal TfLiteDelegate TfLiteDelegateCreate();

// #ifdef __cplusplus  // extern "C"
// #endif  // __cplusplus
// #endif  // TENSORFLOW_LITE_C_COMMON_H_


// Parsed from tensorflow/lite/core/api/error_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_
// #define TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_

// #include <cstdarg>
// Targeting ../ErrorReporter.java



  // namespace tflite

// You should not make bare calls to the error reporter, instead use the
// TF_LITE_REPORT_ERROR macro, since this allows message strings to be
// stripped when the binary size has to be optimized. If you are looking to
// reduce binary size, define TF_LITE_STRIP_ERROR_STRINGS when compiling and
// every call will be stubbed out, taking no memory.
// #ifndef TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
//   do {
//     static_cast<tflite::ErrorReporter*>(reporter)->Report(__VA_ARGS__);
//   } while (false)
// #else  // TF_LITE_STRIP_ERROR_STRINGS
// #define TF_LITE_REPORT_ERROR(reporter, ...)
// #endif  // TF_LITE_STRIP_ERROR_STRINGS

// #endif  // TENSORFLOW_LITE_CORE_API_ERROR_REPORTER_H_


// Parsed from tensorflow/lite/core/api/op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_

// #include <functional>
// #include <memory>
// #include <vector>

// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// Targeting ../OpResolver.java



// Handles the logic for converting between an OperatorCode structure extracted
// from a flatbuffer and information about a registered operator
// implementation.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Cast("const TfLiteRegistration**") PointerPointer registration);
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetRegistrationFromOpCode(@Cast("const tflite::OperatorCode*") Pointer opcode,
                                       @Const @ByRef OpResolver op_resolver,
                                       ErrorReporter error_reporter,
                                       @Const @ByPtrPtr TfLiteRegistration registration);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_API_OP_RESOLVER_H_


// Parsed from tensorflow/lite/core/api/profiler.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_API_PROFILER_H_
// #define TENSORFLOW_LITE_CORE_API_PROFILER_H_

// #include <cstdint>
// Targeting ../Profiler.java


// Targeting ../ScopedProfile.java


// Targeting ../ScopedOperatorProfile.java


// Targeting ../ScopedDelegateOperatorProfile.java


// Targeting ../ScopedRuntimeInstrumentationProfile.java



  // namespace tflite

// #define TFLITE_VARNAME_UNIQ_IMPL(name, ctr) name##ctr
// #define TFLITE_VARNAME_UNIQ(name, ctr) TFLITE_VARNAME_UNIQ_IMPL(name, ctr)

// #define TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler, tag)
//   tflite::ScopedProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag))

// #define TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedOperatorProfile TFLITE_VARNAME_UNIQ(_profile_, __COUNTER__)(
//       (profiler), (tag), (node_index))

// #define TFLITE_SCOPED_DELEGATE_OPERATOR_PROFILE(profiler, tag, node_index)
//   tflite::ScopedDelegateOperatorProfile TFLITE_VARNAME_UNIQ(
//       _profile_, __COUNTER__)((profiler), (tag), (node_index))

// #define TFLITE_ADD_RUNTIME_INSTRUMENTATION_EVENT(
//     profiler, tag, event_metadata1, event_metadata2)
//   do {
//     if (profiler) {
//       const auto handle = profiler->BeginEvent(
//           tag, Profiler::EventType::GENERAL_RUNTIME_INSTRUMENTATION_EVENT,
//           event_metadata1, event_metadata2);
//       profiler->EndEvent(handle);
//     }
//   } while (false);

// #endif  // TENSORFLOW_LITE_CORE_API_PROFILER_H_


// Parsed from tensorflow/lite/core/api/verifier.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Abstract interface for verifying a model. */
// #ifndef TENSORFLOW_LITE_CORE_API_VERIFIER_H_
// #define TENSORFLOW_LITE_CORE_API_VERIFIER_H_

// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../TfLiteVerifier.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_API_VERIFIER_H_


// Parsed from tensorflow/lite/experimental/resource/initialization_status.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_

// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// Targeting ../InitializationStatus.java



/** WARNING: Experimental interface, subject to change. */

@Namespace("tflite::resource") public static native InitializationStatus GetInitializationStatus(@Cast("tflite::resource::InitializationStatusMap*") IntResourceBaseMap map,
                                              int subgraph_id);

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_INITIALIZATION_STATUS_H_


// Parsed from tensorflow/lite/experimental/resource/resource_base.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_
// #define TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_

// #include <cstdint>
// #include <map>
// #include <memory>
// #include <string>
// #include <unordered_map>
// Targeting ../ResourceBase.java



/** WARNING: Experimental interface, subject to change. */

  // namespace resource
  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXPERIMENTAL_RESOURCE_RESOURCE_BASE_H_


// Parsed from tensorflow/lite/allocation.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Memory management for TF Lite. */
// #ifndef TENSORFLOW_LITE_ALLOCATION_H_
// #define TENSORFLOW_LITE_ALLOCATION_H_

// #include <stddef.h>

// #include <cstdio>
// #include <cstdlib>
// #include <memory>

// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../Allocation.java



// Note that not all platforms support MMAP-based allocation.
// Use `IsSupported()` to check.
// Targeting ../FileCopyAllocation.java


// Targeting ../MemoryAllocation.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_ALLOCATION_H_


// Parsed from tensorflow/lite/stderr_reporter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_STDERR_REPORTER_H_
// #define TENSORFLOW_LITE_STDERR_REPORTER_H_

// #include <cstdarg>

// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// Targeting ../StderrReporter.java



// Return the default error reporter (output to stderr).
@Namespace("tflite") public static native ErrorReporter DefaultErrorReporter();

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STDERR_REPORTER_H_


// Parsed from tensorflow/lite/graph_info.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_GRAPH_INFO_H_
// #define TENSORFLOW_LITE_GRAPH_INFO_H_

// #include <stddef.h>

// #include <vector>

// #include "tensorflow/lite/c/common.h"
// Targeting ../GraphInfo.java


// Targeting ../NodeSubset.java



// Partitions a list of node indices `nodes_to_partition` into node sub sets.
// Each node sub set is in dependency order (i.e. all members of the node sub
// sets). `node_subsets` is assumed to be empty.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int PartitionGraphIntoIndependentNodeSubsets(
    @Const GraphInfo info, @Const TfLiteIntArray nodes_to_partition,
    @StdVector NodeSubset node_subsets);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_GRAPH_INFO_H_


// Parsed from tensorflow/lite/memory_planner.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MEMORY_PLANNER_H_
// #define TENSORFLOW_LITE_MEMORY_PLANNER_H_

// #include <vector>

// #include "tensorflow/lite/c/common.h"
// Targeting ../MemoryPlanner.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MEMORY_PLANNER_H_


// Parsed from tensorflow/lite/util.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// This file provides general C++ utility functions in TFLite.
// For example: Converting between `TfLiteIntArray`, `std::vector` and
// Flatbuffer vectors. These functions can't live in `context.h` since it's pure
// C.

// #ifndef TENSORFLOW_LITE_UTIL_H_
// #define TENSORFLOW_LITE_UTIL_H_

// #include <stddef.h>

// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <vector>

// #include "tensorflow/lite/c/common.h"

// Memory allocation parameter used by ArenaPlanner.
// Clients (such as delegates) might look at this to ensure interop between
// TFLite memory & hardware buffers.
// NOTE: This only holds for tensors allocated on the arena.
@Namespace("tflite") @MemberGetter public static native int kDefaultTensorAlignment();

// The prefix of Flex op custom code.
// This will be matched agains the `custom_code` field in `OperatorCode`
// Flatbuffer Table.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kFlexCustomCodePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kFlexCustomCodePrefix();

// Checks whether the prefix of the custom name indicates the operation is an
// Flex operation.
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(@Cast("const char*") BytePointer custom_name);
@Namespace("tflite") public static native @Cast("bool") boolean IsFlexOp(String custom_name);

// Converts a `std::vector` to a `TfLiteIntArray`. The caller takes ownership
// of the returned pointer.
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntPointer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector IntBuffer input);
@Namespace("tflite") public static native TfLiteIntArray ConvertVectorToTfLiteIntArray(@StdVector int[] input);

// Converts an array (of the given size) to a `TfLiteIntArray`. The caller
// takes ownership of the returned pointer, and must make sure 'dims' has at
// least 'rank' elements.
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int rank, @Const IntPointer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int rank, @Const IntBuffer dims);
@Namespace("tflite") public static native TfLiteIntArray ConvertArrayToTfLiteIntArray(int rank, @Const int[] dims);

// Checks whether a `TfLiteIntArray` and an int array have matching elements.
// The caller must guarantee that 'b' has at least 'b_size' elements.
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntPointer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const IntBuffer b);
@Namespace("tflite") public static native @Cast("bool") boolean EqualArrayAndTfLiteIntArray(@Const TfLiteIntArray a, int b_size,
                                 @Const int[] b);
// Targeting ../TfLiteIntArrayDeleter.java



// Helper for Building TfLiteIntArray that is wrapped in a unique_ptr,
// So that it is automatically freed when it goes out of the scope.
@Namespace("tflite") public static native @UniquePtr("TfLiteIntArray,tflite::TfLiteIntArrayDeleter") @ByVal TfLiteIntArray BuildTfLiteIntArray(
    @StdVector IntPointer data);
@Namespace("tflite") public static native @UniquePtr("TfLiteIntArray,tflite::TfLiteIntArrayDeleter") @ByVal TfLiteIntArray BuildTfLiteIntArray(
    @StdVector IntBuffer data);
@Namespace("tflite") public static native @UniquePtr("TfLiteIntArray,tflite::TfLiteIntArrayDeleter") @ByVal TfLiteIntArray BuildTfLiteIntArray(
    @StdVector int[] data);

// Populates the size in bytes of a type into `bytes`. Returns kTfLiteOk for
// valid types, and kTfLiteError otherwise.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int GetSizeOfType(TfLiteContext context, @Cast("const TfLiteType") int type,
                           @Cast("size_t*") SizeTPointer bytes);

// Creates a stub TfLiteRegistration instance with the provided
// `custom_op_name`. The op will fail if invoked, and is useful as a
// placeholder to defer op resolution.
// Note that `custom_op_name` must remain valid for the returned op's lifetime..
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(@Cast("const char*") BytePointer custom_op_name);
@Namespace("tflite") public static native @ByVal TfLiteRegistration CreateUnresolvedCustomOp(String custom_op_name);

// Checks whether the provided op is an unresolved custom op.
@Namespace("tflite") public static native @Cast("bool") boolean IsUnresolvedCustomOp(@Const @ByRef TfLiteRegistration registration);

// Returns a descriptive name with the given op TfLiteRegistration.
@Namespace("tflite") public static native @StdString String GetOpNameByRegistration(@Const @ByRef TfLiteRegistration registration);

// The prefix of a validation subgraph name.
// WARNING: This is an experimental API and subject to change.
@Namespace("tflite") @MemberGetter public static native @Cast("const char") byte kValidationSubgraphNamePrefix(int i);
@Namespace("tflite") @MemberGetter public static native @Cast("const char*") BytePointer kValidationSubgraphNamePrefix();

// Checks whether the prefix of the subgraph name indicates the subgraph is a
// validation subgraph.
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(@Cast("const char*") BytePointer name);
@Namespace("tflite") public static native @Cast("bool") boolean IsValidationSubgraph(String name);

// Multiply two sizes and return true if overflow occurred;
// This is based off tensorflow/overflow.h but is simpler as we already
// have unsigned numbers. It is also generalized to work where sizeof(size_t)
// is not 8.
@Namespace("tflite") public static native @Cast("TfLiteStatus") int MultiplyAndCheckOverflow(@Cast("size_t") long a, @Cast("size_t") long b, @Cast("size_t*") SizeTPointer product);
  // namespace tflite

// #endif  // TENSORFLOW_LITE_UTIL_H_


// Parsed from tensorflow/lite/core/macros.h

/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// This provides utility macros and functions that are inherently platform
// specific.
// #ifndef TENSORFLOW_LITE_CORE_MACROS_H_
// #define TENSORFLOW_LITE_CORE_MACROS_H_

// #ifdef __has_builtin
// #define TFLITE_HAS_BUILTIN(x) __has_builtin(x)
// #else
// #define TFLITE_HAS_BUILTIN(x) 0
// #endif

// #if (!defined(__NVCC__)) && (TFLITE_HAS_BUILTIN(__builtin_expect) ||
//                              (defined(__GNUC__) && __GNUC__ >= 3))
// #define TFLITE_EXPECT_FALSE(cond) __builtin_expect(cond, false)
// #define TFLITE_EXPECT_TRUE(cond) __builtin_expect(!!(cond), true)
// #else
// #define TFLITE_EXPECT_FALSE(cond) (cond)
// #define TFLITE_EXPECT_TRUE(cond) (cond)
// #endif

// Normally we'd use ABSL_HAVE_ATTRIBUTE_WEAK and ABSL_ATTRIBUTE_WEAK, but
// we avoid the absl dependency for binary size reasons.
// #ifdef __has_attribute
// #define TFLITE_HAS_ATTRIBUTE(x) __has_attribute(x)
// #else
// #define TFLITE_HAS_ATTRIBUTE(x) 0
// #endif

// #if (TFLITE_HAS_ATTRIBUTE(weak) ||
//      (defined(__GNUC__) && !defined(__clang__))) &&
//     !(defined(__llvm__) && defined(_WIN32)) && !defined(__MINGW32__)
// #undef TFLITE_ATTRIBUTE_WEAK
// #define TFLITE_ATTRIBUTE_WEAK __attribute__((weak))
public static final int TFLITE_HAS_ATTRIBUTE_WEAK = 1;
// #else
// #define TFLITE_ATTRIBUTE_WEAK
// #endif

// #endif  // TENSORFLOW_LITE_CORE_MACROS_H_


// Parsed from tensorflow/lite/core/subgraph.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SUBGRAPH_H_
// #define TENSORFLOW_LITE_CORE_SUBGRAPH_H_

// #include <stdarg.h>
// #include <stddef.h>

// #include <cstdint>
// #include <cstdlib>
// #include <map>
// #include <memory>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/macros.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/graph_info.h"
// #include "tensorflow/lite/memory_planner.h"
// #include "tensorflow/lite/util.h"
// Targeting ../SingleOpModel.java


// Targeting ../TestDelegate.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../Subgraph.java



  // namespace tflite
// #endif  // TENSORFLOW_LITE_CORE_SUBGRAPH_H_


// Parsed from tensorflow/lite/external_cpu_backend_context.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_
// #define TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_

// #include <memory>
// #include <utility>

// #include "tensorflow/lite/c/common.h"
// Targeting ../TfLiteInternalBackendContext.java


// Targeting ../ExternalCpuBackendContext.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_EXTERNAL_CPU_BACKEND_CONTEXT_H_


// Parsed from tensorflow/lite/portable_type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.

// Arduino build defines abs as a macro here. That is invalid C++, and breaks
// libc++'s <complex> header, undefine it.
// #ifdef abs
// #undef abs
// #endif

// #include <stdint.h>

// #include "tensorflow/lite/c/common.h"

// Map statically from a C++ type to a TfLiteType. Used in interpreter for
// safe casts.
// Example:
//  typeToTfLiteType<bool>() -> kTfLiteBool

// Map from TfLiteType to the corresponding C++ type.
// Example:
//   TfLiteTypeToType<kTfLiteBool>::Type -> bool  // Specializations below

// Template specialization for both typeToTfLiteType and TfLiteTypeToType.
// #define MATCH_TYPE_AND_TFLITE_TYPE(CPP_TYPE, TFLITE_TYPE_ENUM)
//   template <>
//   constexpr TfLiteType typeToTfLiteType<CPP_TYPE>() {
//     return TFLITE_TYPE_ENUM;
//   }
//   template <>
//   struct TfLiteTypeToType<TFLITE_TYPE_ENUM> {
//     using Type = CPP_TYPE;
//   }

// No string mapping is included here, since the TF Lite packed representation
// doesn't correspond to a C++ type well.

// Targeting ../TfLiteTypeToType.java













  // namespace tflite
// #endif  // TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/signature_runner.h

/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_
// #define TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_

// #include <cstddef>
// #include <cstdint>
// #include <string>

// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/internal/signature_def.h"
// Targeting ../SignatureRunnerJNIHelper.java


// Targeting ../TensorHandle.java


// Targeting ../SignatureRunner.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_CORE_SIGNATURE_RUNNER_H_


// Parsed from tensorflow/lite/type_to_tflitetype.h

/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_
// #define TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_

// #include <complex>
// #include <string>

// #include "tensorflow/lite/c/common.h"

// Most of the definitions have been moved to this subheader so that Micro
// can include it without relying on <string> and <complex>, which isn't
// available on all platforms.
// #include "tensorflow/lite/portable_type_to_tflitetype.h"

// TODO(b/163167649): This string conversion means that only the first entry
// in a string tensor will be returned as a std::string, so it's deprecated.





  // namespace tflite
// #endif  // TENSORFLOW_LITE_TYPE_TO_TFLITETYPE_H_


// Parsed from tensorflow/lite/string_type.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// Abstract string. We don't want even absl at this level.
// #ifndef TENSORFLOW_LITE_STRING_TYPE_H_
// #define TENSORFLOW_LITE_STRING_TYPE_H_

// #include <string>

  // namespace tflite

// #endif  // TENSORFLOW_LITE_STRING_TYPE_H_


// Parsed from tensorflow/lite/mutable_op_resolver.h

/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_
// #define TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_

// #include <stddef.h>

// #include <string>
// #include <unordered_map>
// #include <utility>

// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/util.h"
// Targeting ../ValueHasher.java



// Targeting ../MutableOpResolver.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MUTABLE_OP_RESOLVER_H_


// Parsed from tensorflow/lite/interpreter.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Main abstraction controlling the tflite interpreter.
/** See context.h for the API for defining operations (TfLiteRegistration). */
// #ifndef TENSORFLOW_LITE_INTERPRETER_H_
// #define TENSORFLOW_LITE_INTERPRETER_H_

// #include <stddef.h>
// #include <stdint.h>

// #include <complex>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <map>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/c/common.h"  // IWYU pragma: export
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/profiler.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/experimental/resource/initialization_status.h"
// #include "tensorflow/lite/experimental/resource/resource_base.h"
// #include "tensorflow/lite/external_cpu_backend_context.h"
// #include "tensorflow/lite/internal/signature_def.h"
// #include "tensorflow/lite/memory_planner.h"
// #include "tensorflow/lite/portable_type_to_tflitetype.h"
// #include "tensorflow/lite/signature_runner.h"
// #include "tensorflow/lite/stderr_reporter.h"
// #include "tensorflow/lite/string_type.h"
// #include "tensorflow/lite/type_to_tflitetype.h"
// Targeting ../InterpreterTest.java


// Targeting ../InterpreterUtils.java


// Targeting ../TestDelegation.java

  // Class for friend declarations.
  // namespace test_utils

// Targeting ../InterpreterWrapper.java

  // Class for friend declarations.

// Targeting ../Interpreter.java



  // namespace tflite
// #endif  // TENSORFLOW_LITE_INTERPRETER_H_


// Parsed from tensorflow/lite/model_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Deserialization infrastructure for tflite. Provides functionality
/** to go from a serialized tflite model in flatbuffer format to an
/** in-memory representation of the model.
/** */
// #ifndef TENSORFLOW_LITE_MODEL_BUILDER_H_
// #define TENSORFLOW_LITE_MODEL_BUILDER_H_

// #include <stddef.h>

// #include <map>
// #include <memory>
// #include <string>

// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/api/verifier.h"
// #include "tensorflow/lite/mutable_op_resolver.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/stderr_reporter.h"
// #include "tensorflow/lite/string_type.h"
// Targeting ../FlatBufferModel.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_MODEL_BUILDER_H_


// Parsed from tensorflow/lite/interpreter_builder.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Provides functionality to construct an interpreter for a model.
/** */
// #ifndef TENSORFLOW_LITE_INTERPRETER_BUILDER_H_
// #define TENSORFLOW_LITE_INTERPRETER_BUILDER_H_

// #include <map>
// #include <memory>
// #include <string>
// #include <vector>

// #include "flatbuffers/flatbuffers.h"  // from @flatbuffers
// #include "tensorflow/lite/allocation.h"
// #include "tensorflow/lite/c/common.h"
// #include "tensorflow/lite/core/api/error_reporter.h"
// #include "tensorflow/lite/core/api/op_resolver.h"
// #include "tensorflow/lite/core/subgraph.h"
// #include "tensorflow/lite/interpreter.h"
// #include "tensorflow/lite/model_builder.h"
// #include "tensorflow/lite/mutable_op_resolver.h"
// #include "tensorflow/lite/schema/schema_generated.h"
// #include "tensorflow/lite/stderr_reporter.h"
// Targeting ../InterpreterBuilder.java



  // namespace tflite

// #endif  // TENSORFLOW_LITE_INTERPRETER_BUILDER_H_


// Parsed from tensorflow/lite/model.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Defines tflite::Interpreter and tflite::InterpreterBuilder.
/** */
// #ifndef TENSORFLOW_LITE_MODEL_H_
// #define TENSORFLOW_LITE_MODEL_H_

// #include "tensorflow/lite/interpreter_builder.h"
// #include "tensorflow/lite/model_builder.h"

// TODO(b/168725050): Address the issue of proxy header in this file.

// #endif  // TENSORFLOW_LITE_MODEL_H_


// Parsed from tensorflow/lite/kernels/register.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
// #ifndef TENSORFLOW_LITE_KERNELS_REGISTER_H_
// #define TENSORFLOW_LITE_KERNELS_REGISTER_H_

// #include "tensorflow/lite/model.h"  // Legacy.
// #include "tensorflow/lite/mutable_op_resolver.h"
// Targeting ../BuiltinOpResolver.java


// Targeting ../BuiltinOpResolverWithoutDefaultDelegates.java



  // namespace builtin
  // namespace ops
  // namespace tflite

// #endif  // TENSORFLOW_LITE_KERNELS_REGISTER_H_


// Parsed from tensorflow/lite/optional_debug_tools.h

/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
/** \file
/** Optional debugging functionality.
/** For small sized binaries, these are not needed. */
// #ifndef TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_
// #define TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_

// #include "tensorflow/lite/interpreter.h"

// Prints a dump of what tensors and what nodes are in the interpreter.
@Namespace("tflite") public static native void PrintInterpreterState(@Const Interpreter interpreter);

  // namespace tflite

// #endif  // TENSORFLOW_LITE_OPTIONAL_DEBUG_TOOLS_H_


}
